{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb33fce",
   "metadata": {},
   "source": [
    "Download modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d4cf661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset \n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import torch.optim as optim \n",
    "from torch.optim import AdamW\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c17db",
   "metadata": {},
   "source": [
    "Make a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21fc117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Данные.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3c142df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>lu</th>\n",
       "      <th>P0</th>\n",
       "      <th>I</th>\n",
       "      <th>gamma</th>\n",
       "      <th>sgamma</th>\n",
       "      <th>r</th>\n",
       "      <th>Psat</th>\n",
       "      <th>Lsat</th>\n",
       "      <th>loptim</th>\n",
       "      <th>lres</th>\n",
       "      <th>loptim/lres</th>\n",
       "      <th>Psat/Pall</th>\n",
       "      <th>Lsat/lu</th>\n",
       "      <th>f</th>\n",
       "      <th>rho</th>\n",
       "      <th>P0/Pall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.89</td>\n",
       "      <td>3.69</td>\n",
       "      <td>12800000.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2650.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.430000e+08</td>\n",
       "      <td>18.5</td>\n",
       "      <td>2.261250e-08</td>\n",
       "      <td>2.257770e-08</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>5.01</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>1.180000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.89</td>\n",
       "      <td>3.69</td>\n",
       "      <td>12800.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>9.800000e+07</td>\n",
       "      <td>16.7</td>\n",
       "      <td>1.585280e-07</td>\n",
       "      <td>1.580440e-07</td>\n",
       "      <td>0.003065</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>4.52</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>3.130000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.98</td>\n",
       "      <td>3.69</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2830.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.130000e+09</td>\n",
       "      <td>20.3</td>\n",
       "      <td>2.061380e-08</td>\n",
       "      <td>2.057220e-08</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.00156</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>2.070000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.98</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1500000.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>3160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.100000e+09</td>\n",
       "      <td>18.3</td>\n",
       "      <td>1.648760e-08</td>\n",
       "      <td>1.645770e-08</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>4.96</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>1.850000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.98</td>\n",
       "      <td>3.69</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2650.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.180000e+09</td>\n",
       "      <td>19.4</td>\n",
       "      <td>2.356200e-08</td>\n",
       "      <td>2.351110e-08</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.00175</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>2.210000e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      k    lu          P0      I   gamma  sgamma       r          Psat  Lsat  \\\n",
       "0  3.89  3.69  12800000.0   80.0  2650.0     0.0  0.0001  1.430000e+08  18.5   \n",
       "1  3.89  3.69     12800.0   80.0  1000.0     0.0  0.0001  9.800000e+07  16.7   \n",
       "2  3.98  3.69    150000.0  500.0  2830.0     0.0  0.0001  1.130000e+09  20.3   \n",
       "3  3.98  3.69   1500000.0  500.0  3160.0     0.0  0.0001  1.100000e+09  18.3   \n",
       "4  3.98  3.69    150000.0  500.0  2650.0     0.0  0.0001  1.180000e+09  19.4   \n",
       "\n",
       "         loptim          lres  loptim/lres  Psat/Pall  Lsat/lu      f  \\\n",
       "0  2.261250e-08  2.257770e-08     0.001542    0.00132     5.01  0.736   \n",
       "1  1.585280e-07  1.580440e-07     0.003065    0.00240     4.52  0.736   \n",
       "2  2.061380e-08  2.057220e-08     0.002024    0.00156     5.49  0.735   \n",
       "3  1.648760e-08  1.645770e-08     0.001814    0.00136     4.96  0.735   \n",
       "4  2.356200e-08  2.351110e-08     0.002164    0.00175     5.25  0.735   \n",
       "\n",
       "        rho       P0/Pall  \n",
       "0  0.000965  1.180000e-04  \n",
       "1  0.002550  3.130000e-07  \n",
       "2  0.001690  2.070000e-07  \n",
       "3  0.001510  1.850000e-06  \n",
       "4  0.001800  2.210000e-07  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de903ee",
   "metadata": {},
   "source": [
    "Let's do data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "140b811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52e1ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "243cc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Psat/Pall']<0.008]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f6103f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log'] = np.log(df[['Psat/Pall']])\n",
    "df['log0'] = np.log(df['P0']/df['I']/df['gamma']/511000)\n",
    "df['optim'] = df['loptim/lres']/df['Psat/Pall']\n",
    "df['Lsat/lu'] = np.log(df['Lsat/lu'])\n",
    "df['sqgamma'] = np.log(df['gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee8f0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['optim']<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "294bf40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9070138150903294"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)/l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "611d767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['optim'] = df['loptim/lres']/df['rho']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf41df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['optim'] = (df['optim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602d0ab",
   "metadata": {},
   "source": [
    "Make a feature dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f418fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(columns=['gamma', 'P0', 'optim', 'P0/Pall', 'Psat', 'Lsat', 'loptim','lres', 'loptim/lres', 'Psat/Pall', 'Lsat/lu', 'f', 'rho', 'log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3642dca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>lu</th>\n",
       "      <th>I</th>\n",
       "      <th>sgamma</th>\n",
       "      <th>r</th>\n",
       "      <th>log0</th>\n",
       "      <th>sqgamma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.89</td>\n",
       "      <td>3.69</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>-9.043511</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.89</td>\n",
       "      <td>3.69</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>-14.976706</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.98</td>\n",
       "      <td>3.69</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>-15.388374</td>\n",
       "      <td>7.948032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.98</td>\n",
       "      <td>3.69</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>-13.196085</td>\n",
       "      <td>8.058327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.98</td>\n",
       "      <td>3.69</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>-15.322657</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>1.72</td>\n",
       "      <td>1.99</td>\n",
       "      <td>7600.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-16.247034</td>\n",
       "      <td>10.275051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>1.72</td>\n",
       "      <td>1.99</td>\n",
       "      <td>7600.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-11.532665</td>\n",
       "      <td>10.165852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>3.29</td>\n",
       "      <td>1.99</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-11.512708</td>\n",
       "      <td>10.404263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>3.29</td>\n",
       "      <td>1.99</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-11.653787</td>\n",
       "      <td>10.545341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>3.29</td>\n",
       "      <td>1.99</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-15.988667</td>\n",
       "      <td>10.275051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1707 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         k    lu       I   sgamma        r       log0    sqgamma\n",
       "0     3.89  3.69    80.0  0.00000  0.00010  -9.043511   7.882315\n",
       "1     3.89  3.69    80.0  0.00000  0.00010 -14.976706   6.907755\n",
       "2     3.98  3.69   500.0  0.00000  0.00010 -15.388374   7.948032\n",
       "3     3.98  3.69   500.0  0.00000  0.00010 -13.196085   8.058327\n",
       "4     3.98  3.69   500.0  0.00000  0.00010 -15.322657   7.882315\n",
       "...    ...   ...     ...      ...      ...        ...        ...\n",
       "1877  1.72  1.99  7600.0  0.00005  0.00001 -16.247034  10.275051\n",
       "1878  1.72  1.99  7600.0  0.00005  0.00001 -11.532665  10.165852\n",
       "1879  3.29  1.99  4500.0  0.00005  0.00001 -11.512708  10.404263\n",
       "1880  3.29  1.99  4500.0  0.00005  0.00001 -11.653787  10.545341\n",
       "1881  3.29  1.99  4500.0  0.00005  0.00001 -15.988667  10.275051\n",
       "\n",
       "[1707 rows x 7 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17404835",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(columns=['sqgamma', 'k', 'lu', 'I', 'gamma', 'sgamma', 'r', 'log0', 'P0', 'P0/Pall', 'loptim','lres', 'loptim/lres', 'Psat/Pall', 'Lsat', 'f', 'rho', 'Psat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01cfb8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lsat/lu</th>\n",
       "      <th>log</th>\n",
       "      <th>optim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.611436</td>\n",
       "      <td>-6.630124</td>\n",
       "      <td>1.598041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.508512</td>\n",
       "      <td>-6.032287</td>\n",
       "      <td>1.201894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.702928</td>\n",
       "      <td>-6.463069</td>\n",
       "      <td>1.197420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.601406</td>\n",
       "      <td>-6.600271</td>\n",
       "      <td>1.201364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.658228</td>\n",
       "      <td>-6.348139</td>\n",
       "      <td>1.202222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>2.360854</td>\n",
       "      <td>-7.606921</td>\n",
       "      <td>0.798766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>1.829376</td>\n",
       "      <td>-7.086882</td>\n",
       "      <td>1.198454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>1.906575</td>\n",
       "      <td>-7.127156</td>\n",
       "      <td>1.200737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>2.034706</td>\n",
       "      <td>-7.268725</td>\n",
       "      <td>1.197812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>2.202765</td>\n",
       "      <td>-7.360312</td>\n",
       "      <td>0.799514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1707 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Lsat/lu       log     optim\n",
       "0     1.611436 -6.630124  1.598041\n",
       "1     1.508512 -6.032287  1.201894\n",
       "2     1.702928 -6.463069  1.197420\n",
       "3     1.601406 -6.600271  1.201364\n",
       "4     1.658228 -6.348139  1.202222\n",
       "...        ...       ...       ...\n",
       "1877  2.360854 -7.606921  0.798766\n",
       "1878  1.829376 -7.086882  1.198454\n",
       "1879  1.906575 -7.127156  1.200737\n",
       "1880  2.034706 -7.268725  1.197812\n",
       "1881  2.202765 -7.360312  0.799514\n",
       "\n",
       "[1707 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572dd408",
   "metadata": {},
   "source": [
    "Split our data into train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a77acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df1, df2, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc266f",
   "metadata": {},
   "source": [
    "Use scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "515fa4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler2.fit(y_train)\n",
    "\n",
    "y_train = scaler2.transform(y_train)\n",
    "y_test = scaler2.transform(y_test)\n",
    "y_val = scaler2.transform(y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ef2e9",
   "metadata": {},
   "source": [
    "transfer to pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dcc3a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = torch.Tensor(X_train) \n",
    "y_train = torch.Tensor(y_train)\n",
    "X_val = torch.Tensor(X_val)\n",
    "y_val = torch.Tensor(y_val)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "train_set = TensorDataset(X_train, y_train) \n",
    "validate_set = TensorDataset(X_val, y_val) \n",
    "test_set = TensorDataset(X_test, y_test) \n",
    "\n",
    "\n",
    "# Create Dataloader to read the data within batch sizes and put into memory. \n",
    "train_loader = DataLoader(train_set, batch_size = 64, shuffle = True) \n",
    "validate_loader = DataLoader(validate_set, batch_size = 20) \n",
    "test_loader = DataLoader(test_set, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "541c3e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47826087, 0.6688963 , 0.5025381 , 0.1       , 0.11594203,\n",
       "        0.6596927 , 0.48360172],\n",
       "       [0.3043478 , 0.2541806 , 0.06598984, 0.        , 0.13043478,\n",
       "        0.43345717, 0.39940184],\n",
       "       [0.7324415 , 0.64882946, 0.6040609 , 0.1       , 0.02173913,\n",
       "        0.49856123, 0.92849064],\n",
       "       [0.84949833, 0.19063546, 0.6040609 , 0.        , 0.01449275,\n",
       "        0.9969816 , 0.6373941 ],\n",
       "       [0.24080268, 0.9498328 , 0.70558375, 1.        , 0.13043478,\n",
       "        0.48072156, 0.48360172],\n",
       "       [0.8795987 , 0.64214045, 0.5025381 , 0.1       , 0.0173913 ,\n",
       "        0.340046  , 0.90979785],\n",
       "       [0.04682274, 0.729097  , 0.6040609 , 0.1       , 0.71014494,\n",
       "        0.37730882, 0.45869353],\n",
       "       [0.60535115, 0.19063546, 0.04568528, 0.2       , 0.13043478,\n",
       "        0.5735067 , 0.47627386],\n",
       "       [0.16722408, 0.26755852, 0.6040609 , 0.1       , 0.02753623,\n",
       "        0.3214169 , 0.79860777],\n",
       "       [0.01337793, 0.8729097 , 0.07614213, 0.        , 0.04347826,\n",
       "        0.293018  , 0.7573984 ],\n",
       "       [0.9632107 , 0.19063546, 0.05583756, 0.        , 0.05797102,\n",
       "        0.31829378, 0.7217358 ],\n",
       "       [0.4147157 , 0.68561876, 0.78680205, 0.1       , 0.0173913 ,\n",
       "        0.13553086, 0.93969566],\n",
       "       [0.90301   , 0.07023411, 0.00203046, 0.        , 0.42028984,\n",
       "        0.73314303, 0.31506017],\n",
       "       [0.24414715, 0.4715719 , 0.00203046, 0.1       , 0.0768116 ,\n",
       "        0.3207677 , 0.56033206],\n",
       "       [0.8862876 , 0.18060201, 0.04568528, 1.        , 0.71014494,\n",
       "        0.41956246, 0.30689391],\n",
       "       [0.7324415 , 0.32441473, 0.00304569, 0.        , 0.07246377,\n",
       "        0.6907378 , 0.58063394],\n",
       "       [0.8060201 , 0.2374582 , 0.        , 0.        , 0.01449275,\n",
       "        0.49238783, 0.6932179 ],\n",
       "       [0.7558528 , 0.5518395 , 0.35025382, 0.2       , 0.04347826,\n",
       "        0.2897367 , 0.90979785],\n",
       "       [0.16053511, 0.50167227, 0.6040609 , 0.5       , 0.02898551,\n",
       "        0.45561033, 0.58063394],\n",
       "       [0.13043478, 0.6822742 , 0.6040609 , 1.        , 0.10144927,\n",
       "        0.83307165, 0.45869353],\n",
       "       [0.33110368, 0.6822742 , 0.26903552, 0.        , 0.02898551,\n",
       "        0.4995336 , 0.83145845],\n",
       "       [0.27759197, 0.9130435 , 0.00101523, 0.        , 0.10144927,\n",
       "        0.34564257, 0.53170013],\n",
       "       [0.15719064, 0.80936456, 0.00304569, 0.        , 0.13043478,\n",
       "        0.47596872, 0.58063394],\n",
       "       [0.02675585, 0.3779264 , 0.00304569, 0.2       , 0.71014494,\n",
       "        0.7872171 , 0.11273059],\n",
       "       [0.84949833, 0.19063546, 0.6040609 , 0.        , 0.01449275,\n",
       "        0.47559997, 0.62002826],\n",
       "       [0.70234114, 0.4381271 , 0.00203046, 0.5       , 0.2753623 ,\n",
       "        0.5351719 , 0.4350645 ],\n",
       "       [0.7591973 , 0.89966553, 0.8071066 , 0.        , 0.07246377,\n",
       "        0.6338321 , 0.6373941 ],\n",
       "       [0.08026756, 0.7692308 , 0.05583756, 0.        , 0.07246377,\n",
       "        0.71119434, 0.59649855],\n",
       "       [0.5117057 , 0.44816053, 0.06598984, 0.1       , 0.04347826,\n",
       "        0.6865117 , 0.7217358 ],\n",
       "       [0.7658863 , 0.33110368, 0.45177665, 0.05      , 0.        ,\n",
       "        0.67226225, 0.97307026],\n",
       "       [0.9397993 , 0.99331105, 0.5025381 , 0.2       , 0.04347826,\n",
       "        0.3773551 , 0.81927794],\n",
       "       [0.72575253, 0.2541806 , 0.05583756, 1.        , 0.2753623 ,\n",
       "        0.8356973 , 0.28670925],\n",
       "       [0.7591973 , 0.85953176, 0.07614213, 0.        , 0.07246377,\n",
       "        0.33033314, 0.7573984 ],\n",
       "       [0.32441473, 0.69899666, 0.00304569, 0.        , 0.01449275,\n",
       "        0.7099555 , 0.6446678 ],\n",
       "       [0.819398  , 0.15719064, 0.5025381 , 0.        , 0.71014494,\n",
       "        0.30737218, 0.6090432 ],\n",
       "       [0.18394649, 0.26755852, 0.07614213, 0.5       , 0.02898551,\n",
       "        0.5575868 , 0.59649855],\n",
       "       [0.30100334, 0.5250836 , 0.8071066 , 0.        , 0.85507244,\n",
       "        0.81945115, 0.3223339 ],\n",
       "       [0.4381271 , 0.5150502 , 0.00203046, 0.        , 0.11594203,\n",
       "        0.28726745, 0.62002826],\n",
       "       [0.09030101, 0.48494983, 0.05583756, 0.1       , 0.10144927,\n",
       "        0.47929567, 0.6446678 ],\n",
       "       [0.13043478, 0.4381271 , 0.5025381 , 0.        , 0.10144927,\n",
       "        0.5086951 , 0.6932179 ],\n",
       "       [0.71237457, 0.4949833 , 0.07614213, 0.1       , 0.04637681,\n",
       "        0.3009063 , 0.8059356 ],\n",
       "       [0.        , 0.77591974, 0.6852792 , 0.05      , 0.        ,\n",
       "        0.28741863, 0.98525083],\n",
       "       [0.07692308, 0.8528428 , 0.04568528, 0.        , 0.2753623 ,\n",
       "        0.4646029 , 0.6090432 ],\n",
       "       [0.0735786 , 0.6020067 , 0.6040609 , 0.1       , 0.10144927,\n",
       "        0.6665795 , 0.4193661 ],\n",
       "       [0.32107022, 0.7558528 , 0.5025381 , 0.1       , 0.02753623,\n",
       "        0.02251392, 0.77025384],\n",
       "       [0.39464882, 0.5217391 , 0.04568528, 0.1       , 0.13043478,\n",
       "        0.324618  , 0.6292278 ],\n",
       "       [0.59197325, 0.24080268, 0.00203046, 0.2       , 0.42028984,\n",
       "        0.5593144 , 0.31506017],\n",
       "       [0.02341137, 0.13712375, 0.04568528, 0.1       , 0.07246377,\n",
       "        0.72621715, 0.46791616],\n",
       "       [0.00668896, 0.13377926, 0.8071066 , 0.        , 0.04347826,\n",
       "        0.37276822, 0.6292278 ],\n",
       "       [0.12040134, 0.05685619, 0.70558375, 0.2       , 0.20289855,\n",
       "        0.33386707, 0.58063394],\n",
       "       [0.26086956, 0.7458194 , 0.00203046, 0.1       , 0.13043478,\n",
       "        0.41759986, 0.370884  ],\n",
       "       [0.4347826 , 0.909699  , 0.5025381 , 0.        , 0.08695652,\n",
       "        0.33531138, 0.79025006],\n",
       "       [0.16722408, 0.14381272, 0.5025381 , 0.        , 0.04347826,\n",
       "        0.68486583, 0.7217358 ],\n",
       "       [0.62876254, 0.78595316, 0.6040609 , 0.2       , 0.71014494,\n",
       "        0.35165817, 0.59649855],\n",
       "       [0.916388  , 0.7090301 , 0.07614213, 0.1       , 0.1594203 ,\n",
       "        0.2948437 , 0.7573984 ],\n",
       "       [0.18394649, 0.84615386, 0.00304569, 0.        , 0.85507244,\n",
       "        0.20289291, 0.27416465],\n",
       "       [0.37458193, 0.9230769 , 0.07614213, 0.1       , 0.04347826,\n",
       "        0.52801377, 0.7217358 ],\n",
       "       [0.9431438 , 0.71237457, 0.04568528, 0.        , 0.02898551,\n",
       "        0.4794122 , 0.85303766],\n",
       "       [0.89966553, 0.24749164, 0.70558375, 0.1       , 0.08695652,\n",
       "        0.6786592 , 0.79860777],\n",
       "       [0.6020067 , 0.18060201, 0.        , 1.        , 0.71014494,\n",
       "        0.60389864, 0.16126782],\n",
       "       [0.36789298, 0.8729097 , 0.00203046, 0.1       , 0.5652174 ,\n",
       "        0.57796896, 0.29769433],\n",
       "       [0.16722408, 0.19063546, 0.        , 0.2       , 0.13043478,\n",
       "        0.4783484 , 0.48360172],\n",
       "       [0.7993311 , 0.5819398 , 0.8071066 , 0.2       , 0.5652174 ,\n",
       "        0.7930213 , 0.467983  ],\n",
       "       [0.6020067 , 0.23411371, 0.04568528, 0.        , 0.04347826,\n",
       "        0.79728556, 0.48360172]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
    "train_dataset_array "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2d39f",
   "metadata": {},
   "source": [
    "Create our neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db94f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters \n",
    "input_size = list(X_train.shape)[1]   \n",
    "output_size = list(y_train.shape)[1]  \n",
    "\n",
    "\n",
    "\n",
    "# Define neural network \n",
    "class Network(nn.Module): \n",
    "    def __init__(self, input_size, output_size, init_form=\"normal\"): \n",
    "        super().__init__() \n",
    "        self.conv_stack = nn.Sequential(\n",
    "        nn.Linear(input_size, 300), \n",
    "        nn.Tanh(),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(300, 100),\n",
    "        nn.Tanh(),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(100, 100),\n",
    "        nn.Tanh(),\n",
    "        nn.Dropout(p=0.1))\n",
    "        \n",
    "        self.conv_stack1 = nn.Sequential(\n",
    "        nn.Linear(100, 100), \n",
    "        nn.Tanh(),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(100, 30),\n",
    "        nn.Tanh(), \n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(30, 1))\n",
    "                \n",
    "        self.conv_stack2 = nn.Sequential(\n",
    "        nn.Linear(100, 100), \n",
    "        nn.Tanh(),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(100, 30),\n",
    "        nn.Tanh(), \n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(30, 1))\n",
    "        \n",
    "        self.conv_stack3 = nn.Sequential(\n",
    "        nn.Linear(100, 100), \n",
    "        nn.Tanh(),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(100, 30),\n",
    "        nn.Tanh(), \n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(30, 1))\n",
    "            \n",
    "        \n",
    "        self.init_form = init_form\n",
    "        if self.init_form is not None:\n",
    "            self.init()\n",
    "\n",
    "    def forward(self, x): \n",
    "        s = self.conv_stack(x)\n",
    "        out1 = self.conv_stack1(s)\n",
    "        out2 = self.conv_stack2(s)\n",
    "        out3 = self.conv_stack3(s)\n",
    "        return out1, out2, out3\n",
    "    \n",
    "    \n",
    "        # xavier weight initialization\n",
    "    def init(self):\n",
    "        sigmoid_gain = torch.nn.init.calculate_gain(\"tanh\")\n",
    "        for child in self.conv_stack.children():\n",
    "            if isinstance(child, nn.Linear):\n",
    "                if self.init_form == \"normal\":\n",
    "                    torch.nn.init.xavier_normal_(child.weight,\n",
    "                                                 gain=sigmoid_gain)\n",
    "                    if child.bias is not None:\n",
    "                        torch.nn.init.zeros_(child.bias)\n",
    "                elif self.init_form == \"uniform\":\n",
    "                    torch.nn.init.xavier_uniform_(child.weight,\n",
    "                                                  gain=sigmoid_gain)\n",
    "                    if child.bias is not None:\n",
    "                        torch.nn.init.zeros_(child.bias)\n",
    "                else:\n",
    "                    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a50d04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_epoch(model,\n",
    "                optimizer,\n",
    "                criterion,\n",
    "                train_loader):\n",
    "    loss_history = []\n",
    "    for batch in train_loader: \n",
    "        optimizer.zero_grad()\n",
    "        x_train, y_train = batch # parse data\n",
    "        x_train, y_train = x_train.to(device), y_train.to(device) # compute on gpu\n",
    "        y1, y2, y3 = model(x_train) # get predictions\n",
    "        loss = criterion(y1, y_train[:, 0:1]) + criterion(y2, y_train[:,1:2]) + criterion(y3, y_train[:,2:]) # compute loss\n",
    "        loss_history.append(loss.cpu().detach().numpy()) # write loss to log\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a6c7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,\n",
    "             criterion,\n",
    "             val_loader):\n",
    "    cumloss = 0\n",
    "    loss_history = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x_train, y_train = batch # parse data\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device) # compute on gpu\n",
    "            y1, y2, y3 = model(x_train) # get predictions\n",
    "            loss = criterion(y1, y_train[:, 0:1]) + criterion(y2, y_train[:,1:2]) + criterion(y3, y_train[:,2:]) # compute loss\n",
    "            loss_history.append(loss.cpu().detach().numpy()) # write loss to log\n",
    "            cumloss += loss\n",
    "    return cumloss / len(val_loader), loss_history # mean loss and history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f3fed850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, optimizer, model_name=None, n_epochs=5):\n",
    "  \n",
    "    criterion = nn.MSELoss().to(device)\n",
    "\n",
    "    train_history = {}\n",
    "    train_history['model_name'] = model_name\n",
    "    train_history['loss_on_train'] = []\n",
    "    train_history['loss_on_test'] = []\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        loss_on_train = train_epoch(model,\n",
    "                                    optimizer,\n",
    "                                    criterion,\n",
    "                                    train_loader)\n",
    "        _, loss_on_test = validate(model,\n",
    "                                   criterion,\n",
    "                                   validate_loader)\n",
    "        train_history['loss_on_train'].append(np.mean(loss_on_train))\n",
    "        train_history['loss_on_test'].append(np.mean(loss_on_test))\n",
    "        scheduler.step()\n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2e8bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smoothing(scalars, weight):  \n",
    "    last = scalars[0]  \n",
    "    smoothed = []\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  \n",
    "        smoothed.append(smoothed_val)                        \n",
    "        last = smoothed_val                                 \n",
    "\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "def plot_history(history, n_epochs=5, smooth_val=0.9):\n",
    "    fig, ax =  plt.subplots(3, 1, figsize=(12, 14))\n",
    "    for stage_idx, (stage_lbl, stage_title) in enumerate(\n",
    "        zip(['loss_on_train', 'loss_on_test'],\n",
    "            ['train loss', 'val loss'])):\n",
    "        # plot history on each learning step\n",
    "        epoch_len = len(history[stage_lbl])//n_epochs\n",
    "        full_stage_len = len(history[stage_lbl])\n",
    "        ax[stage_idx].plot(exponential_smoothing(history[stage_lbl], smooth_val),\n",
    "                           label='smoothed',\n",
    "                           color='m')\n",
    "        ax[stage_idx].plot(history[stage_lbl],\n",
    "                           label='raw',\n",
    "                           alpha=0.2,\n",
    "                           color='c')\n",
    "        ax[stage_idx].set_title(stage_title)\n",
    "        ax[stage_idx].set_xlabel('epochs')\n",
    "        ax[stage_idx].set_ylabel('loss')\n",
    "        epochs_ticks_positions = np.arange(stop=full_stage_len+1,\n",
    "                                           step=epoch_len)\n",
    "        ax[stage_idx].set_xticks(np.arange(0,1000,100))\n",
    "        ax[stage_idx].set_xticklabels(np.arange(0,1000,100))\n",
    "        ax[stage_idx].legend()\n",
    "\n",
    "        # plot mean train and test loss combined\n",
    "        mean_loss_on_epoch = [np.mean(history[stage_lbl][i:i+epoch_len]) \\\n",
    "                              for i in range(0, full_stage_len, epoch_len)]\n",
    "        std_loss_on_epoch = [np.std(history[stage_lbl][i:i+epoch_len]) \\\n",
    "                              for i in range(0, full_stage_len, epoch_len)]\n",
    "\n",
    "        ax[2].set_title('\\nAverage loss per epoch')\n",
    "        ax[2].errorbar(np.arange(n_epochs) + stage_idx / 30.,\n",
    "                       mean_loss_on_epoch,\n",
    "                       yerr=std_loss_on_epoch,\n",
    "                       capsize=5,\n",
    "                       fmt=\"X--\",\n",
    "                       label=stage_title)\n",
    "        ax[2].set_xticks(np.arange(0,1000,100))\n",
    "        ax[2].set_xticklabels(np.arange(0,1000,100))\n",
    "        ax[2].set_xlabel('epochs')\n",
    "        ax[2].set_ylabel('loss')\n",
    "        ax[2].legend()\n",
    "\n",
    "    fig.suptitle(history['model_name'], fontsize=24)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85cdb03",
   "metadata": {},
   "source": [
    "Fix the seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "150d2bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6b9bcf",
   "metadata": {},
   "source": [
    "Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f030e212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:43<00:00,  9.69it/s]\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "model = Network(input_size,output_size).to(device) \n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "lambda1 = lambda epoch: 0.998 ** epoch\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1, last_epoch = -1)\n",
    "\n",
    "n_epochs = 1000\n",
    "history = train_model(model, optimizer, model_name='model', n_epochs=n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6971052a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x1008 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history, n_epochs=n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc83567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2193f11be50>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,n_epochs+1), (history['loss_on_train']), label='train')\n",
    "plt.plot(range(1,n_epochs+1), (history['loss_on_test']), label='test')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6986b941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv_stack): Sequential(\n",
       "    (0): Linear(in_features=7, out_features=300, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (7): Tanh()\n",
       "    (8): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (conv_stack1): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=100, out_features=30, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       "  (conv_stack2): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=100, out_features=30, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       "  (conv_stack3): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=100, out_features=30, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "55f0e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_abs_error(true, pred):\n",
    "    num = (np.abs(true - pred))\n",
    "    den = (np.abs(true))\n",
    "    squared_error = num/den\n",
    "    rrmse_loss = np.sum(squared_error)\n",
    "    return rrmse_loss/len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c7685f5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc for Lsat= 0.05197312105998495 \n",
      "acc for Psat= 0.0928545544293814 \n",
      "acc for optim= 0.14336201994971248\n"
     ]
    }
   ],
   "source": [
    "def test(model, val_loader):\n",
    "    cumloss1 = 0\n",
    "    cumloss2 = 0\n",
    "    cumloss3 = 0\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    l3 = []\n",
    "    l4 = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x_train, y_train = batch # parse data\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device) # compute on gpu\n",
    "            y_pred = torch.cat(model(x_train),1) # get predictions\n",
    "            y_pred = scaler2.inverse_transform(y_pred.cpu().detach().numpy())\n",
    "            y_train = scaler2.inverse_transform(y_train.cpu().detach().numpy())\n",
    "            loss1 = relative_abs_error(np.exp(y_pred[:,0]), \n",
    "                                                    np.exp(y_train[:,0])) # compute loss\n",
    "            loss2 = relative_abs_error(np.exp(y_pred[:,1]), \n",
    "                                                    np.exp(y_train[:,1]))\n",
    "            loss3 = relative_abs_error((y_pred[:,2]), \n",
    "                                                    (y_train[:,2]))\n",
    "            cumloss1 += loss1\n",
    "            cumloss2 += loss2\n",
    "            cumloss3 += loss3\n",
    "            l1.append(loss1)\n",
    "            l2.append(loss2)\n",
    "            l3.append(loss3)\n",
    "            l4.append(x_train.cpu().detach().numpy())\n",
    "    return cumloss1 / len(val_loader), cumloss2 / len(val_loader), cumloss3 / len(val_loader), l1, l2, l3, l4\n",
    "\n",
    "\n",
    "l = test(model, test_loader)\n",
    "print('acc for Lsat=', l[0],'\\n' 'acc for Psat=', l[1], '\\n' 'acc for optim=', l[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "39faae6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Lsat error')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(l[3])\n",
    "plt.title(\"Lsat error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe589d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Power error')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(l[4])\n",
    "plt.title(\"Power error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4214a0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133    0.201250\n",
       "87     0.216444\n",
       "11     0.216928\n",
       "69     0.218594\n",
       "125    0.220518\n",
       "165    0.223307\n",
       "126    0.227741\n",
       "34     0.231029\n",
       "143    0.241116\n",
       "102    0.243828\n",
       "86     0.247182\n",
       "166    0.252561\n",
       "146    0.252673\n",
       "111    0.296388\n",
       "40     0.311902\n",
       "159    0.312851\n",
       "64     0.331457\n",
       "96     0.365047\n",
       "47     0.378125\n",
       "169    0.477248\n",
       "dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(l[4]).sort_values().tail(20)\n",
    "#scaler.inverse_transform(np.array([X_test[103].cpu().detach().numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c55302e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'optim error')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(l[5])\n",
    "plt.title(\"optim error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e6b13",
   "metadata": {},
   "source": [
    "Now use our model to investigate FELs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6404a9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lsat= 13.647080039978027 Psat= 20210030.326433476 loptim= 5.104742674048302e-07\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import jv\n",
    "X = pd.DataFrame({'k':[2.13], 'lu':[2.8], 'I':[56],  'sgamma':[0.0002], \n",
    "                  'r':[0.00012], 'log0':[10], 'sqgamma': [np.log(300)]})\n",
    "\n",
    "P_0 = X['log0'][0]\n",
    "\n",
    "ksi = X['k']**2/(1+X['k']**2/2)/4\n",
    "f = jv(0,ksi)-jv(1,ksi)\n",
    "rho = 1/2/(np.exp(X['sqgamma']))*(X['I']/X['r']**2/np.pi/4/np.pi/17000*(X['lu']/100*X['k']*f)**2)**(1/3)\n",
    "\n",
    "X['log0'] = np.log(X['log0']/X['I']/np.exp(X['sqgamma'])/511000)\n",
    "\n",
    "X2 = scaler.transform(X)\n",
    "\n",
    "input1 = torch.Tensor(X2)\n",
    "\n",
    "a = (torch.cat(model(input1.to(device)),1)).cpu().detach().numpy()\n",
    "a= scaler2.inverse_transform(a)\n",
    "\n",
    "\n",
    "\n",
    "L_sat = np.exp(a[0][0])*X['lu'][0]\n",
    "P_sat = (np.exp(a[0][1])*np.exp(X['sqgamma'])*511000*X['I'])[0]\n",
    "\n",
    "print('Lsat=', L_sat, 'Psat=', P_sat, \n",
    "'loptim=',(((a[0][2])*rho+1)*X['lu']/100/2/np.exp(X['sqgamma'])/np.exp(X['sqgamma'])*(1+X['k']*X['k']/2))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "38dd6206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20709801.38550309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2193f1e2280>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def P(z,P_0,P_sat,L_sat):\n",
    "    def A(z,L):\n",
    "        return 1/9*(3+2*np.cosh(z/L)+4*np.cos(3**(0.5)/2*z/L)*np.cosh(z/(2*L)))\n",
    "    L = L_sat/1.1/np.log(9*P_sat/P_0)\n",
    "    if z<0.98*L_sat:\n",
    "        return P_0*(A(z,L)*np.exp(0.233*z/L_sat))/(1+P_0/P_sat*(A(z,L)-1))\n",
    "    elif 1.02*L_sat>=z>=0.98*L_sat:\n",
    "        return np.exp(((1.02-z/L_sat)*np.log(P_0*(A(0.98*L_sat,L)*np.exp(0.233*0.98*L_sat/L_sat))/(1+P_0/P_sat*(A(0.98*L_sat,L)-1)))+\n",
    "    (z/L_sat-0.98)*np.log(P_0*(A(L_sat,L)*np.exp(0.233*L_sat/L_sat))/(1+P_0/P_sat*(A(L_sat,L)-1))))/(0.04))\n",
    "    else: \n",
    "        return P_0*(A(L_sat,L)*np.exp(0.233*L_sat/L_sat))/(1+P_0/P_sat*(A(L_sat,L)-1))\n",
    "\n",
    "print(P(L_sat,P_0,P_sat,L_sat))\n",
    "\n",
    " \n",
    "# exponential function y = 10^x\n",
    "data = [P(i,P_0,P_sat,L_sat) for i in np.arange(0, int(1.3*L_sat), X['lu'][0]/100)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "# convert y-axis to Logarithmic scale\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('z, m')\n",
    "plt.ylabel('P, W')\n",
    "plt.title('FEL power evolution')\n",
    "\n",
    " \n",
    "plt.plot(np.arange(0, int(1.3*L_sat), X['lu'][0]/100), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4ba039b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [i.item() for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "15d3a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).to_csv('pandas.txt', header=None, index=None, sep=' ', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d160c283",
   "metadata": {},
   "source": [
    "Do a crossvalidation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "57c36cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split, SubsetRandomSampler, ConcatDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0a148458",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "set_random_seed(42)\n",
    "\n",
    "num_epochs=1000\n",
    "batch_size=64\n",
    "k=5\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "foldperf={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a1a8b1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch:1/1000\n",
      "Loss on train= 0.2438793182373047\n",
      "Loss on test= 0.13895732164382935\n",
      "acc for Lsat= 0.711079936486938 \n",
      "acc for Psat= 1.0441887378692625 \n",
      "acc for optim= 0.23552132290463118\n",
      "Epoch:2/1000\n",
      "Loss on train= 0.1045493632555008\n",
      "Loss on test= 0.09992826730012894\n",
      "acc for Lsat= 0.4146500639741208 \n",
      "acc for Psat= 0.6726808623618066 \n",
      "acc for optim= 0.2250509418522401\n",
      "Epoch:3/1000\n",
      "Loss on train= 0.07979580014944077\n",
      "Loss on test= 0.07291460782289505\n",
      "acc for Lsat= 0.28958181701229263 \n",
      "acc for Psat= 0.47537518268407775 \n",
      "acc for optim= 0.17925043169446161\n",
      "Epoch:4/1000\n",
      "Loss on train= 0.07182149589061737\n",
      "Loss on test= 0.06842219084501266\n",
      "acc for Lsat= 0.310791391510504 \n",
      "acc for Psat= 0.4065532272440253 \n",
      "acc for optim= 0.16862058479128486\n",
      "Epoch:5/1000\n",
      "Loss on train= 0.06333659589290619\n",
      "Loss on test= 0.06404736638069153\n",
      "acc for Lsat= 0.2837209939164577 \n",
      "acc for Psat= 0.39643469284539207 \n",
      "acc for optim= 0.16739513521574656\n",
      "Epoch:6/1000\n",
      "Loss on train= 0.06316057592630386\n",
      "Loss on test= 0.06227077916264534\n",
      "acc for Lsat= 0.2676608794947399 \n",
      "acc for Psat= 0.4115764531978341 \n",
      "acc for optim= 0.17390105924337018\n",
      "Epoch:7/1000\n",
      "Loss on train= 0.06290578842163086\n",
      "Loss on test= 0.05773098021745682\n",
      "acc for Lsat= 0.25791955897182334 \n",
      "acc for Psat= 0.3885961502097374 \n",
      "acc for optim= 0.17477239520842847\n",
      "Epoch:8/1000\n",
      "Loss on train= 0.06077718734741211\n",
      "Loss on test= 0.05824269354343414\n",
      "acc for Lsat= 0.2818298318457367 \n",
      "acc for Psat= 0.34287143593214675 \n",
      "acc for optim= 0.17480639647011742\n",
      "Epoch:9/1000\n",
      "Loss on train= 0.05685102567076683\n",
      "Loss on test= 0.0540543831884861\n",
      "acc for Lsat= 0.2547451694542387 \n",
      "acc for Psat= 0.342176996552667 \n",
      "acc for optim= 0.16560825913451444\n",
      "Epoch:10/1000\n",
      "Loss on train= 0.05486566200852394\n",
      "Loss on test= 0.05524653196334839\n",
      "acc for Lsat= 0.2563664525846311 \n",
      "acc for Psat= 0.36867412269313465 \n",
      "acc for optim= 0.170007743886935\n",
      "Epoch:11/1000\n",
      "Loss on train= 0.053146589547395706\n",
      "Loss on test= 0.06031004339456558\n",
      "acc for Lsat= 0.26766527570363297 \n",
      "acc for Psat= 0.3873652548885028 \n",
      "acc for optim= 0.17434832323033153\n",
      "Epoch:12/1000\n",
      "Loss on train= 0.05632523074746132\n",
      "Loss on test= 0.05632670968770981\n",
      "acc for Lsat= 0.2661385303319886 \n",
      "acc for Psat= 0.33254948169290033 \n",
      "acc for optim= 0.183482743042252\n",
      "Epoch:13/1000\n",
      "Loss on train= 0.05292654410004616\n",
      "Loss on test= 0.05443393811583519\n",
      "acc for Lsat= 0.2807481187918654 \n",
      "acc for Psat= 0.40384727688722827 \n",
      "acc for optim= 0.1682628981892849\n",
      "Epoch:14/1000\n",
      "Loss on train= 0.05102720856666565\n",
      "Loss on test= 0.05205745995044708\n",
      "acc for Lsat= 0.25693585837006166 \n",
      "acc for Psat= 0.3217827562477898 \n",
      "acc for optim= 0.16682374990263654\n",
      "Epoch:15/1000\n",
      "Loss on train= 0.051329392939805984\n",
      "Loss on test= 0.048952214419841766\n",
      "acc for Lsat= 0.25213945260475645 \n",
      "acc for Psat= 0.34035983814353565 \n",
      "acc for optim= 0.16943994148625094\n",
      "Epoch:16/1000\n",
      "Loss on train= 0.049288537353277206\n",
      "Loss on test= 0.04988755285739899\n",
      "acc for Lsat= 0.2516887328553437 \n",
      "acc for Psat= 0.3287084034114977 \n",
      "acc for optim= 0.16908441370111763\n",
      "Epoch:17/1000\n",
      "Loss on train= 0.04772237688302994\n",
      "Loss on test= 0.057781901210546494\n",
      "acc for Lsat= 0.28091321957863846 \n",
      "acc for Psat= 0.4396254957316327 \n",
      "acc for optim= 0.16988979890893066\n",
      "Epoch:18/1000\n",
      "Loss on train= 0.05046515911817551\n",
      "Loss on test= 0.048685189336538315\n",
      "acc for Lsat= 0.2655994375678788 \n",
      "acc for Psat= 0.3043077619764892 \n",
      "acc for optim= 0.1667699055814268\n",
      "Epoch:19/1000\n",
      "Loss on train= 0.04757973551750183\n",
      "Loss on test= 0.04853789880871773\n",
      "acc for Lsat= 0.25476526279386097 \n",
      "acc for Psat= 0.3251728807968951 \n",
      "acc for optim= 0.1666820511073369\n",
      "Epoch:20/1000\n",
      "Loss on train= 0.04692717269062996\n",
      "Loss on test= 0.04753192514181137\n",
      "acc for Lsat= 0.2562495251826669 \n",
      "acc for Psat= 0.298758002254258 \n",
      "acc for optim= 0.16811824466698985\n",
      "Epoch:21/1000\n",
      "Loss on train= 0.047322336584329605\n",
      "Loss on test= 0.04640185460448265\n",
      "acc for Lsat= 0.2549196676162391 \n",
      "acc for Psat= 0.30294558875188476 \n",
      "acc for optim= 0.16673675393345347\n",
      "Epoch:22/1000\n",
      "Loss on train= 0.045399826020002365\n",
      "Loss on test= 0.047802504152059555\n",
      "acc for Lsat= 0.2524395061489752 \n",
      "acc for Psat= 0.369500891354393 \n",
      "acc for optim= 0.17043127136373043\n",
      "Epoch:23/1000\n",
      "Loss on train= 0.04660913348197937\n",
      "Loss on test= 0.04884801805019379\n",
      "acc for Lsat= 0.29834055627303263 \n",
      "acc for Psat= 0.2993743225189538 \n",
      "acc for optim= 0.16599284303544762\n",
      "Epoch:24/1000\n",
      "Loss on train= 0.04771968349814415\n",
      "Loss on test= 0.04703252762556076\n",
      "acc for Lsat= 0.2519319182614551 \n",
      "acc for Psat= 0.3352392886564186 \n",
      "acc for optim= 0.16922358054655334\n",
      "Epoch:25/1000\n",
      "Loss on train= 0.0447547547519207\n",
      "Loss on test= 0.05017967149615288\n",
      "acc for Lsat= 0.2506812292079989 \n",
      "acc for Psat= 0.3689189757619585 \n",
      "acc for optim= 0.17169616204559604\n",
      "Epoch:26/1000\n",
      "Loss on train= 0.04683448746800423\n",
      "Loss on test= 0.04836782068014145\n",
      "acc for Lsat= 0.2892766310923123 \n",
      "acc for Psat= 0.30233842962207974 \n",
      "acc for optim= 0.1652504121742375\n",
      "Epoch:27/1000\n",
      "Loss on train= 0.04614151641726494\n",
      "Loss on test= 0.051743924617767334\n",
      "acc for Lsat= 0.2735744976126078 \n",
      "acc for Psat= 0.38797731744094544 \n",
      "acc for optim= 0.16798368567644162\n",
      "Epoch:28/1000\n",
      "Loss on train= 0.04571661353111267\n",
      "Loss on test= 0.044925257563591\n",
      "acc for Lsat= 0.25419765972219827 \n",
      "acc for Psat= 0.3229611485107395 \n",
      "acc for optim= 0.1664258820868013\n",
      "Epoch:29/1000\n",
      "Loss on train= 0.04444132000207901\n",
      "Loss on test= 0.04574437439441681\n",
      "acc for Lsat= 0.2632325183513553 \n",
      "acc for Psat= 0.3031947250778096 \n",
      "acc for optim= 0.1726469230018185\n",
      "Epoch:30/1000\n",
      "Loss on train= 0.045111048966646194\n",
      "Loss on test= 0.04839145392179489\n",
      "acc for Lsat= 0.27639998811423977 \n",
      "acc for Psat= 0.2927329024999244 \n",
      "acc for optim= 0.17158383887471548\n",
      "Epoch:31/1000\n",
      "Loss on train= 0.04759984835982323\n",
      "Loss on test= 0.05287069454789162\n",
      "acc for Lsat= 0.2808258223771257 \n",
      "acc for Psat= 0.37357692203648457 \n",
      "acc for optim= 0.17872452508175493\n",
      "Epoch:32/1000\n",
      "Loss on train= 0.04692339524626732\n",
      "Loss on test= 0.044713351875543594\n",
      "acc for Lsat= 0.25554326978632774 \n",
      "acc for Psat= 0.292985017711538 \n",
      "acc for optim= 0.1677400254529972\n",
      "Epoch:33/1000\n",
      "Loss on train= 0.04422122240066528\n",
      "Loss on test= 0.04485384002327919\n",
      "acc for Lsat= 0.269573080975352 \n",
      "acc for Psat= 0.293973968314174 \n",
      "acc for optim= 0.17006035621380092\n",
      "Epoch:34/1000\n",
      "Loss on train= 0.045952942222356796\n",
      "Loss on test= 0.04362909123301506\n",
      "acc for Lsat= 0.25256639128903613 \n",
      "acc for Psat= 0.3423462634466812 \n",
      "acc for optim= 0.16577020582566623\n",
      "Epoch:35/1000\n",
      "Loss on train= 0.04263223335146904\n",
      "Loss on test= 0.04369598627090454\n",
      "acc for Lsat= 0.2537045835260537 \n",
      "acc for Psat= 0.3122996557590573 \n",
      "acc for optim= 0.1671368164279532\n",
      "Epoch:36/1000\n",
      "Loss on train= 0.043333377689123154\n",
      "Loss on test= 0.04414419084787369\n",
      "acc for Lsat= 0.2514761133843483 \n",
      "acc for Psat= 0.3083849357212105 \n",
      "acc for optim= 0.17137858435957137\n",
      "Epoch:37/1000\n",
      "Loss on train= 0.04271192103624344\n",
      "Loss on test= 0.04324236139655113\n",
      "acc for Lsat= 0.25444872236727084 \n",
      "acc for Psat= 0.2902576862775607 \n",
      "acc for optim= 0.1707973395867205\n",
      "Epoch:38/1000\n",
      "Loss on train= 0.04112837836146355\n",
      "Loss on test= 0.04582427814602852\n",
      "acc for Lsat= 0.26044757255288054 \n",
      "acc for Psat= 0.3357943604158801 \n",
      "acc for optim= 0.16525608493244134\n",
      "Epoch:39/1000\n",
      "Loss on train= 0.04376466944813728\n",
      "Loss on test= 0.04342078045010567\n",
      "acc for Lsat= 0.250679690101218 \n",
      "acc for Psat= 0.2954997938336725 \n",
      "acc for optim= 0.1656457530897717\n",
      "Epoch:40/1000\n",
      "Loss on train= 0.04193057864904404\n",
      "Loss on test= 0.04330134764313698\n",
      "acc for Lsat= 0.24964885739393017 \n",
      "acc for Psat= 0.30786593431650205 \n",
      "acc for optim= 0.16756085452447306\n",
      "Epoch:41/1000\n",
      "Loss on train= 0.04143816977739334\n",
      "Loss on test= 0.04354611039161682\n",
      "acc for Lsat= 0.25775780356999656 \n",
      "acc for Psat= 0.27908040095009284 \n",
      "acc for optim= 0.16664734956037963\n",
      "Epoch:42/1000\n",
      "Loss on train= 0.041089750826358795\n",
      "Loss on test= 0.04502485692501068\n",
      "acc for Lsat= 0.25824134199326215 \n",
      "acc for Psat= 0.28185649147857467 \n",
      "acc for optim= 0.17084777743317367\n",
      "Epoch:43/1000\n",
      "Loss on train= 0.04181256890296936\n",
      "Loss on test= 0.0461394302546978\n",
      "acc for Lsat= 0.25914740819867665 \n",
      "acc for Psat= 0.3953589440184178 \n",
      "acc for optim= 0.1654979657691183\n",
      "Epoch:44/1000\n",
      "Loss on train= 0.04149963706731796\n",
      "Loss on test= 0.042228128761053085\n",
      "acc for Lsat= 0.24360549014272084 \n",
      "acc for Psat= 0.2978644965098941 \n",
      "acc for optim= 0.16660271950338365\n",
      "Epoch:45/1000\n",
      "Loss on train= 0.039362940937280655\n",
      "Loss on test= 0.04035307466983795\n",
      "acc for Lsat= 0.2443897589100556 \n",
      "acc for Psat= 0.2687067175228334 \n",
      "acc for optim= 0.16711421264366452\n",
      "Epoch:46/1000\n",
      "Loss on train= 0.042309828102588654\n",
      "Loss on test= 0.04293946176767349\n",
      "acc for Lsat= 0.24732471419331234 \n",
      "acc for Psat= 0.26936084804344806 \n",
      "acc for optim= 0.16755645346403913\n",
      "Epoch:47/1000\n",
      "Loss on train= 0.04177078977227211\n",
      "Loss on test= 0.042303428053855896\n",
      "acc for Lsat= 0.24569809179369392 \n",
      "acc for Psat= 0.27106132713267184 \n",
      "acc for optim= 0.1645181964204161\n",
      "Epoch:48/1000\n",
      "Loss on train= 0.042592696845531464\n",
      "Loss on test= 0.04372464492917061\n",
      "acc for Lsat= 0.24475335599576123 \n",
      "acc for Psat= 0.3264885571707917 \n",
      "acc for optim= 0.17168370893231263\n",
      "Epoch:49/1000\n",
      "Loss on train= 0.041042834520339966\n",
      "Loss on test= 0.04366901516914368\n",
      "acc for Lsat= 0.24007808009651416 \n",
      "acc for Psat= 0.30436069680210764 \n",
      "acc for optim= 0.16635233787603157\n",
      "Epoch:50/1000\n",
      "Loss on train= 0.04016878083348274\n",
      "Loss on test= 0.04077718034386635\n",
      "acc for Lsat= 0.24225435779736287 \n",
      "acc for Psat= 0.2885244051483382 \n",
      "acc for optim= 0.1662574177168532\n",
      "Epoch:51/1000\n",
      "Loss on train= 0.03954196721315384\n",
      "Loss on test= 0.038857270032167435\n",
      "acc for Lsat= 0.24649754267594348 \n",
      "acc for Psat= 0.2547876950910321 \n",
      "acc for optim= 0.16630957156320744\n",
      "Epoch:52/1000\n",
      "Loss on train= 0.039185669273138046\n",
      "Loss on test= 0.042520828545093536\n",
      "acc for Lsat= 0.23668477242175137 \n",
      "acc for Psat= 0.3123390477757122 \n",
      "acc for optim= 0.16553689829534865\n",
      "Epoch:53/1000\n",
      "Loss on train= 0.03934093564748764\n",
      "Loss on test= 0.0415104515850544\n",
      "acc for Lsat= 0.2410982247602899 \n",
      "acc for Psat= 0.27149088141134015 \n",
      "acc for optim= 0.1632588813075195\n",
      "Epoch:54/1000\n",
      "Loss on train= 0.039847392588853836\n",
      "Loss on test= 0.03945152834057808\n",
      "acc for Lsat= 0.23992773258804878 \n",
      "acc for Psat= 0.25594521325291986 \n",
      "acc for optim= 0.16459984347669787\n",
      "Epoch:55/1000\n",
      "Loss on train= 0.03822290152311325\n",
      "Loss on test= 0.04018141329288483\n",
      "acc for Lsat= 0.23562451125775458 \n",
      "acc for Psat= 0.2649695409492797 \n",
      "acc for optim= 0.1676092464464447\n",
      "Epoch:56/1000\n",
      "Loss on train= 0.03880815953016281\n",
      "Loss on test= 0.039222441613674164\n",
      "acc for Lsat= 0.24006601587878498 \n",
      "acc for Psat= 0.2603322659220015 \n",
      "acc for optim= 0.1679372437372556\n",
      "Epoch:57/1000\n",
      "Loss on train= 0.03937254101037979\n",
      "Loss on test= 0.03969046100974083\n",
      "acc for Lsat= 0.23556596864497542 \n",
      "acc for Psat= 0.24401082885621792 \n",
      "acc for optim= 0.164522262526509\n",
      "Epoch:58/1000\n",
      "Loss on train= 0.0406978502869606\n",
      "Loss on test= 0.04047900810837746\n",
      "acc for Lsat= 0.23560458203882867 \n",
      "acc for Psat= 0.25713326420102806 \n",
      "acc for optim= 0.16749903981471775\n",
      "Epoch:59/1000\n",
      "Loss on train= 0.03875553607940674\n",
      "Loss on test= 0.040566958487033844\n",
      "acc for Lsat= 0.23728815269628636 \n",
      "acc for Psat= 0.2984884597930402 \n",
      "acc for optim= 0.16438213151158684\n",
      "Epoch:60/1000\n",
      "Loss on train= 0.03799038380384445\n",
      "Loss on test= 0.038751982152462006\n",
      "acc for Lsat= 0.2459511004809129 \n",
      "acc for Psat= 0.25342720640062094 \n",
      "acc for optim= 0.16745003400450928\n",
      "Epoch:61/1000\n",
      "Loss on train= 0.03740542009472847\n",
      "Loss on test= 0.03927602991461754\n",
      "acc for Lsat= 0.2276503743919423 \n",
      "acc for Psat= 0.2803805644726041 \n",
      "acc for optim= 0.16695815558449376\n",
      "Epoch:62/1000\n",
      "Loss on train= 0.03812840208411217\n",
      "Loss on test= 0.038490138947963715\n",
      "acc for Lsat= 0.23842982548811903 \n",
      "acc for Psat= 0.24368767995770985 \n",
      "acc for optim= 0.16646983029438414\n",
      "Epoch:63/1000\n",
      "Loss on train= 0.037090156227350235\n",
      "Loss on test= 0.039559006690979004\n",
      "acc for Lsat= 0.22940019016646082 \n",
      "acc for Psat= 0.25143739321699177 \n",
      "acc for optim= 0.16575987028125116\n",
      "Epoch:64/1000\n",
      "Loss on train= 0.03763389587402344\n",
      "Loss on test= 0.03927820920944214\n",
      "acc for Lsat= 0.23006935095866254 \n",
      "acc for Psat= 0.2565818765234709 \n",
      "acc for optim= 0.16891292107857742\n",
      "Epoch:65/1000\n",
      "Loss on train= 0.03656018525362015\n",
      "Loss on test= 0.040208373218774796\n",
      "acc for Lsat= 0.228976912237085 \n",
      "acc for Psat= 0.2732378261033878 \n",
      "acc for optim= 0.16855370035995285\n",
      "Epoch:66/1000\n",
      "Loss on train= 0.03758852928876877\n",
      "Loss on test= 0.03977192938327789\n",
      "acc for Lsat= 0.23492273504155817 \n",
      "acc for Psat= 0.2475839006148304 \n",
      "acc for optim= 0.16688237136780623\n",
      "Epoch:67/1000\n",
      "Loss on train= 0.03807452321052551\n",
      "Loss on test= 0.03604424372315407\n",
      "acc for Lsat= 0.22879989487784252 \n",
      "acc for Psat= 0.24697720141109833 \n",
      "acc for optim= 0.1648415177961521\n",
      "Epoch:68/1000\n",
      "Loss on train= 0.040367819368839264\n",
      "Loss on test= 0.04114001989364624\n",
      "acc for Lsat= 0.2385428173201424 \n",
      "acc for Psat= 0.2502644469175625 \n",
      "acc for optim= 0.1706475894316486\n",
      "Epoch:69/1000\n",
      "Loss on train= 0.0363558828830719\n",
      "Loss on test= 0.03725934401154518\n",
      "acc for Lsat= 0.2290437788266281 \n",
      "acc for Psat= 0.23977834893223451 \n",
      "acc for optim= 0.16617330867190697\n",
      "Epoch:70/1000\n",
      "Loss on train= 0.03677221015095711\n",
      "Loss on test= 0.038743529468774796\n",
      "acc for Lsat= 0.22904775495148966 \n",
      "acc for Psat= 0.27814865654885185 \n",
      "acc for optim= 0.16616617453058694\n",
      "Epoch:71/1000\n",
      "Loss on train= 0.03619978576898575\n",
      "Loss on test= 0.03836755454540253\n",
      "acc for Lsat= 0.22729985294151936 \n",
      "acc for Psat= 0.23967622981911083 \n",
      "acc for optim= 0.16732728156536514\n",
      "Epoch:72/1000\n",
      "Loss on train= 0.035952936857938766\n",
      "Loss on test= 0.04427917301654816\n",
      "acc for Lsat= 0.2364672475082929 \n",
      "acc for Psat= 0.34406859470760304 \n",
      "acc for optim= 0.16746398955088512\n",
      "Epoch:73/1000\n",
      "Loss on train= 0.03590015694499016\n",
      "Loss on test= 0.03816317394375801\n",
      "acc for Lsat= 0.24274212145726137 \n",
      "acc for Psat= 0.2429326267733526 \n",
      "acc for optim= 0.1612235644727054\n",
      "Epoch:74/1000\n",
      "Loss on train= 0.03596201166510582\n",
      "Loss on test= 0.0371069498360157\n",
      "acc for Lsat= 0.23002952935291673 \n",
      "acc for Psat= 0.23274472876640656 \n",
      "acc for optim= 0.164448721543499\n",
      "Epoch:75/1000\n",
      "Loss on train= 0.03596510738134384\n",
      "Loss on test= 0.03695562109351158\n",
      "acc for Lsat= 0.22603474003928042 \n",
      "acc for Psat= 0.25232906618783646 \n",
      "acc for optim= 0.1647026510927764\n",
      "Epoch:76/1000\n",
      "Loss on train= 0.03635702654719353\n",
      "Loss on test= 0.037350915372371674\n",
      "acc for Lsat= 0.22206964615571537 \n",
      "acc for Psat= 0.24448552143534158 \n",
      "acc for optim= 0.16292522797156808\n",
      "Epoch:77/1000\n",
      "Loss on train= 0.034389350563287735\n",
      "Loss on test= 0.03794806823134422\n",
      "acc for Lsat= 0.22597340697861987 \n",
      "acc for Psat= 0.25033176959155 \n",
      "acc for optim= 0.1622122615476779\n",
      "Epoch:78/1000\n",
      "Loss on train= 0.035420093685388565\n",
      "Loss on test= 0.036432672291994095\n",
      "acc for Lsat= 0.22690032633436086 \n",
      "acc for Psat= 0.25092905502382695 \n",
      "acc for optim= 0.16415669167556635\n",
      "Epoch:79/1000\n",
      "Loss on train= 0.03407038375735283\n",
      "Loss on test= 0.03731744363903999\n",
      "acc for Lsat= 0.23457923917675333 \n",
      "acc for Psat= 0.2397439895278196 \n",
      "acc for optim= 0.16360488128028444\n",
      "Epoch:80/1000\n",
      "Loss on train= 0.033966001123189926\n",
      "Loss on test= 0.03647758811712265\n",
      "acc for Lsat= 0.2224974493251686 \n",
      "acc for Psat= 0.22973507916016433 \n",
      "acc for optim= 0.16370219467486258\n",
      "Epoch:81/1000\n",
      "Loss on train= 0.03583277389407158\n",
      "Loss on test= 0.0402214340865612\n",
      "acc for Lsat= 0.2280323897881365 \n",
      "acc for Psat= 0.2679418735329891 \n",
      "acc for optim= 0.1665229152207359\n",
      "Epoch:82/1000\n",
      "Loss on train= 0.035730279982089996\n",
      "Loss on test= 0.03644171357154846\n",
      "acc for Lsat= 0.2260944941115142 \n",
      "acc for Psat= 0.26459163187746193 \n",
      "acc for optim= 0.1626473495136464\n",
      "Epoch:83/1000\n",
      "Loss on train= 0.03458516299724579\n",
      "Loss on test= 0.035490941256284714\n",
      "acc for Lsat= 0.2481288810108983 \n",
      "acc for Psat= 0.22615871144291572 \n",
      "acc for optim= 0.1637441544239704\n",
      "Epoch:84/1000\n",
      "Loss on train= 0.03602468594908714\n",
      "Loss on test= 0.03701260685920715\n",
      "acc for Lsat= 0.2247104011896837 \n",
      "acc for Psat= 0.2525139001516804 \n",
      "acc for optim= 0.16443426539731576\n",
      "Epoch:85/1000\n",
      "Loss on train= 0.03584902361035347\n",
      "Loss on test= 0.03582841157913208\n",
      "acc for Lsat= 0.246066494399923 \n",
      "acc for Psat= 0.2353805158225405 \n",
      "acc for optim= 0.16461751063004687\n",
      "Epoch:86/1000\n",
      "Loss on train= 0.034881722182035446\n",
      "Loss on test= 0.03544384986162186\n",
      "acc for Lsat= 0.2271583458909956 \n",
      "acc for Psat= 0.2289441318607014 \n",
      "acc for optim= 0.16461591641372225\n",
      "Epoch:87/1000\n",
      "Loss on train= 0.033965613692998886\n",
      "Loss on test= 0.03615199029445648\n",
      "acc for Lsat= 0.2244201361935005 \n",
      "acc for Psat= 0.25301729088209796 \n",
      "acc for optim= 0.16385586655813197\n",
      "Epoch:88/1000\n",
      "Loss on train= 0.032632481306791306\n",
      "Loss on test= 0.0345100462436676\n",
      "acc for Lsat= 0.2184374705105526 \n",
      "acc for Psat= 0.23231204866966937 \n",
      "acc for optim= 0.16547198042125005\n",
      "Epoch:89/1000\n",
      "Loss on train= 0.03322965279221535\n",
      "Loss on test= 0.03641437366604805\n",
      "acc for Lsat= 0.22148665744204857 \n",
      "acc for Psat= 0.2507610093715578 \n",
      "acc for optim= 0.16528859845427576\n",
      "Epoch:90/1000\n",
      "Loss on train= 0.03554665669798851\n",
      "Loss on test= 0.03532800078392029\n",
      "acc for Lsat= 0.22812515536812059 \n",
      "acc for Psat= 0.2269945543865825 \n",
      "acc for optim= 0.1657327880495014\n",
      "Epoch:91/1000\n",
      "Loss on train= 0.03517609462141991\n",
      "Loss on test= 0.038791216909885406\n",
      "acc for Lsat= 0.22276033329409228 \n",
      "acc for Psat= 0.2879607315079319 \n",
      "acc for optim= 0.1675024793195566\n",
      "Epoch:92/1000\n",
      "Loss on train= 0.036404725164175034\n",
      "Loss on test= 0.03579350933432579\n",
      "acc for Lsat= 0.22072251801474943 \n",
      "acc for Psat= 0.2420496861800007 \n",
      "acc for optim= 0.16294187723204148\n",
      "Epoch:93/1000\n",
      "Loss on train= 0.03384067863225937\n",
      "Loss on test= 0.03543826565146446\n",
      "acc for Lsat= 0.22645559061405268 \n",
      "acc for Psat= 0.23310563893017178 \n",
      "acc for optim= 0.1618171901797931\n",
      "Epoch:94/1000\n",
      "Loss on train= 0.03378162160515785\n",
      "Loss on test= 0.03376316651701927\n",
      "acc for Lsat= 0.2199289511208518 \n",
      "acc for Psat= 0.2311114554983437 \n",
      "acc for optim= 0.1638035697596414\n",
      "Epoch:95/1000\n",
      "Loss on train= 0.032846830785274506\n",
      "Loss on test= 0.034429676830768585\n",
      "acc for Lsat= 0.21762031978150934 \n",
      "acc for Psat= 0.23504235300906878 \n",
      "acc for optim= 0.16553241840232646\n",
      "Epoch:96/1000\n",
      "Loss on train= 0.03411521390080452\n",
      "Loss on test= 0.03521248698234558\n",
      "acc for Lsat= 0.21570969934875386 \n",
      "acc for Psat= 0.24977177254781383 \n",
      "acc for optim= 0.16314725604564248\n",
      "Epoch:97/1000\n",
      "Loss on train= 0.03269390016794205\n",
      "Loss on test= 0.03593152016401291\n",
      "acc for Lsat= 0.2180091044039426 \n",
      "acc for Psat= 0.25696303753361743 \n",
      "acc for optim= 0.1648618718912435\n",
      "Epoch:98/1000\n",
      "Loss on train= 0.032910849899053574\n",
      "Loss on test= 0.03470147028565407\n",
      "acc for Lsat= 0.2135937996480948 \n",
      "acc for Psat= 0.22461259741323725 \n",
      "acc for optim= 0.16395218806013312\n",
      "Epoch:99/1000\n",
      "Loss on train= 0.033952828496694565\n",
      "Loss on test= 0.034337710589170456\n",
      "acc for Lsat= 0.21573876471218478 \n",
      "acc for Psat= 0.22158074252233156 \n",
      "acc for optim= 0.1610133846138799\n",
      "Epoch:100/1000\n",
      "Loss on train= 0.03352942317724228\n",
      "Loss on test= 0.03500928357243538\n",
      "acc for Lsat= 0.21705568719939938 \n",
      "acc for Psat= 0.22629894592437236 \n",
      "acc for optim= 0.1646250304391614\n",
      "Epoch:101/1000\n",
      "Loss on train= 0.03387622535228729\n",
      "Loss on test= 0.0354517437517643\n",
      "acc for Lsat= 0.21347258799891933 \n",
      "acc for Psat= 0.2153369893465327 \n",
      "acc for optim= 0.1655083732351512\n",
      "Epoch:102/1000\n",
      "Loss on train= 0.03323831036686897\n",
      "Loss on test= 0.033542416989803314\n",
      "acc for Lsat= 0.22527988162943702 \n",
      "acc for Psat= 0.22928594738938082 \n",
      "acc for optim= 0.16172772226143514\n",
      "Epoch:103/1000\n",
      "Loss on train= 0.03362109512090683\n",
      "Loss on test= 0.03480002284049988\n",
      "acc for Lsat= 0.20990729209196532 \n",
      "acc for Psat= 0.22973403128674652 \n",
      "acc for optim= 0.1630596847035164\n",
      "Epoch:104/1000\n",
      "Loss on train= 0.032712820917367935\n",
      "Loss on test= 0.035515833646059036\n",
      "acc for Lsat= 0.21184444574026579 \n",
      "acc for Psat= 0.2520450544119673 \n",
      "acc for optim= 0.16283573128060252\n",
      "Epoch:105/1000\n",
      "Loss on train= 0.03224733844399452\n",
      "Loss on test= 0.0336320661008358\n",
      "acc for Lsat= 0.20715248061969033 \n",
      "acc for Psat= 0.24030446277504341 \n",
      "acc for optim= 0.16530599344608393\n",
      "Epoch:106/1000\n",
      "Loss on train= 0.033261947333812714\n",
      "Loss on test= 0.03548305109143257\n",
      "acc for Lsat= 0.2100397697318828 \n",
      "acc for Psat= 0.23325572073261597 \n",
      "acc for optim= 0.16266225075404903\n",
      "Epoch:107/1000\n",
      "Loss on train= 0.03290507569909096\n",
      "Loss on test= 0.03599943220615387\n",
      "acc for Lsat= 0.20822531681916243 \n",
      "acc for Psat= 0.269813390467254 \n",
      "acc for optim= 0.16516571627106777\n",
      "Epoch:108/1000\n",
      "Loss on train= 0.032830316573381424\n",
      "Loss on test= 0.03358573466539383\n",
      "acc for Lsat= 0.20411915676142292 \n",
      "acc for Psat= 0.22483730502302862 \n",
      "acc for optim= 0.1621702364512852\n",
      "Epoch:109/1000\n",
      "Loss on train= 0.03228839486837387\n",
      "Loss on test= 0.03471270948648453\n",
      "acc for Lsat= 0.20808413694071215 \n",
      "acc for Psat= 0.2407305572318081 \n",
      "acc for optim= 0.16382383466558997\n",
      "Epoch:110/1000\n",
      "Loss on train= 0.03317108005285263\n",
      "Loss on test= 0.03749063238501549\n",
      "acc for Lsat= 0.21964242228637898 \n",
      "acc for Psat= 0.3296330608402772 \n",
      "acc for optim= 0.16563390285469762\n",
      "Epoch:111/1000\n",
      "Loss on train= 0.033182624727487564\n",
      "Loss on test= 0.03415510430932045\n",
      "acc for Lsat= 0.22266124875046484 \n",
      "acc for Psat= 0.2291624396148315 \n",
      "acc for optim= 0.16156832296586912\n",
      "Epoch:112/1000\n",
      "Loss on train= 0.03147753328084946\n",
      "Loss on test= 0.03367585316300392\n",
      "acc for Lsat= 0.200979608198337 \n",
      "acc for Psat= 0.2231635403395491 \n",
      "acc for optim= 0.16396686658510734\n",
      "Epoch:113/1000\n",
      "Loss on train= 0.03164317086338997\n",
      "Loss on test= 0.03271246701478958\n",
      "acc for Lsat= 0.19973541914030563 \n",
      "acc for Psat= 0.21720438401564404 \n",
      "acc for optim= 0.162110210910588\n",
      "Epoch:114/1000\n",
      "Loss on train= 0.03129635378718376\n",
      "Loss on test= 0.03360914811491966\n",
      "acc for Lsat= 0.2014171870443908 \n",
      "acc for Psat= 0.23275422994480585 \n",
      "acc for optim= 0.16366126273953635\n",
      "Epoch:115/1000\n",
      "Loss on train= 0.03235276788473129\n",
      "Loss on test= 0.034041136503219604\n",
      "acc for Lsat= 0.1989496501181213 \n",
      "acc for Psat= 0.20617785041910464 \n",
      "acc for optim= 0.16884667114958019\n",
      "Epoch:116/1000\n",
      "Loss on train= 0.03176926076412201\n",
      "Loss on test= 0.032419878989458084\n",
      "acc for Lsat= 0.20045375962590062 \n",
      "acc for Psat= 0.2028308717515381 \n",
      "acc for optim= 0.16151405909528765\n",
      "Epoch:117/1000\n",
      "Loss on train= 0.030325377359986305\n",
      "Loss on test= 0.03365455940365791\n",
      "acc for Lsat= 0.19418300635790905 \n",
      "acc for Psat= 0.22135895215395682 \n",
      "acc for optim= 0.16556303550634668\n",
      "Epoch:118/1000\n",
      "Loss on train= 0.030589675530791283\n",
      "Loss on test= 0.03125578165054321\n",
      "acc for Lsat= 0.19152198039812104 \n",
      "acc for Psat= 0.20166193425061302 \n",
      "acc for optim= 0.15901546707977096\n",
      "Epoch:119/1000\n",
      "Loss on train= 0.032048072665929794\n",
      "Loss on test= 0.03181604668498039\n",
      "acc for Lsat= 0.18784344182458035 \n",
      "acc for Psat= 0.19949620732437337 \n",
      "acc for optim= 0.1652386630690375\n",
      "Epoch:120/1000\n",
      "Loss on train= 0.03082050010561943\n",
      "Loss on test= 0.03292638063430786\n",
      "acc for Lsat= 0.18536543077804718 \n",
      "acc for Psat= 0.22588825495140102 \n",
      "acc for optim= 0.16319560289778984\n",
      "Epoch:121/1000\n",
      "Loss on train= 0.030608106404542923\n",
      "Loss on test= 0.03208407014608383\n",
      "acc for Lsat= 0.17998686506106612 \n",
      "acc for Psat= 0.21063465097813902 \n",
      "acc for optim= 0.15894323779499014\n",
      "Epoch:122/1000\n",
      "Loss on train= 0.02911871299147606\n",
      "Loss on test= 0.03326534852385521\n",
      "acc for Lsat= 0.17934138125359417 \n",
      "acc for Psat= 0.23084555135612872 \n",
      "acc for optim= 0.16350629121758217\n",
      "Epoch:123/1000\n",
      "Loss on train= 0.030174562707543373\n",
      "Loss on test= 0.03177839517593384\n",
      "acc for Lsat= 0.18252857337362344 \n",
      "acc for Psat= 0.20542983565219616 \n",
      "acc for optim= 0.16167382801886016\n",
      "Epoch:124/1000\n",
      "Loss on train= 0.03174610808491707\n",
      "Loss on test= 0.03405847027897835\n",
      "acc for Lsat= 0.18701885831712486 \n",
      "acc for Psat= 0.22984170210717914 \n",
      "acc for optim= 0.16402603324465576\n",
      "Epoch:125/1000\n",
      "Loss on train= 0.030176421627402306\n",
      "Loss on test= 0.033721067011356354\n",
      "acc for Lsat= 0.20069494338526675 \n",
      "acc for Psat= 0.21909307233518938 \n",
      "acc for optim= 0.1574994774751885\n",
      "Epoch:126/1000\n",
      "Loss on train= 0.030983438715338707\n",
      "Loss on test= 0.03220107778906822\n",
      "acc for Lsat= 0.17657598162014224 \n",
      "acc for Psat= 0.19934841034024264 \n",
      "acc for optim= 0.1642420348930993\n",
      "Epoch:127/1000\n",
      "Loss on train= 0.029920121654868126\n",
      "Loss on test= 0.031236160546541214\n",
      "acc for Lsat= 0.17814369407603117 \n",
      "acc for Psat= 0.19573511768417104 \n",
      "acc for optim= 0.16133354535926614\n",
      "Epoch:128/1000\n",
      "Loss on train= 0.030479835346341133\n",
      "Loss on test= 0.03282660245895386\n",
      "acc for Lsat= 0.1766029869203156 \n",
      "acc for Psat= 0.20843458868736445 \n",
      "acc for optim= 0.16028447570990884\n",
      "Epoch:129/1000\n",
      "Loss on train= 0.028994819149374962\n",
      "Loss on test= 0.03240804001688957\n",
      "acc for Lsat= 0.18935381918650532 \n",
      "acc for Psat= 0.2218455101168433 \n",
      "acc for optim= 0.16125492839718186\n",
      "Epoch:130/1000\n",
      "Loss on train= 0.029515668749809265\n",
      "Loss on test= 0.0312019195407629\n",
      "acc for Lsat= 0.16733620725200815 \n",
      "acc for Psat= 0.17944278216045167 \n",
      "acc for optim= 0.16271049324856246\n",
      "Epoch:131/1000\n",
      "Loss on train= 0.02898833155632019\n",
      "Loss on test= 0.03145267441868782\n",
      "acc for Lsat= 0.16846316613232182 \n",
      "acc for Psat= 0.18309851633353882 \n",
      "acc for optim= 0.1637451409501491\n",
      "Epoch:132/1000\n",
      "Loss on train= 0.027951568365097046\n",
      "Loss on test= 0.03067905083298683\n",
      "acc for Lsat= 0.17178904566653938 \n",
      "acc for Psat= 0.17248366354311814 \n",
      "acc for optim= 0.16101513405970955\n",
      "Epoch:133/1000\n",
      "Loss on train= 0.02847880683839321\n",
      "Loss on test= 0.031738150864839554\n",
      "acc for Lsat= 0.16357073395751243 \n",
      "acc for Psat= 0.19162950858325264 \n",
      "acc for optim= 0.16205723317358584\n",
      "Epoch:134/1000\n",
      "Loss on train= 0.028463292866945267\n",
      "Loss on test= 0.029101859778165817\n",
      "acc for Lsat= 0.14974928668567114 \n",
      "acc for Psat= 0.16817417362599676 \n",
      "acc for optim= 0.16186917222219446\n",
      "Epoch:135/1000\n",
      "Loss on train= 0.027834339067339897\n",
      "Loss on test= 0.02945992909371853\n",
      "acc for Lsat= 0.1543589818517235 \n",
      "acc for Psat= 0.16400692328266125 \n",
      "acc for optim= 0.16058252085482957\n",
      "Epoch:136/1000\n",
      "Loss on train= 0.027171026915311813\n",
      "Loss on test= 0.02983458712697029\n",
      "acc for Lsat= 0.14360149320573898 \n",
      "acc for Psat= 0.16620353420707473 \n",
      "acc for optim= 0.15807844796450035\n",
      "Epoch:137/1000\n",
      "Loss on train= 0.028285479173064232\n",
      "Loss on test= 0.030990833416581154\n",
      "acc for Lsat= 0.15031015962065258 \n",
      "acc for Psat= 0.16821177300820714 \n",
      "acc for optim= 0.16068988909753054\n",
      "Epoch:138/1000\n",
      "Loss on train= 0.02795146033167839\n",
      "Loss on test= 0.031978655606508255\n",
      "acc for Lsat= 0.16279288868967476 \n",
      "acc for Psat= 0.20303702536611465 \n",
      "acc for optim= 0.16027940988936698\n",
      "Epoch:139/1000\n",
      "Loss on train= 0.02893279679119587\n",
      "Loss on test= 0.03036520443856716\n",
      "acc for Lsat= 0.13950882613460883 \n",
      "acc for Psat= 0.1720942919436483 \n",
      "acc for optim= 0.15971808092934744\n",
      "Epoch:140/1000\n",
      "Loss on train= 0.028227372094988823\n",
      "Loss on test= 0.029011618345975876\n",
      "acc for Lsat= 0.1466169508390648 \n",
      "acc for Psat= 0.16879898893872766 \n",
      "acc for optim= 0.16000085084937335\n",
      "Epoch:141/1000\n",
      "Loss on train= 0.02823437564074993\n",
      "Loss on test= 0.030405567958950996\n",
      "acc for Lsat= 0.13793663344906018 \n",
      "acc for Psat= 0.18663019577530135 \n",
      "acc for optim= 0.16309245191143193\n",
      "Epoch:142/1000\n",
      "Loss on train= 0.027428478002548218\n",
      "Loss on test= 0.029745522886514664\n",
      "acc for Lsat= 0.1476834304903037 \n",
      "acc for Psat= 0.17378559415522601 \n",
      "acc for optim= 0.16102682161964854\n",
      "Epoch:143/1000\n",
      "Loss on train= 0.02649441547691822\n",
      "Loss on test= 0.028812982141971588\n",
      "acc for Lsat= 0.13959512643243785 \n",
      "acc for Psat= 0.1597990450668969 \n",
      "acc for optim= 0.16124725179418767\n",
      "Epoch:144/1000\n",
      "Loss on train= 0.027374889701604843\n",
      "Loss on test= 0.02912253327667713\n",
      "acc for Lsat= 0.13111954448231034 \n",
      "acc for Psat= 0.16469543350495372 \n",
      "acc for optim= 0.16068871134143337\n",
      "Epoch:145/1000\n",
      "Loss on train= 0.027521628886461258\n",
      "Loss on test= 0.030526813119649887\n",
      "acc for Lsat= 0.1519426904445471 \n",
      "acc for Psat= 0.17751410856991512 \n",
      "acc for optim= 0.1594291939885909\n",
      "Epoch:146/1000\n",
      "Loss on train= 0.028330152854323387\n",
      "Loss on test= 0.029287725687026978\n",
      "acc for Lsat= 0.13282413407417626 \n",
      "acc for Psat= 0.16190989318480128 \n",
      "acc for optim= 0.15820560088981425\n",
      "Epoch:147/1000\n",
      "Loss on train= 0.026716576889157295\n",
      "Loss on test= 0.029162004590034485\n",
      "acc for Lsat= 0.131654019688451 \n",
      "acc for Psat= 0.17103874845758227 \n",
      "acc for optim= 0.16113093891413105\n",
      "Epoch:148/1000\n",
      "Loss on train= 0.02575618028640747\n",
      "Loss on test= 0.030011318624019623\n",
      "acc for Lsat= 0.14498208355269954 \n",
      "acc for Psat= 0.18843628496822726 \n",
      "acc for optim= 0.16379959820908963\n",
      "Epoch:149/1000\n",
      "Loss on train= 0.02568488009274006\n",
      "Loss on test= 0.028386957943439484\n",
      "acc for Lsat= 0.13209485156195502 \n",
      "acc for Psat= 0.1548123331561041 \n",
      "acc for optim= 0.16304899501642114\n",
      "Epoch:150/1000\n",
      "Loss on train= 0.02668909542262554\n",
      "Loss on test= 0.028154537081718445\n",
      "acc for Lsat= 0.12912633761615055 \n",
      "acc for Psat= 0.15785341072716194 \n",
      "acc for optim= 0.1586759137156794\n",
      "Epoch:151/1000\n",
      "Loss on train= 0.026046255603432655\n",
      "Loss on test= 0.028373958542943\n",
      "acc for Lsat= 0.1282853793662252 \n",
      "acc for Psat= 0.15153529103412186 \n",
      "acc for optim= 0.16203402027339237\n",
      "Epoch:152/1000\n",
      "Loss on train= 0.025944536551833153\n",
      "Loss on test= 0.026760486885905266\n",
      "acc for Lsat= 0.12484079155811044 \n",
      "acc for Psat= 0.14559606802819972 \n",
      "acc for optim= 0.1592723315736385\n",
      "Epoch:153/1000\n",
      "Loss on train= 0.026642460376024246\n",
      "Loss on test= 0.02861868217587471\n",
      "acc for Lsat= 0.11536889410890219 \n",
      "acc for Psat= 0.14977337951675998 \n",
      "acc for optim= 0.15869983939633417\n",
      "Epoch:154/1000\n",
      "Loss on train= 0.026128530502319336\n",
      "Loss on test= 0.028475256636738777\n",
      "acc for Lsat= 0.12101415560095018 \n",
      "acc for Psat= 0.1573552423339349 \n",
      "acc for optim= 0.1597193147256921\n",
      "Epoch:155/1000\n",
      "Loss on train= 0.027975240722298622\n",
      "Loss on test= 0.027914540842175484\n",
      "acc for Lsat= 0.13335114456094382 \n",
      "acc for Psat= 0.15481189736892215 \n",
      "acc for optim= 0.16130543812564835\n",
      "Epoch:156/1000\n",
      "Loss on train= 0.0256491731852293\n",
      "Loss on test= 0.027432646602392197\n",
      "acc for Lsat= 0.12560907897363074 \n",
      "acc for Psat= 0.15414720246958172 \n",
      "acc for optim= 0.16255299621246186\n",
      "Epoch:157/1000\n",
      "Loss on train= 0.02539675123989582\n",
      "Loss on test= 0.026995914056897163\n",
      "acc for Lsat= 0.1163258798692709 \n",
      "acc for Psat= 0.14975326063228997 \n",
      "acc for optim= 0.16012122066710086\n",
      "Epoch:158/1000\n",
      "Loss on train= 0.024681394919753075\n",
      "Loss on test= 0.027306459844112396\n",
      "acc for Lsat= 0.11254071723979178 \n",
      "acc for Psat= 0.14883019690497767 \n",
      "acc for optim= 0.1594114344975481\n",
      "Epoch:159/1000\n",
      "Loss on train= 0.025015179067850113\n",
      "Loss on test= 0.028309980407357216\n",
      "acc for Lsat= 0.12060913685152301 \n",
      "acc for Psat= 0.1657018751401045 \n",
      "acc for optim= 0.1598019540903972\n",
      "Epoch:160/1000\n",
      "Loss on train= 0.02477448247373104\n",
      "Loss on test= 0.027647966518998146\n",
      "acc for Lsat= 0.11335078426373763 \n",
      "acc for Psat= 0.1463659644324914 \n",
      "acc for optim= 0.157098621446825\n",
      "Epoch:161/1000\n",
      "Loss on train= 0.0255096685141325\n",
      "Loss on test= 0.02710571326315403\n",
      "acc for Lsat= 0.1107718453454813 \n",
      "acc for Psat= 0.14556503222630268 \n",
      "acc for optim= 0.15668701963171217\n",
      "Epoch:162/1000\n",
      "Loss on train= 0.024927571415901184\n",
      "Loss on test= 0.02783956564962864\n",
      "acc for Lsat= 0.12052811267368023 \n",
      "acc for Psat= 0.16118539621267605 \n",
      "acc for optim= 0.15924199853228568\n",
      "Epoch:163/1000\n",
      "Loss on train= 0.0260259211063385\n",
      "Loss on test= 0.02728401869535446\n",
      "acc for Lsat= 0.12204395229634259 \n",
      "acc for Psat= 0.1511199813744554 \n",
      "acc for optim= 0.15830404895088598\n",
      "Epoch:164/1000\n",
      "Loss on train= 0.025752602145075798\n",
      "Loss on test= 0.026371922343969345\n",
      "acc for Lsat= 0.112884130153149 \n",
      "acc for Psat= 0.14121953336107373 \n",
      "acc for optim= 0.15872827455450925\n",
      "Epoch:165/1000\n",
      "Loss on train= 0.025221113115549088\n",
      "Loss on test= 0.026549730449914932\n",
      "acc for Lsat= 0.10787574415191066 \n",
      "acc for Psat= 0.15741613564697218 \n",
      "acc for optim= 0.15632904441253687\n",
      "Epoch:166/1000\n",
      "Loss on train= 0.025076070800423622\n",
      "Loss on test= 0.027496468275785446\n",
      "acc for Lsat= 0.12049881936703807 \n",
      "acc for Psat= 0.14865812679065818 \n",
      "acc for optim= 0.1589316300973544\n",
      "Epoch:167/1000\n",
      "Loss on train= 0.024878067895770073\n",
      "Loss on test= 0.028205713257193565\n",
      "acc for Lsat= 0.12020739724865784 \n",
      "acc for Psat= 0.19927628034373057 \n",
      "acc for optim= 0.1610580771468407\n",
      "Epoch:168/1000\n",
      "Loss on train= 0.024377482011914253\n",
      "Loss on test= 0.02753048948943615\n",
      "acc for Lsat= 0.11137158185540637 \n",
      "acc for Psat= 0.15824322967830287 \n",
      "acc for optim= 0.1597497319264269\n",
      "Epoch:169/1000\n",
      "Loss on train= 0.024392960593104362\n",
      "Loss on test= 0.026764528825879097\n",
      "acc for Lsat= 0.10367411307718274 \n",
      "acc for Psat= 0.1394136903887967 \n",
      "acc for optim= 0.15638627600036195\n",
      "Epoch:170/1000\n",
      "Loss on train= 0.024583933874964714\n",
      "Loss on test= 0.026393471285700798\n",
      "acc for Lsat= 0.10219156985663098 \n",
      "acc for Psat= 0.14018119951815305 \n",
      "acc for optim= 0.1595569945649055\n",
      "Epoch:171/1000\n",
      "Loss on train= 0.023714493960142136\n",
      "Loss on test= 0.027363432571291924\n",
      "acc for Lsat= 0.11487692434130319 \n",
      "acc for Psat= 0.16527058319792004 \n",
      "acc for optim= 0.15889345934224683\n",
      "Epoch:172/1000\n",
      "Loss on train= 0.025108788162469864\n",
      "Loss on test= 0.02624313160777092\n",
      "acc for Lsat= 0.11227483785033619 \n",
      "acc for Psat= 0.16268090779995206 \n",
      "acc for optim= 0.15719046947171922\n",
      "Epoch:173/1000\n",
      "Loss on train= 0.02462405152618885\n",
      "Loss on test= 0.02526470646262169\n",
      "acc for Lsat= 0.10709817213473523 \n",
      "acc for Psat= 0.13793160802502172 \n",
      "acc for optim= 0.15928316629210187\n",
      "Epoch:174/1000\n",
      "Loss on train= 0.025586193427443504\n",
      "Loss on test= 0.02797599695622921\n",
      "acc for Lsat= 0.1281272231344369 \n",
      "acc for Psat= 0.16175180833997124 \n",
      "acc for optim= 0.15704270318101018\n",
      "Epoch:175/1000\n",
      "Loss on train= 0.023625416681170464\n",
      "Loss on test= 0.02648245543241501\n",
      "acc for Lsat= 0.10565340140729255 \n",
      "acc for Psat= 0.14189623740820395 \n",
      "acc for optim= 0.1591829748644781\n",
      "Epoch:176/1000\n",
      "Loss on train= 0.024435240775346756\n",
      "Loss on test= 0.025591013953089714\n",
      "acc for Lsat= 0.10076401205950006 \n",
      "acc for Psat= 0.13559682161309003 \n",
      "acc for optim= 0.15606627175182206\n",
      "Epoch:177/1000\n",
      "Loss on train= 0.024698790162801743\n",
      "Loss on test= 0.02806956321001053\n",
      "acc for Lsat= 0.13212530781264323 \n",
      "acc for Psat= 0.14862944452073487 \n",
      "acc for optim= 0.15907043190889575\n",
      "Epoch:178/1000\n",
      "Loss on train= 0.02548460103571415\n",
      "Loss on test= 0.02650291658937931\n",
      "acc for Lsat= 0.10456554774826149 \n",
      "acc for Psat= 0.14715169751366905 \n",
      "acc for optim= 0.15913521089031055\n",
      "Epoch:179/1000\n",
      "Loss on train= 0.02470020018517971\n",
      "Loss on test= 0.02640511840581894\n",
      "acc for Lsat= 0.10689730222439055 \n",
      "acc for Psat= 0.14471445303422673 \n",
      "acc for optim= 0.1558186863743982\n",
      "Epoch:180/1000\n",
      "Loss on train= 0.02484181523323059\n",
      "Loss on test= 0.027042310684919357\n",
      "acc for Lsat= 0.10852679089454322 \n",
      "acc for Psat= 0.15283823195485974 \n",
      "acc for optim= 0.156045032557459\n",
      "Epoch:181/1000\n",
      "Loss on train= 0.023951247334480286\n",
      "Loss on test= 0.026746980845928192\n",
      "acc for Lsat= 0.11656729509664142 \n",
      "acc for Psat= 0.14821281100428382 \n",
      "acc for optim= 0.1589681972102865\n",
      "Epoch:182/1000\n",
      "Loss on train= 0.025139102712273598\n",
      "Loss on test= 0.026523062959313393\n",
      "acc for Lsat= 0.11042429012713639 \n",
      "acc for Psat= 0.13557899360640893 \n",
      "acc for optim= 0.1547684617612845\n",
      "Epoch:183/1000\n",
      "Loss on train= 0.02418733574450016\n",
      "Loss on test= 0.026828980073332787\n",
      "acc for Lsat= 0.10457745729094725 \n",
      "acc for Psat= 0.14048261276115218 \n",
      "acc for optim= 0.15869230172562834\n",
      "Epoch:184/1000\n",
      "Loss on train= 0.023193052038550377\n",
      "Loss on test= 0.026470093056559563\n",
      "acc for Lsat= 0.11462740868429011 \n",
      "acc for Psat= 0.14525961297691067 \n",
      "acc for optim= 0.15480508641943186\n",
      "Epoch:185/1000\n",
      "Loss on train= 0.02462710067629814\n",
      "Loss on test= 0.02750944346189499\n",
      "acc for Lsat= 0.1263887375692196 \n",
      "acc for Psat= 0.1629702989247154 \n",
      "acc for optim= 0.15998940016344143\n",
      "Epoch:186/1000\n",
      "Loss on train= 0.02424246072769165\n",
      "Loss on test= 0.026780949905514717\n",
      "acc for Lsat= 0.103159914262271 \n",
      "acc for Psat= 0.14288754989934518 \n",
      "acc for optim= 0.1611061202133217\n",
      "Epoch:187/1000\n",
      "Loss on train= 0.024053972214460373\n",
      "Loss on test= 0.02660742960870266\n",
      "acc for Lsat= 0.11489071275704722 \n",
      "acc for Psat= 0.15533242883080264 \n",
      "acc for optim= 0.1555655165170118\n",
      "Epoch:188/1000\n",
      "Loss on train= 0.023900244385004044\n",
      "Loss on test= 0.02523517794907093\n",
      "acc for Lsat= 0.10701357732183511 \n",
      "acc for Psat= 0.15077684565239968 \n",
      "acc for optim= 0.15605315067047296\n",
      "Epoch:189/1000\n",
      "Loss on train= 0.023282770067453384\n",
      "Loss on test= 0.024896996095776558\n",
      "acc for Lsat= 0.10170464533508022 \n",
      "acc for Psat= 0.1360391242361544 \n",
      "acc for optim= 0.15420011254244076\n",
      "Epoch:190/1000\n",
      "Loss on train= 0.022901032119989395\n",
      "Loss on test= 0.026311540976166725\n",
      "acc for Lsat= 0.10883596297910442 \n",
      "acc for Psat= 0.1487518088564128 \n",
      "acc for optim= 0.1557732788035244\n",
      "Epoch:191/1000\n",
      "Loss on train= 0.024258019402623177\n",
      "Loss on test= 0.02703864872455597\n",
      "acc for Lsat= 0.1051356703340017 \n",
      "acc for Psat= 0.1498946117800335 \n",
      "acc for optim= 0.1546311429766721\n",
      "Epoch:192/1000\n",
      "Loss on train= 0.022126298397779465\n",
      "Loss on test= 0.025406774133443832\n",
      "acc for Lsat= 0.1041428304985908 \n",
      "acc for Psat= 0.1488503877902744 \n",
      "acc for optim= 0.1559232961497829\n",
      "Epoch:193/1000\n",
      "Loss on train= 0.023436246439814568\n",
      "Loss on test= 0.025176245719194412\n",
      "acc for Lsat= 0.09507323014379737 \n",
      "acc for Psat= 0.12922740402411784 \n",
      "acc for optim= 0.15605859158443056\n",
      "Epoch:194/1000\n",
      "Loss on train= 0.02366580255329609\n",
      "Loss on test= 0.025273572653532028\n",
      "acc for Lsat= 0.10559554824797418 \n",
      "acc for Psat= 0.1346755196881849 \n",
      "acc for optim= 0.15401646146346562\n",
      "Epoch:195/1000\n",
      "Loss on train= 0.023145008832216263\n",
      "Loss on test= 0.02536032162606716\n",
      "acc for Lsat= 0.10126029181718035 \n",
      "acc for Psat= 0.13232519452358002 \n",
      "acc for optim= 0.15463259253787048\n",
      "Epoch:196/1000\n",
      "Loss on train= 0.023372843861579895\n",
      "Loss on test= 0.02569163590669632\n",
      "acc for Lsat= 0.1070114144851203 \n",
      "acc for Psat= 0.1493189963193431 \n",
      "acc for optim= 0.1541068141642599\n",
      "Epoch:197/1000\n",
      "Loss on train= 0.023255078122019768\n",
      "Loss on test= 0.025771772488951683\n",
      "acc for Lsat= 0.10431688715057119 \n",
      "acc for Psat= 0.14446472507774633 \n",
      "acc for optim= 0.15482460175241744\n",
      "Epoch:198/1000\n",
      "Loss on train= 0.022982418537139893\n",
      "Loss on test= 0.025827327743172646\n",
      "acc for Lsat= 0.09619183486878279 \n",
      "acc for Psat= 0.13409933814178668 \n",
      "acc for optim= 0.16227251390286063\n",
      "Epoch:199/1000\n",
      "Loss on train= 0.023882107809185982\n",
      "Loss on test= 0.02534923143684864\n",
      "acc for Lsat= 0.10754140619423698 \n",
      "acc for Psat= 0.13055138730527552 \n",
      "acc for optim= 0.15784205479479316\n",
      "Epoch:200/1000\n",
      "Loss on train= 0.023693382740020752\n",
      "Loss on test= 0.02534770406782627\n",
      "acc for Lsat= 0.10893493063030055 \n",
      "acc for Psat= 0.1687601657998918 \n",
      "acc for optim= 0.15539171481845387\n",
      "Epoch:201/1000\n",
      "Loss on train= 0.02354631945490837\n",
      "Loss on test= 0.026435017585754395\n",
      "acc for Lsat= 0.10213832090859397 \n",
      "acc for Psat= 0.13576434068505547 \n",
      "acc for optim= 0.1609091127631672\n",
      "Epoch:202/1000\n",
      "Loss on train= 0.022353382781147957\n",
      "Loss on test= 0.02581384778022766\n",
      "acc for Lsat= 0.09990911515448181 \n",
      "acc for Psat= 0.14005418942616232 \n",
      "acc for optim= 0.15424709795321337\n",
      "Epoch:203/1000\n",
      "Loss on train= 0.023024844005703926\n",
      "Loss on test= 0.02527756430208683\n",
      "acc for Lsat= 0.09695365543777364 \n",
      "acc for Psat= 0.13919997551908522 \n",
      "acc for optim= 0.15752887230774892\n",
      "Epoch:204/1000\n",
      "Loss on train= 0.022316357120871544\n",
      "Loss on test= 0.025930916890501976\n",
      "acc for Lsat= 0.09636360448460247 \n",
      "acc for Psat= 0.13665159039719157 \n",
      "acc for optim= 0.15532680644941488\n",
      "Epoch:205/1000\n",
      "Loss on train= 0.02342584729194641\n",
      "Loss on test= 0.02548351138830185\n",
      "acc for Lsat= 0.10312136979990227 \n",
      "acc for Psat= 0.14463999126836707 \n",
      "acc for optim= 0.15427941281930152\n",
      "Epoch:206/1000\n",
      "Loss on train= 0.023299772292375565\n",
      "Loss on test= 0.02613476850092411\n",
      "acc for Lsat= 0.09709620259924985 \n",
      "acc for Psat= 0.17467659684114675 \n",
      "acc for optim= 0.16186212909182043\n",
      "Epoch:207/1000\n",
      "Loss on train= 0.02250201627612114\n",
      "Loss on test= 0.025204051285982132\n",
      "acc for Lsat= 0.09848319898016032 \n",
      "acc for Psat= 0.13516586363513602 \n",
      "acc for optim= 0.15544299417951968\n",
      "Epoch:208/1000\n",
      "Loss on train= 0.02238919958472252\n",
      "Loss on test= 0.026479266583919525\n",
      "acc for Lsat= 0.09859744983654084 \n",
      "acc for Psat= 0.14510228390313465 \n",
      "acc for optim= 0.1562041964245793\n",
      "Epoch:209/1000\n",
      "Loss on train= 0.022401971742510796\n",
      "Loss on test= 0.024800539016723633\n",
      "acc for Lsat= 0.09487304273633862 \n",
      "acc for Psat= 0.1350827163240046 \n",
      "acc for optim= 0.15380541595113634\n",
      "Epoch:210/1000\n",
      "Loss on train= 0.02178027667105198\n",
      "Loss on test= 0.026515530422329903\n",
      "acc for Lsat= 0.10313419576895196 \n",
      "acc for Psat= 0.15346211989060596 \n",
      "acc for optim= 0.15748224965361662\n",
      "Epoch:211/1000\n",
      "Loss on train= 0.02224244363605976\n",
      "Loss on test= 0.02464592456817627\n",
      "acc for Lsat= 0.10186313864004572 \n",
      "acc for Psat= 0.13437006948794242 \n",
      "acc for optim= 0.15436102983563446\n",
      "Epoch:212/1000\n",
      "Loss on train= 0.020862972363829613\n",
      "Loss on test= 0.02511555887758732\n",
      "acc for Lsat= 0.09296623690183771 \n",
      "acc for Psat= 0.1305092438510882 \n",
      "acc for optim= 0.15371203784926785\n",
      "Epoch:213/1000\n",
      "Loss on train= 0.022020487114787102\n",
      "Loss on test= 0.025605030357837677\n",
      "acc for Lsat= 0.11122629836548206 \n",
      "acc for Psat= 0.13915383431602552 \n",
      "acc for optim= 0.15453567255375003\n",
      "Epoch:214/1000\n",
      "Loss on train= 0.022424863651394844\n",
      "Loss on test= 0.023806840181350708\n",
      "acc for Lsat= 0.09177825266736686 \n",
      "acc for Psat= 0.1299039216334638 \n",
      "acc for optim= 0.15340936142344808\n",
      "Epoch:215/1000\n",
      "Loss on train= 0.02189798839390278\n",
      "Loss on test= 0.024174058809876442\n",
      "acc for Lsat= 0.09550944796432292 \n",
      "acc for Psat= 0.13976027733859825 \n",
      "acc for optim= 0.1563578444659908\n",
      "Epoch:216/1000\n",
      "Loss on train= 0.02316286601126194\n",
      "Loss on test= 0.027394145727157593\n",
      "acc for Lsat= 0.11794392450703339 \n",
      "acc for Psat= 0.16914915068205014 \n",
      "acc for optim= 0.16799457041528137\n",
      "Epoch:217/1000\n",
      "Loss on train= 0.022693151608109474\n",
      "Loss on test= 0.024733709171414375\n",
      "acc for Lsat= 0.11016048093966867 \n",
      "acc for Psat= 0.13793925692472742 \n",
      "acc for optim= 0.1554730283065492\n",
      "Epoch:218/1000\n",
      "Loss on train= 0.02229316718876362\n",
      "Loss on test= 0.024144796654582024\n",
      "acc for Lsat= 0.09069637398783154 \n",
      "acc for Psat= 0.13006962914007447 \n",
      "acc for optim= 0.1527467301121582\n",
      "Epoch:219/1000\n",
      "Loss on train= 0.021597985178232193\n",
      "Loss on test= 0.024918852373957634\n",
      "acc for Lsat= 0.09736012004538627 \n",
      "acc for Psat= 0.13543815602891862 \n",
      "acc for optim= 0.15367978385516573\n",
      "Epoch:220/1000\n",
      "Loss on train= 0.021788546815514565\n",
      "Loss on test= 0.026453210040926933\n",
      "acc for Lsat= 0.10199746845964579 \n",
      "acc for Psat= 0.15500827998021902 \n",
      "acc for optim= 0.1525539101754312\n",
      "Epoch:221/1000\n",
      "Loss on train= 0.021510982885956764\n",
      "Loss on test= 0.02570626325905323\n",
      "acc for Lsat= 0.10119810660812151 \n",
      "acc for Psat= 0.15560556265998918 \n",
      "acc for optim= 0.15204950668090994\n",
      "Epoch:222/1000\n",
      "Loss on train= 0.0220620296895504\n",
      "Loss on test= 0.025764869526028633\n",
      "acc for Lsat= 0.10039469560119402 \n",
      "acc for Psat= 0.13383391467835817 \n",
      "acc for optim= 0.1520863897974705\n",
      "Epoch:223/1000\n",
      "Loss on train= 0.02275066263973713\n",
      "Loss on test= 0.024476924911141396\n",
      "acc for Lsat= 0.09806540362858693 \n",
      "acc for Psat= 0.13075401511699256 \n",
      "acc for optim= 0.15327595272333516\n",
      "Epoch:224/1000\n",
      "Loss on train= 0.020590471103787422\n",
      "Loss on test= 0.02447528950870037\n",
      "acc for Lsat= 0.09649223746651434 \n",
      "acc for Psat= 0.14635661565188157 \n",
      "acc for optim= 0.15148800948133498\n",
      "Epoch:225/1000\n",
      "Loss on train= 0.020812442526221275\n",
      "Loss on test= 0.02441047877073288\n",
      "acc for Lsat= 0.09445910560728309 \n",
      "acc for Psat= 0.13349822110115886 \n",
      "acc for optim= 0.15252368549175835\n",
      "Epoch:226/1000\n",
      "Loss on train= 0.022085323929786682\n",
      "Loss on test= 0.024473343044519424\n",
      "acc for Lsat= 0.09964903048502648 \n",
      "acc for Psat= 0.13634029339714304 \n",
      "acc for optim= 0.15434131939149776\n",
      "Epoch:227/1000\n",
      "Loss on train= 0.02182641252875328\n",
      "Loss on test= 0.025193560868501663\n",
      "acc for Lsat= 0.10469688711768367 \n",
      "acc for Psat= 0.15074871923044272 \n",
      "acc for optim= 0.15285546785573229\n",
      "Epoch:228/1000\n",
      "Loss on train= 0.022952068597078323\n",
      "Loss on test= 0.025195132941007614\n",
      "acc for Lsat= 0.09633860027671257 \n",
      "acc for Psat= 0.1353577157785726 \n",
      "acc for optim= 0.1569137871463433\n",
      "Epoch:229/1000\n",
      "Loss on train= 0.02195689082145691\n",
      "Loss on test= 0.024294346570968628\n",
      "acc for Lsat= 0.0944284453146481 \n",
      "acc for Psat= 0.12917358348535937 \n",
      "acc for optim= 0.15115548257415867\n",
      "Epoch:230/1000\n",
      "Loss on train= 0.021065350621938705\n",
      "Loss on test= 0.024306846782565117\n",
      "acc for Lsat= 0.0989833540694658 \n",
      "acc for Psat= 0.13886008777491676 \n",
      "acc for optim= 0.15407272458472518\n",
      "Epoch:231/1000\n",
      "Loss on train= 0.02144119329750538\n",
      "Loss on test= 0.024138012900948524\n",
      "acc for Lsat= 0.09549392915247287 \n",
      "acc for Psat= 0.128068210219228 \n",
      "acc for optim= 0.1560305497377022\n",
      "Epoch:232/1000\n",
      "Loss on train= 0.02160451002418995\n",
      "Loss on test= 0.024763567373156548\n",
      "acc for Lsat= 0.09218678201155804 \n",
      "acc for Psat= 0.13300583421985968 \n",
      "acc for optim= 0.15050605151344376\n",
      "Epoch:233/1000\n",
      "Loss on train= 0.0210181325674057\n",
      "Loss on test= 0.025062531232833862\n",
      "acc for Lsat= 0.08689156408325777 \n",
      "acc for Psat= 0.1275091614834098 \n",
      "acc for optim= 0.15627657903785327\n",
      "Epoch:234/1000\n",
      "Loss on train= 0.02048293687403202\n",
      "Loss on test= 0.023468954488635063\n",
      "acc for Lsat= 0.08615948078640276 \n",
      "acc for Psat= 0.12889326177166147 \n",
      "acc for optim= 0.14956684704634832\n",
      "Epoch:235/1000\n",
      "Loss on train= 0.020269261673092842\n",
      "Loss on test= 0.0242612361907959\n",
      "acc for Lsat= 0.09673569190541771 \n",
      "acc for Psat= 0.12707444138305135 \n",
      "acc for optim= 0.15264701661081412\n",
      "Epoch:236/1000\n",
      "Loss on train= 0.020280322059988976\n",
      "Loss on test= 0.02527899481356144\n",
      "acc for Lsat= 0.10762085649262235 \n",
      "acc for Psat= 0.13713473913281468 \n",
      "acc for optim= 0.1564424716555003\n",
      "Epoch:237/1000\n",
      "Loss on train= 0.020796870812773705\n",
      "Loss on test= 0.025311868637800217\n",
      "acc for Lsat= 0.10382932210681445 \n",
      "acc for Psat= 0.13722487185088497 \n",
      "acc for optim= 0.152059015364346\n",
      "Epoch:238/1000\n",
      "Loss on train= 0.021746540442109108\n",
      "Loss on test= 0.024428879842162132\n",
      "acc for Lsat= 0.09727536643858369 \n",
      "acc for Psat= 0.12778636914155017 \n",
      "acc for optim= 0.15502563964488894\n",
      "Epoch:239/1000\n",
      "Loss on train= 0.020702289417386055\n",
      "Loss on test= 0.024789970368146896\n",
      "acc for Lsat= 0.0914532862232373 \n",
      "acc for Psat= 0.1326572158011883 \n",
      "acc for optim= 0.1580947604290275\n",
      "Epoch:240/1000\n",
      "Loss on train= 0.02051134780049324\n",
      "Loss on test= 0.024672314524650574\n",
      "acc for Lsat= 0.09974289962223597 \n",
      "acc for Psat= 0.1352331068428648 \n",
      "acc for optim= 0.15332835033486453\n",
      "Epoch:241/1000\n",
      "Loss on train= 0.02045573852956295\n",
      "Loss on test= 0.024801919236779213\n",
      "acc for Lsat= 0.09816026177715223 \n",
      "acc for Psat= 0.13320015502530475 \n",
      "acc for optim= 0.15483785138969805\n",
      "Epoch:242/1000\n",
      "Loss on train= 0.02111753262579441\n",
      "Loss on test= 0.02549789473414421\n",
      "acc for Lsat= 0.0999308768300915 \n",
      "acc for Psat= 0.13887923334128044 \n",
      "acc for optim= 0.15074107078618781\n",
      "Epoch:243/1000\n",
      "Loss on train= 0.021551933139562607\n",
      "Loss on test= 0.023700952529907227\n",
      "acc for Lsat= 0.08790763513590409 \n",
      "acc for Psat= 0.12608833192194818 \n",
      "acc for optim= 0.1532058444134025\n",
      "Epoch:244/1000\n",
      "Loss on train= 0.02097533829510212\n",
      "Loss on test= 0.023719416931271553\n",
      "acc for Lsat= 0.09184621176450355 \n",
      "acc for Psat= 0.12689522403815257 \n",
      "acc for optim= 0.151467089122316\n",
      "Epoch:245/1000\n",
      "Loss on train= 0.02095099166035652\n",
      "Loss on test= 0.02451891079545021\n",
      "acc for Lsat= 0.08473039910048744 \n",
      "acc for Psat= 0.12152157459544187 \n",
      "acc for optim= 0.1528615473908839\n",
      "Epoch:246/1000\n",
      "Loss on train= 0.020833192393183708\n",
      "Loss on test= 0.024283716455101967\n",
      "acc for Lsat= 0.09113509110438071 \n",
      "acc for Psat= 0.12961841932562893 \n",
      "acc for optim= 0.15375554621021614\n",
      "Epoch:247/1000\n",
      "Loss on train= 0.02096310630440712\n",
      "Loss on test= 0.02577156573534012\n",
      "acc for Lsat= 0.09796383077916118 \n",
      "acc for Psat= 0.1378344055426081 \n",
      "acc for optim= 0.1562201500533031\n",
      "Epoch:248/1000\n",
      "Loss on train= 0.021341225132346153\n",
      "Loss on test= 0.024150872603058815\n",
      "acc for Lsat= 0.09224360323823566 \n",
      "acc for Psat= 0.1406154047611148 \n",
      "acc for optim= 0.15234340098609167\n",
      "Epoch:249/1000\n",
      "Loss on train= 0.020587923005223274\n",
      "Loss on test= 0.02491438202559948\n",
      "acc for Lsat= 0.09753469921821771 \n",
      "acc for Psat= 0.13448895008064982 \n",
      "acc for optim= 0.15203664847386636\n",
      "Epoch:250/1000\n",
      "Loss on train= 0.020855950191617012\n",
      "Loss on test= 0.024454399943351746\n",
      "acc for Lsat= 0.11088636965054609 \n",
      "acc for Psat= 0.12999457534761524 \n",
      "acc for optim= 0.15123366723424966\n",
      "Epoch:251/1000\n",
      "Loss on train= 0.021619154140353203\n",
      "Loss on test= 0.024082263931632042\n",
      "acc for Lsat= 0.09185502556867377 \n",
      "acc for Psat= 0.13221862223061215 \n",
      "acc for optim= 0.15877039818668684\n",
      "Epoch:252/1000\n",
      "Loss on train= 0.021634874865412712\n",
      "Loss on test= 0.02396024577319622\n",
      "acc for Lsat= 0.09407758516726701 \n",
      "acc for Psat= 0.13512531952604503 \n",
      "acc for optim= 0.15304697225260175\n",
      "Epoch:253/1000\n",
      "Loss on train= 0.020649118348956108\n",
      "Loss on test= 0.02310892939567566\n",
      "acc for Lsat= 0.08359879001826545 \n",
      "acc for Psat= 0.11881574879453034 \n",
      "acc for optim= 0.1498566151259349\n",
      "Epoch:254/1000\n",
      "Loss on train= 0.019951164722442627\n",
      "Loss on test= 0.023305218666791916\n",
      "acc for Lsat= 0.08820619446494651 \n",
      "acc for Psat= 0.13300182055001245 \n",
      "acc for optim= 0.1529630388730388\n",
      "Epoch:255/1000\n",
      "Loss on train= 0.020013442263007164\n",
      "Loss on test= 0.02287575602531433\n",
      "acc for Lsat= 0.09377084590271861 \n",
      "acc for Psat= 0.12996435751550617 \n",
      "acc for optim= 0.15137764185766045\n",
      "Epoch:256/1000\n",
      "Loss on train= 0.020271630957722664\n",
      "Loss on test= 0.02464061789214611\n",
      "acc for Lsat= 0.09150717158452223 \n",
      "acc for Psat= 0.12614058234366868 \n",
      "acc for optim= 0.15090170468998906\n",
      "Epoch:257/1000\n",
      "Loss on train= 0.020988870412111282\n",
      "Loss on test= 0.023350069299340248\n",
      "acc for Lsat= 0.10285526472468713 \n",
      "acc for Psat= 0.14125521834506544 \n",
      "acc for optim= 0.1496820388046214\n",
      "Epoch:258/1000\n",
      "Loss on train= 0.020280471071600914\n",
      "Loss on test= 0.02362888678908348\n",
      "acc for Lsat= 0.08978383250014728 \n",
      "acc for Psat= 0.12277640884500801 \n",
      "acc for optim= 0.14976887782150727\n",
      "Epoch:259/1000\n",
      "Loss on train= 0.019953394308686256\n",
      "Loss on test= 0.023253293707966805\n",
      "acc for Lsat= 0.09780625287084484 \n",
      "acc for Psat= 0.1309880818837505 \n",
      "acc for optim= 0.15225833628660815\n",
      "Epoch:260/1000\n",
      "Loss on train= 0.02023167349398136\n",
      "Loss on test= 0.023458391427993774\n",
      "acc for Lsat= 0.0834388263201793 \n",
      "acc for Psat= 0.12457006215257108 \n",
      "acc for optim= 0.1514948019553656\n",
      "Epoch:261/1000\n",
      "Loss on train= 0.02004314959049225\n",
      "Loss on test= 0.02375953085720539\n",
      "acc for Lsat= 0.09085732233088675 \n",
      "acc for Psat= 0.12242667898783256 \n",
      "acc for optim= 0.14990553842034457\n",
      "Epoch:262/1000\n",
      "Loss on train= 0.020119905471801758\n",
      "Loss on test= 0.02352272905409336\n",
      "acc for Lsat= 0.09819249609776114 \n",
      "acc for Psat= 0.12932547544720166 \n",
      "acc for optim= 0.1498278272706408\n",
      "Epoch:263/1000\n",
      "Loss on train= 0.01864662766456604\n",
      "Loss on test= 0.023360995575785637\n",
      "acc for Lsat= 0.08619830890549378 \n",
      "acc for Psat= 0.12033721331346074 \n",
      "acc for optim= 0.15134675572084824\n",
      "Epoch:264/1000\n",
      "Loss on train= 0.019834117963910103\n",
      "Loss on test= 0.02373286709189415\n",
      "acc for Lsat= 0.0875153920678205 \n",
      "acc for Psat= 0.12401313478764509 \n",
      "acc for optim= 0.1532674416553143\n",
      "Epoch:265/1000\n",
      "Loss on train= 0.019822614267468452\n",
      "Loss on test= 0.023513352498412132\n",
      "acc for Lsat= 0.0867489260791544 \n",
      "acc for Psat= 0.11995084006524959 \n",
      "acc for optim= 0.15087838467965495\n",
      "Epoch:266/1000\n",
      "Loss on train= 0.020228181034326553\n",
      "Loss on test= 0.024296103045344353\n",
      "acc for Lsat= 0.09102718820207541 \n",
      "acc for Psat= 0.13143403767747341 \n",
      "acc for optim= 0.15267945450801793\n",
      "Epoch:267/1000\n",
      "Loss on train= 0.02011987753212452\n",
      "Loss on test= 0.02329549752175808\n",
      "acc for Lsat= 0.09641304073539685 \n",
      "acc for Psat= 0.13135353051546803 \n",
      "acc for optim= 0.15466159487087466\n",
      "Epoch:268/1000\n",
      "Loss on train= 0.021107297390699387\n",
      "Loss on test= 0.023355746641755104\n",
      "acc for Lsat= 0.08614931813506191 \n",
      "acc for Psat= 0.1172047121976301 \n",
      "acc for optim= 0.15431448169720924\n",
      "Epoch:269/1000\n",
      "Loss on train= 0.020864833146333694\n",
      "Loss on test= 0.02422296442091465\n",
      "acc for Lsat= 0.08352968316537594 \n",
      "acc for Psat= 0.11876107093899747 \n",
      "acc for optim= 0.1539042495611894\n",
      "Epoch:270/1000\n",
      "Loss on train= 0.01953260228037834\n",
      "Loss on test= 0.02420654334127903\n",
      "acc for Lsat= 0.08986695470207949 \n",
      "acc for Psat= 0.12228092111622375 \n",
      "acc for optim= 0.15106046938420922\n",
      "Epoch:271/1000\n",
      "Loss on train= 0.020745275542140007\n",
      "Loss on test= 0.02316276915371418\n",
      "acc for Lsat= 0.09306406739146213 \n",
      "acc for Psat= 0.1238701573836051 \n",
      "acc for optim= 0.15329689203306684\n",
      "Epoch:272/1000\n",
      "Loss on train= 0.020427169278264046\n",
      "Loss on test= 0.023243188858032227\n",
      "acc for Lsat= 0.09522912466644846 \n",
      "acc for Psat= 0.12121934360047903 \n",
      "acc for optim= 0.1545459878207045\n",
      "Epoch:273/1000\n",
      "Loss on train= 0.01920759491622448\n",
      "Loss on test= 0.023199941962957382\n",
      "acc for Lsat= 0.08516624686725907 \n",
      "acc for Psat= 0.11532577284150737 \n",
      "acc for optim= 0.1525156300725335\n",
      "Epoch:274/1000\n",
      "Loss on train= 0.020184414461255074\n",
      "Loss on test= 0.023749418556690216\n",
      "acc for Lsat= 0.0832950216393138 \n",
      "acc for Psat= 0.1212096839054082 \n",
      "acc for optim= 0.15151346266467708\n",
      "Epoch:275/1000\n",
      "Loss on train= 0.02017737738788128\n",
      "Loss on test= 0.022632893174886703\n",
      "acc for Lsat= 0.08546724311537121 \n",
      "acc for Psat= 0.12388377716374954 \n",
      "acc for optim= 0.1528018508440633\n",
      "Epoch:276/1000\n",
      "Loss on train= 0.01980818621814251\n",
      "Loss on test= 0.023524034768342972\n",
      "acc for Lsat= 0.10945841832414419 \n",
      "acc for Psat= 0.13794763048621905 \n",
      "acc for optim= 0.1517523127734859\n",
      "Epoch:277/1000\n",
      "Loss on train= 0.0203776266425848\n",
      "Loss on test= 0.023722374811768532\n",
      "acc for Lsat= 0.0951185559513561 \n",
      "acc for Psat= 0.12700957630163806 \n",
      "acc for optim= 0.15260506012907057\n",
      "Epoch:278/1000\n",
      "Loss on train= 0.019453540444374084\n",
      "Loss on test= 0.0238993838429451\n",
      "acc for Lsat= 0.08271868824958803 \n",
      "acc for Psat= 0.11772627727533892 \n",
      "acc for optim= 0.15062210460833933\n",
      "Epoch:279/1000\n",
      "Loss on train= 0.019304092973470688\n",
      "Loss on test= 0.023270584642887115\n",
      "acc for Lsat= 0.08719135271354372 \n",
      "acc for Psat= 0.12202949523925781 \n",
      "acc for optim= 0.1511837153933769\n",
      "Epoch:280/1000\n",
      "Loss on train= 0.019497008994221687\n",
      "Loss on test= 0.023043537512421608\n",
      "acc for Lsat= 0.08717925186173067 \n",
      "acc for Psat= 0.1171513975260662 \n",
      "acc for optim= 0.14940838271201254\n",
      "Epoch:281/1000\n",
      "Loss on train= 0.019794808700680733\n",
      "Loss on test= 0.02344588376581669\n",
      "acc for Lsat= 0.08428895998238727 \n",
      "acc for Psat= 0.12070198391759117 \n",
      "acc for optim= 0.1503644294913029\n",
      "Epoch:282/1000\n",
      "Loss on train= 0.019008098170161247\n",
      "Loss on test= 0.02336862124502659\n",
      "acc for Lsat= 0.08047422861736082 \n",
      "acc for Psat= 0.12226012485763954 \n",
      "acc for optim= 0.15036722851354023\n",
      "Epoch:283/1000\n",
      "Loss on train= 0.019799474626779556\n",
      "Loss on test= 0.023194868117570877\n",
      "acc for Lsat= 0.09636524194894834 \n",
      "acc for Psat= 0.12062477495187142 \n",
      "acc for optim= 0.15145480298124672\n",
      "Epoch:284/1000\n",
      "Loss on train= 0.019676385447382927\n",
      "Loss on test= 0.02349996380507946\n",
      "acc for Lsat= 0.09905933559534956 \n",
      "acc for Psat= 0.12138615593165655 \n",
      "acc for optim= 0.15349508345721175\n",
      "Epoch:285/1000\n",
      "Loss on train= 0.020553363487124443\n",
      "Loss on test= 0.02240481972694397\n",
      "acc for Lsat= 0.08413632105157225 \n",
      "acc for Psat= 0.11668475286113067 \n",
      "acc for optim= 0.15175409059587908\n",
      "Epoch:286/1000\n",
      "Loss on train= 0.0195379089564085\n",
      "Loss on test= 0.023284556344151497\n",
      "acc for Lsat= 0.10217491400202246 \n",
      "acc for Psat= 0.14061966603776546 \n",
      "acc for optim= 0.15396056369293573\n",
      "Epoch:287/1000\n",
      "Loss on train= 0.019293280318379402\n",
      "Loss on test= 0.023829007521271706\n",
      "acc for Lsat= 0.08930459438368334 \n",
      "acc for Psat= 0.13699210460399863 \n",
      "acc for optim= 0.15110160704466988\n",
      "Epoch:288/1000\n",
      "Loss on train= 0.019530585035681725\n",
      "Loss on test= 0.023220954462885857\n",
      "acc for Lsat= 0.08635189823335984 \n",
      "acc for Psat= 0.12152727699755037 \n",
      "acc for optim= 0.14846210224287848\n",
      "Epoch:289/1000\n",
      "Loss on train= 0.019482243806123734\n",
      "Loss on test= 0.02272423729300499\n",
      "acc for Lsat= 0.085800140036697 \n",
      "acc for Psat= 0.12287271066361487 \n",
      "acc for optim= 0.14945744143371964\n",
      "Epoch:290/1000\n",
      "Loss on train= 0.020235756412148476\n",
      "Loss on test= 0.022535962983965874\n",
      "acc for Lsat= 0.0858318195885598 \n",
      "acc for Psat= 0.12110045665126304 \n",
      "acc for optim= 0.15188493481110105\n",
      "Epoch:291/1000\n",
      "Loss on train= 0.01870107278227806\n",
      "Loss on test= 0.023159701377153397\n",
      "acc for Lsat= 0.08314826286512353 \n",
      "acc for Psat= 0.11174103255287753 \n",
      "acc for optim= 0.1489585007147931\n",
      "Epoch:292/1000\n",
      "Loss on train= 0.0199293065816164\n",
      "Loss on test= 0.022864043712615967\n",
      "acc for Lsat= 0.07757027871981015 \n",
      "acc for Psat= 0.1215540153243613 \n",
      "acc for optim= 0.15284278771013918\n",
      "Epoch:293/1000\n",
      "Loss on train= 0.019870499148964882\n",
      "Loss on test= 0.024219956248998642\n",
      "acc for Lsat= 0.10202809900936495 \n",
      "acc for Psat= 0.13396304655708743 \n",
      "acc for optim= 0.15176971041878992\n",
      "Epoch:294/1000\n",
      "Loss on train= 0.020124327391386032\n",
      "Loss on test= 0.02328668162226677\n",
      "acc for Lsat= 0.09359009063917142 \n",
      "acc for Psat= 0.12082871976880932 \n",
      "acc for optim= 0.1509864639206186\n",
      "Epoch:295/1000\n",
      "Loss on train= 0.018747860565781593\n",
      "Loss on test= 0.023110127076506615\n",
      "acc for Lsat= 0.08216443208364951 \n",
      "acc for Psat= 0.12742013054036622 \n",
      "acc for optim= 0.15335179068321408\n",
      "Epoch:296/1000\n",
      "Loss on train= 0.018657328560948372\n",
      "Loss on test= 0.023709552362561226\n",
      "acc for Lsat= 0.08213124961354011 \n",
      "acc for Psat= 0.11974209513379092 \n",
      "acc for optim= 0.15223718782596016\n",
      "Epoch:297/1000\n",
      "Loss on train= 0.01973402500152588\n",
      "Loss on test= 0.02362941950559616\n",
      "acc for Lsat= 0.08243010255981521 \n",
      "acc for Psat= 0.11868495402542063 \n",
      "acc for optim= 0.15089388129323028\n",
      "Epoch:298/1000\n",
      "Loss on train= 0.018654776737093925\n",
      "Loss on test= 0.02308986894786358\n",
      "acc for Lsat= 0.08513169662896974 \n",
      "acc for Psat= 0.12470727852016589 \n",
      "acc for optim= 0.15097500806631048\n",
      "Epoch:299/1000\n",
      "Loss on train= 0.019561465829610825\n",
      "Loss on test= 0.023420339450240135\n",
      "acc for Lsat= 0.08408776696932274 \n",
      "acc for Psat= 0.11748043310602635 \n",
      "acc for optim= 0.14950897521354825\n",
      "Epoch:300/1000\n",
      "Loss on train= 0.018932659178972244\n",
      "Loss on test= 0.022400734946131706\n",
      "acc for Lsat= 0.07963213774056925 \n",
      "acc for Psat= 0.12163343209760927 \n",
      "acc for optim= 0.15483578073225937\n",
      "Epoch:301/1000\n",
      "Loss on train= 0.018634308129549026\n",
      "Loss on test= 0.022982876747846603\n",
      "acc for Lsat= 0.0889566319725442 \n",
      "acc for Psat= 0.11934047648280958 \n",
      "acc for optim= 0.1492766747244965\n",
      "Epoch:302/1000\n",
      "Loss on train= 0.019398583099246025\n",
      "Loss on test= 0.02368251606822014\n",
      "acc for Lsat= 0.09010840808830388 \n",
      "acc for Psat= 0.13590739759495882 \n",
      "acc for optim= 0.15018268572135623\n",
      "Epoch:303/1000\n",
      "Loss on train= 0.019228797405958176\n",
      "Loss on test= 0.024022268131375313\n",
      "acc for Lsat= 0.08790602754318834 \n",
      "acc for Psat= 0.12867086836665967 \n",
      "acc for optim= 0.15005485106544242\n",
      "Epoch:304/1000\n",
      "Loss on train= 0.019597485661506653\n",
      "Loss on test= 0.02424079366028309\n",
      "acc for Lsat= 0.0936270581131362 \n",
      "acc for Psat= 0.1316099363506039 \n",
      "acc for optim= 0.15267669204065568\n",
      "Epoch:305/1000\n",
      "Loss on train= 0.01851530373096466\n",
      "Loss on test= 0.023448137566447258\n",
      "acc for Lsat= 0.0884434399216674 \n",
      "acc for Psat= 0.13415284055808055 \n",
      "acc for optim= 0.1539426514278615\n",
      "Epoch:306/1000\n",
      "Loss on train= 0.019968627020716667\n",
      "Loss on test= 0.023040451109409332\n",
      "acc for Lsat= 0.0862866096718367 \n",
      "acc for Psat= 0.1220544606150187 \n",
      "acc for optim= 0.14937229291149148\n",
      "Epoch:307/1000\n",
      "Loss on train= 0.019018223509192467\n",
      "Loss on test= 0.02206975221633911\n",
      "acc for Lsat= 0.08393681585986754 \n",
      "acc for Psat= 0.11177845165579026 \n",
      "acc for optim= 0.15028271225599749\n",
      "Epoch:308/1000\n",
      "Loss on train= 0.018925469368696213\n",
      "Loss on test= 0.02326187491416931\n",
      "acc for Lsat= 0.07839178238002169 \n",
      "acc for Psat= 0.11690046472406863 \n",
      "acc for optim= 0.15192825588277015\n",
      "Epoch:309/1000\n",
      "Loss on train= 0.018901612609624863\n",
      "Loss on test= 0.023353764787316322\n",
      "acc for Lsat= 0.09683966010907953 \n",
      "acc for Psat= 0.12573424744051553 \n",
      "acc for optim= 0.1477128007483244\n",
      "Epoch:310/1000\n",
      "Loss on train= 0.018842341378331184\n",
      "Loss on test= 0.022909611463546753\n",
      "acc for Lsat= 0.08228849679528677 \n",
      "acc for Psat= 0.11840957737840296 \n",
      "acc for optim= 0.15034718802601002\n",
      "Epoch:311/1000\n",
      "Loss on train= 0.018341582268476486\n",
      "Loss on test= 0.023417262360453606\n",
      "acc for Lsat= 0.08471872254067482 \n",
      "acc for Psat= 0.12038386105698999 \n",
      "acc for optim= 0.14865669623957922\n",
      "Epoch:312/1000\n",
      "Loss on train= 0.01831417717039585\n",
      "Loss on test= 0.02289087139070034\n",
      "acc for Lsat= 0.08764757826874818 \n",
      "acc for Psat= 0.11508958060480036 \n",
      "acc for optim= 0.1521441524210958\n",
      "Epoch:313/1000\n",
      "Loss on train= 0.018864965066313744\n",
      "Loss on test= 0.02255062386393547\n",
      "acc for Lsat= 0.08155273072347291 \n",
      "acc for Psat= 0.11314020826174968 \n",
      "acc for optim= 0.15160554483087355\n",
      "Epoch:314/1000\n",
      "Loss on train= 0.017935674637556076\n",
      "Loss on test= 0.02356550842523575\n",
      "acc for Lsat= 0.08935918274313905 \n",
      "acc for Psat= 0.1342476983601073 \n",
      "acc for optim= 0.15261827925114932\n",
      "Epoch:315/1000\n",
      "Loss on train= 0.019471971318125725\n",
      "Loss on test= 0.022624632343649864\n",
      "acc for Lsat= 0.08810640842811611 \n",
      "acc for Psat= 0.12057425922333598 \n",
      "acc for optim= 0.15113687269711415\n",
      "Epoch:316/1000\n",
      "Loss on train= 0.018820708617568016\n",
      "Loss on test= 0.02319614216685295\n",
      "acc for Lsat= 0.08254223597604174 \n",
      "acc for Psat= 0.11341313800938502 \n",
      "acc for optim= 0.15103899735548965\n",
      "Epoch:317/1000\n",
      "Loss on train= 0.019085833802819252\n",
      "Loss on test= 0.02282099984586239\n",
      "acc for Lsat= 0.09124330694097221 \n",
      "acc for Psat= 0.11612879283404433 \n",
      "acc for optim= 0.1532675639735504\n",
      "Epoch:318/1000\n",
      "Loss on train= 0.01896841451525688\n",
      "Loss on test= 0.022541498765349388\n",
      "acc for Lsat= 0.08305784875768366 \n",
      "acc for Psat= 0.12516415041942533 \n",
      "acc for optim= 0.15012900676838187\n",
      "Epoch:319/1000\n",
      "Loss on train= 0.01808745786547661\n",
      "Loss on test= 0.023569276556372643\n",
      "acc for Lsat= 0.08006998254017182 \n",
      "acc for Psat= 0.11314808952055895 \n",
      "acc for optim= 0.15214755024228774\n",
      "Epoch:320/1000\n",
      "Loss on train= 0.018655432388186455\n",
      "Loss on test= 0.023390905931591988\n",
      "acc for Lsat= 0.08169957859769612 \n",
      "acc for Psat= 0.11698415556619333 \n",
      "acc for optim= 0.15313167207660866\n",
      "Epoch:321/1000\n",
      "Loss on train= 0.018418703228235245\n",
      "Loss on test= 0.022553663700819016\n",
      "acc for Lsat= 0.08252351905816414 \n",
      "acc for Psat= 0.11573796700005513 \n",
      "acc for optim= 0.15215168959674644\n",
      "Epoch:322/1000\n",
      "Loss on train= 0.017838507890701294\n",
      "Loss on test= 0.021907813847064972\n",
      "acc for Lsat= 0.08035490861366755 \n",
      "acc for Psat= 0.11248858402733783 \n",
      "acc for optim= 0.1491595318745141\n",
      "Epoch:323/1000\n",
      "Loss on train= 0.018320996314287186\n",
      "Loss on test= 0.02283238247036934\n",
      "acc for Lsat= 0.08592179399788184 \n",
      "acc for Psat= 0.11976120416508164 \n",
      "acc for optim= 0.14777815409276968\n",
      "Epoch:324/1000\n",
      "Loss on train= 0.018325772136449814\n",
      "Loss on test= 0.0229884572327137\n",
      "acc for Lsat= 0.09613221972883737 \n",
      "acc for Psat= 0.12495292809318467 \n",
      "acc for optim= 0.15289787355055448\n",
      "Epoch:325/1000\n",
      "Loss on train= 0.019107386469841003\n",
      "Loss on test= 0.02311115711927414\n",
      "acc for Lsat= 0.09172161564082401 \n",
      "acc for Psat= 0.12218692290822536 \n",
      "acc for optim= 0.14908015729977045\n",
      "Epoch:326/1000\n",
      "Loss on train= 0.01892891153693199\n",
      "Loss on test= 0.023602696135640144\n",
      "acc for Lsat= 0.0964994854309234 \n",
      "acc for Psat= 0.1374852889399988 \n",
      "acc for optim= 0.15045427321595609\n",
      "Epoch:327/1000\n",
      "Loss on train= 0.018865246325731277\n",
      "Loss on test= 0.02287789061665535\n",
      "acc for Lsat= 0.08871528787470338 \n",
      "acc for Psat= 0.11043169945973495 \n",
      "acc for optim= 0.15210105162918366\n",
      "Epoch:328/1000\n",
      "Loss on train= 0.01832948811352253\n",
      "Loss on test= 0.02303706295788288\n",
      "acc for Lsat= 0.07796757816278262 \n",
      "acc for Psat= 0.1211930820316175 \n",
      "acc for optim= 0.1555497369496925\n",
      "Epoch:329/1000\n",
      "Loss on train= 0.01810634136199951\n",
      "Loss on test= 0.022632980719208717\n",
      "acc for Lsat= 0.07803935840280349 \n",
      "acc for Psat= 0.11134014480137747 \n",
      "acc for optim= 0.149775746435026\n",
      "Epoch:330/1000\n",
      "Loss on train= 0.01794966496527195\n",
      "Loss on test= 0.021870015189051628\n",
      "acc for Lsat= 0.07816303410007315 \n",
      "acc for Psat= 0.10941180042254171 \n",
      "acc for optim= 0.14811324520760596\n",
      "Epoch:331/1000\n",
      "Loss on train= 0.018097931519150734\n",
      "Loss on test= 0.02187078446149826\n",
      "acc for Lsat= 0.07770456563792753 \n",
      "acc for Psat= 0.1149423173099657 \n",
      "acc for optim= 0.14967830052407474\n",
      "Epoch:332/1000\n",
      "Loss on train= 0.018612181767821312\n",
      "Loss on test= 0.022173108533024788\n",
      "acc for Lsat= 0.08098827520280188 \n",
      "acc for Psat= 0.10873545544884133 \n",
      "acc for optim= 0.1503042037898916\n",
      "Epoch:333/1000\n",
      "Loss on train= 0.018303781747817993\n",
      "Loss on test= 0.023150110617280006\n",
      "acc for Lsat= 0.08084745341757209 \n",
      "acc for Psat= 0.12021078897076984 \n",
      "acc for optim= 0.15276467978360248\n",
      "Epoch:334/1000\n",
      "Loss on train= 0.017832638695836067\n",
      "Loss on test= 0.02236860804259777\n",
      "acc for Lsat= 0.07951792282519547 \n",
      "acc for Psat= 0.11441651281724344 \n",
      "acc for optim= 0.1496670783556181\n",
      "Epoch:335/1000\n",
      "Loss on train= 0.019122948870062828\n",
      "Loss on test= 0.023201748728752136\n",
      "acc for Lsat= 0.08270396447656958 \n",
      "acc for Psat= 0.1162597786350504 \n",
      "acc for optim= 0.1497628100488669\n",
      "Epoch:336/1000\n",
      "Loss on train= 0.018861981108784676\n",
      "Loss on test= 0.02182045392692089\n",
      "acc for Lsat= 0.08179535160825101 \n",
      "acc for Psat= 0.12113391164925408 \n",
      "acc for optim= 0.15115822884331512\n",
      "Epoch:337/1000\n",
      "Loss on train= 0.017697090283036232\n",
      "Loss on test= 0.023033030331134796\n",
      "acc for Lsat= 0.08076650378513972 \n",
      "acc for Psat= 0.11828340307818692 \n",
      "acc for optim= 0.15219533861277504\n",
      "Epoch:338/1000\n",
      "Loss on train= 0.01854032836854458\n",
      "Loss on test= 0.022111982107162476\n",
      "acc for Lsat= 0.08155050625238704 \n",
      "acc for Psat= 0.1220319328118004 \n",
      "acc for optim= 0.1515248990534152\n",
      "Epoch:339/1000\n",
      "Loss on train= 0.018679814413189888\n",
      "Loss on test= 0.022212687879800797\n",
      "acc for Lsat= 0.10384027403454449 \n",
      "acc for Psat= 0.1287823513298732 \n",
      "acc for optim= 0.1514985322358204\n",
      "Epoch:340/1000\n",
      "Loss on train= 0.01824803836643696\n",
      "Loss on test= 0.022763829678297043\n",
      "acc for Lsat= 0.07889026964819708 \n",
      "acc for Psat= 0.11598566206190669 \n",
      "acc for optim= 0.15139279361579108\n",
      "Epoch:341/1000\n",
      "Loss on train= 0.017520004883408546\n",
      "Loss on test= 0.022967811673879623\n",
      "acc for Lsat= 0.0764781717248137 \n",
      "acc for Psat= 0.11052181815784241 \n",
      "acc for optim= 0.14869706820807976\n",
      "Epoch:342/1000\n",
      "Loss on train= 0.01833144947886467\n",
      "Loss on test= 0.021199621260166168\n",
      "acc for Lsat= 0.09410451876364676 \n",
      "acc for Psat= 0.11237224615689524 \n",
      "acc for optim= 0.14991920628024893\n",
      "Epoch:343/1000\n",
      "Loss on train= 0.018381714820861816\n",
      "Loss on test= 0.022185273468494415\n",
      "acc for Lsat= 0.07667059695403836 \n",
      "acc for Psat= 0.116590535779332 \n",
      "acc for optim= 0.15226850262115965\n",
      "Epoch:344/1000\n",
      "Loss on train= 0.01831035129725933\n",
      "Loss on test= 0.022939564660191536\n",
      "acc for Lsat= 0.0807255462554602 \n",
      "acc for Psat= 0.1109089292561097 \n",
      "acc for optim= 0.14934320148835548\n",
      "Epoch:345/1000\n",
      "Loss on train= 0.01758037693798542\n",
      "Loss on test= 0.02325916849076748\n",
      "acc for Lsat= 0.08269488447132299 \n",
      "acc for Psat= 0.11581196721606081 \n",
      "acc for optim= 0.15223064184980928\n",
      "Epoch:346/1000\n",
      "Loss on train= 0.01853497326374054\n",
      "Loss on test= 0.023131156340241432\n",
      "acc for Lsat= 0.07903359293739663 \n",
      "acc for Psat= 0.11877828527526602 \n",
      "acc for optim= 0.15084766101995573\n",
      "Epoch:347/1000\n",
      "Loss on train= 0.0175448190420866\n",
      "Loss on test= 0.02306782454252243\n",
      "acc for Lsat= 0.08099089574774236 \n",
      "acc for Psat= 0.10771218918486687 \n",
      "acc for optim= 0.15141926576132794\n",
      "Epoch:348/1000\n",
      "Loss on train= 0.01799103245139122\n",
      "Loss on test= 0.02255425788462162\n",
      "acc for Lsat= 0.08226045781987844 \n",
      "acc for Psat= 0.11441167670230927 \n",
      "acc for optim= 0.1488986744833151\n",
      "Epoch:349/1000\n",
      "Loss on train= 0.017679233103990555\n",
      "Loss on test= 0.02248501405119896\n",
      "acc for Lsat= 0.09131612343051504 \n",
      "acc for Psat= 0.1378194031723314 \n",
      "acc for optim= 0.14974121516725156\n",
      "Epoch:350/1000\n",
      "Loss on train= 0.018451271578669548\n",
      "Loss on test= 0.02307676151394844\n",
      "acc for Lsat= 0.07790937737571042 \n",
      "acc for Psat= 0.11863646517164286 \n",
      "acc for optim= 0.15041181371853599\n",
      "Epoch:351/1000\n",
      "Loss on train= 0.01937561295926571\n",
      "Loss on test= 0.022608136758208275\n",
      "acc for Lsat= 0.07751561751793389 \n",
      "acc for Psat= 0.11143873587399226 \n",
      "acc for optim= 0.14794724280652025\n",
      "Epoch:352/1000\n",
      "Loss on train= 0.01743863709270954\n",
      "Loss on test= 0.023334147408604622\n",
      "acc for Lsat= 0.0885134094280262 \n",
      "acc for Psat= 0.11583699164200462 \n",
      "acc for optim= 0.15292635680829172\n",
      "Epoch:353/1000\n",
      "Loss on train= 0.01827661506831646\n",
      "Loss on test= 0.022904761135578156\n",
      "acc for Lsat= 0.07872221264926303 \n",
      "acc for Psat= 0.11869072541842032 \n",
      "acc for optim= 0.1485319310248492\n",
      "Epoch:354/1000\n",
      "Loss on train= 0.01735268160700798\n",
      "Loss on test= 0.021512310951948166\n",
      "acc for Lsat= 0.08309591176897979 \n",
      "acc for Psat= 0.11230739671922602 \n",
      "acc for optim= 0.15064337130796865\n",
      "Epoch:355/1000\n",
      "Loss on train= 0.018241116777062416\n",
      "Loss on test= 0.022363362833857536\n",
      "acc for Lsat= 0.07936857893221402 \n",
      "acc for Psat= 0.11258456874329383 \n",
      "acc for optim= 0.14906520649444224\n",
      "Epoch:356/1000\n",
      "Loss on train= 0.01757669448852539\n",
      "Loss on test= 0.02230612188577652\n",
      "acc for Lsat= 0.0798417390184941 \n",
      "acc for Psat= 0.10851138038096632 \n",
      "acc for optim= 0.15001281986996975\n",
      "Epoch:357/1000\n",
      "Loss on train= 0.018199797719717026\n",
      "Loss on test= 0.021825702860951424\n",
      "acc for Lsat= 0.08260492600277809 \n",
      "acc for Psat= 0.11553680785866671 \n",
      "acc for optim= 0.15088618514545735\n",
      "Epoch:358/1000\n",
      "Loss on train= 0.0177801251411438\n",
      "Loss on test= 0.02220040187239647\n",
      "acc for Lsat= 0.07784202749348955 \n",
      "acc for Psat= 0.11332203753762864 \n",
      "acc for optim= 0.14865098102544236\n",
      "Epoch:359/1000\n",
      "Loss on train= 0.017868202179670334\n",
      "Loss on test= 0.02280319668352604\n",
      "acc for Lsat= 0.08478765042121225 \n",
      "acc for Psat= 0.11398670964462815 \n",
      "acc for optim= 0.14948501454239263\n",
      "Epoch:360/1000\n",
      "Loss on train= 0.0184915941208601\n",
      "Loss on test= 0.02258627861738205\n",
      "acc for Lsat= 0.0789838802675868 \n",
      "acc for Psat= 0.10743974706659287 \n",
      "acc for optim= 0.1526737415711349\n",
      "Epoch:361/1000\n",
      "Loss on train= 0.01852753385901451\n",
      "Loss on test= 0.02210933156311512\n",
      "acc for Lsat= 0.08773246338201122 \n",
      "acc for Psat= 0.12989105281243687 \n",
      "acc for optim= 0.15020220877720272\n",
      "Epoch:362/1000\n",
      "Loss on train= 0.018354879692196846\n",
      "Loss on test= 0.022635918110609055\n",
      "acc for Lsat= 0.07970693134192217 \n",
      "acc for Psat= 0.11022417604725225 \n",
      "acc for optim= 0.14872005938295507\n",
      "Epoch:363/1000\n",
      "Loss on train= 0.018513651564717293\n",
      "Loss on test= 0.022106226533651352\n",
      "acc for Lsat= 0.075274828016956 \n",
      "acc for Psat= 0.11611499982418808 \n",
      "acc for optim= 0.1502186463916817\n",
      "Epoch:364/1000\n",
      "Loss on train= 0.017549987882375717\n",
      "Loss on test= 0.023500517010688782\n",
      "acc for Lsat= 0.07889393731407152 \n",
      "acc for Psat= 0.12148987332055734 \n",
      "acc for optim= 0.15082823796525743\n",
      "Epoch:365/1000\n",
      "Loss on train= 0.01853545382618904\n",
      "Loss on test= 0.022341633215546608\n",
      "acc for Lsat= 0.08164011058815297 \n",
      "acc for Psat= 0.10865597087283467 \n",
      "acc for optim= 0.15205133717717526\n",
      "Epoch:366/1000\n",
      "Loss on train= 0.017401916906237602\n",
      "Loss on test= 0.023163365200161934\n",
      "acc for Lsat= 0.07789218454463932 \n",
      "acc for Psat= 0.11929080125897427 \n",
      "acc for optim= 0.15237764922091332\n",
      "Epoch:367/1000\n",
      "Loss on train= 0.017468851059675217\n",
      "Loss on test= 0.02175484597682953\n",
      "acc for Lsat= 0.08333197600421717 \n",
      "acc for Psat= 0.10986320299167569 \n",
      "acc for optim= 0.15145793507661534\n",
      "Epoch:368/1000\n",
      "Loss on train= 0.01713215745985508\n",
      "Loss on test= 0.022584078833460808\n",
      "acc for Lsat= 0.07911655140477557 \n",
      "acc for Psat= 0.10601929828178051 \n",
      "acc for optim= 0.14736552388961133\n",
      "Epoch:369/1000\n",
      "Loss on train= 0.016784030944108963\n",
      "Loss on test= 0.022517478093504906\n",
      "acc for Lsat= 0.07791759179676096 \n",
      "acc for Psat= 0.11346205514530804 \n",
      "acc for optim= 0.1510265638661939\n",
      "Epoch:370/1000\n",
      "Loss on train= 0.017820272594690323\n",
      "Loss on test= 0.022650104016065598\n",
      "acc for Lsat= 0.08602318389471186 \n",
      "acc for Psat= 0.11018657519967845 \n",
      "acc for optim= 0.15005770317739825\n",
      "Epoch:371/1000\n",
      "Loss on train= 0.017874272540211678\n",
      "Loss on test= 0.0228801891207695\n",
      "acc for Lsat= 0.08450382244943382 \n",
      "acc for Psat= 0.10795370797778284 \n",
      "acc for optim= 0.14924964748347713\n",
      "Epoch:372/1000\n",
      "Loss on train= 0.017914460971951485\n",
      "Loss on test= 0.022062567993998528\n",
      "acc for Lsat= 0.07377934641022223 \n",
      "acc for Psat= 0.10658366717769459 \n",
      "acc for optim= 0.15023422175863657\n",
      "Epoch:373/1000\n",
      "Loss on train= 0.01795058138668537\n",
      "Loss on test= 0.022097015753388405\n",
      "acc for Lsat= 0.0876544439119358 \n",
      "acc for Psat= 0.11388969302573475 \n",
      "acc for optim= 0.14888152991022383\n",
      "Epoch:374/1000\n",
      "Loss on train= 0.017364580184221268\n",
      "Loss on test= 0.022388340905308723\n",
      "acc for Lsat= 0.07470129655049094 \n",
      "acc for Psat= 0.10584734577277174 \n",
      "acc for optim= 0.148640036602749\n",
      "Epoch:375/1000\n",
      "Loss on train= 0.01760505884885788\n",
      "Loss on test= 0.02250131405889988\n",
      "acc for Lsat= 0.07432755392255183 \n",
      "acc for Psat= 0.10634715066003636 \n",
      "acc for optim= 0.1532470857780241\n",
      "Epoch:376/1000\n",
      "Loss on train= 0.016715828329324722\n",
      "Loss on test= 0.022691717371344566\n",
      "acc for Lsat= 0.08655429704244746 \n",
      "acc for Psat= 0.10926371662323656 \n",
      "acc for optim= 0.14877678749965273\n",
      "Epoch:377/1000\n",
      "Loss on train= 0.0178789384663105\n",
      "Loss on test= 0.022240517660975456\n",
      "acc for Lsat= 0.07909053336346267 \n",
      "acc for Psat= 0.10609328956699055 \n",
      "acc for optim= 0.14972035217522775\n",
      "Epoch:378/1000\n",
      "Loss on train= 0.01740623451769352\n",
      "Loss on test= 0.022556215524673462\n",
      "acc for Lsat= 0.07626796543994217 \n",
      "acc for Psat= 0.10890316998839772 \n",
      "acc for optim= 0.15127121671489704\n",
      "Epoch:379/1000\n",
      "Loss on train= 0.01718275062739849\n",
      "Loss on test= 0.021887730807065964\n",
      "acc for Lsat= 0.08698106391287325 \n",
      "acc for Psat= 0.11330569008260072 \n",
      "acc for optim= 0.14723382039719643\n",
      "Epoch:380/1000\n",
      "Loss on train= 0.01765989325940609\n",
      "Loss on test= 0.02278883568942547\n",
      "acc for Lsat= 0.08299063274828695 \n",
      "acc for Psat= 0.10627302739311294 \n",
      "acc for optim= 0.14900511334900837\n",
      "Epoch:381/1000\n",
      "Loss on train= 0.016903754323720932\n",
      "Loss on test= 0.02249850332736969\n",
      "acc for Lsat= 0.08296249304894988 \n",
      "acc for Psat= 0.11611050989144664 \n",
      "acc for optim= 0.14709787546994282\n",
      "Epoch:382/1000\n",
      "Loss on train= 0.017305776476860046\n",
      "Loss on test= 0.022044988349080086\n",
      "acc for Lsat= 0.07424852675773773 \n",
      "acc for Psat= 0.10683301717934023 \n",
      "acc for optim= 0.14876077785048378\n",
      "Epoch:383/1000\n",
      "Loss on train= 0.01656484231352806\n",
      "Loss on test= 0.022056952118873596\n",
      "acc for Lsat= 0.08538602172734332 \n",
      "acc for Psat= 0.10987085723797745 \n",
      "acc for optim= 0.14820778797631248\n",
      "Epoch:384/1000\n",
      "Loss on train= 0.017552290111780167\n",
      "Loss on test= 0.022715603932738304\n",
      "acc for Lsat= 0.08451458942454518 \n",
      "acc for Psat= 0.11032559689493271 \n",
      "acc for optim= 0.1475897353352898\n",
      "Epoch:385/1000\n",
      "Loss on train= 0.017182722687721252\n",
      "Loss on test= 0.02207307703793049\n",
      "acc for Lsat= 0.07578167732173818 \n",
      "acc for Psat= 0.10680828914293815 \n",
      "acc for optim= 0.1507278268321408\n",
      "Epoch:386/1000\n",
      "Loss on train= 0.016624297946691513\n",
      "Loss on test= 0.021641826257109642\n",
      "acc for Lsat= 0.07969819207326122 \n",
      "acc for Psat= 0.11811901717090924 \n",
      "acc for optim= 0.1483080688109033\n",
      "Epoch:387/1000\n",
      "Loss on train= 0.01739659532904625\n",
      "Loss on test= 0.022666262462735176\n",
      "acc for Lsat= 0.07711611956456964 \n",
      "acc for Psat= 0.11500671262360887 \n",
      "acc for optim= 0.15183752541526208\n",
      "Epoch:388/1000\n",
      "Loss on train= 0.017138676717877388\n",
      "Loss on test= 0.022001327946782112\n",
      "acc for Lsat= 0.0820191973467602 \n",
      "acc for Psat= 0.11175465341224225 \n",
      "acc for optim= 0.14863742643416525\n",
      "Epoch:389/1000\n",
      "Loss on train= 0.01692862994968891\n",
      "Loss on test= 0.022888150066137314\n",
      "acc for Lsat= 0.08186950836070749 \n",
      "acc for Psat= 0.11057435654326528 \n",
      "acc for optim= 0.14826090007129303\n",
      "Epoch:390/1000\n",
      "Loss on train= 0.016846099868416786\n",
      "Loss on test= 0.02178398333489895\n",
      "acc for Lsat= 0.08200565804278732 \n",
      "acc for Psat= 0.10999849072326459 \n",
      "acc for optim= 0.1502674275062409\n",
      "Epoch:391/1000\n",
      "Loss on train= 0.017243865877389908\n",
      "Loss on test= 0.021629245951771736\n",
      "acc for Lsat= 0.07424599636234713 \n",
      "acc for Psat= 0.10560571074089732 \n",
      "acc for optim= 0.14825149942870156\n",
      "Epoch:392/1000\n",
      "Loss on train= 0.017680488526821136\n",
      "Loss on test= 0.02149335853755474\n",
      "acc for Lsat= 0.08038674723666371 \n",
      "acc for Psat= 0.10488636632298315 \n",
      "acc for optim= 0.1476892733098661\n",
      "Epoch:393/1000\n",
      "Loss on train= 0.01688311994075775\n",
      "Loss on test= 0.022259337827563286\n",
      "acc for Lsat= 0.07799170228135942 \n",
      "acc for Psat= 0.1059861643864863 \n",
      "acc for optim= 0.1477463464206239\n",
      "Epoch:394/1000\n",
      "Loss on train= 0.01675995998084545\n",
      "Loss on test= 0.02115318365395069\n",
      "acc for Lsat= 0.07396879371416532 \n",
      "acc for Psat= 0.10448027822464408 \n",
      "acc for optim= 0.1507529121696751\n",
      "Epoch:395/1000\n",
      "Loss on train= 0.017376702278852463\n",
      "Loss on test= 0.021968374028801918\n",
      "acc for Lsat= 0.08113906945897101 \n",
      "acc for Psat= 0.11616512465714617 \n",
      "acc for optim= 0.14764969830101118\n",
      "Epoch:396/1000\n",
      "Loss on train= 0.016659099608659744\n",
      "Loss on test= 0.02179216407239437\n",
      "acc for Lsat= 0.0824485894750519 \n",
      "acc for Psat= 0.11480955600342484 \n",
      "acc for optim= 0.14725089984082704\n",
      "Epoch:397/1000\n",
      "Loss on train= 0.017179293558001518\n",
      "Loss on test= 0.022162379696965218\n",
      "acc for Lsat= 0.0787752777437039 \n",
      "acc for Psat= 0.10638465167478071 \n",
      "acc for optim= 0.14606090562288154\n",
      "Epoch:398/1000\n",
      "Loss on train= 0.01712505705654621\n",
      "Loss on test= 0.021915996447205544\n",
      "acc for Lsat= 0.0793791609745089 \n",
      "acc for Psat= 0.1104892653484281 \n",
      "acc for optim= 0.14900827457342433\n",
      "Epoch:399/1000\n",
      "Loss on train= 0.017169708386063576\n",
      "Loss on test= 0.021477168425917625\n",
      "acc for Lsat= 0.07974011875268235 \n",
      "acc for Psat= 0.10514187505870956 \n",
      "acc for optim= 0.1479210257530212\n",
      "Epoch:400/1000\n",
      "Loss on train= 0.016437342390418053\n",
      "Loss on test= 0.02162850834429264\n",
      "acc for Lsat= 0.07738277383420948 \n",
      "acc for Psat= 0.10161488251234602 \n",
      "acc for optim= 0.14860519424229363\n",
      "Epoch:401/1000\n",
      "Loss on train= 0.017200449481606483\n",
      "Loss on test= 0.022254539653658867\n",
      "acc for Lsat= 0.07902041233061159 \n",
      "acc for Psat= 0.11846580245962177 \n",
      "acc for optim= 0.15070599489829864\n",
      "Epoch:402/1000\n",
      "Loss on train= 0.016829516738653183\n",
      "Loss on test= 0.022189268842339516\n",
      "acc for Lsat= 0.07235508678957474 \n",
      "acc for Psat= 0.10387015376376155 \n",
      "acc for optim= 0.15013876741510687\n",
      "Epoch:403/1000\n",
      "Loss on train= 0.017472920939326286\n",
      "Loss on test= 0.021437957882881165\n",
      "acc for Lsat= 0.07748047567879245 \n",
      "acc for Psat= 0.10353559699169429 \n",
      "acc for optim= 0.14789039423299388\n",
      "Epoch:404/1000\n",
      "Loss on train= 0.01700471341609955\n",
      "Loss on test= 0.0220393855124712\n",
      "acc for Lsat= 0.07320240023128219 \n",
      "acc for Psat= 0.10475989982139229 \n",
      "acc for optim= 0.14774589059360801\n",
      "Epoch:405/1000\n",
      "Loss on train= 0.016138318926095963\n",
      "Loss on test= 0.021683983504772186\n",
      "acc for Lsat= 0.09134903534306243 \n",
      "acc for Psat= 0.11287739795703824 \n",
      "acc for optim= 0.1480371428684539\n",
      "Epoch:406/1000\n",
      "Loss on train= 0.017022544518113136\n",
      "Loss on test= 0.022032596170902252\n",
      "acc for Lsat= 0.07655469929657109 \n",
      "acc for Psat= 0.11350503075756506 \n",
      "acc for optim= 0.14886428224683998\n",
      "Epoch:407/1000\n",
      "Loss on train= 0.016432346776127815\n",
      "Loss on test= 0.022030074149370193\n",
      "acc for Lsat= 0.07514289019511783 \n",
      "acc for Psat= 0.10537433296541043 \n",
      "acc for optim= 0.1504204779170281\n",
      "Epoch:408/1000\n",
      "Loss on train= 0.016115987673401833\n",
      "Loss on test= 0.022210901603102684\n",
      "acc for Lsat= 0.07564742835455163 \n",
      "acc for Psat= 0.11210257861503335 \n",
      "acc for optim= 0.14817598547254288\n",
      "Epoch:409/1000\n",
      "Loss on train= 0.016942167654633522\n",
      "Loss on test= 0.021626034751534462\n",
      "acc for Lsat= 0.07601005584496595 \n",
      "acc for Psat= 0.1037235319812432 \n",
      "acc for optim= 0.1501181945848306\n",
      "Epoch:410/1000\n",
      "Loss on train= 0.01661810278892517\n",
      "Loss on test= 0.021713994443416595\n",
      "acc for Lsat= 0.0882266507295279 \n",
      "acc for Psat= 0.11421045169877844 \n",
      "acc for optim= 0.1473728527658406\n",
      "Epoch:411/1000\n",
      "Loss on train= 0.0169510580599308\n",
      "Loss on test= 0.021603435277938843\n",
      "acc for Lsat= 0.07792808689746349 \n",
      "acc for Psat= 0.10583191597184471 \n",
      "acc for optim= 0.14899500354975959\n",
      "Epoch:412/1000\n",
      "Loss on train= 0.016716506332159042\n",
      "Loss on test= 0.02281644381582737\n",
      "acc for Lsat= 0.08002813855476948 \n",
      "acc for Psat= 0.12143437658433504 \n",
      "acc for optim= 0.14857479951706437\n",
      "Epoch:413/1000\n",
      "Loss on train= 0.016766486689448357\n",
      "Loss on test= 0.02224596217274666\n",
      "acc for Lsat= 0.07444430600567116 \n",
      "acc for Psat= 0.1068005725790892 \n",
      "acc for optim= 0.14641355987007038\n",
      "Epoch:414/1000\n",
      "Loss on train= 0.017090173438191414\n",
      "Loss on test= 0.02200956642627716\n",
      "acc for Lsat= 0.07124131589236844 \n",
      "acc for Psat= 0.1144801574688021 \n",
      "acc for optim= 0.14616993228066405\n",
      "Epoch:415/1000\n",
      "Loss on train= 0.016405178233981133\n",
      "Loss on test= 0.021775448694825172\n",
      "acc for Lsat= 0.08458209316001777 \n",
      "acc for Psat= 0.11316432655847745 \n",
      "acc for optim= 0.14809220686703428\n",
      "Epoch:416/1000\n",
      "Loss on train= 0.01697641983628273\n",
      "Loss on test= 0.02185223624110222\n",
      "acc for Lsat= 0.07647598246205288 \n",
      "acc for Psat= 0.10282886445126264 \n",
      "acc for optim= 0.147825546834952\n",
      "Epoch:417/1000\n",
      "Loss on train= 0.016093311831355095\n",
      "Loss on test= 0.021981224417686462\n",
      "acc for Lsat= 0.0789319503842002 \n",
      "acc for Psat= 0.10434043068822435 \n",
      "acc for optim= 0.14917437285679916\n",
      "Epoch:418/1000\n",
      "Loss on train= 0.017384188249707222\n",
      "Loss on test= 0.0220805574208498\n",
      "acc for Lsat= 0.0774122067167117 \n",
      "acc for Psat= 0.11023930877546136 \n",
      "acc for optim= 0.14810890016761735\n",
      "Epoch:419/1000\n",
      "Loss on train= 0.017072128131985664\n",
      "Loss on test= 0.022341573610901833\n",
      "acc for Lsat= 0.0739971337029308 \n",
      "acc for Psat= 0.11051849083250939 \n",
      "acc for optim= 0.1505735690609561\n",
      "Epoch:420/1000\n",
      "Loss on train= 0.016525352373719215\n",
      "Loss on test= 0.022060172632336617\n",
      "acc for Lsat= 0.0728299295981461 \n",
      "acc for Psat= 0.10371840206095546 \n",
      "acc for optim= 0.14859175422658952\n",
      "Epoch:421/1000\n",
      "Loss on train= 0.017049413174390793\n",
      "Loss on test= 0.022928398102521896\n",
      "acc for Lsat= 0.0938291624732984 \n",
      "acc for Psat= 0.11990662884474593 \n",
      "acc for optim= 0.15073062104244173\n",
      "Epoch:422/1000\n",
      "Loss on train= 0.016081901267170906\n",
      "Loss on test= 0.02168167382478714\n",
      "acc for Lsat= 0.08020838312928463 \n",
      "acc for Psat= 0.1063862514060201 \n",
      "acc for optim= 0.15045447836682646\n",
      "Epoch:423/1000\n",
      "Loss on train= 0.01621607318520546\n",
      "Loss on test= 0.02167130820453167\n",
      "acc for Lsat= 0.07570679319459339 \n",
      "acc for Psat= 0.10541210588426685 \n",
      "acc for optim= 0.1482177606848783\n",
      "Epoch:424/1000\n",
      "Loss on train= 0.016429204493761063\n",
      "Loss on test= 0.02209680899977684\n",
      "acc for Lsat= 0.07669362785214205 \n",
      "acc for Psat= 0.10986228529797042 \n",
      "acc for optim= 0.14945310629483474\n",
      "Epoch:425/1000\n",
      "Loss on train= 0.016193289309740067\n",
      "Loss on test= 0.02281929925084114\n",
      "acc for Lsat= 0.0720152860761085 \n",
      "acc for Psat= 0.1009916608317746 \n",
      "acc for optim= 0.15052022179495853\n",
      "Epoch:426/1000\n",
      "Loss on train= 0.01669752411544323\n",
      "Loss on test= 0.023051796481013298\n",
      "acc for Lsat= 0.0960673278154329 \n",
      "acc for Psat= 0.1124181039507603 \n",
      "acc for optim= 0.15136201258909668\n",
      "Epoch:427/1000\n",
      "Loss on train= 0.017522474750876427\n",
      "Loss on test= 0.02203662507236004\n",
      "acc for Lsat= 0.07433480044139976 \n",
      "acc for Psat= 0.1003826416410085 \n",
      "acc for optim= 0.15016153114182612\n",
      "Epoch:428/1000\n",
      "Loss on train= 0.01652711071074009\n",
      "Loss on test= 0.0225687138736248\n",
      "acc for Lsat= 0.07765372544032 \n",
      "acc for Psat= 0.10844133987379236 \n",
      "acc for optim= 0.1512147442645012\n",
      "Epoch:429/1000\n",
      "Loss on train= 0.01596567966043949\n",
      "Loss on test= 0.022115016356110573\n",
      "acc for Lsat= 0.06985051543807666 \n",
      "acc for Psat= 0.10115548217811457 \n",
      "acc for optim= 0.1489015344369451\n",
      "Epoch:430/1000\n",
      "Loss on train= 0.01672213152050972\n",
      "Loss on test= 0.02232499234378338\n",
      "acc for Lsat= 0.07318836755530778 \n",
      "acc for Psat= 0.10582521070673616 \n",
      "acc for optim= 0.15052890805310984\n",
      "Epoch:431/1000\n",
      "Loss on train= 0.01665499433875084\n",
      "Loss on test= 0.02215314470231533\n",
      "acc for Lsat= 0.07494975545279607 \n",
      "acc for Psat= 0.11030692842315597 \n",
      "acc for optim= 0.1546198146881852\n",
      "Epoch:432/1000\n",
      "Loss on train= 0.016458401456475258\n",
      "Loss on test= 0.021426333114504814\n",
      "acc for Lsat= 0.07468819285548013 \n",
      "acc for Psat= 0.10888888572537621 \n",
      "acc for optim= 0.14959740414967965\n",
      "Epoch:433/1000\n",
      "Loss on train= 0.0161482822149992\n",
      "Loss on test= 0.02246585674583912\n",
      "acc for Lsat= 0.07584541447930955 \n",
      "acc for Psat= 0.11099529313882724 \n",
      "acc for optim= 0.149197519300784\n",
      "Epoch:434/1000\n",
      "Loss on train= 0.016358787193894386\n",
      "Loss on test= 0.021891005337238312\n",
      "acc for Lsat= 0.07890303295316094 \n",
      "acc for Psat= 0.10755417452896153 \n",
      "acc for optim= 0.15014869939053174\n",
      "Epoch:435/1000\n",
      "Loss on train= 0.0152492206543684\n",
      "Loss on test= 0.02266784943640232\n",
      "acc for Lsat= 0.0726159736068542 \n",
      "acc for Psat= 0.10699676538622663 \n",
      "acc for optim= 0.14829386722209845\n",
      "Epoch:436/1000\n",
      "Loss on train= 0.016274837777018547\n",
      "Loss on test= 0.021786384284496307\n",
      "acc for Lsat= 0.07596628382752504 \n",
      "acc for Psat= 0.10330392914356978 \n",
      "acc for optim= 0.14951027675324502\n",
      "Epoch:437/1000\n",
      "Loss on train= 0.01644163765013218\n",
      "Loss on test= 0.02281210571527481\n",
      "acc for Lsat= 0.0750060326534252 \n",
      "acc for Psat= 0.10603684148321119 \n",
      "acc for optim= 0.14818128373139722\n",
      "Epoch:438/1000\n",
      "Loss on train= 0.017111504450440407\n",
      "Loss on test= 0.02246197499334812\n",
      "acc for Lsat= 0.08757196914317995 \n",
      "acc for Psat= 0.12050353019340489 \n",
      "acc for optim= 0.14845398991210915\n",
      "Epoch:439/1000\n",
      "Loss on train= 0.015951529145240784\n",
      "Loss on test= 0.022130755707621574\n",
      "acc for Lsat= 0.07384731747779338 \n",
      "acc for Psat= 0.10965318135248861 \n",
      "acc for optim= 0.14787590596763\n",
      "Epoch:440/1000\n",
      "Loss on train= 0.01651759445667267\n",
      "Loss on test= 0.021358147263526917\n",
      "acc for Lsat= 0.07194759083348651 \n",
      "acc for Psat= 0.10590363935972763 \n",
      "acc for optim= 0.1479679133408885\n",
      "Epoch:441/1000\n",
      "Loss on train= 0.016062796115875244\n",
      "Loss on test= 0.021452365443110466\n",
      "acc for Lsat= 0.07413229101718065 \n",
      "acc for Psat= 0.10142817512105078 \n",
      "acc for optim= 0.15019050218734237\n",
      "Epoch:442/1000\n",
      "Loss on train= 0.015150166116654873\n",
      "Loss on test= 0.02240419201552868\n",
      "acc for Lsat= 0.07573379600958967 \n",
      "acc for Psat= 0.10355415363050378 \n",
      "acc for optim= 0.14850950076730543\n",
      "Epoch:443/1000\n",
      "Loss on train= 0.01654064655303955\n",
      "Loss on test= 0.021973716095089912\n",
      "acc for Lsat= 0.07802003621064943 \n",
      "acc for Psat= 0.10985576308446868 \n",
      "acc for optim= 0.14943817004412904\n",
      "Epoch:444/1000\n",
      "Loss on train= 0.01610618270933628\n",
      "Loss on test= 0.02128252573311329\n",
      "acc for Lsat= 0.07859981177851211 \n",
      "acc for Psat= 0.10386018850082576 \n",
      "acc for optim= 0.14930715230216218\n",
      "Epoch:445/1000\n",
      "Loss on train= 0.015885129570961\n",
      "Loss on test= 0.022016843780875206\n",
      "acc for Lsat= 0.07911943237844893 \n",
      "acc for Psat= 0.10339003476589621 \n",
      "acc for optim= 0.14905810356140134\n",
      "Epoch:446/1000\n",
      "Loss on train= 0.016035836189985275\n",
      "Loss on test= 0.02211890183389187\n",
      "acc for Lsat= 0.07977567495301714 \n",
      "acc for Psat= 0.10628181464252283 \n",
      "acc for optim= 0.14905653435526497\n",
      "Epoch:447/1000\n",
      "Loss on train= 0.016408836469054222\n",
      "Loss on test= 0.021739371120929718\n",
      "acc for Lsat= 0.07663868452821458 \n",
      "acc for Psat= 0.11436640507359044 \n",
      "acc for optim= 0.14707977965820668\n",
      "Epoch:448/1000\n",
      "Loss on train= 0.015924720093607903\n",
      "Loss on test= 0.022010713815689087\n",
      "acc for Lsat= 0.08075060298870569 \n",
      "acc for Psat= 0.1066831611517656 \n",
      "acc for optim= 0.14723510916446925\n",
      "Epoch:449/1000\n",
      "Loss on train= 0.015823420137166977\n",
      "Loss on test= 0.02259768359363079\n",
      "acc for Lsat= 0.0804734470836348 \n",
      "acc for Psat= 0.10869123913917036 \n",
      "acc for optim= 0.15279525609507513\n",
      "Epoch:450/1000\n",
      "Loss on train= 0.01642437092959881\n",
      "Loss on test= 0.023451663553714752\n",
      "acc for Lsat= 0.08938262230930138 \n",
      "acc for Psat= 0.11855796100293284 \n",
      "acc for optim= 0.15084236981068733\n",
      "Epoch:451/1000\n",
      "Loss on train= 0.015727033838629723\n",
      "Loss on test= 0.022514693439006805\n",
      "acc for Lsat= 0.07882591089933019 \n",
      "acc for Psat= 0.10920621491349812 \n",
      "acc for optim= 0.1505985706747568\n",
      "Epoch:452/1000\n",
      "Loss on train= 0.016119705513119698\n",
      "Loss on test= 0.02117428183555603\n",
      "acc for Lsat= 0.07317632990224023 \n",
      "acc for Psat= 0.10369547680762915 \n",
      "acc for optim= 0.1470686125002826\n",
      "Epoch:453/1000\n",
      "Loss on train= 0.015336641110479832\n",
      "Loss on test= 0.021808631718158722\n",
      "acc for Lsat= 0.08119134152450434 \n",
      "acc for Psat= 0.10709046794726604 \n",
      "acc for optim= 0.14809121465762193\n",
      "Epoch:454/1000\n",
      "Loss on train= 0.01616034284234047\n",
      "Loss on test= 0.022996848449110985\n",
      "acc for Lsat= 0.08345047576086861 \n",
      "acc for Psat= 0.11923393775458355 \n",
      "acc for optim= 0.1504491929794071\n",
      "Epoch:455/1000\n",
      "Loss on train= 0.01583595760166645\n",
      "Loss on test= 0.021342474967241287\n",
      "acc for Lsat= 0.07797825832501597 \n",
      "acc for Psat= 0.10552288792854132 \n",
      "acc for optim= 0.15047733282726072\n",
      "Epoch:456/1000\n",
      "Loss on train= 0.015374131500720978\n",
      "Loss on test= 0.021991945803165436\n",
      "acc for Lsat= 0.07285394524022987 \n",
      "acc for Psat= 0.10355068049161537 \n",
      "acc for optim= 0.15165768061761445\n",
      "Epoch:457/1000\n",
      "Loss on train= 0.01556613389402628\n",
      "Loss on test= 0.021473459899425507\n",
      "acc for Lsat= 0.0716918382137717 \n",
      "acc for Psat= 0.10167539432595336 \n",
      "acc for optim= 0.15008113697121708\n",
      "Epoch:458/1000\n",
      "Loss on train= 0.015359573997557163\n",
      "Loss on test= 0.022072717547416687\n",
      "acc for Lsat= 0.07492475466078698 \n",
      "acc for Psat= 0.10401652829591619 \n",
      "acc for optim= 0.14869028880350613\n",
      "Epoch:459/1000\n",
      "Loss on train= 0.015196462161839008\n",
      "Loss on test= 0.021616237238049507\n",
      "acc for Lsat= 0.06967465915157155 \n",
      "acc for Psat= 0.09966649588358362 \n",
      "acc for optim= 0.14881492267019333\n",
      "Epoch:460/1000\n",
      "Loss on train= 0.015827778726816177\n",
      "Loss on test= 0.021741095930337906\n",
      "acc for Lsat= 0.0803075727038209 \n",
      "acc for Psat= 0.10460888895877575 \n",
      "acc for optim= 0.146491220642958\n",
      "Epoch:461/1000\n",
      "Loss on train= 0.016087528318166733\n",
      "Loss on test= 0.02248186059296131\n",
      "acc for Lsat= 0.07173402796552032 \n",
      "acc for Psat= 0.10690701007843016 \n",
      "acc for optim= 0.14778065952747768\n",
      "Epoch:462/1000\n",
      "Loss on train= 0.015640435740351677\n",
      "Loss on test= 0.022321561351418495\n",
      "acc for Lsat= 0.07331464520324503 \n",
      "acc for Psat= 0.10593462480461084 \n",
      "acc for optim= 0.1475557524698517\n",
      "Epoch:463/1000\n",
      "Loss on train= 0.015287446789443493\n",
      "Loss on test= 0.022489264607429504\n",
      "acc for Lsat= 0.08779343513159259 \n",
      "acc for Psat= 0.10642984541151611 \n",
      "acc for optim= 0.14756573595081854\n",
      "Epoch:464/1000\n",
      "Loss on train= 0.01660121977329254\n",
      "Loss on test= 0.022507410496473312\n",
      "acc for Lsat= 0.07786724433748428 \n",
      "acc for Psat= 0.10900539296410013 \n",
      "acc for optim= 0.14612118380014286\n",
      "Epoch:465/1000\n",
      "Loss on train= 0.015432320535182953\n",
      "Loss on test= 0.02176097221672535\n",
      "acc for Lsat= 0.07401872953505216 \n",
      "acc for Psat= 0.10583573419390328 \n",
      "acc for optim= 0.14788485133766732\n",
      "Epoch:466/1000\n",
      "Loss on train= 0.015708979219198227\n",
      "Loss on test= 0.02181120589375496\n",
      "acc for Lsat= 0.0749470729764514 \n",
      "acc for Psat= 0.10208560240229099 \n",
      "acc for optim= 0.14921113646703704\n",
      "Epoch:467/1000\n",
      "Loss on train= 0.01543368212878704\n",
      "Loss on test= 0.021633625030517578\n",
      "acc for Lsat= 0.07315169487680707 \n",
      "acc for Psat= 0.10601257565013592 \n",
      "acc for optim= 0.1473707369593687\n",
      "Epoch:468/1000\n",
      "Loss on train= 0.015331038273870945\n",
      "Loss on test= 0.022169070318341255\n",
      "acc for Lsat= 0.07555584858026221 \n",
      "acc for Psat= 0.10737713115951944 \n",
      "acc for optim= 0.146559958541116\n",
      "Epoch:469/1000\n",
      "Loss on train= 0.015281901694834232\n",
      "Loss on test= 0.021769443526864052\n",
      "acc for Lsat= 0.07754322440917309 \n",
      "acc for Psat= 0.10819556449734886 \n",
      "acc for optim= 0.14852112690079647\n",
      "Epoch:470/1000\n",
      "Loss on train= 0.015520012006163597\n",
      "Loss on test= 0.021071504801511765\n",
      "acc for Lsat= 0.08362673967581652 \n",
      "acc for Psat= 0.11249368167002734 \n",
      "acc for optim= 0.14779899296570462\n",
      "Epoch:471/1000\n",
      "Loss on train= 0.015411237254738808\n",
      "Loss on test= 0.021957669407129288\n",
      "acc for Lsat= 0.07061786944684001 \n",
      "acc for Psat= 0.10176726213325295 \n",
      "acc for optim= 0.1489731942696429\n",
      "Epoch:472/1000\n",
      "Loss on train= 0.015585861168801785\n",
      "Loss on test= 0.021430104970932007\n",
      "acc for Lsat= 0.08083144745161368 \n",
      "acc for Psat= 0.10403660050658291 \n",
      "acc for optim= 0.14963350032651154\n",
      "Epoch:473/1000\n",
      "Loss on train= 0.015191251412034035\n",
      "Loss on test= 0.021582448855042458\n",
      "acc for Lsat= 0.07506886328573639 \n",
      "acc for Psat= 0.101741657552133 \n",
      "acc for optim= 0.14826772278329461\n",
      "Epoch:474/1000\n",
      "Loss on train= 0.015305991284549236\n",
      "Loss on test= 0.022317294031381607\n",
      "acc for Lsat= 0.07714554547669486 \n",
      "acc for Psat= 0.10481850225664058 \n",
      "acc for optim= 0.14840761634994584\n",
      "Epoch:475/1000\n",
      "Loss on train= 0.015313306823372841\n",
      "Loss on test= 0.020820267498493195\n",
      "acc for Lsat= 0.08400833141368094 \n",
      "acc for Psat= 0.10447680017480819 \n",
      "acc for optim= 0.1459413672800476\n",
      "Epoch:476/1000\n",
      "Loss on train= 0.015746185556054115\n",
      "Loss on test= 0.02189723029732704\n",
      "acc for Lsat= 0.07102594626108279 \n",
      "acc for Psat= 0.10154597151319054 \n",
      "acc for optim= 0.15128193782809563\n",
      "Epoch:477/1000\n",
      "Loss on train= 0.014897361397743225\n",
      "Loss on test= 0.02283291704952717\n",
      "acc for Lsat= 0.08448424924449668 \n",
      "acc for Psat= 0.11990988205041601 \n",
      "acc for optim= 0.14887005110515708\n",
      "Epoch:478/1000\n",
      "Loss on train= 0.015604138374328613\n",
      "Loss on test= 0.021819207817316055\n",
      "acc for Lsat= 0.0864907420255813 \n",
      "acc for Psat= 0.12114839233037245 \n",
      "acc for optim= 0.1510953478242868\n",
      "Epoch:479/1000\n",
      "Loss on train= 0.01582999713718891\n",
      "Loss on test= 0.021786745637655258\n",
      "acc for Lsat= 0.07555828654884897 \n",
      "acc for Psat= 0.10206303065797417 \n",
      "acc for optim= 0.1497891306085048\n",
      "Epoch:480/1000\n",
      "Loss on train= 0.015151032246649265\n",
      "Loss on test= 0.021824520081281662\n",
      "acc for Lsat= 0.07598144398179164 \n",
      "acc for Psat= 0.10585258919931329 \n",
      "acc for optim= 0.1480493017803395\n",
      "Epoch:481/1000\n",
      "Loss on train= 0.015280644409358501\n",
      "Loss on test= 0.022359073162078857\n",
      "acc for Lsat= 0.07316823250233535 \n",
      "acc for Psat= 0.10166035394533919 \n",
      "acc for optim= 0.15062788055584678\n",
      "Epoch:482/1000\n",
      "Loss on train= 0.015072716400027275\n",
      "Loss on test= 0.02133185975253582\n",
      "acc for Lsat= 0.07513930889657171 \n",
      "acc for Psat= 0.10283075733438284 \n",
      "acc for optim= 0.14848369443139361\n",
      "Epoch:483/1000\n",
      "Loss on train= 0.014861583709716797\n",
      "Loss on test= 0.02217358909547329\n",
      "acc for Lsat= 0.07058566261169522 \n",
      "acc for Psat= 0.10462847029251908 \n",
      "acc for optim= 0.14745511881932866\n",
      "Epoch:484/1000\n",
      "Loss on train= 0.01543161179870367\n",
      "Loss on test= 0.021465206518769264\n",
      "acc for Lsat= 0.06926904646463171 \n",
      "acc for Psat= 0.10060541378303223 \n",
      "acc for optim= 0.1482230427059224\n",
      "Epoch:485/1000\n",
      "Loss on train= 0.01585010252892971\n",
      "Loss on test= 0.02187291905283928\n",
      "acc for Lsat= 0.0704435374550645 \n",
      "acc for Psat= 0.1030425308550711 \n",
      "acc for optim= 0.1535396512362648\n",
      "Epoch:486/1000\n",
      "Loss on train= 0.014909216202795506\n",
      "Loss on test= 0.022219128906726837\n",
      "acc for Lsat= 0.0852904030650953 \n",
      "acc for Psat= 0.11199319958686828 \n",
      "acc for optim= 0.14843608869666672\n",
      "Epoch:487/1000\n",
      "Loss on train= 0.01515659037977457\n",
      "Loss on test= 0.021215371787548065\n",
      "acc for Lsat= 0.08710230610695394 \n",
      "acc for Psat= 0.10754808692441033 \n",
      "acc for optim= 0.14717792975546115\n",
      "Epoch:488/1000\n",
      "Loss on train= 0.015164305455982685\n",
      "Loss on test= 0.022486846894025803\n",
      "acc for Lsat= 0.07693278013273727 \n",
      "acc for Psat= 0.11769041054668614 \n",
      "acc for optim= 0.1482037171770964\n",
      "Epoch:489/1000\n",
      "Loss on train= 0.01524050347507\n",
      "Loss on test= 0.021726353093981743\n",
      "acc for Lsat= 0.07248344300593253 \n",
      "acc for Psat= 0.10340653477713116 \n",
      "acc for optim= 0.14854681545317763\n",
      "Epoch:490/1000\n",
      "Loss on train= 0.015635833144187927\n",
      "Loss on test= 0.02252398617565632\n",
      "acc for Lsat= 0.09124130081892802 \n",
      "acc for Psat= 0.11419905138966255 \n",
      "acc for optim= 0.1487047552270351\n",
      "Epoch:491/1000\n",
      "Loss on train= 0.015318023040890694\n",
      "Loss on test= 0.0220769215375185\n",
      "acc for Lsat= 0.08131348841214103 \n",
      "acc for Psat= 0.10584404067541675 \n",
      "acc for optim= 0.1514349403175405\n",
      "Epoch:492/1000\n",
      "Loss on train= 0.015064293518662453\n",
      "Loss on test= 0.021035945042967796\n",
      "acc for Lsat= 0.07314733139106205 \n",
      "acc for Psat= 0.10697319196704216 \n",
      "acc for optim= 0.14581565484652093\n",
      "Epoch:493/1000\n",
      "Loss on train= 0.014970357529819012\n",
      "Loss on test= 0.021410508081316948\n",
      "acc for Lsat= 0.07198393965678355 \n",
      "acc for Psat= 0.10171403500724871 \n",
      "acc for optim= 0.1487795674721664\n",
      "Epoch:494/1000\n",
      "Loss on train= 0.01525520347058773\n",
      "Loss on test= 0.021823825314641\n",
      "acc for Lsat= 0.07152899940346563 \n",
      "acc for Psat= 0.1015760953838247 \n",
      "acc for optim= 0.147888085711438\n",
      "Epoch:495/1000\n",
      "Loss on train= 0.01570937968790531\n",
      "Loss on test= 0.02233920991420746\n",
      "acc for Lsat= 0.07794898745427498 \n",
      "acc for Psat= 0.10933061566859781 \n",
      "acc for optim= 0.14754288335179175\n",
      "Epoch:496/1000\n",
      "Loss on train= 0.016026217490434647\n",
      "Loss on test= 0.022162361070513725\n",
      "acc for Lsat= 0.08038045235250478 \n",
      "acc for Psat= 0.11177530932268034 \n",
      "acc for optim= 0.1484395721822086\n",
      "Epoch:497/1000\n",
      "Loss on train= 0.014682156965136528\n",
      "Loss on test= 0.02201620303094387\n",
      "acc for Lsat= 0.07312908757762657 \n",
      "acc for Psat= 0.10638328533235976 \n",
      "acc for optim= 0.14796370244105392\n",
      "Epoch:498/1000\n",
      "Loss on train= 0.014604063704609871\n",
      "Loss on test= 0.021791335195302963\n",
      "acc for Lsat= 0.07594924886955376 \n",
      "acc for Psat= 0.10349364136144568 \n",
      "acc for optim= 0.14931503045202493\n",
      "Epoch:499/1000\n",
      "Loss on train= 0.014830373227596283\n",
      "Loss on test= 0.022211162373423576\n",
      "acc for Lsat= 0.07456824180493718 \n",
      "acc for Psat= 0.11240901711375212 \n",
      "acc for optim= 0.14830217462441453\n",
      "Epoch:500/1000\n",
      "Loss on train= 0.0148133784532547\n",
      "Loss on test= 0.021446891129016876\n",
      "acc for Lsat= 0.0719731909096043 \n",
      "acc for Psat= 0.1041348826647597 \n",
      "acc for optim= 0.15075349645361158\n",
      "Epoch:501/1000\n",
      "Loss on train= 0.014998485334217548\n",
      "Loss on test= 0.021460628136992455\n",
      "acc for Lsat= 0.07089954666125023 \n",
      "acc for Psat= 0.10370322376786674 \n",
      "acc for optim= 0.15017067211806975\n",
      "Epoch:502/1000\n",
      "Loss on train= 0.014947332441806793\n",
      "Loss on test= 0.022608522325754166\n",
      "acc for Lsat= 0.0699177915570744 \n",
      "acc for Psat= 0.10157471273428582 \n",
      "acc for optim= 0.14973877715906034\n",
      "Epoch:503/1000\n",
      "Loss on train= 0.015227793715894222\n",
      "Loss on test= 0.021706901490688324\n",
      "acc for Lsat= 0.0722094524243345 \n",
      "acc for Psat= 0.10593127485525566 \n",
      "acc for optim= 0.14969803548334443\n",
      "Epoch:504/1000\n",
      "Loss on train= 0.014484820887446404\n",
      "Loss on test= 0.022345317527651787\n",
      "acc for Lsat= 0.08119931888342696 \n",
      "acc for Psat= 0.10919291610733613 \n",
      "acc for optim= 0.1494172060212423\n",
      "Epoch:505/1000\n",
      "Loss on train= 0.01503838412463665\n",
      "Loss on test= 0.022118983790278435\n",
      "acc for Lsat= 0.07290343282824736 \n",
      "acc for Psat= 0.10921392292279344 \n",
      "acc for optim= 0.1495223536839913\n",
      "Epoch:506/1000\n",
      "Loss on train= 0.015321592800319195\n",
      "Loss on test= 0.021607160568237305\n",
      "acc for Lsat= 0.07669511582962302 \n",
      "acc for Psat= 0.1067533430070972 \n",
      "acc for optim= 0.14716633267180865\n",
      "Epoch:507/1000\n",
      "Loss on train= 0.014963801018893719\n",
      "Loss on test= 0.022176729515194893\n",
      "acc for Lsat= 0.07088879540909168 \n",
      "acc for Psat= 0.10251511361115795 \n",
      "acc for optim= 0.14890445502493468\n",
      "Epoch:508/1000\n",
      "Loss on train= 0.015636833384633064\n",
      "Loss on test= 0.022142069414258003\n",
      "acc for Lsat= 0.07425556803660535 \n",
      "acc for Psat= 0.10131692468525953 \n",
      "acc for optim= 0.14849973004125683\n",
      "Epoch:509/1000\n",
      "Loss on train= 0.014645458199083805\n",
      "Loss on test= 0.021684659644961357\n",
      "acc for Lsat= 0.07308638333482204 \n",
      "acc for Psat= 0.1074557388244673 \n",
      "acc for optim= 0.1505646002253029\n",
      "Epoch:510/1000\n",
      "Loss on train= 0.015083675272762775\n",
      "Loss on test= 0.02236305922269821\n",
      "acc for Lsat= 0.07368454839106019 \n",
      "acc for Psat= 0.10633616623688374 \n",
      "acc for optim= 0.15149887214071328\n",
      "Epoch:511/1000\n",
      "Loss on train= 0.014450207352638245\n",
      "Loss on test= 0.02343950979411602\n",
      "acc for Lsat= 0.07516659156626643 \n",
      "acc for Psat= 0.11136830488708725 \n",
      "acc for optim= 0.1521579462230404\n",
      "Epoch:512/1000\n",
      "Loss on train= 0.014916989021003246\n",
      "Loss on test= 0.022813227027654648\n",
      "acc for Lsat= 0.0759141925858501 \n",
      "acc for Psat= 0.11947980109243296 \n",
      "acc for optim= 0.1487418774156475\n",
      "Epoch:513/1000\n",
      "Loss on train= 0.015057607553899288\n",
      "Loss on test= 0.02194857969880104\n",
      "acc for Lsat= 0.07893146699251133 \n",
      "acc for Psat= 0.10860876755856991 \n",
      "acc for optim= 0.14916669706569555\n",
      "Epoch:514/1000\n",
      "Loss on train= 0.015063273720443249\n",
      "Loss on test= 0.0221716221421957\n",
      "acc for Lsat= 0.07917942436430542 \n",
      "acc for Psat= 0.11314876257779193 \n",
      "acc for optim= 0.14818239843726555\n",
      "Epoch:515/1000\n",
      "Loss on train= 0.015161102637648582\n",
      "Loss on test= 0.02252676896750927\n",
      "acc for Lsat= 0.07427225624802104 \n",
      "acc for Psat= 0.10808655312687057 \n",
      "acc for optim= 0.15041148046718486\n",
      "Epoch:516/1000\n",
      "Loss on train= 0.015118544921278954\n",
      "Loss on test= 0.02264719270169735\n",
      "acc for Lsat= 0.07963547682841354 \n",
      "acc for Psat= 0.11886792523520337 \n",
      "acc for optim= 0.1494547568088354\n",
      "Epoch:517/1000\n",
      "Loss on train= 0.014573821797966957\n",
      "Loss on test= 0.02256980538368225\n",
      "acc for Lsat= 0.07707194877027276 \n",
      "acc for Psat= 0.10417452345654817 \n",
      "acc for optim= 0.14861244529584716\n",
      "Epoch:518/1000\n",
      "Loss on train= 0.014696233905851841\n",
      "Loss on test= 0.02215089648962021\n",
      "acc for Lsat= 0.07744859202557623 \n",
      "acc for Psat= 0.11374679249386456 \n",
      "acc for optim= 0.14971409343009776\n",
      "Epoch:519/1000\n",
      "Loss on train= 0.014832386747002602\n",
      "Loss on test= 0.021668143570423126\n",
      "acc for Lsat= 0.07724860195701701 \n",
      "acc for Psat= 0.11083438453880261 \n",
      "acc for optim= 0.14785019723679932\n",
      "Epoch:520/1000\n",
      "Loss on train= 0.014851434156298637\n",
      "Loss on test= 0.02197147160768509\n",
      "acc for Lsat= 0.07131154173830417 \n",
      "acc for Psat= 0.10625403665030916 \n",
      "acc for optim= 0.14919039222885208\n",
      "Epoch:521/1000\n",
      "Loss on train= 0.015020759776234627\n",
      "Loss on test= 0.022511132061481476\n",
      "acc for Lsat= 0.07741653472680192 \n",
      "acc for Psat= 0.10517621438368607 \n",
      "acc for optim= 0.14956051849843655\n",
      "Epoch:522/1000\n",
      "Loss on train= 0.014342582784593105\n",
      "Loss on test= 0.021353881806135178\n",
      "acc for Lsat= 0.0766427006535356 \n",
      "acc for Psat= 0.10369321256380938 \n",
      "acc for optim= 0.15166026072644712\n",
      "Epoch:523/1000\n",
      "Loss on train= 0.015420754440128803\n",
      "Loss on test= 0.022024650126695633\n",
      "acc for Lsat= 0.07756859605692548 \n",
      "acc for Psat= 0.11205934117798788 \n",
      "acc for optim= 0.1477582789140682\n",
      "Epoch:524/1000\n",
      "Loss on train= 0.015274284407496452\n",
      "Loss on test= 0.021987885236740112\n",
      "acc for Lsat= 0.07281724681885932 \n",
      "acc for Psat= 0.1026880830625363 \n",
      "acc for optim= 0.15109542899353562\n",
      "Epoch:525/1000\n",
      "Loss on train= 0.014302559196949005\n",
      "Loss on test= 0.021845271810889244\n",
      "acc for Lsat= 0.07074929242118252 \n",
      "acc for Psat= 0.09857728346637712 \n",
      "acc for optim= 0.14727621480476025\n",
      "Epoch:526/1000\n",
      "Loss on train= 0.014621068723499775\n",
      "Loss on test= 0.021596157923340797\n",
      "acc for Lsat= 0.07035197606712479 \n",
      "acc for Psat= 0.10654122271015004 \n",
      "acc for optim= 0.14799367104257855\n",
      "Epoch:527/1000\n",
      "Loss on train= 0.01489034853875637\n",
      "Loss on test= 0.02280060201883316\n",
      "acc for Lsat= 0.08707439025177115 \n",
      "acc for Psat= 0.1042755650721515 \n",
      "acc for optim= 0.14896654629232084\n",
      "Epoch:528/1000\n",
      "Loss on train= 0.015027930028736591\n",
      "Loss on test= 0.021877923980355263\n",
      "acc for Lsat= 0.071017918802575 \n",
      "acc for Psat= 0.1015244177904636 \n",
      "acc for optim= 0.15123217897953783\n",
      "Epoch:529/1000\n",
      "Loss on train= 0.01502919476479292\n",
      "Loss on test= 0.02197575941681862\n",
      "acc for Lsat= 0.07463886953667549 \n",
      "acc for Psat= 0.10229476619004411 \n",
      "acc for optim= 0.14969095154062062\n",
      "Epoch:530/1000\n",
      "Loss on train= 0.014886227436363697\n",
      "Loss on test= 0.022002147510647774\n",
      "acc for Lsat= 0.07033165968533765 \n",
      "acc for Psat= 0.10648454972279826 \n",
      "acc for optim= 0.1496175664009842\n",
      "Epoch:531/1000\n",
      "Loss on train= 0.014691905118525028\n",
      "Loss on test= 0.02167680114507675\n",
      "acc for Lsat= 0.07189659916879328 \n",
      "acc for Psat= 0.10221811525053363 \n",
      "acc for optim= 0.15017887902418242\n",
      "Epoch:532/1000\n",
      "Loss on train= 0.015512590296566486\n",
      "Loss on test= 0.021275972947478294\n",
      "acc for Lsat= 0.0767729267824528 \n",
      "acc for Psat= 0.10324105715434811 \n",
      "acc for optim= 0.14937760833885977\n",
      "Epoch:533/1000\n",
      "Loss on train= 0.014498279429972172\n",
      "Loss on test= 0.022074300795793533\n",
      "acc for Lsat= 0.08065973195522727 \n",
      "acc for Psat= 0.10873152409677095 \n",
      "acc for optim= 0.1488832058304568\n",
      "Epoch:534/1000\n",
      "Loss on train= 0.01486919354647398\n",
      "Loss on test= 0.021896833553910255\n",
      "acc for Lsat= 0.0733843118051358 \n",
      "acc for Psat= 0.10752645441860058 \n",
      "acc for optim= 0.15059343778413795\n",
      "Epoch:535/1000\n",
      "Loss on train= 0.015034946613013744\n",
      "Loss on test= 0.02243977040052414\n",
      "acc for Lsat= 0.07144372969172723 \n",
      "acc for Psat= 0.10896280336617631 \n",
      "acc for optim= 0.1503713922247142\n",
      "Epoch:536/1000\n",
      "Loss on train= 0.015228092670440674\n",
      "Loss on test= 0.022327983751893044\n",
      "acc for Lsat= 0.0683522977504223 \n",
      "acc for Psat= 0.10335676295812736 \n",
      "acc for optim= 0.14857088986821348\n",
      "Epoch:537/1000\n",
      "Loss on train= 0.014694372192025185\n",
      "Loss on test= 0.021958207711577415\n",
      "acc for Lsat= 0.07539051520666012 \n",
      "acc for Psat= 0.10331814073248954 \n",
      "acc for optim= 0.1500330741619351\n",
      "Epoch:538/1000\n",
      "Loss on train= 0.013870085589587688\n",
      "Loss on test= 0.021825170144438744\n",
      "acc for Lsat= 0.07158643764712884 \n",
      "acc for Psat= 0.1057237734232234 \n",
      "acc for optim= 0.15035835417402152\n",
      "Epoch:539/1000\n",
      "Loss on train= 0.01494185347110033\n",
      "Loss on test= 0.022016625851392746\n",
      "acc for Lsat= 0.07404643721952789 \n",
      "acc for Psat= 0.11252692353685831 \n",
      "acc for optim= 0.14966085970995827\n",
      "Epoch:540/1000\n",
      "Loss on train= 0.014187931083142757\n",
      "Loss on test= 0.02213907614350319\n",
      "acc for Lsat= 0.07645725663318188 \n",
      "acc for Psat= 0.11189328202377524 \n",
      "acc for optim= 0.15041535997707578\n",
      "Epoch:541/1000\n",
      "Loss on train= 0.014902284368872643\n",
      "Loss on test= 0.022616513073444366\n",
      "acc for Lsat= 0.08406681135247317 \n",
      "acc for Psat= 0.10429952237693178 \n",
      "acc for optim= 0.1486817551213641\n",
      "Epoch:542/1000\n",
      "Loss on train= 0.014858057722449303\n",
      "Loss on test= 0.022333689033985138\n",
      "acc for Lsat= 0.07205553981552883 \n",
      "acc for Psat= 0.10664366461905929 \n",
      "acc for optim= 0.15045900107222143\n",
      "Epoch:543/1000\n",
      "Loss on train= 0.014512957073748112\n",
      "Loss on test= 0.022078337147831917\n",
      "acc for Lsat= 0.06980055982488337 \n",
      "acc for Psat= 0.10025244545302914 \n",
      "acc for optim= 0.14969680855836587\n",
      "Epoch:544/1000\n",
      "Loss on train= 0.014414801262319088\n",
      "Loss on test= 0.022662676870822906\n",
      "acc for Lsat= 0.06904659770255865 \n",
      "acc for Psat= 0.10008427704687531 \n",
      "acc for optim= 0.14788054811994103\n",
      "Epoch:545/1000\n",
      "Loss on train= 0.014918617904186249\n",
      "Loss on test= 0.021466927602887154\n",
      "acc for Lsat= 0.06903847076766118 \n",
      "acc for Psat= 0.10297180127067819 \n",
      "acc for optim= 0.14969884935407549\n",
      "Epoch:546/1000\n",
      "Loss on train= 0.014234076254069805\n",
      "Loss on test= 0.022365456447005272\n",
      "acc for Lsat= 0.07933115196782489 \n",
      "acc for Psat= 0.10398412042876015 \n",
      "acc for optim= 0.15093134402040626\n",
      "Epoch:547/1000\n",
      "Loss on train= 0.014238230884075165\n",
      "Loss on test= 0.021756980568170547\n",
      "acc for Lsat= 0.07178543561518785 \n",
      "acc for Psat= 0.10380641427943081 \n",
      "acc for optim= 0.1504595262267661\n",
      "Epoch:548/1000\n",
      "Loss on train= 0.014523522928357124\n",
      "Loss on test= 0.021553197875618935\n",
      "acc for Lsat= 0.07084548857520984 \n",
      "acc for Psat= 0.1027303874096601 \n",
      "acc for optim= 0.14827495734160917\n",
      "Epoch:549/1000\n",
      "Loss on train= 0.014338122680783272\n",
      "Loss on test= 0.02290736325085163\n",
      "acc for Lsat= 0.07598264469854855 \n",
      "acc for Psat= 0.10003687138375253 \n",
      "acc for optim= 0.14871334673162317\n",
      "Epoch:550/1000\n",
      "Loss on train= 0.014095327816903591\n",
      "Loss on test= 0.02238205447793007\n",
      "acc for Lsat= 0.08065363175647204 \n",
      "acc for Psat= 0.11087499567836626 \n",
      "acc for optim= 0.14968851474036413\n",
      "Epoch:551/1000\n",
      "Loss on train= 0.014587195590138435\n",
      "Loss on test= 0.02280961535871029\n",
      "acc for Lsat= 0.08375910655208602 \n",
      "acc for Psat= 0.1282278785673883 \n",
      "acc for optim= 0.1503608370144106\n",
      "Epoch:552/1000\n",
      "Loss on train= 0.014077235944569111\n",
      "Loss on test= 0.02198624424636364\n",
      "acc for Lsat= 0.08527466888443579 \n",
      "acc for Psat= 0.10719679495028876 \n",
      "acc for optim= 0.14910488427675442\n",
      "Epoch:553/1000\n",
      "Loss on train= 0.014616789296269417\n",
      "Loss on test= 0.02258230373263359\n",
      "acc for Lsat= 0.07800591185045401 \n",
      "acc for Psat= 0.1030029819851302 \n",
      "acc for optim= 0.14990514558415077\n",
      "Epoch:554/1000\n",
      "Loss on train= 0.014817222952842712\n",
      "Loss on test= 0.02205105498433113\n",
      "acc for Lsat= 0.07599135240249064 \n",
      "acc for Psat= 0.10950613978297212 \n",
      "acc for optim= 0.15031085788609577\n",
      "Epoch:555/1000\n",
      "Loss on train= 0.015732526779174805\n",
      "Loss on test= 0.022582827135920525\n",
      "acc for Lsat= 0.0863448121618987 \n",
      "acc for Psat= 0.12328501868485615 \n",
      "acc for optim= 0.15026190480916599\n",
      "Epoch:556/1000\n",
      "Loss on train= 0.014646075665950775\n",
      "Loss on test= 0.02249033749103546\n",
      "acc for Lsat= 0.0787846725050397 \n",
      "acc for Psat= 0.11038182960000148 \n",
      "acc for optim= 0.15089423593096554\n",
      "Epoch:557/1000\n",
      "Loss on train= 0.014185755513608456\n",
      "Loss on test= 0.02198871783912182\n",
      "acc for Lsat= 0.07065186713423047 \n",
      "acc for Psat= 0.10247949060807593 \n",
      "acc for optim= 0.1507330143570504\n",
      "Epoch:558/1000\n",
      "Loss on train= 0.014312589541077614\n",
      "Loss on test= 0.02218290977180004\n",
      "acc for Lsat= 0.06899706542095871 \n",
      "acc for Psat= 0.09849167294874542 \n",
      "acc for optim= 0.14866131864908924\n",
      "Epoch:559/1000\n",
      "Loss on train= 0.014289123937487602\n",
      "Loss on test= 0.022211933508515358\n",
      "acc for Lsat= 0.07340790768002356 \n",
      "acc for Psat= 0.10007809955020285 \n",
      "acc for optim= 0.14889431104707557\n",
      "Epoch:560/1000\n",
      "Loss on train= 0.013907180167734623\n",
      "Loss on test= 0.02147894725203514\n",
      "acc for Lsat= 0.07380384651331408 \n",
      "acc for Psat= 0.1088488878403787 \n",
      "acc for optim= 0.14997267905263809\n",
      "Epoch:561/1000\n",
      "Loss on train= 0.015322129242122173\n",
      "Loss on test= 0.0223326925188303\n",
      "acc for Lsat= 0.07700246864577069 \n",
      "acc for Psat= 0.10434026510216468 \n",
      "acc for optim= 0.1507879061952382\n",
      "Epoch:562/1000\n",
      "Loss on train= 0.014759272336959839\n",
      "Loss on test= 0.0219400767236948\n",
      "acc for Lsat= 0.07152558444346302 \n",
      "acc for Psat= 0.10291910797258548 \n",
      "acc for optim= 0.15050709784228938\n",
      "Epoch:563/1000\n",
      "Loss on train= 0.01546641904860735\n",
      "Loss on test= 0.022664088755846024\n",
      "acc for Lsat= 0.07349923584746362 \n",
      "acc for Psat= 0.10600740113527671 \n",
      "acc for optim= 0.1484067996673014\n",
      "Epoch:564/1000\n",
      "Loss on train= 0.013768213801085949\n",
      "Loss on test= 0.023012422025203705\n",
      "acc for Lsat= 0.07636032262909845 \n",
      "acc for Psat= 0.10380966639597942 \n",
      "acc for optim= 0.1510250175910139\n",
      "Epoch:565/1000\n",
      "Loss on train= 0.014488671906292439\n",
      "Loss on test= 0.02221299521625042\n",
      "acc for Lsat= 0.08249319707634442 \n",
      "acc for Psat= 0.10957517292214387 \n",
      "acc for optim= 0.15250920237496837\n",
      "Epoch:566/1000\n",
      "Loss on train= 0.014697945676743984\n",
      "Loss on test= 0.022377092391252518\n",
      "acc for Lsat= 0.0746981811384822 \n",
      "acc for Psat= 0.103964204249588 \n",
      "acc for optim= 0.14857725572348437\n",
      "Epoch:567/1000\n",
      "Loss on train= 0.0144426254555583\n",
      "Loss on test= 0.021980348974466324\n",
      "acc for Lsat= 0.0728642534377963 \n",
      "acc for Psat= 0.10438959424281834 \n",
      "acc for optim= 0.14872683557957114\n",
      "Epoch:568/1000\n",
      "Loss on train= 0.014350650832057\n",
      "Loss on test= 0.022373097017407417\n",
      "acc for Lsat= 0.07066323452217635 \n",
      "acc for Psat= 0.10332629771723698 \n",
      "acc for optim= 0.14879906991391484\n",
      "Epoch:569/1000\n",
      "Loss on train= 0.013895775191485882\n",
      "Loss on test= 0.021540731191635132\n",
      "acc for Lsat= 0.07407210695981188 \n",
      "acc for Psat= 0.10289200978025642 \n",
      "acc for optim= 0.14887354874927733\n",
      "Epoch:570/1000\n",
      "Loss on train= 0.014998150058090687\n",
      "Loss on test= 0.021260006353259087\n",
      "acc for Lsat= 0.07227943900019626 \n",
      "acc for Psat= 0.1026524476632723 \n",
      "acc for optim= 0.15012694112090177\n",
      "Epoch:571/1000\n",
      "Loss on train= 0.01469967607408762\n",
      "Loss on test= 0.022629389539361\n",
      "acc for Lsat= 0.08270702787808007 \n",
      "acc for Psat= 0.10917717547115695 \n",
      "acc for optim= 0.1490399709571636\n",
      "Epoch:572/1000\n",
      "Loss on train= 0.014146661385893822\n",
      "Loss on test= 0.021607205271720886\n",
      "acc for Lsat= 0.08527253246386582 \n",
      "acc for Psat= 0.1103142264475458 \n",
      "acc for optim= 0.14958592896841688\n",
      "Epoch:573/1000\n",
      "Loss on train= 0.014644337818026543\n",
      "Loss on test= 0.022075362503528595\n",
      "acc for Lsat= 0.07529214611679218 \n",
      "acc for Psat= 0.10417874247528784 \n",
      "acc for optim= 0.1523824784050748\n",
      "Epoch:574/1000\n",
      "Loss on train= 0.014187772758305073\n",
      "Loss on test= 0.021904118359088898\n",
      "acc for Lsat= 0.08034913707215129 \n",
      "acc for Psat= 0.10967137817528559 \n",
      "acc for optim= 0.14968392714313497\n",
      "Epoch:575/1000\n",
      "Loss on train= 0.014720954932272434\n",
      "Loss on test= 0.021989328786730766\n",
      "acc for Lsat= 0.08240780276911601 \n",
      "acc for Psat= 0.11170497291111867 \n",
      "acc for optim= 0.1487164028063169\n",
      "Epoch:576/1000\n",
      "Loss on train= 0.014201429672539234\n",
      "Loss on test= 0.02201623097062111\n",
      "acc for Lsat= 0.07602317695601833 \n",
      "acc for Psat= 0.11105520194551079 \n",
      "acc for optim= 0.1494109726229379\n",
      "Epoch:577/1000\n",
      "Loss on train= 0.014424430206418037\n",
      "Loss on test= 0.02180328592658043\n",
      "acc for Lsat= 0.07159894425607598 \n",
      "acc for Psat= 0.1077931484114688 \n",
      "acc for optim= 0.152176081005521\n",
      "Epoch:578/1000\n",
      "Loss on train= 0.01397167332470417\n",
      "Loss on test= 0.022338684648275375\n",
      "acc for Lsat= 0.07709917227097128 \n",
      "acc for Psat= 0.10977958504147702 \n",
      "acc for optim= 0.15127726737843\n",
      "Epoch:579/1000\n",
      "Loss on train= 0.014048938639461994\n",
      "Loss on test= 0.020994020625948906\n",
      "acc for Lsat= 0.07070990967196088 \n",
      "acc for Psat= 0.10572083273599316 \n",
      "acc for optim= 0.14999168269658011\n",
      "Epoch:580/1000\n",
      "Loss on train= 0.01418853085488081\n",
      "Loss on test= 0.02222238853573799\n",
      "acc for Lsat= 0.07550465155677541 \n",
      "acc for Psat= 0.10713508489520049 \n",
      "acc for optim= 0.14888597948606622\n",
      "Epoch:581/1000\n",
      "Loss on train= 0.014200037345290184\n",
      "Loss on test= 0.022191911935806274\n",
      "acc for Lsat= 0.07305135782374889 \n",
      "acc for Psat= 0.1051272567225453 \n",
      "acc for optim= 0.1486564563358345\n",
      "Epoch:582/1000\n",
      "Loss on train= 0.014134843833744526\n",
      "Loss on test= 0.021480152383446693\n",
      "acc for Lsat= 0.07133100312017522 \n",
      "acc for Psat= 0.0988811477276178 \n",
      "acc for optim= 0.14990727313333174\n",
      "Epoch:583/1000\n",
      "Loss on train= 0.014748642221093178\n",
      "Loss on test= 0.021178219467401505\n",
      "acc for Lsat= 0.07687167027265922 \n",
      "acc for Psat= 0.10490401294144287 \n",
      "acc for optim= 0.14878760469316246\n",
      "Epoch:584/1000\n",
      "Loss on train= 0.014081771485507488\n",
      "Loss on test= 0.021561063826084137\n",
      "acc for Lsat= 0.07094505194611724 \n",
      "acc for Psat= 0.10529487628081308 \n",
      "acc for optim= 0.15141764568332022\n",
      "Epoch:585/1000\n",
      "Loss on train= 0.013680027797818184\n",
      "Loss on test= 0.02233191952109337\n",
      "acc for Lsat= 0.07284272557675245 \n",
      "acc for Psat= 0.10397859336529854 \n",
      "acc for optim= 0.14875659158459534\n",
      "Epoch:586/1000\n",
      "Loss on train= 0.013718887232244015\n",
      "Loss on test= 0.02111702971160412\n",
      "acc for Lsat= 0.07731549824987138 \n",
      "acc for Psat= 0.11393578765400228 \n",
      "acc for optim= 0.1491343597042996\n",
      "Epoch:587/1000\n",
      "Loss on train= 0.014682580716907978\n",
      "Loss on test= 0.02273191511631012\n",
      "acc for Lsat= 0.08632430379770918 \n",
      "acc for Psat= 0.12072480548655865 \n",
      "acc for optim= 0.15041234849695348\n",
      "Epoch:588/1000\n",
      "Loss on train= 0.01420371700078249\n",
      "Loss on test= 0.021868253126740456\n",
      "acc for Lsat= 0.0774143955735273 \n",
      "acc for Psat= 0.11624907026457232 \n",
      "acc for optim= 0.15136665608795774\n",
      "Epoch:589/1000\n",
      "Loss on train= 0.013861743733286858\n",
      "Loss on test= 0.022219205275177956\n",
      "acc for Lsat= 0.07605187132509049 \n",
      "acc for Psat= 0.11713685822051231 \n",
      "acc for optim= 0.14961255602662346\n",
      "Epoch:590/1000\n",
      "Loss on train= 0.014471187256276608\n",
      "Loss on test= 0.02213936857879162\n",
      "acc for Lsat= 0.08186272589669276 \n",
      "acc for Psat= 0.10655923458032833 \n",
      "acc for optim= 0.14876080448445295\n",
      "Epoch:591/1000\n",
      "Loss on train= 0.014049508608877659\n",
      "Loss on test= 0.02203526720404625\n",
      "acc for Lsat= 0.07398086301116057 \n",
      "acc for Psat= 0.1018096437089863 \n",
      "acc for optim= 0.1507849040023512\n",
      "Epoch:592/1000\n",
      "Loss on train= 0.014225749298930168\n",
      "Loss on test= 0.021886156871914864\n",
      "acc for Lsat= 0.07147416033816101 \n",
      "acc for Psat= 0.0986725971548264 \n",
      "acc for optim= 0.150266952510688\n",
      "Epoch:593/1000\n",
      "Loss on train= 0.013259867206215858\n",
      "Loss on test= 0.021227650344371796\n",
      "acc for Lsat= 0.06838081338476897 \n",
      "acc for Psat= 0.09889082051194786 \n",
      "acc for optim= 0.14989747278318055\n",
      "Epoch:594/1000\n",
      "Loss on train= 0.013434427790343761\n",
      "Loss on test= 0.022062592208385468\n",
      "acc for Lsat= 0.07834051521711571 \n",
      "acc for Psat= 0.1010156127602555 \n",
      "acc for optim= 0.1502489604823217\n",
      "Epoch:595/1000\n",
      "Loss on train= 0.013699074275791645\n",
      "Loss on test= 0.022298229858279228\n",
      "acc for Lsat= 0.08014391020088894 \n",
      "acc for Psat= 0.10817665200296828 \n",
      "acc for optim= 0.1495891802334706\n",
      "Epoch:596/1000\n",
      "Loss on train= 0.015554787591099739\n",
      "Loss on test= 0.022140417248010635\n",
      "acc for Lsat= 0.07577111547769501 \n",
      "acc for Psat= 0.10514950924537504 \n",
      "acc for optim= 0.14979107094761537\n",
      "Epoch:597/1000\n",
      "Loss on train= 0.013987802900373936\n",
      "Loss on test= 0.02178487926721573\n",
      "acc for Lsat= 0.08108867909821166 \n",
      "acc for Psat= 0.10566234428225167 \n",
      "acc for optim= 0.1518273773977527\n",
      "Epoch:598/1000\n",
      "Loss on train= 0.013724075630307198\n",
      "Loss on test= 0.021721627563238144\n",
      "acc for Lsat= 0.071036540452032 \n",
      "acc for Psat= 0.10257473948389986 \n",
      "acc for optim= 0.14985713325069588\n",
      "Epoch:599/1000\n",
      "Loss on train= 0.013207553885877132\n",
      "Loss on test= 0.021495036780834198\n",
      "acc for Lsat= 0.071015285237683 \n",
      "acc for Psat= 0.10215480491964522 \n",
      "acc for optim= 0.14971180603749729\n",
      "Epoch:600/1000\n",
      "Loss on train= 0.013878688216209412\n",
      "Loss on test= 0.021764062345027924\n",
      "acc for Lsat= 0.07042481994312069 \n",
      "acc for Psat= 0.10154057994633417 \n",
      "acc for optim= 0.14916753986745188\n",
      "Epoch:601/1000\n",
      "Loss on train= 0.013846583664417267\n",
      "Loss on test= 0.02177107334136963\n",
      "acc for Lsat= 0.07241881242226128 \n",
      "acc for Psat= 0.10294847316124116 \n",
      "acc for optim= 0.1512888093327367\n",
      "Epoch:602/1000\n",
      "Loss on train= 0.013384358957409859\n",
      "Loss on test= 0.022590396925807\n",
      "acc for Lsat= 0.07496592957910114 \n",
      "acc for Psat= 0.10898776670230981 \n",
      "acc for optim= 0.1503271366274634\n",
      "Epoch:603/1000\n",
      "Loss on train= 0.01384816039353609\n",
      "Loss on test= 0.021665966138243675\n",
      "acc for Lsat= 0.06953199950365527 \n",
      "acc for Psat= 0.10190750689205536 \n",
      "acc for optim= 0.15060135027102853\n",
      "Epoch:604/1000\n",
      "Loss on train= 0.013507066294550896\n",
      "Loss on test= 0.022078072652220726\n",
      "acc for Lsat= 0.07296457990657451 \n",
      "acc for Psat= 0.10074786133940433 \n",
      "acc for optim= 0.1495834907424014\n",
      "Epoch:605/1000\n",
      "Loss on train= 0.013799355365335941\n",
      "Loss on test= 0.02145952172577381\n",
      "acc for Lsat= 0.08633199572761195 \n",
      "acc for Psat= 0.10904642396987083 \n",
      "acc for optim= 0.1500785290204805\n",
      "Epoch:606/1000\n",
      "Loss on train= 0.013357975520193577\n",
      "Loss on test= 0.02217746339738369\n",
      "acc for Lsat= 0.07498022405016068 \n",
      "acc for Psat= 0.10010893005271287 \n",
      "acc for optim= 0.1499096210414785\n",
      "Epoch:607/1000\n",
      "Loss on train= 0.013754921033978462\n",
      "Loss on test= 0.02153591997921467\n",
      "acc for Lsat= 0.07102022580926204 \n",
      "acc for Psat= 0.10848773840653936 \n",
      "acc for optim= 0.1501631805271009\n",
      "Epoch:608/1000\n",
      "Loss on train= 0.0138711454346776\n",
      "Loss on test= 0.021936459466814995\n",
      "acc for Lsat= 0.07422563168297575 \n",
      "acc for Psat= 0.10720099711734986 \n",
      "acc for optim= 0.14964481172371544\n",
      "Epoch:609/1000\n",
      "Loss on train= 0.013680999167263508\n",
      "Loss on test= 0.022844409570097923\n",
      "acc for Lsat= 0.08487145104677575 \n",
      "acc for Psat= 0.1131569565332609 \n",
      "acc for optim= 0.1511835654114568\n",
      "Epoch:610/1000\n",
      "Loss on train= 0.013505951501429081\n",
      "Loss on test= 0.021381163969635963\n",
      "acc for Lsat= 0.07401071573016653 \n",
      "acc for Psat= 0.10754729869753815 \n",
      "acc for optim= 0.14877406624068454\n",
      "Epoch:611/1000\n",
      "Loss on train= 0.014034238643944263\n",
      "Loss on test= 0.022833190858364105\n",
      "acc for Lsat= 0.06971726889824152 \n",
      "acc for Psat= 0.1036558611547432 \n",
      "acc for optim= 0.14896571147085422\n",
      "Epoch:612/1000\n",
      "Loss on train= 0.013368276879191399\n",
      "Loss on test= 0.022333117201924324\n",
      "acc for Lsat= 0.07748447956634913 \n",
      "acc for Psat= 0.11171320009469196 \n",
      "acc for optim= 0.15027097849750834\n",
      "Epoch:613/1000\n",
      "Loss on train= 0.013629022054374218\n",
      "Loss on test= 0.022095542401075363\n",
      "acc for Lsat= 0.07577917152068941 \n",
      "acc for Psat= 0.1057576337129967 \n",
      "acc for optim= 0.15090516463070616\n",
      "Epoch:614/1000\n",
      "Loss on train= 0.013629875145852566\n",
      "Loss on test= 0.022000771015882492\n",
      "acc for Lsat= 0.06981340469514016 \n",
      "acc for Psat= 0.09986957395789631 \n",
      "acc for optim= 0.15013247552108128\n",
      "Epoch:615/1000\n",
      "Loss on train= 0.013558662496507168\n",
      "Loss on test= 0.02222764678299427\n",
      "acc for Lsat= 0.08215241116344729 \n",
      "acc for Psat= 0.11688960869842986 \n",
      "acc for optim= 0.14895874806416787\n",
      "Epoch:616/1000\n",
      "Loss on train= 0.01363293081521988\n",
      "Loss on test= 0.021992452442646027\n",
      "acc for Lsat= 0.07628980958184531 \n",
      "acc for Psat= 0.10660344556320542 \n",
      "acc for optim= 0.1511445716171962\n",
      "Epoch:617/1000\n",
      "Loss on train= 0.014031553640961647\n",
      "Loss on test= 0.02234533429145813\n",
      "acc for Lsat= 0.07137226817813822 \n",
      "acc for Psat= 0.10073876012599346 \n",
      "acc for optim= 0.1498284541491259\n",
      "Epoch:618/1000\n",
      "Loss on train= 0.013136711902916431\n",
      "Loss on test= 0.021759891882538795\n",
      "acc for Lsat= 0.07193768008602813 \n",
      "acc for Psat= 0.09825315123974682 \n",
      "acc for optim= 0.15061645753359876\n",
      "Epoch:619/1000\n",
      "Loss on train= 0.013584019616246223\n",
      "Loss on test= 0.021819252520799637\n",
      "acc for Lsat= 0.07037928491533793 \n",
      "acc for Psat= 0.10214620326642578 \n",
      "acc for optim= 0.15050163140328615\n",
      "Epoch:620/1000\n",
      "Loss on train= 0.014348791912198067\n",
      "Loss on test= 0.021474407985806465\n",
      "acc for Lsat= 0.06918869765097914 \n",
      "acc for Psat= 0.09778101840288535 \n",
      "acc for optim= 0.14936036392303792\n",
      "Epoch:621/1000\n",
      "Loss on train= 0.014105724170804024\n",
      "Loss on test= 0.021973296999931335\n",
      "acc for Lsat= 0.06675503761863391 \n",
      "acc for Psat= 0.10154991197427642 \n",
      "acc for optim= 0.15022715013288576\n",
      "Epoch:622/1000\n",
      "Loss on train= 0.014035096392035484\n",
      "Loss on test= 0.022527312859892845\n",
      "acc for Lsat= 0.07532289973723137 \n",
      "acc for Psat= 0.10335242159144822 \n",
      "acc for optim= 0.1494200071821181\n",
      "Epoch:623/1000\n",
      "Loss on train= 0.012873881496489048\n",
      "Loss on test= 0.02249755896627903\n",
      "acc for Lsat= 0.06946074883407136 \n",
      "acc for Psat= 0.10412924960998206 \n",
      "acc for optim= 0.1488998062388842\n",
      "Epoch:624/1000\n",
      "Loss on train= 0.013556249439716339\n",
      "Loss on test= 0.02181810885667801\n",
      "acc for Lsat= 0.07192381093668385 \n",
      "acc for Psat= 0.1025349149672296 \n",
      "acc for optim= 0.15039505419937085\n",
      "Epoch:625/1000\n",
      "Loss on train= 0.013659103773534298\n",
      "Loss on test= 0.021976901218295097\n",
      "acc for Lsat= 0.06671955683500663 \n",
      "acc for Psat= 0.10045370577379714 \n",
      "acc for optim= 0.15018891163442616\n",
      "Epoch:626/1000\n",
      "Loss on train= 0.013414436019957066\n",
      "Loss on test= 0.02159452624619007\n",
      "acc for Lsat= 0.07617474825081239 \n",
      "acc for Psat= 0.1064615631024307 \n",
      "acc for optim= 0.1495102752482772\n",
      "Epoch:627/1000\n",
      "Loss on train= 0.013568279333412647\n",
      "Loss on test= 0.021711861714720726\n",
      "acc for Lsat= 0.06796141313358003 \n",
      "acc for Psat= 0.10339207140314226 \n",
      "acc for optim= 0.14779623327065142\n",
      "Epoch:628/1000\n",
      "Loss on train= 0.013516630046069622\n",
      "Loss on test= 0.021543191745877266\n",
      "acc for Lsat= 0.06831217515112158 \n",
      "acc for Psat= 0.10140827947676773 \n",
      "acc for optim= 0.14915342564598663\n",
      "Epoch:629/1000\n",
      "Loss on train= 0.012907465919852257\n",
      "Loss on test= 0.02239503152668476\n",
      "acc for Lsat= 0.06991441238956199 \n",
      "acc for Psat= 0.10548031324167981 \n",
      "acc for optim= 0.14883057322613027\n",
      "Epoch:630/1000\n",
      "Loss on train= 0.013719995506107807\n",
      "Loss on test= 0.022306660190224648\n",
      "acc for Lsat= 0.07186958627447337 \n",
      "acc for Psat= 0.09927002393526096 \n",
      "acc for optim= 0.15049940941737738\n",
      "Epoch:631/1000\n",
      "Loss on train= 0.013409691862761974\n",
      "Loss on test= 0.021985402330756187\n",
      "acc for Lsat= 0.0673198901240612 \n",
      "acc for Psat= 0.09792090382092813 \n",
      "acc for optim= 0.14867865688777046\n",
      "Epoch:632/1000\n",
      "Loss on train= 0.013196569867432117\n",
      "Loss on test= 0.02222047932446003\n",
      "acc for Lsat= 0.07070024131342424 \n",
      "acc for Psat= 0.10357938883114493 \n",
      "acc for optim= 0.14932543861113515\n",
      "Epoch:633/1000\n",
      "Loss on train= 0.013833574019372463\n",
      "Loss on test= 0.022596852853894234\n",
      "acc for Lsat= 0.07066465397411802 \n",
      "acc for Psat= 0.1006917825370928 \n",
      "acc for optim= 0.15047875306138958\n",
      "Epoch:634/1000\n",
      "Loss on train= 0.013746047392487526\n",
      "Loss on test= 0.022008413448929787\n",
      "acc for Lsat= 0.07296871277581021 \n",
      "acc for Psat= 0.10116064186112037 \n",
      "acc for optim= 0.15066128137499793\n",
      "Epoch:635/1000\n",
      "Loss on train= 0.013496414758265018\n",
      "Loss on test= 0.02213452383875847\n",
      "acc for Lsat= 0.07458524094071499 \n",
      "acc for Psat= 0.10096853779399909 \n",
      "acc for optim= 0.15060157864988838\n",
      "Epoch:636/1000\n",
      "Loss on train= 0.013776254840195179\n",
      "Loss on test= 0.02192896418273449\n",
      "acc for Lsat= 0.07600566964608885 \n",
      "acc for Psat= 0.09875538511529715 \n",
      "acc for optim= 0.14921688797862032\n",
      "Epoch:637/1000\n",
      "Loss on train= 0.01380025502294302\n",
      "Loss on test= 0.022691309452056885\n",
      "acc for Lsat= 0.06960787449366229 \n",
      "acc for Psat= 0.09949445411612425 \n",
      "acc for optim= 0.14931551839822158\n",
      "Epoch:638/1000\n",
      "Loss on train= 0.014102154411375523\n",
      "Loss on test= 0.022150220349431038\n",
      "acc for Lsat= 0.0746943159752906 \n",
      "acc for Psat= 0.10024580498074376 \n",
      "acc for optim= 0.15221870666326476\n",
      "Epoch:639/1000\n",
      "Loss on train= 0.013671264983713627\n",
      "Loss on test= 0.02167021855711937\n",
      "acc for Lsat= 0.07505330181993124 \n",
      "acc for Psat= 0.10652402506714248 \n",
      "acc for optim= 0.14950964745889075\n",
      "Epoch:640/1000\n",
      "Loss on train= 0.013487900607287884\n",
      "Loss on test= 0.02169794775545597\n",
      "acc for Lsat= 0.07773027564600064 \n",
      "acc for Psat= 0.10924751287282904 \n",
      "acc for optim= 0.15012720304470129\n",
      "Epoch:641/1000\n",
      "Loss on train= 0.013272106647491455\n",
      "Loss on test= 0.0220757108181715\n",
      "acc for Lsat= 0.07090946014141322 \n",
      "acc for Psat= 0.10019932694213336 \n",
      "acc for optim= 0.15080058812303004\n",
      "Epoch:642/1000\n",
      "Loss on train= 0.012920008972287178\n",
      "Loss on test= 0.02186644822359085\n",
      "acc for Lsat= 0.07063036221404409 \n",
      "acc for Psat= 0.10014813136420771 \n",
      "acc for optim= 0.14875132742514247\n",
      "Epoch:643/1000\n",
      "Loss on train= 0.013644250109791756\n",
      "Loss on test= 0.022158896550536156\n",
      "acc for Lsat= 0.07164347003464684 \n",
      "acc for Psat= 0.10352950636730635 \n",
      "acc for optim= 0.15045938978955592\n",
      "Epoch:644/1000\n",
      "Loss on train= 0.01315750740468502\n",
      "Loss on test= 0.021686388179659843\n",
      "acc for Lsat= 0.06845159152417485 \n",
      "acc for Psat= 0.0998011094985214 \n",
      "acc for optim= 0.14975580184958703\n",
      "Epoch:645/1000\n",
      "Loss on train= 0.013312071561813354\n",
      "Loss on test= 0.022489160299301147\n",
      "acc for Lsat= 0.07165349358538059 \n",
      "acc for Psat= 0.10487454313772462 \n",
      "acc for optim= 0.14914709598915118\n",
      "Epoch:646/1000\n",
      "Loss on train= 0.013337632641196251\n",
      "Loss on test= 0.021488841623067856\n",
      "acc for Lsat= 0.06936519961816529 \n",
      "acc for Psat= 0.09716130554478038 \n",
      "acc for optim= 0.14857879952338843\n",
      "Epoch:647/1000\n",
      "Loss on train= 0.01313342060893774\n",
      "Loss on test= 0.021595081314444542\n",
      "acc for Lsat= 0.0685982369802323 \n",
      "acc for Psat= 0.09825797510701557 \n",
      "acc for optim= 0.14786516004226533\n",
      "Epoch:648/1000\n",
      "Loss on train= 0.012534716166555882\n",
      "Loss on test= 0.021790577098727226\n",
      "acc for Lsat= 0.06800117851095737 \n",
      "acc for Psat= 0.09717532659685892 \n",
      "acc for optim= 0.14812560574556902\n",
      "Epoch:649/1000\n",
      "Loss on train= 0.013271143659949303\n",
      "Loss on test= 0.021955717355012894\n",
      "acc for Lsat= 0.07469909060437022 \n",
      "acc for Psat= 0.10839319961807656 \n",
      "acc for optim= 0.15018260205702924\n",
      "Epoch:650/1000\n",
      "Loss on train= 0.013484463095664978\n",
      "Loss on test= 0.02190828137099743\n",
      "acc for Lsat= 0.07490719643938185 \n",
      "acc for Psat= 0.10410477443786952 \n",
      "acc for optim= 0.1489703522172085\n",
      "Epoch:651/1000\n",
      "Loss on train= 0.013659334741532803\n",
      "Loss on test= 0.022988250479102135\n",
      "acc for Lsat= 0.07040381242468507 \n",
      "acc for Psat= 0.09970082440645592 \n",
      "acc for optim= 0.15171863088180057\n",
      "Epoch:652/1000\n",
      "Loss on train= 0.013335751369595528\n",
      "Loss on test= 0.022455299273133278\n",
      "acc for Lsat= 0.0706515326848458 \n",
      "acc for Psat= 0.10448649308808222 \n",
      "acc for optim= 0.148094113918634\n",
      "Epoch:653/1000\n",
      "Loss on train= 0.013354643248021603\n",
      "Loss on test= 0.022103851661086082\n",
      "acc for Lsat= 0.06905323765008552 \n",
      "acc for Psat= 0.10185949216253336 \n",
      "acc for optim= 0.15008496343099398\n",
      "Epoch:654/1000\n",
      "Loss on train= 0.01281063910573721\n",
      "Loss on test= 0.021699387580156326\n",
      "acc for Lsat= 0.06992664375772507 \n",
      "acc for Psat= 0.10275891477681474 \n",
      "acc for optim= 0.14990396323394142\n",
      "Epoch:655/1000\n",
      "Loss on train= 0.013171867467463017\n",
      "Loss on test= 0.02241380512714386\n",
      "acc for Lsat= 0.06768861435774552 \n",
      "acc for Psat= 0.09990978547901017 \n",
      "acc for optim= 0.15125062608243625\n",
      "Epoch:656/1000\n",
      "Loss on train= 0.012785383500158787\n",
      "Loss on test= 0.022656649351119995\n",
      "acc for Lsat= 0.07035973678989661 \n",
      "acc for Psat= 0.10193729353109467 \n",
      "acc for optim= 0.15148289055127237\n",
      "Epoch:657/1000\n",
      "Loss on train= 0.013684921897947788\n",
      "Loss on test= 0.021682925522327423\n",
      "acc for Lsat= 0.06875242180998539 \n",
      "acc for Psat= 0.10010151411607814 \n",
      "acc for optim= 0.14976536908815072\n",
      "Epoch:658/1000\n",
      "Loss on train= 0.013687139376997948\n",
      "Loss on test= 0.022730285301804543\n",
      "acc for Lsat= 0.08072095586611978 \n",
      "acc for Psat= 0.11027072178961035 \n",
      "acc for optim= 0.15087996547008273\n",
      "Epoch:659/1000\n",
      "Loss on train= 0.013355395756661892\n",
      "Loss on test= 0.02259347215294838\n",
      "acc for Lsat= 0.07004539716085326 \n",
      "acc for Psat= 0.10379023363819946 \n",
      "acc for optim= 0.1516545461657831\n",
      "Epoch:660/1000\n",
      "Loss on train= 0.013057793490588665\n",
      "Loss on test= 0.0225753802806139\n",
      "acc for Lsat= 0.07164688544019908 \n",
      "acc for Psat= 0.09966662609299948 \n",
      "acc for optim= 0.15011242436016117\n",
      "Epoch:661/1000\n",
      "Loss on train= 0.013128147460520267\n",
      "Loss on test= 0.022950829938054085\n",
      "acc for Lsat= 0.06880653793629618 \n",
      "acc for Psat= 0.09904519233394697 \n",
      "acc for optim= 0.15085985420946263\n",
      "Epoch:662/1000\n",
      "Loss on train= 0.012896317057311535\n",
      "Loss on test= 0.022642256692051888\n",
      "acc for Lsat= 0.07737314557514315 \n",
      "acc for Psat= 0.1077971116846978 \n",
      "acc for optim= 0.15143766868550115\n",
      "Epoch:663/1000\n",
      "Loss on train= 0.012556107714772224\n",
      "Loss on test= 0.02205670438706875\n",
      "acc for Lsat= 0.06789819692258421 \n",
      "acc for Psat= 0.09883522523994066 \n",
      "acc for optim= 0.15062021147769153\n",
      "Epoch:664/1000\n",
      "Loss on train= 0.013471168465912342\n",
      "Loss on test= 0.02167258970439434\n",
      "acc for Lsat= 0.07833578997474176 \n",
      "acc for Psat= 0.11043143088239371 \n",
      "acc for optim= 0.15067515727689496\n",
      "Epoch:665/1000\n",
      "Loss on train= 0.013477307744324207\n",
      "Loss on test= 0.02260674349963665\n",
      "acc for Lsat= 0.06975027478216496 \n",
      "acc for Psat= 0.10021609295642256 \n",
      "acc for optim= 0.15128619952059272\n",
      "Epoch:666/1000\n",
      "Loss on train= 0.013311986811459064\n",
      "Loss on test= 0.021881060674786568\n",
      "acc for Lsat= 0.0682182665382113 \n",
      "acc for Psat= 0.09871827716447193 \n",
      "acc for optim= 0.14982990887870024\n",
      "Epoch:667/1000\n",
      "Loss on train= 0.012629983946681023\n",
      "Loss on test= 0.022254684939980507\n",
      "acc for Lsat= 0.06758174265539925 \n",
      "acc for Psat= 0.10088231533864804 \n",
      "acc for optim= 0.1499134442734956\n",
      "Epoch:668/1000\n",
      "Loss on train= 0.012921846471726894\n",
      "Loss on test= 0.02223713882267475\n",
      "acc for Lsat= 0.06691731581854267 \n",
      "acc for Psat= 0.10066628982854446 \n",
      "acc for optim= 0.14948079982073204\n",
      "Epoch:669/1000\n",
      "Loss on train= 0.013424095697700977\n",
      "Loss on test= 0.022264277562499046\n",
      "acc for Lsat= 0.06699146359069798 \n",
      "acc for Psat= 0.10192926185471672 \n",
      "acc for optim= 0.1511795810290745\n",
      "Epoch:670/1000\n",
      "Loss on train= 0.012827700935304165\n",
      "Loss on test= 0.02205132693052292\n",
      "acc for Lsat= 0.06663939417200629 \n",
      "acc for Psat= 0.09937349052920295 \n",
      "acc for optim= 0.15027346240721665\n",
      "Epoch:671/1000\n",
      "Loss on train= 0.013045037165284157\n",
      "Loss on test= 0.022114615887403488\n",
      "acc for Lsat= 0.07018692766709184 \n",
      "acc for Psat= 0.09709769057673077 \n",
      "acc for optim= 0.14968834788696314\n",
      "Epoch:672/1000\n",
      "Loss on train= 0.013150474056601524\n",
      "Loss on test= 0.02278515323996544\n",
      "acc for Lsat= 0.07068661390942989 \n",
      "acc for Psat= 0.09973307814312928 \n",
      "acc for optim= 0.1507644489358034\n",
      "Epoch:673/1000\n",
      "Loss on train= 0.013208834454417229\n",
      "Loss on test= 0.022341236472129822\n",
      "acc for Lsat= 0.06862193665829211 \n",
      "acc for Psat= 0.1007803450985208 \n",
      "acc for optim= 0.14914625653396807\n",
      "Epoch:674/1000\n",
      "Loss on train= 0.013564066961407661\n",
      "Loss on test= 0.022704917937517166\n",
      "acc for Lsat= 0.07525231528321771 \n",
      "acc for Psat= 0.10053697866062784 \n",
      "acc for optim= 0.15157167210531397\n",
      "Epoch:675/1000\n",
      "Loss on train= 0.01334329042583704\n",
      "Loss on test= 0.022344736382365227\n",
      "acc for Lsat= 0.07040913674324453 \n",
      "acc for Psat= 0.10067066522531731 \n",
      "acc for optim= 0.15061358916403053\n",
      "Epoch:676/1000\n",
      "Loss on train= 0.012955903075635433\n",
      "Loss on test= 0.02200842648744583\n",
      "acc for Lsat= 0.0719128694150139 \n",
      "acc for Psat= 0.09988287942750117 \n",
      "acc for optim= 0.15041667595258185\n",
      "Epoch:677/1000\n",
      "Loss on train= 0.014091410674154758\n",
      "Loss on test= 0.0222962386906147\n",
      "acc for Lsat= 0.06868270089063931 \n",
      "acc for Psat= 0.10139007954502421 \n",
      "acc for optim= 0.1510709178408119\n",
      "Epoch:678/1000\n",
      "Loss on train= 0.013123063370585442\n",
      "Loss on test= 0.022268332540988922\n",
      "acc for Lsat= 0.07321927828646184 \n",
      "acc for Psat= 0.10239901739695541 \n",
      "acc for optim= 0.15049089993353307\n",
      "Epoch:679/1000\n",
      "Loss on train= 0.012889106757938862\n",
      "Loss on test= 0.02194267138838768\n",
      "acc for Lsat= 0.07003922958508678 \n",
      "acc for Psat= 0.10014242820961532 \n",
      "acc for optim= 0.1514608851896964\n",
      "Epoch:680/1000\n",
      "Loss on train= 0.013018217869102955\n",
      "Loss on test= 0.02324342541396618\n",
      "acc for Lsat= 0.06906357007565292 \n",
      "acc for Psat= 0.10265107693466236 \n",
      "acc for optim= 0.15086646456259034\n",
      "Epoch:681/1000\n",
      "Loss on train= 0.014056759886443615\n",
      "Loss on test= 0.022276800125837326\n",
      "acc for Lsat= 0.07445044602666583 \n",
      "acc for Psat= 0.10074963162705748 \n",
      "acc for optim= 0.15300337729659985\n",
      "Epoch:682/1000\n",
      "Loss on train= 0.012752415612339973\n",
      "Loss on test= 0.022690419107675552\n",
      "acc for Lsat= 0.06784106815772199 \n",
      "acc for Psat= 0.098683450398255 \n",
      "acc for optim= 0.1512535847302687\n",
      "Epoch:683/1000\n",
      "Loss on train= 0.013335230760276318\n",
      "Loss on test= 0.022158553823828697\n",
      "acc for Lsat= 0.07736600305748935 \n",
      "acc for Psat= 0.10203914386885508 \n",
      "acc for optim= 0.15167566405182267\n",
      "Epoch:684/1000\n",
      "Loss on train= 0.012642664834856987\n",
      "Loss on test= 0.022057147696614265\n",
      "acc for Lsat= 0.07257239492826684 \n",
      "acc for Psat= 0.10674237993072436 \n",
      "acc for optim= 0.15229078630276296\n",
      "Epoch:685/1000\n",
      "Loss on train= 0.013458718545734882\n",
      "Loss on test= 0.02219303883612156\n",
      "acc for Lsat= 0.08054980367323092 \n",
      "acc for Psat= 0.11324009194326559 \n",
      "acc for optim= 0.15107651594469318\n",
      "Epoch:686/1000\n",
      "Loss on train= 0.01316688023507595\n",
      "Loss on test= 0.022626787424087524\n",
      "acc for Lsat= 0.09002089573695414 \n",
      "acc for Psat= 0.1208387511909206 \n",
      "acc for optim= 0.1514561883040837\n",
      "Epoch:687/1000\n",
      "Loss on train= 0.01368616707623005\n",
      "Loss on test= 0.022271454334259033\n",
      "acc for Lsat= 0.07018314472266605 \n",
      "acc for Psat= 0.10269616212559696 \n",
      "acc for optim= 0.15055362085171317\n",
      "Epoch:688/1000\n",
      "Loss on train= 0.012972196564078331\n",
      "Loss on test= 0.022280707955360413\n",
      "acc for Lsat= 0.07022267272108021 \n",
      "acc for Psat= 0.09930799918317317 \n",
      "acc for optim= 0.15085976343614316\n",
      "Epoch:689/1000\n",
      "Loss on train= 0.012919658794999123\n",
      "Loss on test= 0.021663540974259377\n",
      "acc for Lsat= 0.07193817654717405 \n",
      "acc for Psat= 0.10154106775391541 \n",
      "acc for optim= 0.14898080833726549\n",
      "Epoch:690/1000\n",
      "Loss on train= 0.013246140442788601\n",
      "Loss on test= 0.022129280492663383\n",
      "acc for Lsat= 0.06969095355648534 \n",
      "acc for Psat= 0.10111340719996102 \n",
      "acc for optim= 0.14986344938262353\n",
      "Epoch:691/1000\n",
      "Loss on train= 0.013151166029274464\n",
      "Loss on test= 0.022029096260666847\n",
      "acc for Lsat= 0.06943040494110893 \n",
      "acc for Psat= 0.09977512777445723 \n",
      "acc for optim= 0.15152059900404205\n",
      "Epoch:692/1000\n",
      "Loss on train= 0.012846649624407291\n",
      "Loss on test= 0.022225767374038696\n",
      "acc for Lsat= 0.07247927851257133 \n",
      "acc for Psat= 0.10527475097052677 \n",
      "acc for optim= 0.15095765873839292\n",
      "Epoch:693/1000\n",
      "Loss on train= 0.01329532265663147\n",
      "Loss on test= 0.022740717977285385\n",
      "acc for Lsat= 0.07666593453416794 \n",
      "acc for Psat= 0.10004645409774149 \n",
      "acc for optim= 0.15205572807907666\n",
      "Epoch:694/1000\n",
      "Loss on train= 0.012499338947236538\n",
      "Loss on test= 0.02272002212703228\n",
      "acc for Lsat= 0.06744331744224129 \n",
      "acc for Psat= 0.0986773478232349 \n",
      "acc for optim= 0.15073957934332055\n",
      "Epoch:695/1000\n",
      "Loss on train= 0.012589679099619389\n",
      "Loss on test= 0.022418413311243057\n",
      "acc for Lsat= 0.07420998845385554 \n",
      "acc for Psat= 0.10308230790584984 \n",
      "acc for optim= 0.15027289915322467\n",
      "Epoch:696/1000\n",
      "Loss on train= 0.012625274248421192\n",
      "Loss on test= 0.021732062101364136\n",
      "acc for Lsat= 0.07300627950418033 \n",
      "acc for Psat= 0.09903946340678142 \n",
      "acc for optim= 0.15147477519076527\n",
      "Epoch:697/1000\n",
      "Loss on train= 0.012137175537645817\n",
      "Loss on test= 0.02183683030307293\n",
      "acc for Lsat= 0.06686965759806457 \n",
      "acc for Psat= 0.0993898507368525 \n",
      "acc for optim= 0.14961841351962168\n",
      "Epoch:698/1000\n",
      "Loss on train= 0.012988029979169369\n",
      "Loss on test= 0.02186744660139084\n",
      "acc for Lsat= 0.06676302029444925 \n",
      "acc for Psat= 0.09887909243669225 \n",
      "acc for optim= 0.14983362359461994\n",
      "Epoch:699/1000\n",
      "Loss on train= 0.012885330244898796\n",
      "Loss on test= 0.02196255885064602\n",
      "acc for Lsat= 0.06917993207508544 \n",
      "acc for Psat= 0.0975800322139778 \n",
      "acc for optim= 0.14946165672964432\n",
      "Epoch:700/1000\n",
      "Loss on train= 0.012701360508799553\n",
      "Loss on test= 0.021528607234358788\n",
      "acc for Lsat= 0.07197843206879309 \n",
      "acc for Psat= 0.09984987162276361 \n",
      "acc for optim= 0.15074817647173555\n",
      "Epoch:701/1000\n",
      "Loss on train= 0.013202912174165249\n",
      "Loss on test= 0.02222464606165886\n",
      "acc for Lsat= 0.07230211799326926 \n",
      "acc for Psat= 0.1025335504169084 \n",
      "acc for optim= 0.1514011991380457\n",
      "Epoch:702/1000\n",
      "Loss on train= 0.012800545431673527\n",
      "Loss on test= 0.023100292310118675\n",
      "acc for Lsat= 0.0768224356677445 \n",
      "acc for Psat= 0.10288669979849528 \n",
      "acc for optim= 0.1507537062183963\n",
      "Epoch:703/1000\n",
      "Loss on train= 0.01277188491076231\n",
      "Loss on test= 0.022854581475257874\n",
      "acc for Lsat= 0.07988346873525766 \n",
      "acc for Psat= 0.10583272562470544 \n",
      "acc for optim= 0.15025264157409288\n",
      "Epoch:704/1000\n",
      "Loss on train= 0.013732561841607094\n",
      "Loss on test= 0.022693749517202377\n",
      "acc for Lsat= 0.06901995570358642 \n",
      "acc for Psat= 0.09836006089302393 \n",
      "acc for optim= 0.15374318295935072\n",
      "Epoch:705/1000\n",
      "Loss on train= 0.0129753602668643\n",
      "Loss on test= 0.022563345730304718\n",
      "acc for Lsat= 0.07005374290024323 \n",
      "acc for Psat= 0.09954869350127603 \n",
      "acc for optim= 0.15122096392007364\n",
      "Epoch:706/1000\n",
      "Loss on train= 0.013130511157214642\n",
      "Loss on test= 0.022593380883336067\n",
      "acc for Lsat= 0.06704849963172331 \n",
      "acc for Psat= 0.1008242423550235 \n",
      "acc for optim= 0.14930950359648645\n",
      "Epoch:707/1000\n",
      "Loss on train= 0.012889952398836613\n",
      "Loss on test= 0.021924221888184547\n",
      "acc for Lsat= 0.07104295263456743 \n",
      "acc for Psat= 0.10343101323641019 \n",
      "acc for optim= 0.15040045036033933\n",
      "Epoch:708/1000\n",
      "Loss on train= 0.012437083758413792\n",
      "Loss on test= 0.022339053452014923\n",
      "acc for Lsat= 0.06905835731282979 \n",
      "acc for Psat= 0.10217509382586942 \n",
      "acc for optim= 0.15083579274507058\n",
      "Epoch:709/1000\n",
      "Loss on train= 0.012892910279333591\n",
      "Loss on test= 0.02186867780983448\n",
      "acc for Lsat= 0.06913295457529467 \n",
      "acc for Psat= 0.10201118503298079 \n",
      "acc for optim= 0.1518905162613257\n",
      "Epoch:710/1000\n",
      "Loss on train= 0.013091033324599266\n",
      "Loss on test= 0.02185678854584694\n",
      "acc for Lsat= 0.07038482803838991 \n",
      "acc for Psat= 0.10252723771076266 \n",
      "acc for optim= 0.15123258454855099\n",
      "Epoch:711/1000\n",
      "Loss on train= 0.013357560150325298\n",
      "Loss on test= 0.023010190576314926\n",
      "acc for Lsat= 0.06685890922712723 \n",
      "acc for Psat= 0.09907972347300709 \n",
      "acc for optim= 0.15026286751328904\n",
      "Epoch:712/1000\n",
      "Loss on train= 0.01311207003891468\n",
      "Loss on test= 0.02256552316248417\n",
      "acc for Lsat= 0.07094329860915378 \n",
      "acc for Psat= 0.09862120367363837 \n",
      "acc for optim= 0.15110265893793579\n",
      "Epoch:713/1000\n",
      "Loss on train= 0.012247628532350063\n",
      "Loss on test= 0.022106384858489037\n",
      "acc for Lsat= 0.06656010463784304 \n",
      "acc for Psat= 0.09973057468864213 \n",
      "acc for optim= 0.15012873728013915\n",
      "Epoch:714/1000\n",
      "Loss on train= 0.01333015225827694\n",
      "Loss on test= 0.021838484331965446\n",
      "acc for Lsat= 0.06973733350486058 \n",
      "acc for Psat= 0.10034679515021185 \n",
      "acc for optim= 0.1515725867890836\n",
      "Epoch:715/1000\n",
      "Loss on train= 0.012778607197105885\n",
      "Loss on test= 0.02186896838247776\n",
      "acc for Lsat= 0.06933004160062022 \n",
      "acc for Psat= 0.10414479251121764 \n",
      "acc for optim= 0.15116290137221253\n",
      "Epoch:716/1000\n",
      "Loss on train= 0.012402809225022793\n",
      "Loss on test= 0.021878056228160858\n",
      "acc for Lsat= 0.07025017895769833 \n",
      "acc for Psat= 0.10113171341015256 \n",
      "acc for optim= 0.15068858275777872\n",
      "Epoch:717/1000\n",
      "Loss on train= 0.012864222750067711\n",
      "Loss on test= 0.02272510528564453\n",
      "acc for Lsat= 0.06883464196195634 \n",
      "acc for Psat= 0.10102360662431809 \n",
      "acc for optim= 0.15033752817252147\n",
      "Epoch:718/1000\n",
      "Loss on train= 0.012594310566782951\n",
      "Loss on test= 0.022627275437116623\n",
      "acc for Lsat= 0.0717333658391059 \n",
      "acc for Psat= 0.09958077590924955 \n",
      "acc for optim= 0.15015964438748913\n",
      "Epoch:719/1000\n",
      "Loss on train= 0.012985631823539734\n",
      "Loss on test= 0.022683031857013702\n",
      "acc for Lsat= 0.07003040721646178 \n",
      "acc for Psat= 0.10005603436615775 \n",
      "acc for optim= 0.15152383959570598\n",
      "Epoch:720/1000\n",
      "Loss on train= 0.012426698580384254\n",
      "Loss on test= 0.021812044084072113\n",
      "acc for Lsat= 0.07588385770289212 \n",
      "acc for Psat= 0.10175763564648424 \n",
      "acc for optim= 0.15115944353052943\n",
      "Epoch:721/1000\n",
      "Loss on train= 0.013047786429524422\n",
      "Loss on test= 0.022825246676802635\n",
      "acc for Lsat= 0.07160631126145589 \n",
      "acc for Psat= 0.10016993810964182 \n",
      "acc for optim= 0.15114144029411367\n",
      "Epoch:722/1000\n",
      "Loss on train= 0.01216187048703432\n",
      "Loss on test= 0.022504324093461037\n",
      "acc for Lsat= 0.07836982330610587 \n",
      "acc for Psat= 0.10609337109267915 \n",
      "acc for optim= 0.15083871341226895\n",
      "Epoch:723/1000\n",
      "Loss on train= 0.012721163220703602\n",
      "Loss on test= 0.022270115092396736\n",
      "acc for Lsat= 0.07438734718335427 \n",
      "acc for Psat= 0.10477349815574594 \n",
      "acc for optim= 0.15051369829431321\n",
      "Epoch:724/1000\n",
      "Loss on train= 0.01232911180704832\n",
      "Loss on test= 0.023127365857362747\n",
      "acc for Lsat= 0.06993373924513593 \n",
      "acc for Psat= 0.10247296001824033 \n",
      "acc for optim= 0.150369734403699\n",
      "Epoch:725/1000\n",
      "Loss on train= 0.012307380326092243\n",
      "Loss on test= 0.022108184173703194\n",
      "acc for Lsat= 0.07223843055904111 \n",
      "acc for Psat= 0.099352204027366 \n",
      "acc for optim= 0.1506981147088086\n",
      "Epoch:726/1000\n",
      "Loss on train= 0.012742040678858757\n",
      "Loss on test= 0.021350262686610222\n",
      "acc for Lsat= 0.07069092972136017 \n",
      "acc for Psat= 0.09830664625001506 \n",
      "acc for optim= 0.1501611419096342\n",
      "Epoch:727/1000\n",
      "Loss on train= 0.012205200269818306\n",
      "Loss on test= 0.02275259606540203\n",
      "acc for Lsat= 0.06802364445207922 \n",
      "acc for Psat= 0.10135245939029806 \n",
      "acc for optim= 0.15065327413454396\n",
      "Epoch:728/1000\n",
      "Loss on train= 0.013551999814808369\n",
      "Loss on test= 0.022465674206614494\n",
      "acc for Lsat= 0.07637419251112446 \n",
      "acc for Psat= 0.09994061256563941 \n",
      "acc for optim= 0.15106236704956258\n",
      "Epoch:729/1000\n",
      "Loss on train= 0.01275467686355114\n",
      "Loss on test= 0.02197483740746975\n",
      "acc for Lsat= 0.07070281822420038 \n",
      "acc for Psat= 0.10295010300173713 \n",
      "acc for optim= 0.15021950867881018\n",
      "Epoch:730/1000\n",
      "Loss on train= 0.013124854303896427\n",
      "Loss on test= 0.02244039997458458\n",
      "acc for Lsat= 0.07509062563462116 \n",
      "acc for Psat= 0.1052902466831017 \n",
      "acc for optim= 0.1500903162449301\n",
      "Epoch:731/1000\n",
      "Loss on train= 0.012709914706647396\n",
      "Loss on test= 0.023008711636066437\n",
      "acc for Lsat= 0.07384973612932667 \n",
      "acc for Psat= 0.10061854559717781 \n",
      "acc for optim= 0.15062470527186345\n",
      "Epoch:732/1000\n",
      "Loss on train= 0.012616568244993687\n",
      "Loss on test= 0.02227657660841942\n",
      "acc for Lsat= 0.07404032732759205 \n",
      "acc for Psat= 0.10215546421434399 \n",
      "acc for optim= 0.15187761633103075\n",
      "Epoch:733/1000\n",
      "Loss on train= 0.012313866056501865\n",
      "Loss on test= 0.022724615409970284\n",
      "acc for Lsat= 0.07014282566566406 \n",
      "acc for Psat= 0.09999122690916853 \n",
      "acc for optim= 0.1516674640764826\n",
      "Epoch:734/1000\n",
      "Loss on train= 0.012427620589733124\n",
      "Loss on test= 0.022436492145061493\n",
      "acc for Lsat= 0.06954171076367462 \n",
      "acc for Psat= 0.10529483904276178 \n",
      "acc for optim= 0.15095064475686845\n",
      "Epoch:735/1000\n",
      "Loss on train= 0.012613952159881592\n",
      "Loss on test= 0.022467007860541344\n",
      "acc for Lsat= 0.07128199976544047 \n",
      "acc for Psat= 0.09924060595391992 \n",
      "acc for optim= 0.15066045170210524\n",
      "Epoch:736/1000\n",
      "Loss on train= 0.01298880111426115\n",
      "Loss on test= 0.021465033292770386\n",
      "acc for Lsat= 0.0686562188538998 \n",
      "acc for Psat= 0.09774841701865593 \n",
      "acc for optim= 0.1508970470523518\n",
      "Epoch:737/1000\n",
      "Loss on train= 0.012936652638018131\n",
      "Loss on test= 0.02159540355205536\n",
      "acc for Lsat= 0.0743385987028331 \n",
      "acc for Psat= 0.1015529469002125 \n",
      "acc for optim= 0.1503384546782091\n",
      "Epoch:738/1000\n",
      "Loss on train= 0.012226840481162071\n",
      "Loss on test= 0.02149403840303421\n",
      "acc for Lsat= 0.07395148965409427 \n",
      "acc for Psat= 0.10449255835574328 \n",
      "acc for optim= 0.15113483720839618\n",
      "Epoch:739/1000\n",
      "Loss on train= 0.012362778186798096\n",
      "Loss on test= 0.022265898063778877\n",
      "acc for Lsat= 0.07104074458941273 \n",
      "acc for Psat= 0.10265962560707549 \n",
      "acc for optim= 0.14991703102754994\n",
      "Epoch:740/1000\n",
      "Loss on train= 0.012376943603157997\n",
      "Loss on test= 0.022544028237462044\n",
      "acc for Lsat= 0.07681160094928108 \n",
      "acc for Psat= 0.11123105962807157 \n",
      "acc for optim= 0.1515767468962559\n",
      "Epoch:741/1000\n",
      "Loss on train= 0.012049408629536629\n",
      "Loss on test= 0.023049060255289078\n",
      "acc for Lsat= 0.07219909543412867 \n",
      "acc for Psat= 0.09993803524891798 \n",
      "acc for optim= 0.15008284322843202\n",
      "Epoch:742/1000\n",
      "Loss on train= 0.012244248762726784\n",
      "Loss on test= 0.021860396489501\n",
      "acc for Lsat= 0.07494747938705837 \n",
      "acc for Psat= 0.1001045885672205 \n",
      "acc for optim= 0.1511527912363261\n",
      "Epoch:743/1000\n",
      "Loss on train= 0.012605028226971626\n",
      "Loss on test= 0.022897224873304367\n",
      "acc for Lsat= 0.07337695173052854 \n",
      "acc for Psat= 0.10020195115048226 \n",
      "acc for optim= 0.1512835932134394\n",
      "Epoch:744/1000\n",
      "Loss on train= 0.012719101272523403\n",
      "Loss on test= 0.022486256435513496\n",
      "acc for Lsat= 0.06789223149765371 \n",
      "acc for Psat= 0.09936445755419937 \n",
      "acc for optim= 0.1528790813347826\n",
      "Epoch:745/1000\n",
      "Loss on train= 0.01251655351370573\n",
      "Loss on test= 0.02293042652308941\n",
      "acc for Lsat= 0.06978314199122874 \n",
      "acc for Psat= 0.10356918864471969 \n",
      "acc for optim= 0.15319831694083355\n",
      "Epoch:746/1000\n",
      "Loss on train= 0.012172234244644642\n",
      "Loss on test= 0.02213277667760849\n",
      "acc for Lsat= 0.06798160175152396 \n",
      "acc for Psat= 0.09772006297032303 \n",
      "acc for optim= 0.1535214221160673\n",
      "Epoch:747/1000\n",
      "Loss on train= 0.012419464066624641\n",
      "Loss on test= 0.023082619532942772\n",
      "acc for Lsat= 0.06656256136902147 \n",
      "acc for Psat= 0.09917106457128874 \n",
      "acc for optim= 0.15341849590456766\n",
      "Epoch:748/1000\n",
      "Loss on train= 0.012363324873149395\n",
      "Loss on test= 0.022068748250603676\n",
      "acc for Lsat= 0.07203724423912279 \n",
      "acc for Psat= 0.10016268283821818 \n",
      "acc for optim= 0.15108477293058883\n",
      "Epoch:749/1000\n",
      "Loss on train= 0.01290441956371069\n",
      "Loss on test= 0.022309647873044014\n",
      "acc for Lsat= 0.07093592140563701 \n",
      "acc for Psat= 0.10028902581363819 \n",
      "acc for optim= 0.15112296594337768\n",
      "Epoch:750/1000\n",
      "Loss on train= 0.012149105779826641\n",
      "Loss on test= 0.02177301049232483\n",
      "acc for Lsat= 0.06900038153824221 \n",
      "acc for Psat= 0.10041172680664695 \n",
      "acc for optim= 0.15062524909988986\n",
      "Epoch:751/1000\n",
      "Loss on train= 0.012582245282828808\n",
      "Loss on test= 0.02253931388258934\n",
      "acc for Lsat= 0.07211282639408427 \n",
      "acc for Psat= 0.09939888305838324 \n",
      "acc for optim= 0.1507201426052968\n",
      "Epoch:752/1000\n",
      "Loss on train= 0.012524555437266827\n",
      "Loss on test= 0.022313831374049187\n",
      "acc for Lsat= 0.06796454322892566 \n",
      "acc for Psat= 0.09583835500815382 \n",
      "acc for optim= 0.1508703024284388\n",
      "Epoch:753/1000\n",
      "Loss on train= 0.012933192774653435\n",
      "Loss on test= 0.021891910582780838\n",
      "acc for Lsat= 0.06879538431318098 \n",
      "acc for Psat= 0.09927955703481882 \n",
      "acc for optim= 0.15151241962299783\n",
      "Epoch:754/1000\n",
      "Loss on train= 0.012402678839862347\n",
      "Loss on test= 0.022832104936242104\n",
      "acc for Lsat= 0.06906816198976336 \n",
      "acc for Psat= 0.10082814839987259 \n",
      "acc for optim= 0.15179587467960343\n",
      "Epoch:755/1000\n",
      "Loss on train= 0.012759781442582607\n",
      "Loss on test= 0.022985579445958138\n",
      "acc for Lsat= 0.06667711894179501 \n",
      "acc for Psat= 0.09761374769020714 \n",
      "acc for optim= 0.15085696350300426\n",
      "Epoch:756/1000\n",
      "Loss on train= 0.01233178935945034\n",
      "Loss on test= 0.021495528519153595\n",
      "acc for Lsat= 0.06691925570220253 \n",
      "acc for Psat= 0.09912681862761413 \n",
      "acc for optim= 0.15209435462159576\n",
      "Epoch:757/1000\n",
      "Loss on train= 0.012098684906959534\n",
      "Loss on test= 0.02245001122355461\n",
      "acc for Lsat= 0.06724190491180483 \n",
      "acc for Psat= 0.09833776331027083 \n",
      "acc for optim= 0.15200487002977892\n",
      "Epoch:758/1000\n",
      "Loss on train= 0.01275816559791565\n",
      "Loss on test= 0.021762318909168243\n",
      "acc for Lsat= 0.06758021259822718 \n",
      "acc for Psat= 0.09633315912512849 \n",
      "acc for optim= 0.15123796930344788\n",
      "Epoch:759/1000\n",
      "Loss on train= 0.012498252093791962\n",
      "Loss on test= 0.022784072905778885\n",
      "acc for Lsat= 0.0724470759745056 \n",
      "acc for Psat= 0.09774571704706085 \n",
      "acc for optim= 0.15295911969932607\n",
      "Epoch:760/1000\n",
      "Loss on train= 0.012660549022257328\n",
      "Loss on test= 0.022228075191378593\n",
      "acc for Lsat= 0.0687909120341076 \n",
      "acc for Psat= 0.09835809948436448 \n",
      "acc for optim= 0.1517724629454438\n",
      "Epoch:761/1000\n",
      "Loss on train= 0.012269954197108746\n",
      "Loss on test= 0.021409155800938606\n",
      "acc for Lsat= 0.06805880433103176 \n",
      "acc for Psat= 0.09646137879536398 \n",
      "acc for optim= 0.1506454181037472\n",
      "Epoch:762/1000\n",
      "Loss on train= 0.01219099760055542\n",
      "Loss on test= 0.022928887978196144\n",
      "acc for Lsat= 0.06869309928726121 \n",
      "acc for Psat= 0.09927310113891019 \n",
      "acc for optim= 0.15172057397341807\n",
      "Epoch:763/1000\n",
      "Loss on train= 0.01234530657529831\n",
      "Loss on test= 0.02211414836347103\n",
      "acc for Lsat= 0.06657378123052095 \n",
      "acc for Psat= 0.09801590535727843 \n",
      "acc for optim= 0.1518113556296327\n",
      "Epoch:764/1000\n",
      "Loss on train= 0.012967241927981377\n",
      "Loss on test= 0.02237466536462307\n",
      "acc for Lsat= 0.06638021981003277 \n",
      "acc for Psat= 0.09737851809821653 \n",
      "acc for optim= 0.1511975690771971\n",
      "Epoch:765/1000\n",
      "Loss on train= 0.013305963017046452\n",
      "Loss on test= 0.021562756970524788\n",
      "acc for Lsat= 0.06861938238143919 \n",
      "acc for Psat= 0.09712889566770026 \n",
      "acc for optim= 0.15078990142606818\n",
      "Epoch:766/1000\n",
      "Loss on train= 0.012082035653293133\n",
      "Loss on test= 0.022099392488598824\n",
      "acc for Lsat= 0.06987332957131523 \n",
      "acc for Psat= 0.09556751065079949 \n",
      "acc for optim= 0.15039311166221517\n",
      "Epoch:767/1000\n",
      "Loss on train= 0.01217497419565916\n",
      "Loss on test= 0.02243749424815178\n",
      "acc for Lsat= 0.06507172248886271 \n",
      "acc for Psat= 0.09694890482877183 \n",
      "acc for optim= 0.15167187470138274\n",
      "Epoch:768/1000\n",
      "Loss on train= 0.012222463265061378\n",
      "Loss on test= 0.022180061787366867\n",
      "acc for Lsat= 0.07230126103293462 \n",
      "acc for Psat= 0.09864971043659605 \n",
      "acc for optim= 0.15072432154436838\n",
      "Epoch:769/1000\n",
      "Loss on train= 0.0125351557508111\n",
      "Loss on test= 0.0227047111839056\n",
      "acc for Lsat= 0.06802441381734867 \n",
      "acc for Psat= 0.09760398035033596 \n",
      "acc for optim= 0.15303664670830153\n",
      "Epoch:770/1000\n",
      "Loss on train= 0.011953109875321388\n",
      "Loss on test= 0.023026691749691963\n",
      "acc for Lsat= 0.068416570141862 \n",
      "acc for Psat= 0.0978657307220852 \n",
      "acc for optim= 0.1517420211701694\n",
      "Epoch:771/1000\n",
      "Loss on train= 0.012562165968120098\n",
      "Loss on test= 0.02220233529806137\n",
      "acc for Lsat= 0.06781099456489281 \n",
      "acc for Psat= 0.10199275828675178 \n",
      "acc for optim= 0.1525277894200677\n",
      "Epoch:772/1000\n",
      "Loss on train= 0.011855785734951496\n",
      "Loss on test= 0.022391105070710182\n",
      "acc for Lsat= 0.06707308136941585 \n",
      "acc for Psat= 0.09872740881387576 \n",
      "acc for optim= 0.15074045135333294\n",
      "Epoch:773/1000\n",
      "Loss on train= 0.01203942857682705\n",
      "Loss on test= 0.022414611652493477\n",
      "acc for Lsat= 0.06920965351535632 \n",
      "acc for Psat= 0.09875674146750439 \n",
      "acc for optim= 0.15217512998470042\n",
      "Epoch:774/1000\n",
      "Loss on train= 0.012684312649071217\n",
      "Loss on test= 0.022226743400096893\n",
      "acc for Lsat= 0.06821299548759016 \n",
      "acc for Psat= 0.10318162290756884 \n",
      "acc for optim= 0.15323134551808681\n",
      "Epoch:775/1000\n",
      "Loss on train= 0.012467438355088234\n",
      "Loss on test= 0.022600611671805382\n",
      "acc for Lsat= 0.06585831384722181 \n",
      "acc for Psat= 0.09599292573342687 \n",
      "acc for optim= 0.1522526891920654\n",
      "Epoch:776/1000\n",
      "Loss on train= 0.012226486578583717\n",
      "Loss on test= 0.02256150357425213\n",
      "acc for Lsat= 0.06631939816118476 \n",
      "acc for Psat= 0.09713564136891663 \n",
      "acc for optim= 0.15194865225557466\n",
      "Epoch:777/1000\n",
      "Loss on train= 0.01256119180470705\n",
      "Loss on test= 0.021894561126828194\n",
      "acc for Lsat= 0.06710609377816665 \n",
      "acc for Psat= 0.09790286253457053 \n",
      "acc for optim= 0.14992469509574669\n",
      "Epoch:778/1000\n",
      "Loss on train= 0.012281552888453007\n",
      "Loss on test= 0.021686729043722153\n",
      "acc for Lsat= 0.06714908517278309 \n",
      "acc for Psat= 0.09806661708806441 \n",
      "acc for optim= 0.15001012478951997\n",
      "Epoch:779/1000\n",
      "Loss on train= 0.012547134421765804\n",
      "Loss on test= 0.022297507151961327\n",
      "acc for Lsat= 0.06606249937979486 \n",
      "acc for Psat= 0.09851602557489642 \n",
      "acc for optim= 0.14894356446408746\n",
      "Epoch:780/1000\n",
      "Loss on train= 0.011977693066000938\n",
      "Loss on test= 0.02157241851091385\n",
      "acc for Lsat= 0.0657959074079 \n",
      "acc for Psat= 0.09776950702318721 \n",
      "acc for optim= 0.15056164462701027\n",
      "Epoch:781/1000\n",
      "Loss on train= 0.011919953860342503\n",
      "Loss on test= 0.021922875195741653\n",
      "acc for Lsat= 0.06582765380211446 \n",
      "acc for Psat= 0.09740493813424411 \n",
      "acc for optim= 0.14979257797482004\n",
      "Epoch:782/1000\n",
      "Loss on train= 0.011934652924537659\n",
      "Loss on test= 0.021999727934598923\n",
      "acc for Lsat= 0.06912144052229847 \n",
      "acc for Psat= 0.10271020167293739 \n",
      "acc for optim= 0.15044059345492491\n",
      "Epoch:783/1000\n",
      "Loss on train= 0.012640324421226978\n",
      "Loss on test= 0.022524919360876083\n",
      "acc for Lsat= 0.06595718837853683 \n",
      "acc for Psat= 0.09737890333036252 \n",
      "acc for optim= 0.1498040690374533\n",
      "Epoch:784/1000\n",
      "Loss on train= 0.012156945653259754\n",
      "Loss on test= 0.02218891680240631\n",
      "acc for Lsat= 0.07035582027958079 \n",
      "acc for Psat= 0.10107779126626708 \n",
      "acc for optim= 0.1480123213953354\n",
      "Epoch:785/1000\n",
      "Loss on train= 0.0117607731372118\n",
      "Loss on test= 0.022053463384509087\n",
      "acc for Lsat= 0.06617745690765572 \n",
      "acc for Psat= 0.09776560751306657 \n",
      "acc for optim= 0.14925679144669204\n",
      "Epoch:786/1000\n",
      "Loss on train= 0.011873182840645313\n",
      "Loss on test= 0.021253712475299835\n",
      "acc for Lsat= 0.07326135240321933 \n",
      "acc for Psat= 0.10200884785763052 \n",
      "acc for optim= 0.14944186475981905\n",
      "Epoch:787/1000\n",
      "Loss on train= 0.012136456556618214\n",
      "Loss on test= 0.022381843999028206\n",
      "acc for Lsat= 0.06612211641085108 \n",
      "acc for Psat= 0.09627544786842956 \n",
      "acc for optim= 0.1504861259777284\n",
      "Epoch:788/1000\n",
      "Loss on train= 0.012446442618966103\n",
      "Loss on test= 0.022791074588894844\n",
      "acc for Lsat= 0.06603900287238466 \n",
      "acc for Psat= 0.09728263527451955 \n",
      "acc for optim= 0.15100114444561574\n",
      "Epoch:789/1000\n",
      "Loss on train= 0.012105901725590229\n",
      "Loss on test= 0.02219085767865181\n",
      "acc for Lsat= 0.06716612828926391 \n",
      "acc for Psat= 0.09740297263642878 \n",
      "acc for optim= 0.15107993507306042\n",
      "Epoch:790/1000\n",
      "Loss on train= 0.01160125620663166\n",
      "Loss on test= 0.022728364914655685\n",
      "acc for Lsat= 0.0666532858859661 \n",
      "acc for Psat= 0.09567526620487833 \n",
      "acc for optim= 0.15097299451843843\n",
      "Epoch:791/1000\n",
      "Loss on train= 0.011853900738060474\n",
      "Loss on test= 0.021956482902169228\n",
      "acc for Lsat= 0.06867420481288948 \n",
      "acc for Psat= 0.09832178427531474 \n",
      "acc for optim= 0.15185240079398168\n",
      "Epoch:792/1000\n",
      "Loss on train= 0.011934422887861729\n",
      "Loss on test= 0.022603165358304977\n",
      "acc for Lsat= 0.07218592454032645 \n",
      "acc for Psat= 0.09973158769037241 \n",
      "acc for optim= 0.1512518637798157\n",
      "Epoch:793/1000\n",
      "Loss on train= 0.012273130007088184\n",
      "Loss on test= 0.02302142046391964\n",
      "acc for Lsat= 0.06853576759959376 \n",
      "acc for Psat= 0.09750953827585493 \n",
      "acc for optim= 0.15124326679001618\n",
      "Epoch:794/1000\n",
      "Loss on train= 0.01248809415847063\n",
      "Loss on test= 0.02217567153275013\n",
      "acc for Lsat= 0.07209931838750044 \n",
      "acc for Psat= 0.09971816551645728 \n",
      "acc for optim= 0.15209316155839203\n",
      "Epoch:795/1000\n",
      "Loss on train= 0.012462608516216278\n",
      "Loss on test= 0.022360028699040413\n",
      "acc for Lsat= 0.06877335787215501 \n",
      "acc for Psat= 0.09695390108415851 \n",
      "acc for optim= 0.15212119401887406\n",
      "Epoch:796/1000\n",
      "Loss on train= 0.012114286422729492\n",
      "Loss on test= 0.022523002699017525\n",
      "acc for Lsat= 0.06905842765027106 \n",
      "acc for Psat= 0.0979359435876739 \n",
      "acc for optim= 0.15091199270910602\n",
      "Epoch:797/1000\n",
      "Loss on train= 0.01329406164586544\n",
      "Loss on test= 0.02231687307357788\n",
      "acc for Lsat= 0.06863063732254941 \n",
      "acc for Psat= 0.10589739763459494 \n",
      "acc for optim= 0.15046031492889134\n",
      "Epoch:798/1000\n",
      "Loss on train= 0.01190950907766819\n",
      "Loss on test= 0.022215761244297028\n",
      "acc for Lsat= 0.06982308088544992 \n",
      "acc for Psat= 0.1052139527377892 \n",
      "acc for optim= 0.15118337816178204\n",
      "Epoch:799/1000\n",
      "Loss on train= 0.01243682112544775\n",
      "Loss on test= 0.022499365732073784\n",
      "acc for Lsat= 0.07513675714450024 \n",
      "acc for Psat= 0.10230063760399424 \n",
      "acc for optim= 0.15077436328726354\n",
      "Epoch:800/1000\n",
      "Loss on train= 0.012651652097702026\n",
      "Loss on test= 0.023252181708812714\n",
      "acc for Lsat= 0.0749850147091273 \n",
      "acc for Psat= 0.10293647987501962 \n",
      "acc for optim= 0.15110531936452237\n",
      "Epoch:801/1000\n",
      "Loss on train= 0.011985999532043934\n",
      "Loss on test= 0.02212519198656082\n",
      "acc for Lsat= 0.07301274502396188 \n",
      "acc for Psat= 0.10613940821533586 \n",
      "acc for optim= 0.15069927232606073\n",
      "Epoch:802/1000\n",
      "Loss on train= 0.012352976016700268\n",
      "Loss on test= 0.022702548652887344\n",
      "acc for Lsat= 0.06864107333346457 \n",
      "acc for Psat= 0.09911039734995639 \n",
      "acc for optim= 0.15125557683234991\n",
      "Epoch:803/1000\n",
      "Loss on train= 0.011519601568579674\n",
      "Loss on test= 0.02205791510641575\n",
      "acc for Lsat= 0.06757554798031172 \n",
      "acc for Psat= 0.09916120949773694 \n",
      "acc for optim= 0.15168191119285912\n",
      "Epoch:804/1000\n",
      "Loss on train= 0.012313536368310452\n",
      "Loss on test= 0.021221403032541275\n",
      "acc for Lsat= 0.06878522050341102 \n",
      "acc for Psat= 0.09886277650677881 \n",
      "acc for optim= 0.1514709803155094\n",
      "Epoch:805/1000\n",
      "Loss on train= 0.011583109386265278\n",
      "Loss on test= 0.021869903430342674\n",
      "acc for Lsat= 0.06864963460998282 \n",
      "acc for Psat= 0.10064768830803145 \n",
      "acc for optim= 0.15142612924607482\n",
      "Epoch:806/1000\n",
      "Loss on train= 0.012537440285086632\n",
      "Loss on test= 0.021506842225790024\n",
      "acc for Lsat= 0.06893587719364419 \n",
      "acc for Psat= 0.09643748206553666 \n",
      "acc for optim= 0.15140601998547776\n",
      "Epoch:807/1000\n",
      "Loss on train= 0.011609809473156929\n",
      "Loss on test= 0.02234543301165104\n",
      "acc for Lsat= 0.0682891008168756 \n",
      "acc for Psat= 0.10190514371640658 \n",
      "acc for optim= 0.15279715385943948\n",
      "Epoch:808/1000\n",
      "Loss on train= 0.011916190385818481\n",
      "Loss on test= 0.02263117954134941\n",
      "acc for Lsat= 0.06959848551457108 \n",
      "acc for Psat= 0.09699411400132796 \n",
      "acc for optim= 0.15181243684996804\n",
      "Epoch:809/1000\n",
      "Loss on train= 0.01204106118530035\n",
      "Loss on test= 0.022426467388868332\n",
      "acc for Lsat= 0.06854426795462042 \n",
      "acc for Psat= 0.09733476496218049 \n",
      "acc for optim= 0.15044651380012997\n",
      "Epoch:810/1000\n",
      "Loss on train= 0.012492111884057522\n",
      "Loss on test= 0.021812772378325462\n",
      "acc for Lsat= 0.06743121609537309 \n",
      "acc for Psat= 0.09722123686657395 \n",
      "acc for optim= 0.15154891954703978\n",
      "Epoch:811/1000\n",
      "Loss on train= 0.012641837820410728\n",
      "Loss on test= 0.022969035431742668\n",
      "acc for Lsat= 0.06841650918076601 \n",
      "acc for Psat= 0.09713347621534354 \n",
      "acc for optim= 0.15130562386243449\n",
      "Epoch:812/1000\n",
      "Loss on train= 0.011779497377574444\n",
      "Loss on test= 0.022119397297501564\n",
      "acc for Lsat= 0.06756609245590195 \n",
      "acc for Psat= 0.10245988176906624 \n",
      "acc for optim= 0.1499325377006468\n",
      "Epoch:813/1000\n",
      "Loss on train= 0.011945279315114021\n",
      "Loss on test= 0.021834127604961395\n",
      "acc for Lsat= 0.06696074764395868 \n",
      "acc for Psat= 0.09686918805207922 \n",
      "acc for optim= 0.15192803595945284\n",
      "Epoch:814/1000\n",
      "Loss on train= 0.01199182216078043\n",
      "Loss on test= 0.02216973900794983\n",
      "acc for Lsat= 0.07119914283388079 \n",
      "acc for Psat= 0.09956783551314347 \n",
      "acc for optim= 0.15107575557952702\n",
      "Epoch:815/1000\n",
      "Loss on train= 0.01163398940116167\n",
      "Loss on test= 0.022310758009552956\n",
      "acc for Lsat= 0.06840312248250577 \n",
      "acc for Psat= 0.09996277749340401 \n",
      "acc for optim= 0.15301283370220775\n",
      "Epoch:816/1000\n",
      "Loss on train= 0.01219804398715496\n",
      "Loss on test= 0.022538697347044945\n",
      "acc for Lsat= 0.06841486313216316 \n",
      "acc for Psat= 0.09682072484216026 \n",
      "acc for optim= 0.15118490154561023\n",
      "Epoch:817/1000\n",
      "Loss on train= 0.01255632285028696\n",
      "Loss on test= 0.02309909649193287\n",
      "acc for Lsat= 0.06864973612599991 \n",
      "acc for Psat= 0.09834897233006172 \n",
      "acc for optim= 0.1520629173101381\n",
      "Epoch:818/1000\n",
      "Loss on train= 0.011761419475078583\n",
      "Loss on test= 0.021841391921043396\n",
      "acc for Lsat= 0.06809911706122844 \n",
      "acc for Psat= 0.0999971676704495 \n",
      "acc for optim= 0.15081592103175542\n",
      "Epoch:819/1000\n",
      "Loss on train= 0.0114377336576581\n",
      "Loss on test= 0.022077137604355812\n",
      "acc for Lsat= 0.06744811494287067 \n",
      "acc for Psat= 0.09560378298806985 \n",
      "acc for optim= 0.15221494743198258\n",
      "Epoch:820/1000\n",
      "Loss on train= 0.012063457630574703\n",
      "Loss on test= 0.0225042924284935\n",
      "acc for Lsat= 0.06665051012142158 \n",
      "acc for Psat= 0.09817313670716014 \n",
      "acc for optim= 0.15093622738340765\n",
      "Epoch:821/1000\n",
      "Loss on train= 0.012223335914313793\n",
      "Loss on test= 0.02216433361172676\n",
      "acc for Lsat= 0.06966525291485645 \n",
      "acc for Psat= 0.10036872706540005 \n",
      "acc for optim= 0.15137224543926325\n",
      "Epoch:822/1000\n",
      "Loss on train= 0.012168790213763714\n",
      "Loss on test= 0.021744800731539726\n",
      "acc for Lsat= 0.07030141739750224 \n",
      "acc for Psat= 0.09640493070168356 \n",
      "acc for optim= 0.15287596965549002\n",
      "Epoch:823/1000\n",
      "Loss on train= 0.01167235430330038\n",
      "Loss on test= 0.02231534756720066\n",
      "acc for Lsat= 0.06795557460119558 \n",
      "acc for Psat= 0.09751863596447283 \n",
      "acc for optim= 0.15098132247544604\n",
      "Epoch:824/1000\n",
      "Loss on train= 0.012482525780797005\n",
      "Loss on test= 0.0215426217764616\n",
      "acc for Lsat= 0.07071986295456109 \n",
      "acc for Psat= 0.0959364584910117 \n",
      "acc for optim= 0.1518085590034624\n",
      "Epoch:825/1000\n",
      "Loss on train= 0.013006682507693768\n",
      "Loss on test= 0.022392887622117996\n",
      "acc for Lsat= 0.06693211725581921 \n",
      "acc for Psat= 0.09627061923872988 \n",
      "acc for optim= 0.1521018863120348\n",
      "Epoch:826/1000\n",
      "Loss on train= 0.01227926928550005\n",
      "Loss on test= 0.02216477319598198\n",
      "acc for Lsat= 0.0707294309555098 \n",
      "acc for Psat= 0.10061201938363008 \n",
      "acc for optim= 0.1511351976877827\n",
      "Epoch:827/1000\n",
      "Loss on train= 0.01230780128389597\n",
      "Loss on test= 0.02270440012216568\n",
      "acc for Lsat= 0.0665506118951842 \n",
      "acc for Psat= 0.09781106851029633 \n",
      "acc for optim= 0.15184011853414517\n",
      "Epoch:828/1000\n",
      "Loss on train= 0.012480397708714008\n",
      "Loss on test= 0.022452274337410927\n",
      "acc for Lsat= 0.07185595440508123 \n",
      "acc for Psat= 0.11063810709703008 \n",
      "acc for optim= 0.1518679517052102\n",
      "Epoch:829/1000\n",
      "Loss on train= 0.012174329720437527\n",
      "Loss on test= 0.022075476124882698\n",
      "acc for Lsat= 0.0702811150950847 \n",
      "acc for Psat= 0.09611045840966742 \n",
      "acc for optim= 0.15330496277920036\n",
      "Epoch:830/1000\n",
      "Loss on train= 0.012094488367438316\n",
      "Loss on test= 0.021971214562654495\n",
      "acc for Lsat= 0.06604778553758348 \n",
      "acc for Psat= 0.09516169450607806 \n",
      "acc for optim= 0.15246055886594959\n",
      "Epoch:831/1000\n",
      "Loss on train= 0.012420046143233776\n",
      "Loss on test= 0.021470099687576294\n",
      "acc for Lsat= 0.06789758388584238 \n",
      "acc for Psat= 0.09869164260122863 \n",
      "acc for optim= 0.15286963063221037\n",
      "Epoch:832/1000\n",
      "Loss on train= 0.011725025251507759\n",
      "Loss on test= 0.023369504138827324\n",
      "acc for Lsat= 0.07387938849949757 \n",
      "acc for Psat= 0.10292105902469038 \n",
      "acc for optim= 0.15319557740442769\n",
      "Epoch:833/1000\n",
      "Loss on train= 0.012452295981347561\n",
      "Loss on test= 0.022884687408804893\n",
      "acc for Lsat= 0.06890432500958049 \n",
      "acc for Psat= 0.10146015514171006 \n",
      "acc for optim= 0.15247063294201596\n",
      "Epoch:834/1000\n",
      "Loss on train= 0.01264403760433197\n",
      "Loss on test= 0.021706731989979744\n",
      "acc for Lsat= 0.07399743232022092 \n",
      "acc for Psat= 0.09846179588292525 \n",
      "acc for optim= 0.1527644060378851\n",
      "Epoch:835/1000\n",
      "Loss on train= 0.012157722376286983\n",
      "Loss on test= 0.02186845801770687\n",
      "acc for Lsat= 0.06725392839639291 \n",
      "acc for Psat= 0.09980675128607257 \n",
      "acc for optim= 0.15181964552679728\n",
      "Epoch:836/1000\n",
      "Loss on train= 0.011770294979214668\n",
      "Loss on test= 0.022187143564224243\n",
      "acc for Lsat= 0.06838978916109598 \n",
      "acc for Psat= 0.09689826410870221 \n",
      "acc for optim= 0.15208309744283602\n",
      "Epoch:837/1000\n",
      "Loss on train= 0.012520101852715015\n",
      "Loss on test= 0.02226758748292923\n",
      "acc for Lsat= 0.06812591687389385 \n",
      "acc for Psat= 0.10429166903923519 \n",
      "acc for optim= 0.15239014255248035\n",
      "Epoch:838/1000\n",
      "Loss on train= 0.011908902786672115\n",
      "Loss on test= 0.023000134155154228\n",
      "acc for Lsat= 0.06916270134258903 \n",
      "acc for Psat= 0.1006063785861893 \n",
      "acc for optim= 0.15301756039014286\n",
      "Epoch:839/1000\n",
      "Loss on train= 0.011778871528804302\n",
      "Loss on test= 0.022359423339366913\n",
      "acc for Lsat= 0.07039065402607585 \n",
      "acc for Psat= 0.1041808181030806 \n",
      "acc for optim= 0.1518600791197678\n",
      "Epoch:840/1000\n",
      "Loss on train= 0.01148703508079052\n",
      "Loss on test= 0.02323552407324314\n",
      "acc for Lsat= 0.06716669454329037 \n",
      "acc for Psat= 0.0981924792262803 \n",
      "acc for optim= 0.15148474370126308\n",
      "Epoch:841/1000\n",
      "Loss on train= 0.011665339581668377\n",
      "Loss on test= 0.022370707243680954\n",
      "acc for Lsat= 0.06946452986758413 \n",
      "acc for Psat= 0.1006816540841644 \n",
      "acc for optim= 0.15151821442220692\n",
      "Epoch:842/1000\n",
      "Loss on train= 0.011760310269892216\n",
      "Loss on test= 0.02277371659874916\n",
      "acc for Lsat= 0.0680463046231539 \n",
      "acc for Psat= 0.09801228733553838 \n",
      "acc for optim= 0.15074946672417397\n",
      "Epoch:843/1000\n",
      "Loss on train= 0.013085816986858845\n",
      "Loss on test= 0.022581005468964577\n",
      "acc for Lsat= 0.07062439415541993 \n",
      "acc for Psat= 0.09696882927536571 \n",
      "acc for optim= 0.1509744567728518\n",
      "Epoch:844/1000\n",
      "Loss on train= 0.011946504935622215\n",
      "Loss on test= 0.021796001121401787\n",
      "acc for Lsat= 0.06665381146625823 \n",
      "acc for Psat= 0.09882718399909646 \n",
      "acc for optim= 0.15197823330809512\n",
      "Epoch:845/1000\n",
      "Loss on train= 0.01175262313336134\n",
      "Loss on test= 0.02214190922677517\n",
      "acc for Lsat= 0.0712153691092995 \n",
      "acc for Psat= 0.0998368331760267 \n",
      "acc for optim= 0.152653050759306\n",
      "Epoch:846/1000\n",
      "Loss on train= 0.012019767425954342\n",
      "Loss on test= 0.022720597684383392\n",
      "acc for Lsat= 0.06684011182317702 \n",
      "acc for Psat= 0.09758930265705452 \n",
      "acc for optim= 0.15165907908911724\n",
      "Epoch:847/1000\n",
      "Loss on train= 0.011857415549457073\n",
      "Loss on test= 0.022854965180158615\n",
      "acc for Lsat= 0.06940932456045054 \n",
      "acc for Psat= 0.09690480279764067 \n",
      "acc for optim= 0.15319724477010707\n",
      "Epoch:848/1000\n",
      "Loss on train= 0.012218067422509193\n",
      "Loss on test= 0.022542966529726982\n",
      "acc for Lsat= 0.0677530926426384 \n",
      "acc for Psat= 0.09751673744762458 \n",
      "acc for optim= 0.1524369401591165\n",
      "Epoch:849/1000\n",
      "Loss on train= 0.011989349499344826\n",
      "Loss on test= 0.02284174971282482\n",
      "acc for Lsat= 0.06886020910700295 \n",
      "acc for Psat= 0.10099862693156118 \n",
      "acc for optim= 0.1519489241002802\n",
      "Epoch:850/1000\n",
      "Loss on train= 0.011930779553949833\n",
      "Loss on test= 0.021783700212836266\n",
      "acc for Lsat= 0.06775947119112428 \n",
      "acc for Psat= 0.0989332716132319 \n",
      "acc for optim= 0.15061530146091873\n",
      "Epoch:851/1000\n",
      "Loss on train= 0.011823435313999653\n",
      "Loss on test= 0.023091360926628113\n",
      "acc for Lsat= 0.06846599949158705 \n",
      "acc for Psat= 0.09858227950393954 \n",
      "acc for optim= 0.15152462691167665\n",
      "Epoch:852/1000\n",
      "Loss on train= 0.011526061221957207\n",
      "Loss on test= 0.022318940609693527\n",
      "acc for Lsat= 0.06765012631186616 \n",
      "acc for Psat= 0.09673151827333773 \n",
      "acc for optim= 0.1532423105746805\n",
      "Epoch:853/1000\n",
      "Loss on train= 0.011837719939649105\n",
      "Loss on test= 0.023033246397972107\n",
      "acc for Lsat= 0.07024100932766034 \n",
      "acc for Psat= 0.10167609461122175 \n",
      "acc for optim= 0.15196992291564568\n",
      "Epoch:854/1000\n",
      "Loss on train= 0.012023291550576687\n",
      "Loss on test= 0.02183149941265583\n",
      "acc for Lsat= 0.0662265015004877 \n",
      "acc for Psat= 0.09559543348626044 \n",
      "acc for optim= 0.15269694175831108\n",
      "Epoch:855/1000\n",
      "Loss on train= 0.011510984040796757\n",
      "Loss on test= 0.022716699168086052\n",
      "acc for Lsat= 0.06723042843349747 \n",
      "acc for Psat= 0.0995587577851508 \n",
      "acc for optim= 0.15104477534262445\n",
      "Epoch:856/1000\n",
      "Loss on train= 0.011726908385753632\n",
      "Loss on test= 0.021709276363253593\n",
      "acc for Lsat= 0.06882094723045629 \n",
      "acc for Psat= 0.09882940782661057 \n",
      "acc for optim= 0.15102890161580823\n",
      "Epoch:857/1000\n",
      "Loss on train= 0.011868881061673164\n",
      "Loss on test= 0.021729260683059692\n",
      "acc for Lsat= 0.06995393518989666 \n",
      "acc for Psat= 0.10068711654292388 \n",
      "acc for optim= 0.15067794132866336\n",
      "Epoch:858/1000\n",
      "Loss on train= 0.012226521037518978\n",
      "Loss on test= 0.022463839501142502\n",
      "acc for Lsat= 0.06798680002903226 \n",
      "acc for Psat= 0.09848847749621369 \n",
      "acc for optim= 0.15106657082060249\n",
      "Epoch:859/1000\n",
      "Loss on train= 0.011880457401275635\n",
      "Loss on test= 0.02268134243786335\n",
      "acc for Lsat= 0.06812636732461048 \n",
      "acc for Psat= 0.09905945112142849 \n",
      "acc for optim= 0.15248715358715118\n",
      "Epoch:860/1000\n",
      "Loss on train= 0.011870061978697777\n",
      "Loss on test= 0.022372541949152946\n",
      "acc for Lsat= 0.06806755426318147 \n",
      "acc for Psat= 0.09813709581809187 \n",
      "acc for optim= 0.15204524001805886\n",
      "Epoch:861/1000\n",
      "Loss on train= 0.011432928033173084\n",
      "Loss on test= 0.02215713821351528\n",
      "acc for Lsat= 0.0688532782254425 \n",
      "acc for Psat= 0.10092767298815655 \n",
      "acc for optim= 0.1532751427536391\n",
      "Epoch:862/1000\n",
      "Loss on train= 0.011489366181194782\n",
      "Loss on test= 0.02300262823700905\n",
      "acc for Lsat= 0.06897394824463664 \n",
      "acc for Psat= 0.098782773984231 \n",
      "acc for optim= 0.15250157658443897\n",
      "Epoch:863/1000\n",
      "Loss on train= 0.01162900310009718\n",
      "Loss on test= 0.021822096779942513\n",
      "acc for Lsat= 0.06523523344550024 \n",
      "acc for Psat= 0.09613981475663738 \n",
      "acc for optim= 0.15241065926330033\n",
      "Epoch:864/1000\n",
      "Loss on train= 0.011644360609352589\n",
      "Loss on test= 0.02244322933256626\n",
      "acc for Lsat= 0.06701022321203619 \n",
      "acc for Psat= 0.0943975478806765 \n",
      "acc for optim= 0.15094301268903915\n",
      "Epoch:865/1000\n",
      "Loss on train= 0.011644992046058178\n",
      "Loss on test= 0.021892815828323364\n",
      "acc for Lsat= 0.0680418466808788 \n",
      "acc for Psat= 0.09841086070402913 \n",
      "acc for optim= 0.15127231066805186\n",
      "Epoch:866/1000\n",
      "Loss on train= 0.011764741502702236\n",
      "Loss on test= 0.02257062867283821\n",
      "acc for Lsat= 0.06923495690489925 \n",
      "acc for Psat= 0.09949547030600998 \n",
      "acc for optim= 0.15109369386470203\n",
      "Epoch:867/1000\n",
      "Loss on train= 0.011874007992446423\n",
      "Loss on test= 0.021326350048184395\n",
      "acc for Lsat= 0.06724681388896168 \n",
      "acc for Psat= 0.10410984269408292 \n",
      "acc for optim= 0.15143097310367218\n",
      "Epoch:868/1000\n",
      "Loss on train= 0.011024695821106434\n",
      "Loss on test= 0.02180822193622589\n",
      "acc for Lsat= 0.06674890473633507 \n",
      "acc for Psat= 0.09674107953163473 \n",
      "acc for optim= 0.1510033557581347\n",
      "Epoch:869/1000\n",
      "Loss on train= 0.011887600645422935\n",
      "Loss on test= 0.02319631353020668\n",
      "acc for Lsat= 0.06721813970824018 \n",
      "acc for Psat= 0.09974069159688348 \n",
      "acc for optim= 0.15304230839707134\n",
      "Epoch:870/1000\n",
      "Loss on train= 0.011941430158913136\n",
      "Loss on test= 0.022477207705378532\n",
      "acc for Lsat= 0.06706803550949919 \n",
      "acc for Psat= 0.09768921045765927 \n",
      "acc for optim= 0.15208265943780688\n",
      "Epoch:871/1000\n",
      "Loss on train= 0.011435353197157383\n",
      "Loss on test= 0.02178148552775383\n",
      "acc for Lsat= 0.06855253349506975 \n",
      "acc for Psat= 0.09719564952327561 \n",
      "acc for optim= 0.15143456466966293\n",
      "Epoch:872/1000\n",
      "Loss on train= 0.012425852008163929\n",
      "Loss on test= 0.021841902285814285\n",
      "acc for Lsat= 0.07098659336368901 \n",
      "acc for Psat= 0.10050323924352957 \n",
      "acc for optim= 0.15165316145681465\n",
      "Epoch:873/1000\n",
      "Loss on train= 0.011184667237102985\n",
      "Loss on test= 0.022564852610230446\n",
      "acc for Lsat= 0.06625631650618935 \n",
      "acc for Psat= 0.09635443034956229 \n",
      "acc for optim= 0.15106005922108395\n",
      "Epoch:874/1000\n",
      "Loss on train= 0.012198134325444698\n",
      "Loss on test= 0.0223905798047781\n",
      "acc for Lsat= 0.06915854208691176 \n",
      "acc for Psat= 0.10250960463701295 \n",
      "acc for optim= 0.1512029330595783\n",
      "Epoch:875/1000\n",
      "Loss on train= 0.011557896621525288\n",
      "Loss on test= 0.022047923877835274\n",
      "acc for Lsat= 0.07618489564455226 \n",
      "acc for Psat= 0.10170537318502154 \n",
      "acc for optim= 0.15176755146330775\n",
      "Epoch:876/1000\n",
      "Loss on train= 0.011110058054327965\n",
      "Loss on test= 0.022850822657346725\n",
      "acc for Lsat= 0.06657740372755995 \n",
      "acc for Psat= 0.09607353779762685 \n",
      "acc for optim= 0.15151876137898215\n",
      "Epoch:877/1000\n",
      "Loss on train= 0.011838157661259174\n",
      "Loss on test= 0.022344179451465607\n",
      "acc for Lsat= 0.06657677825503173 \n",
      "acc for Psat= 0.09667500146599702 \n",
      "acc for optim= 0.15087216597458852\n",
      "Epoch:878/1000\n",
      "Loss on train= 0.011989250779151917\n",
      "Loss on test= 0.02217625267803669\n",
      "acc for Lsat= 0.06521513036319186 \n",
      "acc for Psat= 0.09597910315887478 \n",
      "acc for optim= 0.15283907010309727\n",
      "Epoch:879/1000\n",
      "Loss on train= 0.011457978747785091\n",
      "Loss on test= 0.023469330742955208\n",
      "acc for Lsat= 0.06557176401250783 \n",
      "acc for Psat= 0.09636904977484796 \n",
      "acc for optim= 0.15198775436791073\n",
      "Epoch:880/1000\n",
      "Loss on train= 0.01189403049647808\n",
      "Loss on test= 0.021853270009160042\n",
      "acc for Lsat= 0.06587006589304965 \n",
      "acc for Psat= 0.10137623427714228 \n",
      "acc for optim= 0.15159751077823067\n",
      "Epoch:881/1000\n",
      "Loss on train= 0.011743418872356415\n",
      "Loss on test= 0.022259818390011787\n",
      "acc for Lsat= 0.06568459596150737 \n",
      "acc for Psat= 0.09660649536257963 \n",
      "acc for optim= 0.15207983358357827\n",
      "Epoch:882/1000\n",
      "Loss on train= 0.01236930675804615\n",
      "Loss on test= 0.022533519193530083\n",
      "acc for Lsat= 0.06750782294526844 \n",
      "acc for Psat= 0.10045150393663452 \n",
      "acc for optim= 0.15218006952656468\n",
      "Epoch:883/1000\n",
      "Loss on train= 0.011958847753703594\n",
      "Loss on test= 0.022195875644683838\n",
      "acc for Lsat= 0.06952933317601089 \n",
      "acc for Psat= 0.09814519595070138 \n",
      "acc for optim= 0.15231297982491523\n",
      "Epoch:884/1000\n",
      "Loss on train= 0.011267128400504589\n",
      "Loss on test= 0.02176850475370884\n",
      "acc for Lsat= 0.06549071649776345 \n",
      "acc for Psat= 0.09849572809431639 \n",
      "acc for optim= 0.15292311471166006\n",
      "Epoch:885/1000\n",
      "Loss on train= 0.011798955500125885\n",
      "Loss on test= 0.02229822240769863\n",
      "acc for Lsat= 0.07025937188503356 \n",
      "acc for Psat= 0.09951165916515743 \n",
      "acc for optim= 0.15152294877359634\n",
      "Epoch:886/1000\n",
      "Loss on train= 0.011455501429736614\n",
      "Loss on test= 0.022022828459739685\n",
      "acc for Lsat= 0.06598779962110361 \n",
      "acc for Psat= 0.09937287422509684 \n",
      "acc for optim= 0.15299187241202572\n",
      "Epoch:887/1000\n",
      "Loss on train= 0.011936746537685394\n",
      "Loss on test= 0.022693106904625893\n",
      "acc for Lsat= 0.06688232902078534 \n",
      "acc for Psat= 0.097168820304332 \n",
      "acc for optim= 0.1528645414054592\n",
      "Epoch:888/1000\n",
      "Loss on train= 0.011633756570518017\n",
      "Loss on test= 0.02226373367011547\n",
      "acc for Lsat= 0.06834510978670218 \n",
      "acc for Psat= 0.10048428134268703 \n",
      "acc for optim= 0.15259301810961626\n",
      "Epoch:889/1000\n",
      "Loss on train= 0.011746356263756752\n",
      "Loss on test= 0.021652955561876297\n",
      "acc for Lsat= 0.06947626770532805 \n",
      "acc for Psat= 0.09787578582763673 \n",
      "acc for optim= 0.15271212525938038\n",
      "Epoch:890/1000\n",
      "Loss on train= 0.01168303843587637\n",
      "Loss on test= 0.022350823506712914\n",
      "acc for Lsat= 0.06612414703812709 \n",
      "acc for Psat= 0.09551674884023063 \n",
      "acc for optim= 0.15187836509210328\n",
      "Epoch:891/1000\n",
      "Loss on train= 0.011681091971695423\n",
      "Loss on test= 0.021969161927700043\n",
      "acc for Lsat= 0.06808466596460817 \n",
      "acc for Psat= 0.10171991582724738 \n",
      "acc for optim= 0.15236180014784545\n",
      "Epoch:892/1000\n",
      "Loss on train= 0.010967752896249294\n",
      "Loss on test= 0.02166561223566532\n",
      "acc for Lsat= 0.06914915520685457 \n",
      "acc for Psat= 0.09785819049689459 \n",
      "acc for optim= 0.15219511209532272\n",
      "Epoch:893/1000\n",
      "Loss on train= 0.012006319127976894\n",
      "Loss on test= 0.022575393319129944\n",
      "acc for Lsat= 0.06772678661782083 \n",
      "acc for Psat= 0.0979085445998119 \n",
      "acc for optim= 0.15386863265718734\n",
      "Epoch:894/1000\n",
      "Loss on train= 0.011769215576350689\n",
      "Loss on test= 0.022357013076543808\n",
      "acc for Lsat= 0.0681204750490347 \n",
      "acc for Psat= 0.09968108087283035 \n",
      "acc for optim= 0.15356137839662673\n",
      "Epoch:895/1000\n",
      "Loss on train= 0.011843807995319366\n",
      "Loss on test= 0.021786443889141083\n",
      "acc for Lsat= 0.06981414522839544 \n",
      "acc for Psat= 0.10142911605264654 \n",
      "acc for optim= 0.1516355344821448\n",
      "Epoch:896/1000\n",
      "Loss on train= 0.011260861530900002\n",
      "Loss on test= 0.021852057427167892\n",
      "acc for Lsat= 0.06639913705694318 \n",
      "acc for Psat= 0.09582745737807696 \n",
      "acc for optim= 0.15086169056718135\n",
      "Epoch:897/1000\n",
      "Loss on train= 0.011375906877219677\n",
      "Loss on test= 0.022235054522752762\n",
      "acc for Lsat= 0.06875782222842854 \n",
      "acc for Psat= 0.09730578668885849 \n",
      "acc for optim= 0.15178507829821383\n",
      "Epoch:898/1000\n",
      "Loss on train= 0.011368662118911743\n",
      "Loss on test= 0.02236412651836872\n",
      "acc for Lsat= 0.06598837231084755 \n",
      "acc for Psat= 0.097180068928538 \n",
      "acc for optim= 0.15275119927238387\n",
      "Epoch:899/1000\n",
      "Loss on train= 0.011492850258946419\n",
      "Loss on test= 0.021753808483481407\n",
      "acc for Lsat= 0.0661668988458342 \n",
      "acc for Psat= 0.09904305552723398 \n",
      "acc for optim= 0.1518474451529227\n",
      "Epoch:900/1000\n",
      "Loss on train= 0.011523752473294735\n",
      "Loss on test= 0.023263046517968178\n",
      "acc for Lsat= 0.06996909344315133 \n",
      "acc for Psat= 0.09821096004243703 \n",
      "acc for optim= 0.15240238720792473\n",
      "Epoch:901/1000\n",
      "Loss on train= 0.011296513490378857\n",
      "Loss on test= 0.021233869716525078\n",
      "acc for Lsat= 0.07214070892215169 \n",
      "acc for Psat= 0.09890665640664655 \n",
      "acc for optim= 0.15313583790661883\n",
      "Epoch:902/1000\n",
      "Loss on train= 0.011811068281531334\n",
      "Loss on test= 0.022363068535923958\n",
      "acc for Lsat= 0.06744640168557532 \n",
      "acc for Psat= 0.09769551247061291 \n",
      "acc for optim= 0.15229120595114568\n",
      "Epoch:903/1000\n",
      "Loss on train= 0.011589722707867622\n",
      "Loss on test= 0.023154787719249725\n",
      "acc for Lsat= 0.06818694269340297 \n",
      "acc for Psat= 0.09671525336381208 \n",
      "acc for optim= 0.15281379953571336\n",
      "Epoch:904/1000\n",
      "Loss on train= 0.01129316259175539\n",
      "Loss on test= 0.0222287829965353\n",
      "acc for Lsat= 0.06710472426739246 \n",
      "acc for Psat= 0.09886758729468946 \n",
      "acc for optim= 0.1534387176417037\n",
      "Epoch:905/1000\n",
      "Loss on train= 0.01104988157749176\n",
      "Loss on test= 0.022731540724635124\n",
      "acc for Lsat= 0.07087694011455357 \n",
      "acc for Psat= 0.10320696523815294 \n",
      "acc for optim= 0.15358398338885004\n",
      "Epoch:906/1000\n",
      "Loss on train= 0.0114362807944417\n",
      "Loss on test= 0.02211463265120983\n",
      "acc for Lsat= 0.07761492618294648 \n",
      "acc for Psat= 0.10473128672058003 \n",
      "acc for optim= 0.15408433424277954\n",
      "Epoch:907/1000\n",
      "Loss on train= 0.011733044870197773\n",
      "Loss on test= 0.02230595238506794\n",
      "acc for Lsat= 0.06835854768357007 \n",
      "acc for Psat= 0.10000599798966087 \n",
      "acc for optim= 0.15315221391643005\n",
      "Epoch:908/1000\n",
      "Loss on train= 0.011303137056529522\n",
      "Loss on test= 0.022695565596222878\n",
      "acc for Lsat= 0.06762965400749663 \n",
      "acc for Psat= 0.09628088315856022 \n",
      "acc for optim= 0.1528368955632777\n",
      "Epoch:909/1000\n",
      "Loss on train= 0.011448726058006287\n",
      "Loss on test= 0.02331172674894333\n",
      "acc for Lsat= 0.07638774824102851 \n",
      "acc for Psat= 0.10230294434335144 \n",
      "acc for optim= 0.15469933476955\n",
      "Epoch:910/1000\n",
      "Loss on train= 0.011461123824119568\n",
      "Loss on test= 0.02276213839650154\n",
      "acc for Lsat= 0.07113009201133766 \n",
      "acc for Psat= 0.10365007895963928 \n",
      "acc for optim= 0.15391034327472167\n",
      "Epoch:911/1000\n",
      "Loss on train= 0.011454210616648197\n",
      "Loss on test= 0.0227602981030941\n",
      "acc for Lsat= 0.06627382982608883 \n",
      "acc for Psat= 0.09682109102458258 \n",
      "acc for optim= 0.15377966440793286\n",
      "Epoch:912/1000\n",
      "Loss on train= 0.011850729584693909\n",
      "Loss on test= 0.02181052416563034\n",
      "acc for Lsat= 0.06619052059230615 \n",
      "acc for Psat= 0.10135212194087892 \n",
      "acc for optim= 0.1527644376620106\n",
      "Epoch:913/1000\n",
      "Loss on train= 0.011583219282329082\n",
      "Loss on test= 0.02324618585407734\n",
      "acc for Lsat= 0.06733140202851785 \n",
      "acc for Psat= 0.09696400351698614 \n",
      "acc for optim= 0.15365451542642028\n",
      "Epoch:914/1000\n",
      "Loss on train= 0.011446232907474041\n",
      "Loss on test= 0.02252631075680256\n",
      "acc for Lsat= 0.07125699626052895 \n",
      "acc for Psat= 0.10565044992785916 \n",
      "acc for optim= 0.15372656847155364\n",
      "Epoch:915/1000\n",
      "Loss on train= 0.011663241311907768\n",
      "Loss on test= 0.022471686825156212\n",
      "acc for Lsat= 0.06683806267093582 \n",
      "acc for Psat= 0.09713854041210439 \n",
      "acc for optim= 0.1533006590466167\n",
      "Epoch:916/1000\n",
      "Loss on train= 0.011576377786695957\n",
      "Loss on test= 0.023054642602801323\n",
      "acc for Lsat= 0.06878552975448658 \n",
      "acc for Psat= 0.09699118099735422 \n",
      "acc for optim= 0.15301336289640277\n",
      "Epoch:917/1000\n",
      "Loss on train= 0.010903501883149147\n",
      "Loss on test= 0.022131534293293953\n",
      "acc for Lsat= 0.06701375649022899 \n",
      "acc for Psat= 0.09746793001593153 \n",
      "acc for optim= 0.15369099519181492\n",
      "Epoch:918/1000\n",
      "Loss on train= 0.011441375128924847\n",
      "Loss on test= 0.02187352441251278\n",
      "acc for Lsat= 0.06879386348383765 \n",
      "acc for Psat= 0.0967659458864567 \n",
      "acc for optim= 0.1536541262338328\n",
      "Epoch:919/1000\n",
      "Loss on train= 0.011527192778885365\n",
      "Loss on test= 0.02271682769060135\n",
      "acc for Lsat= 0.06803197959728811 \n",
      "acc for Psat= 0.09572139908110976 \n",
      "acc for optim= 0.15270485818584098\n",
      "Epoch:920/1000\n",
      "Loss on train= 0.011477501131594181\n",
      "Loss on test= 0.023070914670825005\n",
      "acc for Lsat= 0.0733613238679214 \n",
      "acc for Psat= 0.10642577138454022 \n",
      "acc for optim= 0.15309536969938944\n",
      "Epoch:921/1000\n",
      "Loss on train= 0.011437567882239819\n",
      "Loss on test= 0.0224229134619236\n",
      "acc for Lsat= 0.06854047201994647 \n",
      "acc for Psat= 0.09622685891449255 \n",
      "acc for optim= 0.15215455616431386\n",
      "Epoch:922/1000\n",
      "Loss on train= 0.011465751565992832\n",
      "Loss on test= 0.022716477513313293\n",
      "acc for Lsat= 0.06619099569677118 \n",
      "acc for Psat= 0.09720015222844094 \n",
      "acc for optim= 0.15179552388349637\n",
      "Epoch:923/1000\n",
      "Loss on train= 0.012064558453857899\n",
      "Loss on test= 0.023128589615225792\n",
      "acc for Lsat= 0.06765425469590186 \n",
      "acc for Psat= 0.09715410800669282 \n",
      "acc for optim= 0.15380848262001115\n",
      "Epoch:924/1000\n",
      "Loss on train= 0.011265152134001255\n",
      "Loss on test= 0.022594155743718147\n",
      "acc for Lsat= 0.06629379307510845 \n",
      "acc for Psat= 0.0957973678939762 \n",
      "acc for optim= 0.1526961730168111\n",
      "Epoch:925/1000\n",
      "Loss on train= 0.011541667394340038\n",
      "Loss on test= 0.022508174180984497\n",
      "acc for Lsat= 0.07057663846847624 \n",
      "acc for Psat= 0.09915762894177357 \n",
      "acc for optim= 0.1539290786383556\n",
      "Epoch:926/1000\n",
      "Loss on train= 0.010917121544480324\n",
      "Loss on test= 0.022783659398555756\n",
      "acc for Lsat= 0.06648009576868774 \n",
      "acc for Psat= 0.1002033126512635 \n",
      "acc for optim= 0.15194875879145142\n",
      "Epoch:927/1000\n",
      "Loss on train= 0.011136406101286411\n",
      "Loss on test= 0.022857913747429848\n",
      "acc for Lsat= 0.06997352513760031 \n",
      "acc for Psat= 0.09803476955407481 \n",
      "acc for optim= 0.15263178200024707\n",
      "Epoch:928/1000\n",
      "Loss on train= 0.011382036842405796\n",
      "Loss on test= 0.022229332476854324\n",
      "acc for Lsat= 0.07001900719447787 \n",
      "acc for Psat= 0.09996025391987393 \n",
      "acc for optim= 0.15315707502175016\n",
      "Epoch:929/1000\n",
      "Loss on train= 0.011080784723162651\n",
      "Loss on test= 0.0228309016674757\n",
      "acc for Lsat= 0.06702325211410903 \n",
      "acc for Psat= 0.09711609704549923 \n",
      "acc for optim= 0.15221658141113995\n",
      "Epoch:930/1000\n",
      "Loss on train= 0.011290819384157658\n",
      "Loss on test= 0.022012243047356606\n",
      "acc for Lsat= 0.06760298262204838 \n",
      "acc for Psat= 0.09804663915570787 \n",
      "acc for optim= 0.1510971887563154\n",
      "Epoch:931/1000\n",
      "Loss on train= 0.011407168582081795\n",
      "Loss on test= 0.02265596203505993\n",
      "acc for Lsat= 0.06934787444894101 \n",
      "acc for Psat= 0.09762666340286157 \n",
      "acc for optim= 0.15203047485842658\n",
      "Epoch:932/1000\n",
      "Loss on train= 0.01099299918860197\n",
      "Loss on test= 0.022164732217788696\n",
      "acc for Lsat= 0.06768021470684547 \n",
      "acc for Psat= 0.09811801411384762 \n",
      "acc for optim= 0.15183672184167904\n",
      "Epoch:933/1000\n",
      "Loss on train= 0.011889434419572353\n",
      "Loss on test= 0.021545030176639557\n",
      "acc for Lsat= 0.0646572054521586 \n",
      "acc for Psat= 0.09563684679345036 \n",
      "acc for optim= 0.1528454288295733\n",
      "Epoch:934/1000\n",
      "Loss on train= 0.011393974535167217\n",
      "Loss on test= 0.02242567576467991\n",
      "acc for Lsat= 0.06620345906759811 \n",
      "acc for Psat= 0.09754075625806154 \n",
      "acc for optim= 0.1519836559446151\n",
      "Epoch:935/1000\n",
      "Loss on train= 0.01134033314883709\n",
      "Loss on test= 0.02230343036353588\n",
      "acc for Lsat= 0.06484905509061589 \n",
      "acc for Psat= 0.09647590381362509 \n",
      "acc for optim= 0.15223794387424505\n",
      "Epoch:936/1000\n",
      "Loss on train= 0.011229856871068478\n",
      "Loss on test= 0.022339247167110443\n",
      "acc for Lsat= 0.0707507825075986 \n",
      "acc for Psat= 0.09774545140044634 \n",
      "acc for optim= 0.15114047036218484\n",
      "Epoch:937/1000\n",
      "Loss on train= 0.011300459504127502\n",
      "Loss on test= 0.022657278925180435\n",
      "acc for Lsat= 0.06632371124041041 \n",
      "acc for Psat= 0.09716485168450692 \n",
      "acc for optim= 0.15230422845710548\n",
      "Epoch:938/1000\n",
      "Loss on train= 0.011424027383327484\n",
      "Loss on test= 0.022110627964138985\n",
      "acc for Lsat= 0.06634123761986577 \n",
      "acc for Psat= 0.09643914323312498 \n",
      "acc for optim= 0.15193655007701382\n",
      "Epoch:939/1000\n",
      "Loss on train= 0.012355436570942402\n",
      "Loss on test= 0.022798428311944008\n",
      "acc for Lsat= 0.06735729326639461 \n",
      "acc for Psat= 0.09587306468391736 \n",
      "acc for optim= 0.15254170478776444\n",
      "Epoch:940/1000\n",
      "Loss on train= 0.011513381265103817\n",
      "Loss on test= 0.02347085438668728\n",
      "acc for Lsat= 0.06843353383168824 \n",
      "acc for Psat= 0.0977273255289591 \n",
      "acc for optim= 0.1528163441391878\n",
      "Epoch:941/1000\n",
      "Loss on train= 0.011421133764088154\n",
      "Loss on test= 0.02249637246131897\n",
      "acc for Lsat= 0.06980213440731911 \n",
      "acc for Psat= 0.09621236696591808 \n",
      "acc for optim= 0.15194123054659645\n",
      "Epoch:942/1000\n",
      "Loss on train= 0.011267255060374737\n",
      "Loss on test= 0.021909845992922783\n",
      "acc for Lsat= 0.06918238035072123 \n",
      "acc for Psat= 0.0964417972338952 \n",
      "acc for optim= 0.15141823846240376\n",
      "Epoch:943/1000\n",
      "Loss on train= 0.011199326254427433\n",
      "Loss on test= 0.022779542952775955\n",
      "acc for Lsat= 0.07266440966794657 \n",
      "acc for Psat= 0.10483239716865693 \n",
      "acc for optim= 0.15162890399017212\n",
      "Epoch:944/1000\n",
      "Loss on train= 0.01223438885062933\n",
      "Loss on test= 0.021881235763430595\n",
      "acc for Lsat= 0.06583157673824666 \n",
      "acc for Psat= 0.09795417738119234 \n",
      "acc for optim= 0.1515236350785062\n",
      "Epoch:945/1000\n",
      "Loss on train= 0.011522719636559486\n",
      "Loss on test= 0.0218854621052742\n",
      "acc for Lsat= 0.06854121282053151 \n",
      "acc for Psat= 0.09971145238987233 \n",
      "acc for optim= 0.15231131509292958\n",
      "Epoch:946/1000\n",
      "Loss on train= 0.010906738229095936\n",
      "Loss on test= 0.022554880008101463\n",
      "acc for Lsat= 0.06643960837311919 \n",
      "acc for Psat= 0.09719783360775923 \n",
      "acc for optim= 0.15182606219453282\n",
      "Epoch:947/1000\n",
      "Loss on train= 0.011063532903790474\n",
      "Loss on test= 0.02266736887395382\n",
      "acc for Lsat= 0.06891761460771595 \n",
      "acc for Psat= 0.10010655110460577 \n",
      "acc for optim= 0.15246250504671144\n",
      "Epoch:948/1000\n",
      "Loss on train= 0.011591657064855099\n",
      "Loss on test= 0.02276262827217579\n",
      "acc for Lsat= 0.06996133095798301 \n",
      "acc for Psat= 0.10489287073430034 \n",
      "acc for optim= 0.15200469779017756\n",
      "Epoch:949/1000\n",
      "Loss on train= 0.011084926314651966\n",
      "Loss on test= 0.022118743509054184\n",
      "acc for Lsat= 0.06639075892114164 \n",
      "acc for Psat= 0.09806615575603472 \n",
      "acc for optim= 0.15332228479987361\n",
      "Epoch:950/1000\n",
      "Loss on train= 0.011287404224276543\n",
      "Loss on test= 0.022440366446971893\n",
      "acc for Lsat= 0.0680712289489385 \n",
      "acc for Psat= 0.09729848313965275 \n",
      "acc for optim= 0.1518231916863261\n",
      "Epoch:951/1000\n",
      "Loss on train= 0.01152112614363432\n",
      "Loss on test= 0.02178902179002762\n",
      "acc for Lsat= 0.06953408063646171 \n",
      "acc for Psat= 0.09969082464807452 \n",
      "acc for optim= 0.1518818495083489\n",
      "Epoch:952/1000\n",
      "Loss on train= 0.011652759276330471\n",
      "Loss on test= 0.022839147597551346\n",
      "acc for Lsat= 0.0681999747044224 \n",
      "acc for Psat= 0.09827976069379092 \n",
      "acc for optim= 0.15346169000448182\n",
      "Epoch:953/1000\n",
      "Loss on train= 0.011533212848007679\n",
      "Loss on test= 0.02183900959789753\n",
      "acc for Lsat= 0.06763252798108958 \n",
      "acc for Psat= 0.09948776611854072 \n",
      "acc for optim= 0.15188017327128067\n",
      "Epoch:954/1000\n",
      "Loss on train= 0.010924406349658966\n",
      "Loss on test= 0.022573044523596764\n",
      "acc for Lsat= 0.06892222552204448 \n",
      "acc for Psat= 0.09930468820456255 \n",
      "acc for optim= 0.15069641868933492\n",
      "Epoch:955/1000\n",
      "Loss on train= 0.01167312916368246\n",
      "Loss on test= 0.021831220015883446\n",
      "acc for Lsat= 0.07097027386937822 \n",
      "acc for Psat= 0.09936795055470195 \n",
      "acc for optim= 0.15212104175970012\n",
      "Epoch:956/1000\n",
      "Loss on train= 0.011207480914890766\n",
      "Loss on test= 0.021466616541147232\n",
      "acc for Lsat= 0.07060454564039098 \n",
      "acc for Psat= 0.10076787428206385 \n",
      "acc for optim= 0.15255134814601404\n",
      "Epoch:957/1000\n",
      "Loss on train= 0.010823252610862255\n",
      "Loss on test= 0.022187024354934692\n",
      "acc for Lsat= 0.06850880653755215 \n",
      "acc for Psat= 0.09860204975074315 \n",
      "acc for optim= 0.15222221981251358\n",
      "Epoch:958/1000\n",
      "Loss on train= 0.01165862288326025\n",
      "Loss on test= 0.02227986603975296\n",
      "acc for Lsat= 0.06853250680967819 \n",
      "acc for Psat= 0.09849328646232125 \n",
      "acc for optim= 0.15247690679623044\n",
      "Epoch:959/1000\n",
      "Loss on train= 0.010980822145938873\n",
      "Loss on test= 0.022286104038357735\n",
      "acc for Lsat= 0.06619288960960615 \n",
      "acc for Psat= 0.09555679481488923 \n",
      "acc for optim= 0.15141457713323575\n",
      "Epoch:960/1000\n",
      "Loss on train= 0.011141183786094189\n",
      "Loss on test= 0.022046232596039772\n",
      "acc for Lsat= 0.07108344597079824 \n",
      "acc for Psat= 0.09827050390433632 \n",
      "acc for optim= 0.15130347318427506\n",
      "Epoch:961/1000\n",
      "Loss on train= 0.011018775403499603\n",
      "Loss on test= 0.021870220080018044\n",
      "acc for Lsat= 0.0700627944299153 \n",
      "acc for Psat= 0.09990530920979195 \n",
      "acc for optim= 0.1530621916352713\n",
      "Epoch:962/1000\n",
      "Loss on train= 0.010731393471360207\n",
      "Loss on test= 0.022364448755979538\n",
      "acc for Lsat= 0.06774958772120682 \n",
      "acc for Psat= 0.0983247770027465 \n",
      "acc for optim= 0.1521521791469219\n",
      "Epoch:963/1000\n",
      "Loss on train= 0.011575124226510525\n",
      "Loss on test= 0.022736208513379097\n",
      "acc for Lsat= 0.06956485866312173 \n",
      "acc for Psat= 0.09907882298742021 \n",
      "acc for optim= 0.1527392503431073\n",
      "Epoch:964/1000\n",
      "Loss on train= 0.01084282249212265\n",
      "Loss on test= 0.02251099795103073\n",
      "acc for Lsat= 0.06574223472232438 \n",
      "acc for Psat= 0.09686057536903016 \n",
      "acc for optim= 0.15287944767562253\n",
      "Epoch:965/1000\n",
      "Loss on train= 0.01106205116957426\n",
      "Loss on test= 0.022750796750187874\n",
      "acc for Lsat= 0.06685768557941399 \n",
      "acc for Psat= 0.09769268432922934 \n",
      "acc for optim= 0.15235093449833384\n",
      "Epoch:966/1000\n",
      "Loss on train= 0.01067877747118473\n",
      "Loss on test= 0.02384691685438156\n",
      "acc for Lsat= 0.07022995550767133 \n",
      "acc for Psat= 0.09756945034990275 \n",
      "acc for optim= 0.15257630397711083\n",
      "Epoch:967/1000\n",
      "Loss on train= 0.011015357449650764\n",
      "Loss on test= 0.02237403206527233\n",
      "acc for Lsat= 0.0687042474845715 \n",
      "acc for Psat= 0.0987866660091172 \n",
      "acc for optim= 0.15479888979382683\n",
      "Epoch:968/1000\n",
      "Loss on train= 0.010798905976116657\n",
      "Loss on test= 0.02310260757803917\n",
      "acc for Lsat= 0.07071551560761523 \n",
      "acc for Psat= 0.095954951950878 \n",
      "acc for optim= 0.15424632226113866\n",
      "Epoch:969/1000\n",
      "Loss on train= 0.010298630222678185\n",
      "Loss on test= 0.022793296724557877\n",
      "acc for Lsat= 0.0668265901728722 \n",
      "acc for Psat= 0.09882742681376562 \n",
      "acc for optim= 0.1543905839769547\n",
      "Epoch:970/1000\n",
      "Loss on train= 0.011344173923134804\n",
      "Loss on test= 0.022277912124991417\n",
      "acc for Lsat= 0.06686883134105276 \n",
      "acc for Psat= 0.09657928637689928 \n",
      "acc for optim= 0.15357659797731824\n",
      "Epoch:971/1000\n",
      "Loss on train= 0.011175788007676601\n",
      "Loss on test= 0.022212129086256027\n",
      "acc for Lsat= 0.06665878603822767 \n",
      "acc for Psat= 0.10201142669912194 \n",
      "acc for optim= 0.15395649909181053\n",
      "Epoch:972/1000\n",
      "Loss on train= 0.011166015639901161\n",
      "Loss on test= 0.023534350097179413\n",
      "acc for Lsat= 0.06810378175835276 \n",
      "acc for Psat= 0.0995842890683995 \n",
      "acc for optim= 0.15401647974090318\n",
      "Epoch:973/1000\n",
      "Loss on train= 0.011386564932763577\n",
      "Loss on test= 0.022282283753156662\n",
      "acc for Lsat= 0.07029235275876879 \n",
      "acc for Psat= 0.09836737513542175 \n",
      "acc for optim= 0.15352812769801116\n",
      "Epoch:974/1000\n",
      "Loss on train= 0.010392582975327969\n",
      "Loss on test= 0.022840676829218864\n",
      "acc for Lsat= 0.0675848896024235 \n",
      "acc for Psat= 0.09747843484941905 \n",
      "acc for optim= 0.15307260819051746\n",
      "Epoch:975/1000\n",
      "Loss on train= 0.010606749914586544\n",
      "Loss on test= 0.02212749607861042\n",
      "acc for Lsat= 0.06762410445862829 \n",
      "acc for Psat= 0.09864243017478637 \n",
      "acc for optim= 0.15214709364298573\n",
      "Epoch:976/1000\n",
      "Loss on train= 0.01136865932494402\n",
      "Loss on test= 0.02203540690243244\n",
      "acc for Lsat= 0.07063600764520145 \n",
      "acc for Psat= 0.09806173998256063 \n",
      "acc for optim= 0.15271482146855603\n",
      "Epoch:977/1000\n",
      "Loss on train= 0.011840629391372204\n",
      "Loss on test= 0.021970974281430244\n",
      "acc for Lsat= 0.06813344345536342 \n",
      "acc for Psat= 0.09680690032699178 \n",
      "acc for optim= 0.15322164128785118\n",
      "Epoch:978/1000\n",
      "Loss on train= 0.011480406858026981\n",
      "Loss on test= 0.022286010906100273\n",
      "acc for Lsat= 0.06959332845535786 \n",
      "acc for Psat= 0.09962867320177961 \n",
      "acc for optim= 0.15303545940754026\n",
      "Epoch:979/1000\n",
      "Loss on train= 0.011211737059056759\n",
      "Loss on test= 0.022264428436756134\n",
      "acc for Lsat= 0.06643016598747419 \n",
      "acc for Psat= 0.09880015867889123 \n",
      "acc for optim= 0.15111578334605572\n",
      "Epoch:980/1000\n",
      "Loss on train= 0.011114719323813915\n",
      "Loss on test= 0.022709794342517853\n",
      "acc for Lsat= 0.06651219967195755 \n",
      "acc for Psat= 0.09735378042804045 \n",
      "acc for optim= 0.1528439815258267\n",
      "Epoch:981/1000\n",
      "Loss on train= 0.011578981764614582\n",
      "Loss on test= 0.02336549572646618\n",
      "acc for Lsat= 0.06715550758315876 \n",
      "acc for Psat= 0.09728815363491097 \n",
      "acc for optim= 0.15382955565405051\n",
      "Epoch:982/1000\n",
      "Loss on train= 0.010643137618899345\n",
      "Loss on test= 0.0227202195674181\n",
      "acc for Lsat= 0.06902905167931338 \n",
      "acc for Psat= 0.09804491147250433 \n",
      "acc for optim= 0.15190957364845914\n",
      "Epoch:983/1000\n",
      "Loss on train= 0.011286674067378044\n",
      "Loss on test= 0.022558262571692467\n",
      "acc for Lsat= 0.06634439958686446 \n",
      "acc for Psat= 0.1018526310738535 \n",
      "acc for optim= 0.15461729757017473\n",
      "Epoch:984/1000\n",
      "Loss on train= 0.011044367216527462\n",
      "Loss on test= 0.022816047072410583\n",
      "acc for Lsat= 0.06562793554855739 \n",
      "acc for Psat= 0.09757685285073973 \n",
      "acc for optim= 0.1526807248592377\n",
      "Epoch:985/1000\n",
      "Loss on train= 0.011242381297051907\n",
      "Loss on test= 0.023094097152352333\n",
      "acc for Lsat= 0.06477779233178427 \n",
      "acc for Psat= 0.09860200499775403 \n",
      "acc for optim= 0.15299586980445834\n",
      "Epoch:986/1000\n",
      "Loss on train= 0.011352461762726307\n",
      "Loss on test= 0.021388433873653412\n",
      "acc for Lsat= 0.06500719807472738 \n",
      "acc for Psat= 0.09642476376108951 \n",
      "acc for optim= 0.1522422694486637\n",
      "Epoch:987/1000\n",
      "Loss on train= 0.0110812708735466\n",
      "Loss on test= 0.022257888689637184\n",
      "acc for Lsat= 0.06729773241420127 \n",
      "acc for Psat= 0.10016001165903284 \n",
      "acc for optim= 0.15189607608753974\n",
      "Epoch:988/1000\n",
      "Loss on train= 0.011333961971104145\n",
      "Loss on test= 0.022936038672924042\n",
      "acc for Lsat= 0.06929096124100922 \n",
      "acc for Psat= 0.10259034172245039 \n",
      "acc for optim= 0.15264262402572504\n",
      "Epoch:989/1000\n",
      "Loss on train= 0.010785107500851154\n",
      "Loss on test= 0.022903477773070335\n",
      "acc for Lsat= 0.06652102348614375 \n",
      "acc for Psat= 0.09882795103760654 \n",
      "acc for optim= 0.1532697631671183\n",
      "Epoch:990/1000\n",
      "Loss on train= 0.010875006206333637\n",
      "Loss on test= 0.022574689239263535\n",
      "acc for Lsat= 0.06690390505069911 \n",
      "acc for Psat= 0.09873696050770656 \n",
      "acc for optim= 0.1529018982304291\n",
      "Epoch:991/1000\n",
      "Loss on train= 0.011256949976086617\n",
      "Loss on test= 0.022663120180368423\n",
      "acc for Lsat= 0.06530061976062103 \n",
      "acc for Psat= 0.0996389522504965 \n",
      "acc for optim= 0.1525246077201691\n",
      "Epoch:992/1000\n",
      "Loss on train= 0.01064006332308054\n",
      "Loss on test= 0.022640252485871315\n",
      "acc for Lsat= 0.06511660732106118 \n",
      "acc for Psat= 0.100129668102708 \n",
      "acc for optim= 0.15195122664156943\n",
      "Epoch:993/1000\n",
      "Loss on train= 0.011750509962439537\n",
      "Loss on test= 0.023520946502685547\n",
      "acc for Lsat= 0.0662995429629107 \n",
      "acc for Psat= 0.09820954461034348 \n",
      "acc for optim= 0.15218188224044743\n",
      "Epoch:994/1000\n",
      "Loss on train= 0.01145276241004467\n",
      "Loss on test= 0.022382522001862526\n",
      "acc for Lsat= 0.06755283477892512 \n",
      "acc for Psat= 0.09721625271033608 \n",
      "acc for optim= 0.15255263713111117\n",
      "Epoch:995/1000\n",
      "Loss on train= 0.010823712684214115\n",
      "Loss on test= 0.0227360799908638\n",
      "acc for Lsat= 0.06642003405925839 \n",
      "acc for Psat= 0.09720416702701405 \n",
      "acc for optim= 0.1526881252214362\n",
      "Epoch:996/1000\n",
      "Loss on train= 0.011073416098952293\n",
      "Loss on test= 0.02279416285455227\n",
      "acc for Lsat= 0.06667875444572234 \n",
      "acc for Psat= 0.09796411870325916 \n",
      "acc for optim= 0.15290285160771241\n",
      "Epoch:997/1000\n",
      "Loss on train= 0.010283470153808594\n",
      "Loss on test= 0.02227054163813591\n",
      "acc for Lsat= 0.06678705303573923 \n",
      "acc for Psat= 0.09713268773104262 \n",
      "acc for optim= 0.15408041378192322\n",
      "Epoch:998/1000\n",
      "Loss on train= 0.010969276539981365\n",
      "Loss on test= 0.022282036021351814\n",
      "acc for Lsat= 0.06592813386671566 \n",
      "acc for Psat= 0.09899271544427966 \n",
      "acc for optim= 0.15156834076013279\n",
      "Epoch:999/1000\n",
      "Loss on train= 0.010882080532610416\n",
      "Loss on test= 0.02276526764035225\n",
      "acc for Lsat= 0.0673731055271586 \n",
      "acc for Psat= 0.09961957448344694 \n",
      "acc for optim= 0.15339777289434925\n",
      "Epoch:1000/1000\n",
      "Loss on train= 0.010980802588164806\n",
      "Loss on test= 0.022473560646176338\n",
      "acc for Lsat= 0.06794206990158043 \n",
      "acc for Psat= 0.09867262414523534 \n",
      "acc for optim= 0.1521469564334895\n",
      "Fold 2\n",
      "Epoch:1/1000\n",
      "Loss on train= 0.28489094972610474\n",
      "Loss on test= 0.13428321480751038\n",
      "acc for Lsat= 0.42339837035040956 \n",
      "acc for Psat= 1.0071389137312423 \n",
      "acc for optim= 0.2287465453787652\n",
      "Epoch:2/1000\n",
      "Loss on train= 0.13856860995292664\n",
      "Loss on test= 0.10653115063905716\n",
      "acc for Lsat= 0.4014396598385997 \n",
      "acc for Psat= 0.5566352654015226 \n",
      "acc for optim= 0.2678618960815594\n",
      "Epoch:3/1000\n",
      "Loss on train= 0.09162484854459763\n",
      "Loss on test= 0.07882220298051834\n",
      "acc for Lsat= 0.31644131921911506 \n",
      "acc for Psat= 0.512184594632901 \n",
      "acc for optim= 0.18253163461821661\n",
      "Epoch:4/1000\n",
      "Loss on train= 0.07478653639554977\n",
      "Loss on test= 0.07028743624687195\n",
      "acc for Lsat= 0.27740134484866 \n",
      "acc for Psat= 0.46657363890748543 \n",
      "acc for optim= 0.16954091708434074\n",
      "Epoch:5/1000\n",
      "Loss on train= 0.06536304205656052\n",
      "Loss on test= 0.06716499477624893\n",
      "acc for Lsat= 0.296545307730093 \n",
      "acc for Psat= 0.4049973875124255 \n",
      "acc for optim= 0.16794792971372183\n",
      "Epoch:6/1000\n",
      "Loss on train= 0.06409696489572525\n",
      "Loss on test= 0.06048760935664177\n",
      "acc for Lsat= 0.2610327273447313 \n",
      "acc for Psat= 0.4108429245317559 \n",
      "acc for optim= 0.16875915958117924\n",
      "Epoch:7/1000\n",
      "Loss on train= 0.06256689131259918\n",
      "Loss on test= 0.05943781137466431\n",
      "acc for Lsat= 0.2757542113925136 \n",
      "acc for Psat= 0.38397284948761873 \n",
      "acc for optim= 0.1733136952243252\n",
      "Epoch:8/1000\n",
      "Loss on train= 0.059777941554784775\n",
      "Loss on test= 0.05992526933550835\n",
      "acc for Lsat= 0.31388928250464637 \n",
      "acc for Psat= 0.3608390243407439 \n",
      "acc for optim= 0.1764628023922124\n",
      "Epoch:9/1000\n",
      "Loss on train= 0.05767855420708656\n",
      "Loss on test= 0.059180762618780136\n",
      "acc for Lsat= 0.27291543549843045 \n",
      "acc for Psat= 0.39590327210844306 \n",
      "acc for optim= 0.18164569448699683\n",
      "Epoch:10/1000\n",
      "Loss on train= 0.055316705256700516\n",
      "Loss on test= 0.056585971266031265\n",
      "acc for Lsat= 0.2558976671776746 \n",
      "acc for Psat= 0.3743439346605208 \n",
      "acc for optim= 0.18180485933965776\n",
      "Epoch:11/1000\n",
      "Loss on train= 0.056798163801431656\n",
      "Loss on test= 0.0542997382581234\n",
      "acc for Lsat= 0.2985192301661469 \n",
      "acc for Psat= 0.33877725063794845 \n",
      "acc for optim= 0.16843125116419919\n",
      "Epoch:12/1000\n",
      "Loss on train= 0.05486161634325981\n",
      "Loss on test= 0.05525239184498787\n",
      "acc for Lsat= 0.2821521464955188 \n",
      "acc for Psat= 0.33920015041218243 \n",
      "acc for optim= 0.16795134322587837\n",
      "Epoch:13/1000\n",
      "Loss on train= 0.0498393252491951\n",
      "Loss on test= 0.051294904202222824\n",
      "acc for Lsat= 0.2587993207259339 \n",
      "acc for Psat= 0.3358894926704106 \n",
      "acc for optim= 0.16644055177999098\n",
      "Epoch:14/1000\n",
      "Loss on train= 0.05141127109527588\n",
      "Loss on test= 0.05187511444091797\n",
      "acc for Lsat= 0.26018807216279205 \n",
      "acc for Psat= 0.32734698758782127 \n",
      "acc for optim= 0.17265490421455532\n",
      "Epoch:15/1000\n",
      "Loss on train= 0.04923843964934349\n",
      "Loss on test= 0.053488753736019135\n",
      "acc for Lsat= 0.28963480006390263 \n",
      "acc for Psat= 0.3277145852343128 \n",
      "acc for optim= 0.18405550123327327\n",
      "Epoch:16/1000\n",
      "Loss on train= 0.050264935940504074\n",
      "Loss on test= 0.05012863129377365\n",
      "acc for Lsat= 0.2517602601716685 \n",
      "acc for Psat= 0.332040482791464 \n",
      "acc for optim= 0.1750969184319321\n",
      "Epoch:17/1000\n",
      "Loss on train= 0.04745476320385933\n",
      "Loss on test= 0.05422025918960571\n",
      "acc for Lsat= 0.30218433248335647 \n",
      "acc for Psat= 0.31814767602091404 \n",
      "acc for optim= 0.16590223743152108\n",
      "Epoch:18/1000\n",
      "Loss on train= 0.05173366516828537\n",
      "Loss on test= 0.04975859448313713\n",
      "acc for Lsat= 0.25962710320842924 \n",
      "acc for Psat= 0.32449189740131495 \n",
      "acc for optim= 0.16699782737892296\n",
      "Epoch:19/1000\n",
      "Loss on train= 0.046957917511463165\n",
      "Loss on test= 0.050031717866659164\n",
      "acc for Lsat= 0.27413318402438785 \n",
      "acc for Psat= 0.32084655175268745 \n",
      "acc for optim= 0.16681473515328016\n",
      "Epoch:20/1000\n",
      "Loss on train= 0.047840725630521774\n",
      "Loss on test= 0.04791194200515747\n",
      "acc for Lsat= 0.3025461403329811 \n",
      "acc for Psat= 0.31128895805219003 \n",
      "acc for optim= 0.16558051390809964\n",
      "Epoch:21/1000\n",
      "Loss on train= 0.04744962230324745\n",
      "Loss on test= 0.04604543372988701\n",
      "acc for Lsat= 0.25492165020412116 \n",
      "acc for Psat= 0.3133143014046288 \n",
      "acc for optim= 0.1694863958947347\n",
      "Epoch:22/1000\n",
      "Loss on train= 0.04958696290850639\n",
      "Loss on test= 0.054153695702552795\n",
      "acc for Lsat= 0.2620737477363968 \n",
      "acc for Psat= 0.4017961685900612 \n",
      "acc for optim= 0.18117920996251394\n",
      "Epoch:23/1000\n",
      "Loss on train= 0.048499803990125656\n",
      "Loss on test= 0.04689810052514076\n",
      "acc for Lsat= 0.27478346912080354 \n",
      "acc for Psat= 0.31984774981073916 \n",
      "acc for optim= 0.17507053684036725\n",
      "Epoch:24/1000\n",
      "Loss on train= 0.0452800951898098\n",
      "Loss on test= 0.04686262458562851\n",
      "acc for Lsat= 0.26147590609057425 \n",
      "acc for Psat= 0.34000700857217075 \n",
      "acc for optim= 0.1748386874395108\n",
      "Epoch:25/1000\n",
      "Loss on train= 0.044667910784482956\n",
      "Loss on test= 0.04739554598927498\n",
      "acc for Lsat= 0.28579870803624874 \n",
      "acc for Psat= 0.30249902129386536 \n",
      "acc for optim= 0.16614631934754537\n",
      "Epoch:26/1000\n",
      "Loss on train= 0.04708559438586235\n",
      "Loss on test= 0.04523037374019623\n",
      "acc for Lsat= 0.267124310994191 \n",
      "acc for Psat= 0.34489802096525535 \n",
      "acc for optim= 0.17301566788463557\n",
      "Epoch:27/1000\n",
      "Loss on train= 0.04434036463499069\n",
      "Loss on test= 0.04677616059780121\n",
      "acc for Lsat= 0.3274468875741703 \n",
      "acc for Psat= 0.3023563667998545 \n",
      "acc for optim= 0.16610330695116457\n",
      "Epoch:28/1000\n",
      "Loss on train= 0.04615585878491402\n",
      "Loss on test= 0.045575059950351715\n",
      "acc for Lsat= 0.28332911989983156 \n",
      "acc for Psat= 0.3021338931349822 \n",
      "acc for optim= 0.16620062259100846\n",
      "Epoch:29/1000\n",
      "Loss on train= 0.04370463639497757\n",
      "Loss on test= 0.044279906898736954\n",
      "acc for Lsat= 0.2566467193030287 \n",
      "acc for Psat= 0.3476132039094013 \n",
      "acc for optim= 0.18149575425815065\n",
      "Epoch:30/1000\n",
      "Loss on train= 0.044093914330005646\n",
      "Loss on test= 0.04613075032830238\n",
      "acc for Lsat= 0.292804743693425 \n",
      "acc for Psat= 0.2997890666686998 \n",
      "acc for optim= 0.1653100111616746\n",
      "Epoch:31/1000\n",
      "Loss on train= 0.04325366020202637\n",
      "Loss on test= 0.04402105137705803\n",
      "acc for Lsat= 0.27015108465082105 \n",
      "acc for Psat= 0.3171449918653117 \n",
      "acc for optim= 0.1678823173365994\n",
      "Epoch:32/1000\n",
      "Loss on train= 0.04430804401636124\n",
      "Loss on test= 0.042785707861185074\n",
      "acc for Lsat= 0.2589452514708149 \n",
      "acc for Psat= 0.3129176327282287 \n",
      "acc for optim= 0.1676671400266385\n",
      "Epoch:33/1000\n",
      "Loss on train= 0.04305120185017586\n",
      "Loss on test= 0.047273602336645126\n",
      "acc for Lsat= 0.2802771547825904 \n",
      "acc for Psat= 0.29661769954377715 \n",
      "acc for optim= 0.17228358395410992\n",
      "Epoch:34/1000\n",
      "Loss on train= 0.04276134818792343\n",
      "Loss on test= 0.04377441480755806\n",
      "acc for Lsat= 0.2443509350522474 \n",
      "acc for Psat= 0.2957230407351458 \n",
      "acc for optim= 0.17582042620305724\n",
      "Epoch:35/1000\n",
      "Loss on train= 0.04249120503664017\n",
      "Loss on test= 0.04383442923426628\n",
      "acc for Lsat= 0.2950021220234511 \n",
      "acc for Psat= 0.2981226944113033 \n",
      "acc for optim= 0.16661830187270382\n",
      "Epoch:36/1000\n",
      "Loss on train= 0.041568707674741745\n",
      "Loss on test= 0.04196134954690933\n",
      "acc for Lsat= 0.26066692370208316 \n",
      "acc for Psat= 0.28748073140709046 \n",
      "acc for optim= 0.16735582976520377\n",
      "Epoch:37/1000\n",
      "Loss on train= 0.04139237478375435\n",
      "Loss on test= 0.04146051034331322\n",
      "acc for Lsat= 0.2409674367879072 \n",
      "acc for Psat= 0.30032777820375606 \n",
      "acc for optim= 0.16582418791083406\n",
      "Epoch:38/1000\n",
      "Loss on train= 0.04048581048846245\n",
      "Loss on test= 0.04273098707199097\n",
      "acc for Lsat= 0.24629822965171558 \n",
      "acc for Psat= 0.3127066180829711 \n",
      "acc for optim= 0.1874546082373808\n",
      "Epoch:39/1000\n",
      "Loss on train= 0.04098189249634743\n",
      "Loss on test= 0.043181560933589935\n",
      "acc for Lsat= 0.2632313489487432 \n",
      "acc for Psat= 0.2846903608822012 \n",
      "acc for optim= 0.1739752721701197\n",
      "Epoch:40/1000\n",
      "Loss on train= 0.04035621136426926\n",
      "Loss on test= 0.04171588644385338\n",
      "acc for Lsat= 0.2446156606691255 \n",
      "acc for Psat= 0.29262484578625675 \n",
      "acc for optim= 0.17877392097108055\n",
      "Epoch:41/1000\n",
      "Loss on train= 0.04163452982902527\n",
      "Loss on test= 0.03877053037285805\n",
      "acc for Lsat= 0.26075900443338107 \n",
      "acc for Psat= 0.26819039216408364 \n",
      "acc for optim= 0.16474749698621857\n",
      "Epoch:42/1000\n",
      "Loss on train= 0.040277160704135895\n",
      "Loss on test= 0.04244542494416237\n",
      "acc for Lsat= 0.2526724172192951 \n",
      "acc for Psat= 0.27126846272958205 \n",
      "acc for optim= 0.1664116177767888\n",
      "Epoch:43/1000\n",
      "Loss on train= 0.04049894958734512\n",
      "Loss on test= 0.04140715301036835\n",
      "acc for Lsat= 0.24901330758932447 \n",
      "acc for Psat= 0.2689487802107986 \n",
      "acc for optim= 0.1663330535662622\n",
      "Epoch:44/1000\n",
      "Loss on train= 0.03929682448506355\n",
      "Loss on test= 0.04158768430352211\n",
      "acc for Lsat= 0.24688069321388428 \n",
      "acc for Psat= 0.3068130339834046 \n",
      "acc for optim= 0.18152185066441523\n",
      "Epoch:45/1000\n",
      "Loss on train= 0.042346399277448654\n",
      "Loss on test= 0.041979774832725525\n",
      "acc for Lsat= 0.29622359843074947 \n",
      "acc for Psat= 0.260771359889793 \n",
      "acc for optim= 0.1769586397411571\n",
      "Epoch:46/1000\n",
      "Loss on train= 0.03927486762404442\n",
      "Loss on test= 0.039173975586891174\n",
      "acc for Lsat= 0.23834417970961022 \n",
      "acc for Psat= 0.2649653624763215 \n",
      "acc for optim= 0.16640629425031772\n",
      "Epoch:47/1000\n",
      "Loss on train= 0.037524301558732986\n",
      "Loss on test= 0.038588713854551315\n",
      "acc for Lsat= 0.2473275515815653 \n",
      "acc for Psat= 0.2706045180932355 \n",
      "acc for optim= 0.17044992048113414\n",
      "Epoch:48/1000\n",
      "Loss on train= 0.037247441709041595\n",
      "Loss on test= 0.03944239392876625\n",
      "acc for Lsat= 0.25507809296916767 \n",
      "acc for Psat= 0.2526951044318075 \n",
      "acc for optim= 0.16418587854381964\n",
      "Epoch:49/1000\n",
      "Loss on train= 0.03856126591563225\n",
      "Loss on test= 0.03846989572048187\n",
      "acc for Lsat= 0.24495728762290558 \n",
      "acc for Psat= 0.279510110968554 \n",
      "acc for optim= 0.17346535908301527\n",
      "Epoch:50/1000\n",
      "Loss on train= 0.037766266614198685\n",
      "Loss on test= 0.04011271148920059\n",
      "acc for Lsat= 0.23040213791756803 \n",
      "acc for Psat= 0.2965367382668852 \n",
      "acc for optim= 0.16663522302358008\n",
      "Epoch:51/1000\n",
      "Loss on train= 0.04214213415980339\n",
      "Loss on test= 0.04791659116744995\n",
      "acc for Lsat= 0.37783435717464997 \n",
      "acc for Psat= 0.2773936347586098 \n",
      "acc for optim= 0.16708776835678726\n",
      "Epoch:52/1000\n",
      "Loss on train= 0.040017228573560715\n",
      "Loss on test= 0.03887678682804108\n",
      "acc for Lsat= 0.23848808924925774 \n",
      "acc for Psat= 0.26552952409643615 \n",
      "acc for optim= 0.16454453690107476\n",
      "Epoch:53/1000\n",
      "Loss on train= 0.038580507040023804\n",
      "Loss on test= 0.037537895143032074\n",
      "acc for Lsat= 0.23797300186054873 \n",
      "acc for Psat= 0.25266173686665583 \n",
      "acc for optim= 0.1814004660292473\n",
      "Epoch:54/1000\n",
      "Loss on train= 0.03753785416483879\n",
      "Loss on test= 0.03878973051905632\n",
      "acc for Lsat= 0.24796171727888475 \n",
      "acc for Psat= 0.2540561272441168 \n",
      "acc for optim= 0.1808745792812013\n",
      "Epoch:55/1000\n",
      "Loss on train= 0.036507103592157364\n",
      "Loss on test= 0.038932036608457565\n",
      "acc for Lsat= 0.24938252407237754 \n",
      "acc for Psat= 0.2513879987656111 \n",
      "acc for optim= 0.17205247305373594\n",
      "Epoch:56/1000\n",
      "Loss on train= 0.03688139095902443\n",
      "Loss on test= 0.037442080676555634\n",
      "acc for Lsat= 0.2329148112128162 \n",
      "acc for Psat= 0.255293116607905 \n",
      "acc for optim= 0.16501247774083197\n",
      "Epoch:57/1000\n",
      "Loss on train= 0.03797551989555359\n",
      "Loss on test= 0.039064519107341766\n",
      "acc for Lsat= 0.2809822898004903 \n",
      "acc for Psat= 0.24975354628827365 \n",
      "acc for optim= 0.1667511424779465\n",
      "Epoch:58/1000\n",
      "Loss on train= 0.035508327186107635\n",
      "Loss on test= 0.03808813914656639\n",
      "acc for Lsat= 0.22646380739689725 \n",
      "acc for Psat= 0.2941542793471827 \n",
      "acc for optim= 0.18079736109069927\n",
      "Epoch:59/1000\n",
      "Loss on train= 0.03638213127851486\n",
      "Loss on test= 0.03678742051124573\n",
      "acc for Lsat= 0.24551402274952375 \n",
      "acc for Psat= 0.24430143436506946 \n",
      "acc for optim= 0.17079104669192186\n",
      "Epoch:60/1000\n",
      "Loss on train= 0.03516233339905739\n",
      "Loss on test= 0.03816618025302887\n",
      "acc for Lsat= 0.2365007684354492 \n",
      "acc for Psat= 0.26585608102341246 \n",
      "acc for optim= 0.16548180590801886\n",
      "Epoch:61/1000\n",
      "Loss on train= 0.034796278923749924\n",
      "Loss on test= 0.0346074253320694\n",
      "acc for Lsat= 0.24126747375310845 \n",
      "acc for Psat= 0.24447915933639713 \n",
      "acc for optim= 0.1644765504144385\n",
      "Epoch:62/1000\n",
      "Loss on train= 0.0354558601975441\n",
      "Loss on test= 0.03870896250009537\n",
      "acc for Lsat= 0.2546402036610571 \n",
      "acc for Psat= 0.246569666655631 \n",
      "acc for optim= 0.16340603734598177\n",
      "Epoch:63/1000\n",
      "Loss on train= 0.03674132749438286\n",
      "Loss on test= 0.03894689306616783\n",
      "acc for Lsat= 0.2178188108800775 \n",
      "acc for Psat= 0.31955604357028466 \n",
      "acc for optim= 0.16323775805812832\n",
      "Epoch:64/1000\n",
      "Loss on train= 0.03604494407773018\n",
      "Loss on test= 0.03729231655597687\n",
      "acc for Lsat= 0.2494175748236491 \n",
      "acc for Psat= 0.24471356904997169 \n",
      "acc for optim= 0.16312313081966529\n",
      "Epoch:65/1000\n",
      "Loss on train= 0.03523363173007965\n",
      "Loss on test= 0.037080638110637665\n",
      "acc for Lsat= 0.23118025226541847 \n",
      "acc for Psat= 0.2864320638141394 \n",
      "acc for optim= 0.16800743723171552\n",
      "Epoch:66/1000\n",
      "Loss on train= 0.03539140522480011\n",
      "Loss on test= 0.03651491552591324\n",
      "acc for Lsat= 0.24474244838537176 \n",
      "acc for Psat= 0.24154522635429193 \n",
      "acc for optim= 0.16862387844829524\n",
      "Epoch:67/1000\n",
      "Loss on train= 0.03511394187808037\n",
      "Loss on test= 0.03814162686467171\n",
      "acc for Lsat= 0.25487828229109166 \n",
      "acc for Psat= 0.24072223283310473 \n",
      "acc for optim= 0.16302027047640097\n",
      "Epoch:68/1000\n",
      "Loss on train= 0.03470220789313316\n",
      "Loss on test= 0.036485396325588226\n",
      "acc for Lsat= 0.2498161118657517 \n",
      "acc for Psat= 0.23541219861434917 \n",
      "acc for optim= 0.17118229642110558\n",
      "Epoch:69/1000\n",
      "Loss on train= 0.03289986774325371\n",
      "Loss on test= 0.03729454427957535\n",
      "acc for Lsat= 0.2293143318249629 \n",
      "acc for Psat= 0.2620317222393903 \n",
      "acc for optim= 0.1631080722979441\n",
      "Epoch:70/1000\n",
      "Loss on train= 0.03375207260251045\n",
      "Loss on test= 0.036030422896146774\n",
      "acc for Lsat= 0.25396952972429165 \n",
      "acc for Psat= 0.22904472500352394 \n",
      "acc for optim= 0.1632456851133507\n",
      "Epoch:71/1000\n",
      "Loss on train= 0.03364747390151024\n",
      "Loss on test= 0.035151559859514236\n",
      "acc for Lsat= 0.2501533118258223 \n",
      "acc for Psat= 0.23566134801277747 \n",
      "acc for optim= 0.1702343914504554\n",
      "Epoch:72/1000\n",
      "Loss on train= 0.03402722254395485\n",
      "Loss on test= 0.03546373173594475\n",
      "acc for Lsat= 0.22993761251992106 \n",
      "acc for Psat= 0.2344897719210076 \n",
      "acc for optim= 0.1767938020617463\n",
      "Epoch:73/1000\n",
      "Loss on train= 0.03330238163471222\n",
      "Loss on test= 0.03528531268239021\n",
      "acc for Lsat= 0.2332294374117911 \n",
      "acc for Psat= 0.22590531963781724 \n",
      "acc for optim= 0.1640714190087293\n",
      "Epoch:74/1000\n",
      "Loss on train= 0.03309713676571846\n",
      "Loss on test= 0.03838237375020981\n",
      "acc for Lsat= 0.24609797525491192 \n",
      "acc for Psat= 0.24694429542169585 \n",
      "acc for optim= 0.16183742485234048\n",
      "Epoch:75/1000\n",
      "Loss on train= 0.03270014375448227\n",
      "Loss on test= 0.03424200788140297\n",
      "acc for Lsat= 0.21482742859553783 \n",
      "acc for Psat= 0.22501331589089735 \n",
      "acc for optim= 0.164264501932482\n",
      "Epoch:76/1000\n",
      "Loss on train= 0.03293647617101669\n",
      "Loss on test= 0.0339534729719162\n",
      "acc for Lsat= 0.21581893865025747 \n",
      "acc for Psat= 0.225422972739276 \n",
      "acc for optim= 0.16229059969163329\n",
      "Epoch:77/1000\n",
      "Loss on train= 0.032818324863910675\n",
      "Loss on test= 0.034544430673122406\n",
      "acc for Lsat= 0.22158462433994133 \n",
      "acc for Psat= 0.2300954397761118 \n",
      "acc for optim= 0.17090873703333903\n",
      "Epoch:78/1000\n",
      "Loss on train= 0.03226396441459656\n",
      "Loss on test= 0.033686310052871704\n",
      "acc for Lsat= 0.23122091197370417 \n",
      "acc for Psat= 0.22358462748672536 \n",
      "acc for optim= 0.1675772989158767\n",
      "Epoch:79/1000\n",
      "Loss on train= 0.03074466623365879\n",
      "Loss on test= 0.034038666635751724\n",
      "acc for Lsat= 0.2038386673961428 \n",
      "acc for Psat= 0.24771018188200186 \n",
      "acc for optim= 0.16985193697838963\n",
      "Epoch:80/1000\n",
      "Loss on train= 0.032759055495262146\n",
      "Loss on test= 0.033451758325099945\n",
      "acc for Lsat= 0.2262934384704276 \n",
      "acc for Psat= 0.2206439458620143 \n",
      "acc for optim= 0.16334168680665323\n",
      "Epoch:81/1000\n",
      "Loss on train= 0.03318944200873375\n",
      "Loss on test= 0.036647435277700424\n",
      "acc for Lsat= 0.20937126064556438 \n",
      "acc for Psat= 0.3071734078241803 \n",
      "acc for optim= 0.1626199028573011\n",
      "Epoch:82/1000\n",
      "Loss on train= 0.03190380334854126\n",
      "Loss on test= 0.033407632261514664\n",
      "acc for Lsat= 0.20072400648819952 \n",
      "acc for Psat= 0.2177946858201343 \n",
      "acc for optim= 0.16258819209868236\n",
      "Epoch:83/1000\n",
      "Loss on train= 0.031993258744478226\n",
      "Loss on test= 0.03514222055673599\n",
      "acc for Lsat= 0.21498000254997843 \n",
      "acc for Psat= 0.23694664800742873 \n",
      "acc for optim= 0.1662397609627098\n",
      "Epoch:84/1000\n",
      "Loss on train= 0.032165031880140305\n",
      "Loss on test= 0.03278263658285141\n",
      "acc for Lsat= 0.20292475731087073 \n",
      "acc for Psat= 0.22826526214057097 \n",
      "acc for optim= 0.1622895204956928\n",
      "Epoch:85/1000\n",
      "Loss on train= 0.031591352075338364\n",
      "Loss on test= 0.032909370958805084\n",
      "acc for Lsat= 0.2099474183242095 \n",
      "acc for Psat= 0.21293077170315708 \n",
      "acc for optim= 0.16282998549490368\n",
      "Epoch:86/1000\n",
      "Loss on train= 0.03041244111955166\n",
      "Loss on test= 0.0329069159924984\n",
      "acc for Lsat= 0.19584497580161467 \n",
      "acc for Psat= 0.21155109310619305 \n",
      "acc for optim= 0.16069106200513342\n",
      "Epoch:87/1000\n",
      "Loss on train= 0.030925992876291275\n",
      "Loss on test= 0.03387368097901344\n",
      "acc for Lsat= 0.21167407413288178 \n",
      "acc for Psat= 0.21349848340363747 \n",
      "acc for optim= 0.16115441584629744\n",
      "Epoch:88/1000\n",
      "Loss on train= 0.030571801587939262\n",
      "Loss on test= 0.03306230902671814\n",
      "acc for Lsat= 0.2083629365897136 \n",
      "acc for Psat= 0.21068547469345528 \n",
      "acc for optim= 0.16203028795331023\n",
      "Epoch:89/1000\n",
      "Loss on train= 0.031055545434355736\n",
      "Loss on test= 0.032845091074705124\n",
      "acc for Lsat= 0.20364310259469287 \n",
      "acc for Psat= 0.24711037021203627 \n",
      "acc for optim= 0.16434786140598853\n",
      "Epoch:90/1000\n",
      "Loss on train= 0.030488578602671623\n",
      "Loss on test= 0.03357510641217232\n",
      "acc for Lsat= 0.21786930386196948 \n",
      "acc for Psat= 0.21059875638625702 \n",
      "acc for optim= 0.16283731174810204\n",
      "Epoch:91/1000\n",
      "Loss on train= 0.0304569061845541\n",
      "Loss on test= 0.03113486059010029\n",
      "acc for Lsat= 0.1771069907971486 \n",
      "acc for Psat= 0.21867804739470986 \n",
      "acc for optim= 0.1602356039134249\n",
      "Epoch:92/1000\n",
      "Loss on train= 0.030855728313326836\n",
      "Loss on test= 0.03192978352308273\n",
      "acc for Lsat= 0.18110501321569114 \n",
      "acc for Psat= 0.2160737463528014 \n",
      "acc for optim= 0.16174086110536443\n",
      "Epoch:93/1000\n",
      "Loss on train= 0.029960602521896362\n",
      "Loss on test= 0.032661084085702896\n",
      "acc for Lsat= 0.19581000804901125 \n",
      "acc for Psat= 0.2339029339643625 \n",
      "acc for optim= 0.16222989940899218\n",
      "Epoch:94/1000\n",
      "Loss on train= 0.030190307646989822\n",
      "Loss on test= 0.03011779487133026\n",
      "acc for Lsat= 0.18902776749914577 \n",
      "acc for Psat= 0.21524131372704275 \n",
      "acc for optim= 0.16235933005276648\n",
      "Epoch:95/1000\n",
      "Loss on train= 0.029115479439496994\n",
      "Loss on test= 0.030893880873918533\n",
      "acc for Lsat= 0.17242724028597575 \n",
      "acc for Psat= 0.2006928121041313 \n",
      "acc for optim= 0.16143400376939174\n",
      "Epoch:96/1000\n",
      "Loss on train= 0.029117675498127937\n",
      "Loss on test= 0.030088726431131363\n",
      "acc for Lsat= 0.1688633405778404 \n",
      "acc for Psat= 0.1951655591856388 \n",
      "acc for optim= 0.16039154830260438\n",
      "Epoch:97/1000\n",
      "Loss on train= 0.02871572971343994\n",
      "Loss on test= 0.03152494505047798\n",
      "acc for Lsat= 0.17041510487498449 \n",
      "acc for Psat= 0.21838496676071384 \n",
      "acc for optim= 0.16070179728148024\n",
      "Epoch:98/1000\n",
      "Loss on train= 0.028904424980282784\n",
      "Loss on test= 0.030306249856948853\n",
      "acc for Lsat= 0.16524569943467277 \n",
      "acc for Psat= 0.19650401782264432 \n",
      "acc for optim= 0.16343998446234217\n",
      "Epoch:99/1000\n",
      "Loss on train= 0.028294596821069717\n",
      "Loss on test= 0.02946094609797001\n",
      "acc for Lsat= 0.1610951484742446 \n",
      "acc for Psat= 0.19276901810881494 \n",
      "acc for optim= 0.16170394548576505\n",
      "Epoch:100/1000\n",
      "Loss on train= 0.02869206853210926\n",
      "Loss on test= 0.03136822581291199\n",
      "acc for Lsat= 0.16437454944433166 \n",
      "acc for Psat= 0.20200413578524787 \n",
      "acc for optim= 0.1596390340110697\n",
      "Epoch:101/1000\n",
      "Loss on train= 0.02887178771197796\n",
      "Loss on test= 0.029765479266643524\n",
      "acc for Lsat= 0.16448069329978313 \n",
      "acc for Psat= 0.20025496713164026 \n",
      "acc for optim= 0.16212498582966214\n",
      "Epoch:102/1000\n",
      "Loss on train= 0.028528543189167976\n",
      "Loss on test= 0.031239576637744904\n",
      "acc for Lsat= 0.18318388884310646 \n",
      "acc for Psat= 0.19715533729955845 \n",
      "acc for optim= 0.1675643584596023\n",
      "Epoch:103/1000\n",
      "Loss on train= 0.02863943763077259\n",
      "Loss on test= 0.029511526226997375\n",
      "acc for Lsat= 0.16815788532625586 \n",
      "acc for Psat= 0.2021770720618355 \n",
      "acc for optim= 0.16937367926769908\n",
      "Epoch:104/1000\n",
      "Loss on train= 0.02782154083251953\n",
      "Loss on test= 0.03061164729297161\n",
      "acc for Lsat= 0.172517158051083 \n",
      "acc for Psat= 0.22558972641479133 \n",
      "acc for optim= 0.16751670459941803\n",
      "Epoch:105/1000\n",
      "Loss on train= 0.027828557416796684\n",
      "Loss on test= 0.02863004244863987\n",
      "acc for Lsat= 0.1400460177116189 \n",
      "acc for Psat= 0.1888916324417578 \n",
      "acc for optim= 0.16240676178701877\n",
      "Epoch:106/1000\n",
      "Loss on train= 0.026576563715934753\n",
      "Loss on test= 0.0307049248367548\n",
      "acc for Lsat= 0.14815391922679486 \n",
      "acc for Psat= 0.1892450870682814 \n",
      "acc for optim= 0.15981202878337858\n",
      "Epoch:107/1000\n",
      "Loss on train= 0.028245938941836357\n",
      "Loss on test= 0.02839912287890911\n",
      "acc for Lsat= 0.14453049152822953 \n",
      "acc for Psat= 0.1812164825702183 \n",
      "acc for optim= 0.15998109031022129\n",
      "Epoch:108/1000\n",
      "Loss on train= 0.026671526953577995\n",
      "Loss on test= 0.028505800291895866\n",
      "acc for Lsat= 0.15310777641153078 \n",
      "acc for Psat= 0.18454685496942727 \n",
      "acc for optim= 0.16134386930568062\n",
      "Epoch:109/1000\n",
      "Loss on train= 0.027629047632217407\n",
      "Loss on test= 0.028854263946413994\n",
      "acc for Lsat= 0.15149901503953614 \n",
      "acc for Psat= 0.18304834083282454 \n",
      "acc for optim= 0.16123861191737632\n",
      "Epoch:110/1000\n",
      "Loss on train= 0.026438191533088684\n",
      "Loss on test= 0.028860921040177345\n",
      "acc for Lsat= 0.1515457761522269 \n",
      "acc for Psat= 0.17699512612201232 \n",
      "acc for optim= 0.16722169935383396\n",
      "Epoch:111/1000\n",
      "Loss on train= 0.02704860083758831\n",
      "Loss on test= 0.028781874105334282\n",
      "acc for Lsat= 0.15378513484393547 \n",
      "acc for Psat= 0.18916105524585158 \n",
      "acc for optim= 0.16031448237584614\n",
      "Epoch:112/1000\n",
      "Loss on train= 0.026056362316012383\n",
      "Loss on test= 0.02792620100080967\n",
      "acc for Lsat= 0.1539931878954012 \n",
      "acc for Psat= 0.2118006804761392 \n",
      "acc for optim= 0.1614797240506344\n",
      "Epoch:113/1000\n",
      "Loss on train= 0.02634446695446968\n",
      "Loss on test= 0.027545226737856865\n",
      "acc for Lsat= 0.14597211772299412 \n",
      "acc for Psat= 0.18106730842632984 \n",
      "acc for optim= 0.1604929942137866\n",
      "Epoch:114/1000\n",
      "Loss on train= 0.0261900145560503\n",
      "Loss on test= 0.028490565717220306\n",
      "acc for Lsat= 0.13976487625050416 \n",
      "acc for Psat= 0.17360654406129564 \n",
      "acc for optim= 0.1619146359841171\n",
      "Epoch:115/1000\n",
      "Loss on train= 0.026382844895124435\n",
      "Loss on test= 0.026937557384371758\n",
      "acc for Lsat= 0.12883928915042572 \n",
      "acc for Psat= 0.16947924184457988 \n",
      "acc for optim= 0.1613054351218058\n",
      "Epoch:116/1000\n",
      "Loss on train= 0.027086134999990463\n",
      "Loss on test= 0.027402836829423904\n",
      "acc for Lsat= 0.1305422772128595 \n",
      "acc for Psat= 0.16825618428279762 \n",
      "acc for optim= 0.16422519246666076\n",
      "Epoch:117/1000\n",
      "Loss on train= 0.02622462436556816\n",
      "Loss on test= 0.02810439094901085\n",
      "acc for Lsat= 0.14375885613062728 \n",
      "acc for Psat= 0.17938740795115024 \n",
      "acc for optim= 0.16349802497034632\n",
      "Epoch:118/1000\n",
      "Loss on train= 0.027880758047103882\n",
      "Loss on test= 0.028095591813325882\n",
      "acc for Lsat= 0.1734575625608987 \n",
      "acc for Psat= 0.1687575558217139 \n",
      "acc for optim= 0.17080288656282505\n",
      "Epoch:119/1000\n",
      "Loss on train= 0.026946280151605606\n",
      "Loss on test= 0.028205281123518944\n",
      "acc for Lsat= 0.12976886601268925 \n",
      "acc for Psat= 0.1823142614262262 \n",
      "acc for optim= 0.1736559461822237\n",
      "Epoch:120/1000\n",
      "Loss on train= 0.025283707305788994\n",
      "Loss on test= 0.027199557051062584\n",
      "acc for Lsat= 0.12440440223980465 \n",
      "acc for Psat= 0.18235776290057598 \n",
      "acc for optim= 0.16208558991044925\n",
      "Epoch:121/1000\n",
      "Loss on train= 0.025673331692814827\n",
      "Loss on test= 0.027154820039868355\n",
      "acc for Lsat= 0.13462126493880489 \n",
      "acc for Psat= 0.1904477165295528 \n",
      "acc for optim= 0.15883374455149998\n",
      "Epoch:122/1000\n",
      "Loss on train= 0.02529851533472538\n",
      "Loss on test= 0.027275562286376953\n",
      "acc for Lsat= 0.1403621648206694 \n",
      "acc for Psat= 0.1745695352447694 \n",
      "acc for optim= 0.15877223513847172\n",
      "Epoch:123/1000\n",
      "Loss on train= 0.025086481124162674\n",
      "Loss on test= 0.026419300585985184\n",
      "acc for Lsat= 0.1302263956803542 \n",
      "acc for Psat= 0.16734573267440253 \n",
      "acc for optim= 0.15832070700810935\n",
      "Epoch:124/1000\n",
      "Loss on train= 0.025827137753367424\n",
      "Loss on test= 0.028585264459252357\n",
      "acc for Lsat= 0.14220809471628534 \n",
      "acc for Psat= 0.197812114333424 \n",
      "acc for optim= 0.15805552185967908\n",
      "Epoch:125/1000\n",
      "Loss on train= 0.027225377038121223\n",
      "Loss on test= 0.027542676776647568\n",
      "acc for Lsat= 0.12379251717454842 \n",
      "acc for Psat= 0.16467694339470704 \n",
      "acc for optim= 0.1660257624172354\n",
      "Epoch:126/1000\n",
      "Loss on train= 0.0252088475972414\n",
      "Loss on test= 0.027094166725873947\n",
      "acc for Lsat= 0.13098003434580424 \n",
      "acc for Psat= 0.16638272384411534 \n",
      "acc for optim= 0.16209006452389818\n",
      "Epoch:127/1000\n",
      "Loss on train= 0.025290073826909065\n",
      "Loss on test= 0.02637678012251854\n",
      "acc for Lsat= 0.12536681936337402 \n",
      "acc for Psat= 0.16697313687455892 \n",
      "acc for optim= 0.16391098433189188\n",
      "Epoch:128/1000\n",
      "Loss on train= 0.02473614178597927\n",
      "Loss on test= 0.026007935404777527\n",
      "acc for Lsat= 0.1278590299255733 \n",
      "acc for Psat= 0.1634459180993983 \n",
      "acc for optim= 0.15825680475115567\n",
      "Epoch:129/1000\n",
      "Loss on train= 0.024891134351491928\n",
      "Loss on test= 0.026355724781751633\n",
      "acc for Lsat= 0.1280627005748544 \n",
      "acc for Psat= 0.17281177231578793 \n",
      "acc for optim= 0.1611872069950821\n",
      "Epoch:130/1000\n",
      "Loss on train= 0.024320324882864952\n",
      "Loss on test= 0.02702414244413376\n",
      "acc for Lsat= 0.1345499157052561 \n",
      "acc for Psat= 0.18822401663911578 \n",
      "acc for optim= 0.1579885331164106\n",
      "Epoch:131/1000\n",
      "Loss on train= 0.026154939085245132\n",
      "Loss on test= 0.026243556290864944\n",
      "acc for Lsat= 0.12457623612902025 \n",
      "acc for Psat= 0.1564057024412377 \n",
      "acc for optim= 0.159363616408517\n",
      "Epoch:132/1000\n",
      "Loss on train= 0.023668313398957253\n",
      "Loss on test= 0.02691393345594406\n",
      "acc for Lsat= 0.144038370918076 \n",
      "acc for Psat= 0.16456183976692043 \n",
      "acc for optim= 0.1627301755446228\n",
      "Epoch:133/1000\n",
      "Loss on train= 0.024972811341285706\n",
      "Loss on test= 0.026435308158397675\n",
      "acc for Lsat= 0.1387987325571091 \n",
      "acc for Psat= 0.17325756861186836 \n",
      "acc for optim= 0.16866386911310324\n",
      "Epoch:134/1000\n",
      "Loss on train= 0.02482980117201805\n",
      "Loss on test= 0.027825111523270607\n",
      "acc for Lsat= 0.13074534238984203 \n",
      "acc for Psat= 0.1726736934752285 \n",
      "acc for optim= 0.1755275773660868\n",
      "Epoch:135/1000\n",
      "Loss on train= 0.024944165721535683\n",
      "Loss on test= 0.0256833303719759\n",
      "acc for Lsat= 0.12620141816480432 \n",
      "acc for Psat= 0.16234091714371082 \n",
      "acc for optim= 0.1579030924704932\n",
      "Epoch:136/1000\n",
      "Loss on train= 0.024934379383921623\n",
      "Loss on test= 0.025204645469784737\n",
      "acc for Lsat= 0.11741739564803505 \n",
      "acc for Psat= 0.16439706720478423 \n",
      "acc for optim= 0.15934084031151102\n",
      "Epoch:137/1000\n",
      "Loss on train= 0.02569444477558136\n",
      "Loss on test= 0.02456493489444256\n",
      "acc for Lsat= 0.11629501660096199 \n",
      "acc for Psat= 0.16257959721553308 \n",
      "acc for optim= 0.16099556705819468\n",
      "Epoch:138/1000\n",
      "Loss on train= 0.025250528007745743\n",
      "Loss on test= 0.02612609602510929\n",
      "acc for Lsat= 0.12436102925131702 \n",
      "acc for Psat= 0.17815055108240976 \n",
      "acc for optim= 0.1683148623152581\n",
      "Epoch:139/1000\n",
      "Loss on train= 0.02508150041103363\n",
      "Loss on test= 0.027576101943850517\n",
      "acc for Lsat= 0.14193481059748286 \n",
      "acc for Psat= 0.18492846247975006 \n",
      "acc for optim= 0.1739603015092703\n",
      "Epoch:140/1000\n",
      "Loss on train= 0.02580314688384533\n",
      "Loss on test= 0.025059591978788376\n",
      "acc for Lsat= 0.12641286394677131 \n",
      "acc for Psat= 0.15857973083827065 \n",
      "acc for optim= 0.1599177200380506\n",
      "Epoch:141/1000\n",
      "Loss on train= 0.025482961907982826\n",
      "Loss on test= 0.02518044225871563\n",
      "acc for Lsat= 0.11534588905267935 \n",
      "acc for Psat= 0.17468360688050874 \n",
      "acc for optim= 0.15954123325979133\n",
      "Epoch:142/1000\n",
      "Loss on train= 0.024825727567076683\n",
      "Loss on test= 0.024638544768095016\n",
      "acc for Lsat= 0.11450284836757162 \n",
      "acc for Psat= 0.1620182877469788 \n",
      "acc for optim= 0.16108178751199959\n",
      "Epoch:143/1000\n",
      "Loss on train= 0.023766128346323967\n",
      "Loss on test= 0.025633521378040314\n",
      "acc for Lsat= 0.120082018317818 \n",
      "acc for Psat= 0.17844606742449443 \n",
      "acc for optim= 0.1586324285949066\n",
      "Epoch:144/1000\n",
      "Loss on train= 0.024157077074050903\n",
      "Loss on test= 0.025608647614717484\n",
      "acc for Lsat= 0.1483104274503234 \n",
      "acc for Psat= 0.17063909724274773 \n",
      "acc for optim= 0.15637039061309183\n",
      "Epoch:145/1000\n",
      "Loss on train= 0.023478461429476738\n",
      "Loss on test= 0.0242780689150095\n",
      "acc for Lsat= 0.11141182193815813 \n",
      "acc for Psat= 0.16218384002103792 \n",
      "acc for optim= 0.15922771572853459\n",
      "Epoch:146/1000\n",
      "Loss on train= 0.024037763476371765\n",
      "Loss on test= 0.02565591037273407\n",
      "acc for Lsat= 0.10944186740143355 \n",
      "acc for Psat= 0.1580743818995351 \n",
      "acc for optim= 0.15765469972050897\n",
      "Epoch:147/1000\n",
      "Loss on train= 0.023808110505342484\n",
      "Loss on test= 0.02471528761088848\n",
      "acc for Lsat= 0.12514288726134462 \n",
      "acc for Psat= 0.1585920319254369 \n",
      "acc for optim= 0.1585130952125372\n",
      "Epoch:148/1000\n",
      "Loss on train= 0.023209650069475174\n",
      "Loss on test= 0.024303555488586426\n",
      "acc for Lsat= 0.114602690892484 \n",
      "acc for Psat= 0.1535322966217355 \n",
      "acc for optim= 0.15875090654932753\n",
      "Epoch:149/1000\n",
      "Loss on train= 0.024173619225621223\n",
      "Loss on test= 0.025102484971284866\n",
      "acc for Lsat= 0.12314355170790757 \n",
      "acc for Psat= 0.17090261362106507 \n",
      "acc for optim= 0.1563777494729098\n",
      "Epoch:150/1000\n",
      "Loss on train= 0.022353218868374825\n",
      "Loss on test= 0.024166753515601158\n",
      "acc for Lsat= 0.10885757704753568 \n",
      "acc for Psat= 0.1519569228288313 \n",
      "acc for optim= 0.15863676706665533\n",
      "Epoch:151/1000\n",
      "Loss on train= 0.022532612085342407\n",
      "Loss on test= 0.024901550263166428\n",
      "acc for Lsat= 0.10375602667574804 \n",
      "acc for Psat= 0.14814372799477551 \n",
      "acc for optim= 0.1576068686884503\n",
      "Epoch:152/1000\n",
      "Loss on train= 0.023678844794631004\n",
      "Loss on test= 0.025126297026872635\n",
      "acc for Lsat= 0.10464680597905819 \n",
      "acc for Psat= 0.1497694331332908 \n",
      "acc for optim= 0.1583697711205866\n",
      "Epoch:153/1000\n",
      "Loss on train= 0.023427005857229233\n",
      "Loss on test= 0.024349933490157127\n",
      "acc for Lsat= 0.11050129483765483 \n",
      "acc for Psat= 0.1561244731619021 \n",
      "acc for optim= 0.15676654709473067\n",
      "Epoch:154/1000\n",
      "Loss on train= 0.022820178419351578\n",
      "Loss on test= 0.02456808276474476\n",
      "acc for Lsat= 0.1184307044222018 \n",
      "acc for Psat= 0.1817622123549364 \n",
      "acc for optim= 0.15834736516949102\n",
      "Epoch:155/1000\n",
      "Loss on train= 0.022396717220544815\n",
      "Loss on test= 0.025129925459623337\n",
      "acc for Lsat= 0.1215038011986796 \n",
      "acc for Psat= 0.15967239409098688 \n",
      "acc for optim= 0.15658993249715758\n",
      "Epoch:156/1000\n",
      "Loss on train= 0.023240791633725166\n",
      "Loss on test= 0.024104982614517212\n",
      "acc for Lsat= 0.10648875854949404 \n",
      "acc for Psat= 0.15761455928703544 \n",
      "acc for optim= 0.15848895155679774\n",
      "Epoch:157/1000\n",
      "Loss on train= 0.02241666428744793\n",
      "Loss on test= 0.024502255022525787\n",
      "acc for Lsat= 0.11219466462971268 \n",
      "acc for Psat= 0.1495143662838262 \n",
      "acc for optim= 0.15652723984129743\n",
      "Epoch:158/1000\n",
      "Loss on train= 0.023239850997924805\n",
      "Loss on test= 0.026449447497725487\n",
      "acc for Lsat= 0.12080581257509632 \n",
      "acc for Psat= 0.25256806396627685 \n",
      "acc for optim= 0.15735694735122707\n",
      "Epoch:159/1000\n",
      "Loss on train= 0.024305591359734535\n",
      "Loss on test= 0.0247946847230196\n",
      "acc for Lsat= 0.11998712623055378 \n",
      "acc for Psat= 0.1469889638995442 \n",
      "acc for optim= 0.15776987299722936\n",
      "Epoch:160/1000\n",
      "Loss on train= 0.022502513602375984\n",
      "Loss on test= 0.02526748739182949\n",
      "acc for Lsat= 0.14042056189026944 \n",
      "acc for Psat= 0.15749595094878688 \n",
      "acc for optim= 0.15791075918456954\n",
      "Epoch:161/1000\n",
      "Loss on train= 0.02271335944533348\n",
      "Loss on test= 0.023987479507923126\n",
      "acc for Lsat= 0.10156525319078955 \n",
      "acc for Psat= 0.1809980937321412 \n",
      "acc for optim= 0.16070108200441743\n",
      "Epoch:162/1000\n",
      "Loss on train= 0.024397382512688637\n",
      "Loss on test= 0.024270063266158104\n",
      "acc for Lsat= 0.12019746247045898 \n",
      "acc for Psat= 0.15136013546015575 \n",
      "acc for optim= 0.1630675892497218\n",
      "Epoch:163/1000\n",
      "Loss on train= 0.023058589547872543\n",
      "Loss on test= 0.025254692882299423\n",
      "acc for Lsat= 0.13140617030037965 \n",
      "acc for Psat= 0.17054701941171146 \n",
      "acc for optim= 0.15826805812941466\n",
      "Epoch:164/1000\n",
      "Loss on train= 0.02268262393772602\n",
      "Loss on test= 0.02420749142765999\n",
      "acc for Lsat= 0.11904514356248064 \n",
      "acc for Psat= 0.18193269599102477 \n",
      "acc for optim= 0.15710813954392575\n",
      "Epoch:165/1000\n",
      "Loss on train= 0.02151556685566902\n",
      "Loss on test= 0.023062080144882202\n",
      "acc for Lsat= 0.10595362784610879 \n",
      "acc for Psat= 0.1519054876670428 \n",
      "acc for optim= 0.15500288815745725\n",
      "Epoch:166/1000\n",
      "Loss on train= 0.021592404693365097\n",
      "Loss on test= 0.023113930597901344\n",
      "acc for Lsat= 0.10930224810601984 \n",
      "acc for Psat= 0.15894534351147568 \n",
      "acc for optim= 0.15749165742043306\n",
      "Epoch:167/1000\n",
      "Loss on train= 0.022994453087449074\n",
      "Loss on test= 0.023068949580192566\n",
      "acc for Lsat= 0.10788875368285478 \n",
      "acc for Psat= 0.1503457339377224 \n",
      "acc for optim= 0.15548397859221924\n",
      "Epoch:168/1000\n",
      "Loss on train= 0.022485533729195595\n",
      "Loss on test= 0.023595621809363365\n",
      "acc for Lsat= 0.1035662085617591 \n",
      "acc for Psat= 0.15498440927810872 \n",
      "acc for optim= 0.15863085757001352\n",
      "Epoch:169/1000\n",
      "Loss on train= 0.022761061787605286\n",
      "Loss on test= 0.024539653211832047\n",
      "acc for Lsat= 0.11625882556698618 \n",
      "acc for Psat= 0.1517401221933007 \n",
      "acc for optim= 0.15752023164189566\n",
      "Epoch:170/1000\n",
      "Loss on train= 0.023513436317443848\n",
      "Loss on test= 0.0266218651086092\n",
      "acc for Lsat= 0.15025041838024944 \n",
      "acc for Psat= 0.19278281005849132 \n",
      "acc for optim= 0.15640637158068005\n",
      "Epoch:171/1000\n",
      "Loss on train= 0.023337867110967636\n",
      "Loss on test= 0.024481790140271187\n",
      "acc for Lsat= 0.11576963317628836 \n",
      "acc for Psat= 0.18056633732613167 \n",
      "acc for optim= 0.15734666327884242\n",
      "Epoch:172/1000\n",
      "Loss on train= 0.021468473598361015\n",
      "Loss on test= 0.023266475647687912\n",
      "acc for Lsat= 0.11627781677971159 \n",
      "acc for Psat= 0.14811703823973324 \n",
      "acc for optim= 0.16062832052464557\n",
      "Epoch:173/1000\n",
      "Loss on train= 0.022257938981056213\n",
      "Loss on test= 0.023569852113723755\n",
      "acc for Lsat= 0.11352899357116711 \n",
      "acc for Psat= 0.14584809293260725 \n",
      "acc for optim= 0.15620206941005793\n",
      "Epoch:174/1000\n",
      "Loss on train= 0.02219436876475811\n",
      "Loss on test= 0.02283150888979435\n",
      "acc for Lsat= 0.10217896723363394 \n",
      "acc for Psat= 0.14889765120576232 \n",
      "acc for optim= 0.1594574590595123\n",
      "Epoch:175/1000\n",
      "Loss on train= 0.021723110228776932\n",
      "Loss on test= 0.02293272130191326\n",
      "acc for Lsat= 0.11456737134452369 \n",
      "acc for Psat= 0.1501906158992344 \n",
      "acc for optim= 0.16232862961953357\n",
      "Epoch:176/1000\n",
      "Loss on train= 0.02179100550711155\n",
      "Loss on test= 0.02316153235733509\n",
      "acc for Lsat= 0.11037370186159137 \n",
      "acc for Psat= 0.1455332231116423 \n",
      "acc for optim= 0.15608879578348137\n",
      "Epoch:177/1000\n",
      "Loss on train= 0.021892013028264046\n",
      "Loss on test= 0.023092979565262794\n",
      "acc for Lsat= 0.09922464563509645 \n",
      "acc for Psat= 0.15706666959632912 \n",
      "acc for optim= 0.15846002353538557\n",
      "Epoch:178/1000\n",
      "Loss on train= 0.021914932876825333\n",
      "Loss on test= 0.022850971668958664\n",
      "acc for Lsat= 0.09585570928662324 \n",
      "acc for Psat= 0.15055680038250835 \n",
      "acc for optim= 0.15600106241664646\n",
      "Epoch:179/1000\n",
      "Loss on train= 0.021292541176080704\n",
      "Loss on test= 0.024349648505449295\n",
      "acc for Lsat= 0.12885769576847234 \n",
      "acc for Psat= 0.15422661015513975 \n",
      "acc for optim= 0.15697955827167076\n",
      "Epoch:180/1000\n",
      "Loss on train= 0.021870221942663193\n",
      "Loss on test= 0.02377278544008732\n",
      "acc for Lsat= 0.1181468321620245 \n",
      "acc for Psat= 0.1792275525376281 \n",
      "acc for optim= 0.16094332613544088\n",
      "Epoch:181/1000\n",
      "Loss on train= 0.02207900583744049\n",
      "Loss on test= 0.022329626604914665\n",
      "acc for Lsat= 0.09369429332838929 \n",
      "acc for Psat= 0.15223622356203245 \n",
      "acc for optim= 0.1618885987453256\n",
      "Epoch:182/1000\n",
      "Loss on train= 0.02083928883075714\n",
      "Loss on test= 0.02257145754992962\n",
      "acc for Lsat= 0.1018926257097657 \n",
      "acc for Psat= 0.13973594283375207 \n",
      "acc for optim= 0.16283356988792558\n",
      "Epoch:183/1000\n",
      "Loss on train= 0.020741356536746025\n",
      "Loss on test= 0.023010587319731712\n",
      "acc for Lsat= 0.10052346542930775 \n",
      "acc for Psat= 0.14815558468079948 \n",
      "acc for optim= 0.1577806638263845\n",
      "Epoch:184/1000\n",
      "Loss on train= 0.02196511998772621\n",
      "Loss on test= 0.023368561640381813\n",
      "acc for Lsat= 0.1076300762527104 \n",
      "acc for Psat= 0.14536971631118348 \n",
      "acc for optim= 0.1564477618990303\n",
      "Epoch:185/1000\n",
      "Loss on train= 0.021983971819281578\n",
      "Loss on test= 0.021941492334008217\n",
      "acc for Lsat= 0.10074235895025496 \n",
      "acc for Psat= 0.15107689702446858 \n",
      "acc for optim= 0.157015792745597\n",
      "Epoch:186/1000\n",
      "Loss on train= 0.02182728610932827\n",
      "Loss on test= 0.023808537051081657\n",
      "acc for Lsat= 0.11132100251151754 \n",
      "acc for Psat= 0.14853360421542405 \n",
      "acc for optim= 0.1653695690397287\n",
      "Epoch:187/1000\n",
      "Loss on train= 0.021867303177714348\n",
      "Loss on test= 0.02312537468969822\n",
      "acc for Lsat= 0.11366361430804504 \n",
      "acc for Psat= 0.16986817827378276 \n",
      "acc for optim= 0.15627916817801582\n",
      "Epoch:188/1000\n",
      "Loss on train= 0.021677915006875992\n",
      "Loss on test= 0.021760959178209305\n",
      "acc for Lsat= 0.09248552277604244 \n",
      "acc for Psat= 0.14193084887187257 \n",
      "acc for optim= 0.15709886619144775\n",
      "Epoch:189/1000\n",
      "Loss on train= 0.020679892972111702\n",
      "Loss on test= 0.022307967767119408\n",
      "acc for Lsat= 0.0931741725886657 \n",
      "acc for Psat= 0.13915276398385792 \n",
      "acc for optim= 0.16534501540213026\n",
      "Epoch:190/1000\n",
      "Loss on train= 0.02133287861943245\n",
      "Loss on test= 0.02223481796681881\n",
      "acc for Lsat= 0.11150926021002698 \n",
      "acc for Psat= 0.15572161297038972 \n",
      "acc for optim= 0.15763617822223996\n",
      "Epoch:191/1000\n",
      "Loss on train= 0.020659813657402992\n",
      "Loss on test= 0.023396866396069527\n",
      "acc for Lsat= 0.10639183882737202 \n",
      "acc for Psat= 0.13712442308291128 \n",
      "acc for optim= 0.1585005244543386\n",
      "Epoch:192/1000\n",
      "Loss on train= 0.020793389528989792\n",
      "Loss on test= 0.022013725712895393\n",
      "acc for Lsat= 0.10089131723789493 \n",
      "acc for Psat= 0.1474066416671323 \n",
      "acc for optim= 0.158208956680059\n",
      "Epoch:193/1000\n",
      "Loss on train= 0.020610123872756958\n",
      "Loss on test= 0.023656923323869705\n",
      "acc for Lsat= 0.10916705377839758 \n",
      "acc for Psat= 0.14223117701482685 \n",
      "acc for optim= 0.16736315952643938\n",
      "Epoch:194/1000\n",
      "Loss on train= 0.020184054970741272\n",
      "Loss on test= 0.022900784388184547\n",
      "acc for Lsat= 0.10350602945189573 \n",
      "acc for Psat= 0.14371579605053061 \n",
      "acc for optim= 0.15548996890167002\n",
      "Epoch:195/1000\n",
      "Loss on train= 0.020225953310728073\n",
      "Loss on test= 0.021868860349059105\n",
      "acc for Lsat= 0.09915052760585688 \n",
      "acc for Psat= 0.15069625746585388 \n",
      "acc for optim= 0.15867686477458112\n",
      "Epoch:196/1000\n",
      "Loss on train= 0.020318398252129555\n",
      "Loss on test= 0.022608233615756035\n",
      "acc for Lsat= 0.11404096846716985 \n",
      "acc for Psat= 0.14275455344768245 \n",
      "acc for optim= 0.15808529811169897\n",
      "Epoch:197/1000\n",
      "Loss on train= 0.021078461781144142\n",
      "Loss on test= 0.022065190598368645\n",
      "acc for Lsat= 0.11048114302972988 \n",
      "acc for Psat= 0.16347570197526795 \n",
      "acc for optim= 0.15884812974332696\n",
      "Epoch:198/1000\n",
      "Loss on train= 0.020263124257326126\n",
      "Loss on test= 0.023462140932679176\n",
      "acc for Lsat= 0.10421816877261043 \n",
      "acc for Psat= 0.140521461230058 \n",
      "acc for optim= 0.1683687420991751\n",
      "Epoch:199/1000\n",
      "Loss on train= 0.021014807745814323\n",
      "Loss on test= 0.02422703616321087\n",
      "acc for Lsat= 0.129819228866659 \n",
      "acc for Psat= 0.1643952469706322 \n",
      "acc for optim= 0.15634714543073036\n",
      "Epoch:200/1000\n",
      "Loss on train= 0.02078813500702381\n",
      "Loss on test= 0.023021649569272995\n",
      "acc for Lsat= 0.09856329312904576 \n",
      "acc for Psat= 0.18205238729981918 \n",
      "acc for optim= 0.16465898240401622\n",
      "Epoch:201/1000\n",
      "Loss on train= 0.02136196196079254\n",
      "Loss on test= 0.022393707185983658\n",
      "acc for Lsat= 0.10624305878854183 \n",
      "acc for Psat= 0.14477174004843069 \n",
      "acc for optim= 0.15527194906859146\n",
      "Epoch:202/1000\n",
      "Loss on train= 0.020293358713388443\n",
      "Loss on test= 0.02169729210436344\n",
      "acc for Lsat= 0.0941736660511208 \n",
      "acc for Psat= 0.1555074377755146 \n",
      "acc for optim= 0.15896396792638703\n",
      "Epoch:203/1000\n",
      "Loss on train= 0.02020394429564476\n",
      "Loss on test= 0.021778235211968422\n",
      "acc for Lsat= 0.114668591376494 \n",
      "acc for Psat= 0.14632988846152753 \n",
      "acc for optim= 0.15617517778826528\n",
      "Epoch:204/1000\n",
      "Loss on train= 0.019907360896468163\n",
      "Loss on test= 0.021958978846669197\n",
      "acc for Lsat= 0.09782635943834171 \n",
      "acc for Psat= 0.1306533833948998 \n",
      "acc for optim= 0.15652285362185647\n",
      "Epoch:205/1000\n",
      "Loss on train= 0.019380658864974976\n",
      "Loss on test= 0.021026935428380966\n",
      "acc for Lsat= 0.08883028952721407 \n",
      "acc for Psat= 0.1323266529119932 \n",
      "acc for optim= 0.15762399955384418\n",
      "Epoch:206/1000\n",
      "Loss on train= 0.02045430615544319\n",
      "Loss on test= 0.02242426760494709\n",
      "acc for Lsat= 0.10775404094796705 \n",
      "acc for Psat= 0.1432346389843867 \n",
      "acc for optim= 0.15547902552087753\n",
      "Epoch:207/1000\n",
      "Loss on train= 0.02029300294816494\n",
      "Loss on test= 0.020411215722560883\n",
      "acc for Lsat= 0.08845782800309347 \n",
      "acc for Psat= 0.1341422766820263 \n",
      "acc for optim= 0.1569168728558023\n",
      "Epoch:208/1000\n",
      "Loss on train= 0.020354479551315308\n",
      "Loss on test= 0.022433103993535042\n",
      "acc for Lsat= 0.09317386207938833 \n",
      "acc for Psat= 0.1389854405028662 \n",
      "acc for optim= 0.15575001262168334\n",
      "Epoch:209/1000\n",
      "Loss on train= 0.02026243507862091\n",
      "Loss on test= 0.021338248625397682\n",
      "acc for Lsat= 0.08995029393696828 \n",
      "acc for Psat= 0.13022426616313842 \n",
      "acc for optim= 0.15689213177812342\n",
      "Epoch:210/1000\n",
      "Loss on train= 0.020871620625257492\n",
      "Loss on test= 0.02098863199353218\n",
      "acc for Lsat= 0.0954268957399938 \n",
      "acc for Psat= 0.15559079137385856 \n",
      "acc for optim= 0.15759570564481568\n",
      "Epoch:211/1000\n",
      "Loss on train= 0.020187625661492348\n",
      "Loss on test= 0.02223975956439972\n",
      "acc for Lsat= 0.09279777312747908 \n",
      "acc for Psat= 0.13218195807315375 \n",
      "acc for optim= 0.15554003203704234\n",
      "Epoch:212/1000\n",
      "Loss on train= 0.019774388521909714\n",
      "Loss on test= 0.02207721583545208\n",
      "acc for Lsat= 0.09550263043592996 \n",
      "acc for Psat= 0.15378129527052745 \n",
      "acc for optim= 0.15982670696562218\n",
      "Epoch:213/1000\n",
      "Loss on train= 0.02037612535059452\n",
      "Loss on test= 0.021043293178081512\n",
      "acc for Lsat= 0.08901994864394712 \n",
      "acc for Psat= 0.15645900237538948 \n",
      "acc for optim= 0.1559859428401497\n",
      "Epoch:214/1000\n",
      "Loss on train= 0.020154088735580444\n",
      "Loss on test= 0.021672794595360756\n",
      "acc for Lsat= 0.1054453986275388 \n",
      "acc for Psat= 0.13600904390295848 \n",
      "acc for optim= 0.1543099726781862\n",
      "Epoch:215/1000\n",
      "Loss on train= 0.020681003108620644\n",
      "Loss on test= 0.022450251504778862\n",
      "acc for Lsat= 0.11987048098567557 \n",
      "acc for Psat= 0.1491566506609294 \n",
      "acc for optim= 0.15651053029224146\n",
      "Epoch:216/1000\n",
      "Loss on train= 0.02053811587393284\n",
      "Loss on test= 0.02155441790819168\n",
      "acc for Lsat= 0.09701381769291188 \n",
      "acc for Psat= 0.13717896310289346 \n",
      "acc for optim= 0.1565022787275809\n",
      "Epoch:217/1000\n",
      "Loss on train= 0.01919839344918728\n",
      "Loss on test= 0.02122698351740837\n",
      "acc for Lsat= 0.09116058237650738 \n",
      "acc for Psat= 0.1490435704882115 \n",
      "acc for optim= 0.15369488477493654\n",
      "Epoch:218/1000\n",
      "Loss on train= 0.022600028663873672\n",
      "Loss on test= 0.021242231130599976\n",
      "acc for Lsat= 0.10126490334918546 \n",
      "acc for Psat= 0.13389927164813056 \n",
      "acc for optim= 0.15550197569756688\n",
      "Epoch:219/1000\n",
      "Loss on train= 0.020026978105306625\n",
      "Loss on test= 0.021500850096344948\n",
      "acc for Lsat= 0.09901272765425749 \n",
      "acc for Psat= 0.12716274739164782 \n",
      "acc for optim= 0.15730519313931673\n",
      "Epoch:220/1000\n",
      "Loss on train= 0.01957204006612301\n",
      "Loss on test= 0.021329563111066818\n",
      "acc for Lsat= 0.10248513882829803 \n",
      "acc for Psat= 0.15753756657056178 \n",
      "acc for optim= 0.15524982852671357\n",
      "Epoch:221/1000\n",
      "Loss on train= 0.019756123423576355\n",
      "Loss on test= 0.021796084940433502\n",
      "acc for Lsat= 0.10683465263071557 \n",
      "acc for Psat= 0.12857877012868685 \n",
      "acc for optim= 0.16022880998621683\n",
      "Epoch:222/1000\n",
      "Loss on train= 0.018782198429107666\n",
      "Loss on test= 0.02099030837416649\n",
      "acc for Lsat= 0.09517342369863516 \n",
      "acc for Psat= 0.15146352895044043 \n",
      "acc for optim= 0.1548672344688866\n",
      "Epoch:223/1000\n",
      "Loss on train= 0.019412193447351456\n",
      "Loss on test= 0.0206619743257761\n",
      "acc for Lsat= 0.08844259194696524 \n",
      "acc for Psat= 0.12818444256066 \n",
      "acc for optim= 0.15575482478508587\n",
      "Epoch:224/1000\n",
      "Loss on train= 0.018695423379540443\n",
      "Loss on test= 0.021529240533709526\n",
      "acc for Lsat= 0.09896239387754464 \n",
      "acc for Psat= 0.17623629141365693 \n",
      "acc for optim= 0.1560368250009199\n",
      "Epoch:225/1000\n",
      "Loss on train= 0.019362276419997215\n",
      "Loss on test= 0.02088833972811699\n",
      "acc for Lsat= 0.10351692433860521 \n",
      "acc for Psat= 0.13443429707841067 \n",
      "acc for optim= 0.15504675074638755\n",
      "Epoch:226/1000\n",
      "Loss on train= 0.018969720229506493\n",
      "Loss on test= 0.020783042535185814\n",
      "acc for Lsat= 0.09493923359992042 \n",
      "acc for Psat= 0.1255398808097157 \n",
      "acc for optim= 0.15333649074668745\n",
      "Epoch:227/1000\n",
      "Loss on train= 0.018946900963783264\n",
      "Loss on test= 0.021046891808509827\n",
      "acc for Lsat= 0.09206147995745773 \n",
      "acc for Psat= 0.13237270490001485 \n",
      "acc for optim= 0.16008185309246317\n",
      "Epoch:228/1000\n",
      "Loss on train= 0.019135277718305588\n",
      "Loss on test= 0.021453090012073517\n",
      "acc for Lsat= 0.08843047779340009 \n",
      "acc for Psat= 0.14376901466006242 \n",
      "acc for optim= 0.15493452729180798\n",
      "Epoch:229/1000\n",
      "Loss on train= 0.02012387290596962\n",
      "Loss on test= 0.021216196939349174\n",
      "acc for Lsat= 0.09593107000339864 \n",
      "acc for Psat= 0.1310079040488958 \n",
      "acc for optim= 0.15972743980146692\n",
      "Epoch:230/1000\n",
      "Loss on train= 0.019743138924241066\n",
      "Loss on test= 0.02087808959186077\n",
      "acc for Lsat= 0.09870213662575736 \n",
      "acc for Psat= 0.13552934941110972 \n",
      "acc for optim= 0.1618404528962478\n",
      "Epoch:231/1000\n",
      "Loss on train= 0.019473502412438393\n",
      "Loss on test= 0.02117452584207058\n",
      "acc for Lsat= 0.09044185446072137 \n",
      "acc for Psat= 0.13614039383762 \n",
      "acc for optim= 0.1524428218016684\n",
      "Epoch:232/1000\n",
      "Loss on train= 0.018748998641967773\n",
      "Loss on test= 0.02124142460525036\n",
      "acc for Lsat= 0.09048837510225813 \n",
      "acc for Psat= 0.17668991982403723 \n",
      "acc for optim= 0.15625046489490377\n",
      "Epoch:233/1000\n",
      "Loss on train= 0.019127268344163895\n",
      "Loss on test= 0.021406369283795357\n",
      "acc for Lsat= 0.0932191893964846 \n",
      "acc for Psat= 0.1627307427164054 \n",
      "acc for optim= 0.15669724323881762\n",
      "Epoch:234/1000\n",
      "Loss on train= 0.01897827722132206\n",
      "Loss on test= 0.02082129754126072\n",
      "acc for Lsat= 0.08608191874884538 \n",
      "acc for Psat= 0.12349969254196864 \n",
      "acc for optim= 0.1551173522242067\n",
      "Epoch:235/1000\n",
      "Loss on train= 0.01911003701388836\n",
      "Loss on test= 0.020647980272769928\n",
      "acc for Lsat= 0.09889596526332749 \n",
      "acc for Psat= 0.13206600618277123 \n",
      "acc for optim= 0.15555728194112217\n",
      "Epoch:236/1000\n",
      "Loss on train= 0.01925942860543728\n",
      "Loss on test= 0.02105221152305603\n",
      "acc for Lsat= 0.09713504387462285 \n",
      "acc for Psat= 0.1353027236909474 \n",
      "acc for optim= 0.15354937172106645\n",
      "Epoch:237/1000\n",
      "Loss on train= 0.019044864922761917\n",
      "Loss on test= 0.020985232666134834\n",
      "acc for Lsat= 0.10109488174505865 \n",
      "acc for Psat= 0.15823266458212798 \n",
      "acc for optim= 0.15767631381483543\n",
      "Epoch:238/1000\n",
      "Loss on train= 0.019840529188513756\n",
      "Loss on test= 0.02131679095327854\n",
      "acc for Lsat= 0.10169748156356473 \n",
      "acc for Psat= 0.13273929095225598 \n",
      "acc for optim= 0.15730077423861716\n",
      "Epoch:239/1000\n",
      "Loss on train= 0.018879199400544167\n",
      "Loss on test= 0.021730002015829086\n",
      "acc for Lsat= 0.09111822776376456 \n",
      "acc for Psat= 0.13542350011349577 \n",
      "acc for optim= 0.16861734306023246\n",
      "Epoch:240/1000\n",
      "Loss on train= 0.019400447607040405\n",
      "Loss on test= 0.02032981812953949\n",
      "acc for Lsat= 0.09098973288092503 \n",
      "acc for Psat= 0.13728711090062298 \n",
      "acc for optim= 0.15915083034307245\n",
      "Epoch:241/1000\n",
      "Loss on train= 0.019975366070866585\n",
      "Loss on test= 0.020609553903341293\n",
      "acc for Lsat= 0.09599786091575896 \n",
      "acc for Psat= 0.15418199409525812 \n",
      "acc for optim= 0.1532477455510223\n",
      "Epoch:242/1000\n",
      "Loss on train= 0.019016185775399208\n",
      "Loss on test= 0.02126789651811123\n",
      "acc for Lsat= 0.09658700708626419 \n",
      "acc for Psat= 0.13534670065895177 \n",
      "acc for optim= 0.15490405103175073\n",
      "Epoch:243/1000\n",
      "Loss on train= 0.018994007259607315\n",
      "Loss on test= 0.020812444388866425\n",
      "acc for Lsat= 0.10934182402272982 \n",
      "acc for Psat= 0.12399108420544319 \n",
      "acc for optim= 0.15449094779802774\n",
      "Epoch:244/1000\n",
      "Loss on train= 0.01814338192343712\n",
      "Loss on test= 0.02124938927590847\n",
      "acc for Lsat= 0.08499362972533342 \n",
      "acc for Psat= 0.1331681479921494 \n",
      "acc for optim= 0.1553330129822167\n",
      "Epoch:245/1000\n",
      "Loss on train= 0.019222334027290344\n",
      "Loss on test= 0.020949453115463257\n",
      "acc for Lsat= 0.08801596680779364 \n",
      "acc for Psat= 0.12293955092993104 \n",
      "acc for optim= 0.15167698932675003\n",
      "Epoch:246/1000\n",
      "Loss on train= 0.018654128536581993\n",
      "Loss on test= 0.020489782094955444\n",
      "acc for Lsat= 0.10176083817678189 \n",
      "acc for Psat= 0.1251951439009583 \n",
      "acc for optim= 0.15645208384355191\n",
      "Epoch:247/1000\n",
      "Loss on train= 0.019125986844301224\n",
      "Loss on test= 0.02077774703502655\n",
      "acc for Lsat= 0.09031166911445065 \n",
      "acc for Psat= 0.13381477853479878 \n",
      "acc for optim= 0.16439017460469904\n",
      "Epoch:248/1000\n",
      "Loss on train= 0.019440727308392525\n",
      "Loss on test= 0.021215559914708138\n",
      "acc for Lsat= 0.09627767148725888 \n",
      "acc for Psat= 0.15301958392899026 \n",
      "acc for optim= 0.15500447534704467\n",
      "Epoch:249/1000\n",
      "Loss on train= 0.01855943724513054\n",
      "Loss on test= 0.022039882838726044\n",
      "acc for Lsat= 0.12049131050092805 \n",
      "acc for Psat= 0.13647041931775045 \n",
      "acc for optim= 0.152390389685554\n",
      "Epoch:250/1000\n",
      "Loss on train= 0.01884380914270878\n",
      "Loss on test= 0.02091878466308117\n",
      "acc for Lsat= 0.0906025587552775 \n",
      "acc for Psat= 0.14555297630417544 \n",
      "acc for optim= 0.15581644617807464\n",
      "Epoch:251/1000\n",
      "Loss on train= 0.018818838521838188\n",
      "Loss on test= 0.020484574139118195\n",
      "acc for Lsat= 0.08674290093838208 \n",
      "acc for Psat= 0.12434851951590595 \n",
      "acc for optim= 0.1536437126306387\n",
      "Epoch:252/1000\n",
      "Loss on train= 0.01922704465687275\n",
      "Loss on test= 0.021227380260825157\n",
      "acc for Lsat= 0.11270874970074417 \n",
      "acc for Psat= 0.152170006789547 \n",
      "acc for optim= 0.1579310722875681\n",
      "Epoch:253/1000\n",
      "Loss on train= 0.01949273981153965\n",
      "Loss on test= 0.01969235949218273\n",
      "acc for Lsat= 0.0905451677832919 \n",
      "acc for Psat= 0.1315514237275064 \n",
      "acc for optim= 0.15710088406991018\n",
      "Epoch:254/1000\n",
      "Loss on train= 0.01840689592063427\n",
      "Loss on test= 0.02034572884440422\n",
      "acc for Lsat= 0.0983063046535567 \n",
      "acc for Psat= 0.13457208344036442 \n",
      "acc for optim= 0.15370814368422006\n",
      "Epoch:255/1000\n",
      "Loss on train= 0.01889793574810028\n",
      "Loss on test= 0.019930116832256317\n",
      "acc for Lsat= 0.08463324070083432 \n",
      "acc for Psat= 0.1191676650789428 \n",
      "acc for optim= 0.15369455996694206\n",
      "Epoch:256/1000\n",
      "Loss on train= 0.01867484673857689\n",
      "Loss on test= 0.019428499042987823\n",
      "acc for Lsat= 0.0870675854904707 \n",
      "acc for Psat= 0.12038684468149924 \n",
      "acc for optim= 0.15163476287998753\n",
      "Epoch:257/1000\n",
      "Loss on train= 0.01909346505999565\n",
      "Loss on test= 0.020779987797141075\n",
      "acc for Lsat= 0.09145836203703941 \n",
      "acc for Psat= 0.13217507825767846 \n",
      "acc for optim= 0.15384557546784497\n",
      "Epoch:258/1000\n",
      "Loss on train= 0.018109140917658806\n",
      "Loss on test= 0.02097596414387226\n",
      "acc for Lsat= 0.09754966966581259 \n",
      "acc for Psat= 0.1251347915964178 \n",
      "acc for optim= 0.1541626077538526\n",
      "Epoch:259/1000\n",
      "Loss on train= 0.018165405839681625\n",
      "Loss on test= 0.02165498211979866\n",
      "acc for Lsat= 0.09754512883682798 \n",
      "acc for Psat= 0.1586219796874229 \n",
      "acc for optim= 0.16880197578338052\n",
      "Epoch:260/1000\n",
      "Loss on train= 0.018793130293488503\n",
      "Loss on test= 0.020170867443084717\n",
      "acc for Lsat= 0.085860564099868 \n",
      "acc for Psat= 0.1210726113460145 \n",
      "acc for optim= 0.15640865759474218\n",
      "Epoch:261/1000\n",
      "Loss on train= 0.01871020905673504\n",
      "Loss on test= 0.020736543461680412\n",
      "acc for Lsat= 0.10884586788034184 \n",
      "acc for Psat= 0.12742992040296358 \n",
      "acc for optim= 0.1535640117200842\n",
      "Epoch:262/1000\n",
      "Loss on train= 0.017744820564985275\n",
      "Loss on test= 0.020458541810512543\n",
      "acc for Lsat= 0.09146898063116296 \n",
      "acc for Psat= 0.12420520820856518 \n",
      "acc for optim= 0.15881378997203915\n",
      "Epoch:263/1000\n",
      "Loss on train= 0.018007364124059677\n",
      "Loss on test= 0.020532280206680298\n",
      "acc for Lsat= 0.08714474027506569 \n",
      "acc for Psat= 0.12989295172989898 \n",
      "acc for optim= 0.15641891169420077\n",
      "Epoch:264/1000\n",
      "Loss on train= 0.017863404005765915\n",
      "Loss on test= 0.019824013113975525\n",
      "acc for Lsat= 0.08784778352713543 \n",
      "acc for Psat= 0.14176176757633369 \n",
      "acc for optim= 0.15467377260247367\n",
      "Epoch:265/1000\n",
      "Loss on train= 0.018375525251030922\n",
      "Loss on test= 0.020244598388671875\n",
      "acc for Lsat= 0.0976130353210224 \n",
      "acc for Psat= 0.1333062951168135 \n",
      "acc for optim= 0.15269260003443058\n",
      "Epoch:266/1000\n",
      "Loss on train= 0.01794610545039177\n",
      "Loss on test= 0.020403960719704628\n",
      "acc for Lsat= 0.08148186963871255 \n",
      "acc for Psat= 0.12556495100313092 \n",
      "acc for optim= 0.15339870922040852\n",
      "Epoch:267/1000\n",
      "Loss on train= 0.01731976494193077\n",
      "Loss on test= 0.019786300137639046\n",
      "acc for Lsat= 0.08531793045123276 \n",
      "acc for Psat= 0.12303675598663169 \n",
      "acc for optim= 0.15341643969360103\n",
      "Epoch:268/1000\n",
      "Loss on train= 0.018139008432626724\n",
      "Loss on test= 0.01993625983595848\n",
      "acc for Lsat= 0.08772800255546843 \n",
      "acc for Psat= 0.1197131592280536 \n",
      "acc for optim= 0.15108508522054168\n",
      "Epoch:269/1000\n",
      "Loss on train= 0.018246931955218315\n",
      "Loss on test= 0.019945373758673668\n",
      "acc for Lsat= 0.08747197090513165 \n",
      "acc for Psat= 0.11965905921617002 \n",
      "acc for optim= 0.15558115240712925\n",
      "Epoch:270/1000\n",
      "Loss on train= 0.018459472805261612\n",
      "Loss on test= 0.020333124324679375\n",
      "acc for Lsat= 0.09047121333094957 \n",
      "acc for Psat= 0.13058352619676128 \n",
      "acc for optim= 0.1526244694943505\n",
      "Epoch:271/1000\n",
      "Loss on train= 0.017674656584858894\n",
      "Loss on test= 0.020437508821487427\n",
      "acc for Lsat= 0.0885683267508935 \n",
      "acc for Psat= 0.12280258287897262 \n",
      "acc for optim= 0.15268177088343396\n",
      "Epoch:272/1000\n",
      "Loss on train= 0.018424343317747116\n",
      "Loss on test= 0.02078329771757126\n",
      "acc for Lsat= 0.08786942185784068 \n",
      "acc for Psat= 0.12608607445505313 \n",
      "acc for optim= 0.15399325448413226\n",
      "Epoch:273/1000\n",
      "Loss on train= 0.018450845032930374\n",
      "Loss on test= 0.019923288375139236\n",
      "acc for Lsat= 0.0999619169397303 \n",
      "acc for Psat= 0.14860998768499373 \n",
      "acc for optim= 0.16062080666076303\n",
      "Epoch:274/1000\n",
      "Loss on train= 0.01866307482123375\n",
      "Loss on test= 0.019834157079458237\n",
      "acc for Lsat= 0.08595293828008001 \n",
      "acc for Psat= 0.12072188746523986 \n",
      "acc for optim= 0.15416591495861948\n",
      "Epoch:275/1000\n",
      "Loss on train= 0.01777772419154644\n",
      "Loss on test= 0.019268879666924477\n",
      "acc for Lsat= 0.09087915182220271 \n",
      "acc for Psat= 0.11854006631003296 \n",
      "acc for optim= 0.15357959927301287\n",
      "Epoch:276/1000\n",
      "Loss on train= 0.017826925963163376\n",
      "Loss on test= 0.01952163316309452\n",
      "acc for Lsat= 0.08424245385663028 \n",
      "acc for Psat= 0.11704572361355817 \n",
      "acc for optim= 0.15801889446425735\n",
      "Epoch:277/1000\n",
      "Loss on train= 0.01818743348121643\n",
      "Loss on test= 0.01917775347828865\n",
      "acc for Lsat= 0.08872742046183892 \n",
      "acc for Psat= 0.12752232737106162 \n",
      "acc for optim= 0.15921470500488832\n",
      "Epoch:278/1000\n",
      "Loss on train= 0.017972499132156372\n",
      "Loss on test= 0.020137960091233253\n",
      "acc for Lsat= 0.10094535066104744 \n",
      "acc for Psat= 0.12286985028195255 \n",
      "acc for optim= 0.15428366704788954\n",
      "Epoch:279/1000\n",
      "Loss on train= 0.018744533881545067\n",
      "Loss on test= 0.02033197693526745\n",
      "acc for Lsat= 0.08307099022997512 \n",
      "acc for Psat= 0.13038607036065117 \n",
      "acc for optim= 0.1538667875666951\n",
      "Epoch:280/1000\n",
      "Loss on train= 0.017990441992878914\n",
      "Loss on test= 0.019162626937031746\n",
      "acc for Lsat= 0.08146552759654195 \n",
      "acc for Psat= 0.1276637058885025 \n",
      "acc for optim= 0.15265764068405616\n",
      "Epoch:281/1000\n",
      "Loss on train= 0.017796959728002548\n",
      "Loss on test= 0.02012641727924347\n",
      "acc for Lsat= 0.08571885087942184 \n",
      "acc for Psat= 0.14449986189550493 \n",
      "acc for optim= 0.1535187328011064\n",
      "Epoch:282/1000\n",
      "Loss on train= 0.017660053446888924\n",
      "Loss on test= 0.019515467807650566\n",
      "acc for Lsat= 0.09786486131987122 \n",
      "acc for Psat= 0.12162056066269099 \n",
      "acc for optim= 0.15013880582529657\n",
      "Epoch:283/1000\n",
      "Loss on train= 0.017681410536170006\n",
      "Loss on test= 0.020033886656165123\n",
      "acc for Lsat= 0.08549768913517274 \n",
      "acc for Psat= 0.13508067801728013 \n",
      "acc for optim= 0.15683050741023366\n",
      "Epoch:284/1000\n",
      "Loss on train= 0.01734759844839573\n",
      "Loss on test= 0.019536036998033524\n",
      "acc for Lsat= 0.07994031053109758 \n",
      "acc for Psat= 0.12971031914243547 \n",
      "acc for optim= 0.152179361541284\n",
      "Epoch:285/1000\n",
      "Loss on train= 0.0172352846711874\n",
      "Loss on test= 0.019467873498797417\n",
      "acc for Lsat= 0.0883309020872406 \n",
      "acc for Psat= 0.12312720545929107 \n",
      "acc for optim= 0.15223888939309846\n",
      "Epoch:286/1000\n",
      "Loss on train= 0.01730651780962944\n",
      "Loss on test= 0.019133256748318672\n",
      "acc for Lsat= 0.08990234727296506 \n",
      "acc for Psat= 0.11909167960632683 \n",
      "acc for optim= 0.15808791138618278\n",
      "Epoch:287/1000\n",
      "Loss on train= 0.01739758439362049\n",
      "Loss on test= 0.020103856921195984\n",
      "acc for Lsat= 0.09084202572570077 \n",
      "acc for Psat= 0.12173377650794914 \n",
      "acc for optim= 0.1524891309640062\n",
      "Epoch:288/1000\n",
      "Loss on train= 0.017729416489601135\n",
      "Loss on test= 0.020516803488135338\n",
      "acc for Lsat= 0.08743855735163784 \n",
      "acc for Psat= 0.1657351377718351 \n",
      "acc for optim= 0.15620361474844122\n",
      "Epoch:289/1000\n",
      "Loss on train= 0.017137350514531136\n",
      "Loss on test= 0.019429676234722137\n",
      "acc for Lsat= 0.09022753914694877 \n",
      "acc for Psat= 0.12279465123877759 \n",
      "acc for optim= 0.160652117353859\n",
      "Epoch:290/1000\n",
      "Loss on train= 0.017196927219629288\n",
      "Loss on test= 0.01911281608045101\n",
      "acc for Lsat= 0.08629463471219023 \n",
      "acc for Psat= 0.11508575399574529 \n",
      "acc for optim= 0.1553425367595045\n",
      "Epoch:291/1000\n",
      "Loss on train= 0.01691785454750061\n",
      "Loss on test= 0.019798699766397476\n",
      "acc for Lsat= 0.08781825944861274 \n",
      "acc for Psat= 0.14918987937604805 \n",
      "acc for optim= 0.15024661043249002\n",
      "Epoch:292/1000\n",
      "Loss on train= 0.0178497564047575\n",
      "Loss on test= 0.01905732788145542\n",
      "acc for Lsat= 0.09436462225129225 \n",
      "acc for Psat= 0.11836785128163521 \n",
      "acc for optim= 0.1509840080051388\n",
      "Epoch:293/1000\n",
      "Loss on train= 0.017497705295681953\n",
      "Loss on test= 0.020123781636357307\n",
      "acc for Lsat= 0.1020001999487391 \n",
      "acc for Psat= 0.13779695925431093 \n",
      "acc for optim= 0.1572070501464851\n",
      "Epoch:294/1000\n",
      "Loss on train= 0.017152519896626472\n",
      "Loss on test= 0.019648609682917595\n",
      "acc for Lsat= 0.09546348626370507 \n",
      "acc for Psat= 0.12364245993293461 \n",
      "acc for optim= 0.15309814939771865\n",
      "Epoch:295/1000\n",
      "Loss on train= 0.017547115683555603\n",
      "Loss on test= 0.018656320869922638\n",
      "acc for Lsat= 0.08582780209231675 \n",
      "acc for Psat= 0.12117523931646601 \n",
      "acc for optim= 0.1550491491882447\n",
      "Epoch:296/1000\n",
      "Loss on train= 0.0175436120480299\n",
      "Loss on test= 0.020391274243593216\n",
      "acc for Lsat= 0.09210216239867781 \n",
      "acc for Psat= 0.16735380607555503 \n",
      "acc for optim= 0.15505178770140374\n",
      "Epoch:297/1000\n",
      "Loss on train= 0.01837102137506008\n",
      "Loss on test= 0.019053837284445763\n",
      "acc for Lsat= 0.08967987035596094 \n",
      "acc for Psat= 0.11862993334188447 \n",
      "acc for optim= 0.14968213515759368\n",
      "Epoch:298/1000\n",
      "Loss on train= 0.01724362187087536\n",
      "Loss on test= 0.02014716900885105\n",
      "acc for Lsat= 0.1160816879413209 \n",
      "acc for Psat= 0.1337640017324356 \n",
      "acc for optim= 0.1512954083026415\n",
      "Epoch:299/1000\n",
      "Loss on train= 0.017197078093886375\n",
      "Loss on test= 0.01956564374268055\n",
      "acc for Lsat= 0.08817511991546063 \n",
      "acc for Psat= 0.16238036454257043 \n",
      "acc for optim= 0.1584457113086004\n",
      "Epoch:300/1000\n",
      "Loss on train= 0.017605535686016083\n",
      "Loss on test= 0.018830474466085434\n",
      "acc for Lsat= 0.08578512172792808 \n",
      "acc for Psat= 0.12837891753543038 \n",
      "acc for optim= 0.15436143854862888\n",
      "Epoch:301/1000\n",
      "Loss on train= 0.017210496589541435\n",
      "Loss on test= 0.01989460363984108\n",
      "acc for Lsat= 0.08616790759222241 \n",
      "acc for Psat= 0.1310260356218956 \n",
      "acc for optim= 0.1510856098693684\n",
      "Epoch:302/1000\n",
      "Loss on train= 0.016938062384724617\n",
      "Loss on test= 0.019117038697004318\n",
      "acc for Lsat= 0.08344122780243697 \n",
      "acc for Psat= 0.11853677224174594 \n",
      "acc for optim= 0.15452569893732904\n",
      "Epoch:303/1000\n",
      "Loss on train= 0.017056304961442947\n",
      "Loss on test= 0.01886516995728016\n",
      "acc for Lsat= 0.07723699047868281 \n",
      "acc for Psat= 0.11897345884754744 \n",
      "acc for optim= 0.1668727895015041\n",
      "Epoch:304/1000\n",
      "Loss on train= 0.01725838892161846\n",
      "Loss on test= 0.02033236436545849\n",
      "acc for Lsat= 0.08516732525740199 \n",
      "acc for Psat= 0.11551229039970154 \n",
      "acc for optim= 0.15530167628272917\n",
      "Epoch:305/1000\n",
      "Loss on train= 0.016975119709968567\n",
      "Loss on test= 0.018970118835568428\n",
      "acc for Lsat= 0.07925100492876629 \n",
      "acc for Psat= 0.11612832250876588 \n",
      "acc for optim= 0.15334376230009555\n",
      "Epoch:306/1000\n",
      "Loss on train= 0.017065690830349922\n",
      "Loss on test= 0.018936656415462494\n",
      "acc for Lsat= 0.08592331231386803 \n",
      "acc for Psat= 0.12744381361655985 \n",
      "acc for optim= 0.15267816766003592\n",
      "Epoch:307/1000\n",
      "Loss on train= 0.01701323501765728\n",
      "Loss on test= 0.018657540902495384\n",
      "acc for Lsat= 0.07954775138703143 \n",
      "acc for Psat= 0.1197558034404659 \n",
      "acc for optim= 0.15205464451598782\n",
      "Epoch:308/1000\n",
      "Loss on train= 0.017124254256486893\n",
      "Loss on test= 0.019394759088754654\n",
      "acc for Lsat= 0.0886951808959299 \n",
      "acc for Psat= 0.13259940574975262 \n",
      "acc for optim= 0.1531417060623442\n",
      "Epoch:309/1000\n",
      "Loss on train= 0.016871796920895576\n",
      "Loss on test= 0.018755294382572174\n",
      "acc for Lsat= 0.09045582368889947 \n",
      "acc for Psat= 0.11386160993405446 \n",
      "acc for optim= 0.15274527245642675\n",
      "Epoch:310/1000\n",
      "Loss on train= 0.01691426895558834\n",
      "Loss on test= 0.019467011094093323\n",
      "acc for Lsat= 0.10269941149116198 \n",
      "acc for Psat= 0.14518396443885637 \n",
      "acc for optim= 0.15399021914691957\n",
      "Epoch:311/1000\n",
      "Loss on train= 0.017464743927121162\n",
      "Loss on test= 0.01900436170399189\n",
      "acc for Lsat= 0.08533985497486613 \n",
      "acc for Psat= 0.11876516352825811 \n",
      "acc for optim= 0.1528873163173792\n",
      "Epoch:312/1000\n",
      "Loss on train= 0.017096415162086487\n",
      "Loss on test= 0.019879881292581558\n",
      "acc for Lsat= 0.09464943820547332 \n",
      "acc for Psat= 0.12086772066749274 \n",
      "acc for optim= 0.15475332005505915\n",
      "Epoch:313/1000\n",
      "Loss on train= 0.01751765049993992\n",
      "Loss on test= 0.018894923850893974\n",
      "acc for Lsat= 0.09693171798863011 \n",
      "acc for Psat= 0.12934573207430425 \n",
      "acc for optim= 0.15124219781810783\n",
      "Epoch:314/1000\n",
      "Loss on train= 0.016305873170495033\n",
      "Loss on test= 0.01832360029220581\n",
      "acc for Lsat= 0.08105583141869424 \n",
      "acc for Psat= 0.11643636407920414 \n",
      "acc for optim= 0.15307967592437277\n",
      "Epoch:315/1000\n",
      "Loss on train= 0.015861568972468376\n",
      "Loss on test= 0.019010312855243683\n",
      "acc for Lsat= 0.07719052675478363 \n",
      "acc for Psat= 0.11853888070007558 \n",
      "acc for optim= 0.15102373797693064\n",
      "Epoch:316/1000\n",
      "Loss on train= 0.0171776432543993\n",
      "Loss on test= 0.019558314234018326\n",
      "acc for Lsat= 0.0808550776810467 \n",
      "acc for Psat= 0.12005710232876277 \n",
      "acc for optim= 0.15880069147282294\n",
      "Epoch:317/1000\n",
      "Loss on train= 0.017223384231328964\n",
      "Loss on test= 0.019927171990275383\n",
      "acc for Lsat= 0.09361545348103442 \n",
      "acc for Psat= 0.127165933713077 \n",
      "acc for optim= 0.15340235639556787\n",
      "Epoch:318/1000\n",
      "Loss on train= 0.015843430534005165\n",
      "Loss on test= 0.018587442114949226\n",
      "acc for Lsat= 0.083249455647733 \n",
      "acc for Psat= 0.11284251890156907 \n",
      "acc for optim= 0.1507423877822692\n",
      "Epoch:319/1000\n",
      "Loss on train= 0.01666213944554329\n",
      "Loss on test= 0.019297422841191292\n",
      "acc for Lsat= 0.08173232796153784 \n",
      "acc for Psat= 0.11983011763722823 \n",
      "acc for optim= 0.15159929388324347\n",
      "Epoch:320/1000\n",
      "Loss on train= 0.016673704609274864\n",
      "Loss on test= 0.018422668799757957\n",
      "acc for Lsat= 0.0796101738653797 \n",
      "acc for Psat= 0.12166350274904894 \n",
      "acc for optim= 0.15428196089425533\n",
      "Epoch:321/1000\n",
      "Loss on train= 0.0170898400247097\n",
      "Loss on test= 0.019891642034053802\n",
      "acc for Lsat= 0.10959851099894602 \n",
      "acc for Psat= 0.12092109507013091 \n",
      "acc for optim= 0.15838057165282354\n",
      "Epoch:322/1000\n",
      "Loss on train= 0.017253650352358818\n",
      "Loss on test= 0.01833040453493595\n",
      "acc for Lsat= 0.07712071311921681 \n",
      "acc for Psat= 0.11965231803747324 \n",
      "acc for optim= 0.15031372900512438\n",
      "Epoch:323/1000\n",
      "Loss on train= 0.016638970002532005\n",
      "Loss on test= 0.01922031305730343\n",
      "acc for Lsat= 0.08850044809641693 \n",
      "acc for Psat= 0.11364093792886343 \n",
      "acc for optim= 0.15012596689951013\n",
      "Epoch:324/1000\n",
      "Loss on train= 0.017187708988785744\n",
      "Loss on test= 0.019632192328572273\n",
      "acc for Lsat= 0.08343327267758537 \n",
      "acc for Psat= 0.1480923089870187 \n",
      "acc for optim= 0.15394251805085404\n",
      "Epoch:325/1000\n",
      "Loss on train= 0.01662442460656166\n",
      "Loss on test= 0.019448017701506615\n",
      "acc for Lsat= 0.08603551659686408 \n",
      "acc for Psat= 0.11556236754589727 \n",
      "acc for optim= 0.1506527294626389\n",
      "Epoch:326/1000\n",
      "Loss on train= 0.016884032636880875\n",
      "Loss on test= 0.0192393958568573\n",
      "acc for Lsat= 0.08311152824455596 \n",
      "acc for Psat= 0.1134285192583456 \n",
      "acc for optim= 0.1536843755270799\n",
      "Epoch:327/1000\n",
      "Loss on train= 0.016056571155786514\n",
      "Loss on test= 0.019201379269361496\n",
      "acc for Lsat= 0.08869000084711527 \n",
      "acc for Psat= 0.11982265873970416 \n",
      "acc for optim= 0.15679849295582032\n",
      "Epoch:328/1000\n",
      "Loss on train= 0.01667904667556286\n",
      "Loss on test= 0.018772751092910767\n",
      "acc for Lsat= 0.08656551109656879 \n",
      "acc for Psat= 0.1197057913475685 \n",
      "acc for optim= 0.15009771315697476\n",
      "Epoch:329/1000\n",
      "Loss on train= 0.01657891273498535\n",
      "Loss on test= 0.018970420584082603\n",
      "acc for Lsat= 0.08357135267505066 \n",
      "acc for Psat= 0.12276009696540763 \n",
      "acc for optim= 0.1520676008490629\n",
      "Epoch:330/1000\n",
      "Loss on train= 0.016683300957083702\n",
      "Loss on test= 0.019166501238942146\n",
      "acc for Lsat= 0.0847092239289463 \n",
      "acc for Psat= 0.13574200540194567 \n",
      "acc for optim= 0.1512870394479823\n",
      "Epoch:331/1000\n",
      "Loss on train= 0.016258487477898598\n",
      "Loss on test= 0.01952960342168808\n",
      "acc for Lsat= 0.09048947767622782 \n",
      "acc for Psat= 0.11844037210365528 \n",
      "acc for optim= 0.16467027000962933\n",
      "Epoch:332/1000\n",
      "Loss on train= 0.01620718277990818\n",
      "Loss on test= 0.019193142652511597\n",
      "acc for Lsat= 0.08460294725856542 \n",
      "acc for Psat= 0.11365607117497645 \n",
      "acc for optim= 0.15191937079369916\n",
      "Epoch:333/1000\n",
      "Loss on train= 0.016212379559874535\n",
      "Loss on test= 0.019084099680185318\n",
      "acc for Lsat= 0.082268069127378 \n",
      "acc for Psat= 0.11540712360192708 \n",
      "acc for optim= 0.1517798193665438\n",
      "Epoch:334/1000\n",
      "Loss on train= 0.017193343490362167\n",
      "Loss on test= 0.018327802419662476\n",
      "acc for Lsat= 0.0857769228904533 \n",
      "acc for Psat= 0.12295632345305359 \n",
      "acc for optim= 0.16034815046996254\n",
      "Epoch:335/1000\n",
      "Loss on train= 0.016431467607617378\n",
      "Loss on test= 0.01812998577952385\n",
      "acc for Lsat= 0.0854614464776034 \n",
      "acc for Psat= 0.12186451592684215 \n",
      "acc for optim= 0.1512107800285803\n",
      "Epoch:336/1000\n",
      "Loss on train= 0.01711866445839405\n",
      "Loss on test= 0.018731044605374336\n",
      "acc for Lsat= 0.09503221053343555 \n",
      "acc for Psat= 0.1196636683932357 \n",
      "acc for optim= 0.1522920249719739\n",
      "Epoch:337/1000\n",
      "Loss on train= 0.016866708174347878\n",
      "Loss on test= 0.01846027746796608\n",
      "acc for Lsat= 0.08684646057528117 \n",
      "acc for Psat= 0.11308135773073581 \n",
      "acc for optim= 0.15434624305351471\n",
      "Epoch:338/1000\n",
      "Loss on train= 0.01657174900174141\n",
      "Loss on test= 0.0191855076700449\n",
      "acc for Lsat= 0.08627905733683455 \n",
      "acc for Psat= 0.1224132904852865 \n",
      "acc for optim= 0.15204161217899356\n",
      "Epoch:339/1000\n",
      "Loss on train= 0.01640605367720127\n",
      "Loss on test= 0.019225023686885834\n",
      "acc for Lsat= 0.09729386922605135 \n",
      "acc for Psat= 0.11866530499859232 \n",
      "acc for optim= 0.15057934408537604\n",
      "Epoch:340/1000\n",
      "Loss on train= 0.016825826838612556\n",
      "Loss on test= 0.018665945157408714\n",
      "acc for Lsat= 0.08211107117012276 \n",
      "acc for Psat= 0.11874951476061281 \n",
      "acc for optim= 0.155707233237879\n",
      "Epoch:341/1000\n",
      "Loss on train= 0.01624610461294651\n",
      "Loss on test= 0.019008325412869453\n",
      "acc for Lsat= 0.10634242407111234 \n",
      "acc for Psat= 0.12786329979333555 \n",
      "acc for optim= 0.15638033419261038\n",
      "Epoch:342/1000\n",
      "Loss on train= 0.016582006588578224\n",
      "Loss on test= 0.01831740327179432\n",
      "acc for Lsat= 0.07704747625241765 \n",
      "acc for Psat= 0.12312246020450149 \n",
      "acc for optim= 0.15101427071209672\n",
      "Epoch:343/1000\n",
      "Loss on train= 0.0161458570510149\n",
      "Loss on test= 0.018498608842492104\n",
      "acc for Lsat= 0.07902951512439094 \n",
      "acc for Psat= 0.10951397697273008 \n",
      "acc for optim= 0.15125947176663734\n",
      "Epoch:344/1000\n",
      "Loss on train= 0.016612183302640915\n",
      "Loss on test= 0.018766898661851883\n",
      "acc for Lsat= 0.07947688359160755 \n",
      "acc for Psat= 0.11979913343683764 \n",
      "acc for optim= 0.15306503493372148\n",
      "Epoch:345/1000\n",
      "Loss on train= 0.016697445884346962\n",
      "Loss on test= 0.01911875046789646\n",
      "acc for Lsat= 0.08118610975460845 \n",
      "acc for Psat= 0.11932958535090327 \n",
      "acc for optim= 0.15578804655876907\n",
      "Epoch:346/1000\n",
      "Loss on train= 0.01634075492620468\n",
      "Loss on test= 0.018411708995699883\n",
      "acc for Lsat= 0.09822772955638563 \n",
      "acc for Psat= 0.11371046976454567 \n",
      "acc for optim= 0.15169207008025729\n",
      "Epoch:347/1000\n",
      "Loss on train= 0.01646885834634304\n",
      "Loss on test= 0.01872255466878414\n",
      "acc for Lsat= 0.08644321992166991 \n",
      "acc for Psat= 0.1155130534884328 \n",
      "acc for optim= 0.15209967936514102\n",
      "Epoch:348/1000\n",
      "Loss on train= 0.01686358079314232\n",
      "Loss on test= 0.01881570741534233\n",
      "acc for Lsat= 0.08490195703527796 \n",
      "acc for Psat= 0.12480935819033866 \n",
      "acc for optim= 0.15261920223509048\n",
      "Epoch:349/1000\n",
      "Loss on train= 0.016824398189783096\n",
      "Loss on test= 0.017704477533698082\n",
      "acc for Lsat= 0.0829871814931228 \n",
      "acc for Psat= 0.11229391215312458 \n",
      "acc for optim= 0.15029989564141566\n",
      "Epoch:350/1000\n",
      "Loss on train= 0.016363831236958504\n",
      "Loss on test= 0.019302012398838997\n",
      "acc for Lsat= 0.08163468445349678 \n",
      "acc for Psat= 0.11699602028125086 \n",
      "acc for optim= 0.15058896116152645\n",
      "Epoch:351/1000\n",
      "Loss on train= 0.01654902845621109\n",
      "Loss on test= 0.01854308694601059\n",
      "acc for Lsat= 0.08444682346900162 \n",
      "acc for Psat= 0.10966221212487742 \n",
      "acc for optim= 0.15660231147554562\n",
      "Epoch:352/1000\n",
      "Loss on train= 0.016787473112344742\n",
      "Loss on test= 0.01853785291314125\n",
      "acc for Lsat= 0.09308532596913989 \n",
      "acc for Psat= 0.11571633910663653 \n",
      "acc for optim= 0.15669732250553123\n",
      "Epoch:353/1000\n",
      "Loss on train= 0.017265422269701958\n",
      "Loss on test= 0.01969853602349758\n",
      "acc for Lsat= 0.08987736941876694 \n",
      "acc for Psat= 0.1466372269957138 \n",
      "acc for optim= 0.15620640909735764\n",
      "Epoch:354/1000\n",
      "Loss on train= 0.016365153715014458\n",
      "Loss on test= 0.01930861361324787\n",
      "acc for Lsat= 0.10248983702847271 \n",
      "acc for Psat= 0.11388203300175809 \n",
      "acc for optim= 0.1512829263436347\n",
      "Epoch:355/1000\n",
      "Loss on train= 0.016835030168294907\n",
      "Loss on test= 0.0186686459928751\n",
      "acc for Lsat= 0.08902565091581806 \n",
      "acc for Psat= 0.14242731404219205 \n",
      "acc for optim= 0.152931655220994\n",
      "Epoch:356/1000\n",
      "Loss on train= 0.01556763332337141\n",
      "Loss on test= 0.01846608892083168\n",
      "acc for Lsat= 0.08034539294690692 \n",
      "acc for Psat= 0.11780834299293955 \n",
      "acc for optim= 0.15032575567634462\n",
      "Epoch:357/1000\n",
      "Loss on train= 0.015461250208318233\n",
      "Loss on test= 0.01864219456911087\n",
      "acc for Lsat= 0.0866907943126767 \n",
      "acc for Psat= 0.11555728209679797 \n",
      "acc for optim= 0.15143317586620722\n",
      "Epoch:358/1000\n",
      "Loss on train= 0.016512015834450722\n",
      "Loss on test= 0.020059363916516304\n",
      "acc for Lsat= 0.08912749090007038 \n",
      "acc for Psat= 0.1781373587831828 \n",
      "acc for optim= 0.15604940707227202\n",
      "Epoch:359/1000\n",
      "Loss on train= 0.016373364254832268\n",
      "Loss on test= 0.019431736320257187\n",
      "acc for Lsat= 0.09892635352922892 \n",
      "acc for Psat= 0.1472000574489825 \n",
      "acc for optim= 0.15071543471117996\n",
      "Epoch:360/1000\n",
      "Loss on train= 0.016298970207571983\n",
      "Loss on test= 0.019418464973568916\n",
      "acc for Lsat= 0.08523569813994475 \n",
      "acc for Psat= 0.11194655998661605 \n",
      "acc for optim= 0.15004367696151322\n",
      "Epoch:361/1000\n",
      "Loss on train= 0.016358455643057823\n",
      "Loss on test= 0.019434593617916107\n",
      "acc for Lsat= 0.10197281398692158 \n",
      "acc for Psat= 0.11749790907972402 \n",
      "acc for optim= 0.15911129324935203\n",
      "Epoch:362/1000\n",
      "Loss on train= 0.01590130850672722\n",
      "Loss on test= 0.018720146268606186\n",
      "acc for Lsat= 0.08282920041643016 \n",
      "acc for Psat= 0.11073206351780084 \n",
      "acc for optim= 0.1533780138693043\n",
      "Epoch:363/1000\n",
      "Loss on train= 0.01603085733950138\n",
      "Loss on test= 0.01880006119608879\n",
      "acc for Lsat= 0.09215888383136878 \n",
      "acc for Psat= 0.11675550654024046 \n",
      "acc for optim= 0.15051928950550308\n",
      "Epoch:364/1000\n",
      "Loss on train= 0.01630580797791481\n",
      "Loss on test= 0.018649743869900703\n",
      "acc for Lsat= 0.09580027219008035 \n",
      "acc for Psat= 0.1175253957456681 \n",
      "acc for optim= 0.1511471947531794\n",
      "Epoch:365/1000\n",
      "Loss on train= 0.015863163396716118\n",
      "Loss on test= 0.01963806338608265\n",
      "acc for Lsat= 0.09726206781292643 \n",
      "acc for Psat= 0.12897801938764952 \n",
      "acc for optim= 0.15794933769911904\n",
      "Epoch:366/1000\n",
      "Loss on train= 0.01607181504368782\n",
      "Loss on test= 0.018407607451081276\n",
      "acc for Lsat= 0.08352501538443866 \n",
      "acc for Psat= 0.11822817941897673 \n",
      "acc for optim= 0.151201284838064\n",
      "Epoch:367/1000\n",
      "Loss on train= 0.015540576539933681\n",
      "Loss on test= 0.018759572878479958\n",
      "acc for Lsat= 0.08087835640834781 \n",
      "acc for Psat= 0.10987432212010693 \n",
      "acc for optim= 0.15415514443129677\n",
      "Epoch:368/1000\n",
      "Loss on train= 0.015871942043304443\n",
      "Loss on test= 0.01937326416373253\n",
      "acc for Lsat= 0.08632369504738366 \n",
      "acc for Psat= 0.11589898861584805 \n",
      "acc for optim= 0.1545766176712534\n",
      "Epoch:369/1000\n",
      "Loss on train= 0.016560958698391914\n",
      "Loss on test= 0.019027957692742348\n",
      "acc for Lsat= 0.08557336058514275 \n",
      "acc for Psat= 0.15166442587251952 \n",
      "acc for optim= 0.15452836024739874\n",
      "Epoch:370/1000\n",
      "Loss on train= 0.01638984866440296\n",
      "Loss on test= 0.018825167790055275\n",
      "acc for Lsat= 0.07846746443635874 \n",
      "acc for Psat= 0.1366976585925585 \n",
      "acc for optim= 0.15073127939577396\n",
      "Epoch:371/1000\n",
      "Loss on train= 0.015776338055729866\n",
      "Loss on test= 0.018025796860456467\n",
      "acc for Lsat= 0.07364765351275 \n",
      "acc for Psat= 0.11659324628722473 \n",
      "acc for optim= 0.15065595688888128\n",
      "Epoch:372/1000\n",
      "Loss on train= 0.015780329704284668\n",
      "Loss on test= 0.018767179921269417\n",
      "acc for Lsat= 0.07792834056298083 \n",
      "acc for Psat= 0.10904852199852999 \n",
      "acc for optim= 0.1585355382592606\n",
      "Epoch:373/1000\n",
      "Loss on train= 0.01515092235058546\n",
      "Loss on test= 0.018159208819270134\n",
      "acc for Lsat= 0.08125405867112556 \n",
      "acc for Psat= 0.11403965281459214 \n",
      "acc for optim= 0.15011023295800038\n",
      "Epoch:374/1000\n",
      "Loss on train= 0.015275748446583748\n",
      "Loss on test= 0.017289815470576286\n",
      "acc for Lsat= 0.07215439082791426 \n",
      "acc for Psat= 0.11665404564792659 \n",
      "acc for optim= 0.15021841609200767\n",
      "Epoch:375/1000\n",
      "Loss on train= 0.015731235966086388\n",
      "Loss on test= 0.018433989956974983\n",
      "acc for Lsat= 0.07505057201935694 \n",
      "acc for Psat= 0.14790257054492695 \n",
      "acc for optim= 0.15195916049808852\n",
      "Epoch:376/1000\n",
      "Loss on train= 0.01598958671092987\n",
      "Loss on test= 0.018590392544865608\n",
      "acc for Lsat= 0.08119383247465908 \n",
      "acc for Psat= 0.11039694209644751 \n",
      "acc for optim= 0.1517270515877787\n",
      "Epoch:377/1000\n",
      "Loss on train= 0.01686430349946022\n",
      "Loss on test= 0.019028611481189728\n",
      "acc for Lsat= 0.08061999561321757 \n",
      "acc for Psat= 0.11216093377478432 \n",
      "acc for optim= 0.15143348941436183\n",
      "Epoch:378/1000\n",
      "Loss on train= 0.015875879675149918\n",
      "Loss on test= 0.01837209425866604\n",
      "acc for Lsat= 0.08584103720772455 \n",
      "acc for Psat= 0.13742822871438506 \n",
      "acc for optim= 0.15115613275222572\n",
      "Epoch:379/1000\n",
      "Loss on train= 0.01637796126306057\n",
      "Loss on test= 0.01779239997267723\n",
      "acc for Lsat= 0.07529443465639739 \n",
      "acc for Psat= 0.11576475115069762 \n",
      "acc for optim= 0.15053904217556252\n",
      "Epoch:380/1000\n",
      "Loss on train= 0.01580011658370495\n",
      "Loss on test= 0.017891930416226387\n",
      "acc for Lsat= 0.07630142323766069 \n",
      "acc for Psat= 0.10944462199970309 \n",
      "acc for optim= 0.15437715597357432\n",
      "Epoch:381/1000\n",
      "Loss on train= 0.01592470146715641\n",
      "Loss on test= 0.018102707341313362\n",
      "acc for Lsat= 0.08367323133834573 \n",
      "acc for Psat= 0.10902581188247966 \n",
      "acc for optim= 0.15459777772533229\n",
      "Epoch:382/1000\n",
      "Loss on train= 0.015512978658080101\n",
      "Loss on test= 0.01829843409359455\n",
      "acc for Lsat= 0.07587660776587847 \n",
      "acc for Psat= 0.11452447862659242 \n",
      "acc for optim= 0.14900942936353057\n",
      "Epoch:383/1000\n",
      "Loss on train= 0.01580052450299263\n",
      "Loss on test= 0.019059043377637863\n",
      "acc for Lsat= 0.09609815757688343 \n",
      "acc for Psat= 0.13273631898363075 \n",
      "acc for optim= 0.1485743470716562\n",
      "Epoch:384/1000\n",
      "Loss on train= 0.015927346423268318\n",
      "Loss on test= 0.01802193373441696\n",
      "acc for Lsat= 0.08444958814887965 \n",
      "acc for Psat= 0.10848243953077012 \n",
      "acc for optim= 0.1537522194104032\n",
      "Epoch:385/1000\n",
      "Loss on train= 0.015095655806362629\n",
      "Loss on test= 0.018439549952745438\n",
      "acc for Lsat= 0.08268521754387667 \n",
      "acc for Psat= 0.11583090133445206 \n",
      "acc for optim= 0.151317038497686\n",
      "Epoch:386/1000\n",
      "Loss on train= 0.015282899141311646\n",
      "Loss on test= 0.018728526309132576\n",
      "acc for Lsat= 0.08887013379277825 \n",
      "acc for Psat= 0.1243936567805534 \n",
      "acc for optim= 0.15067393309528368\n",
      "Epoch:387/1000\n",
      "Loss on train= 0.015467973425984383\n",
      "Loss on test= 0.017541350796818733\n",
      "acc for Lsat= 0.08437860864006343 \n",
      "acc for Psat= 0.1081820742068862 \n",
      "acc for optim= 0.1507331496594417\n",
      "Epoch:388/1000\n",
      "Loss on train= 0.014692715369164944\n",
      "Loss on test= 0.01846376620233059\n",
      "acc for Lsat= 0.07637736411875179 \n",
      "acc for Psat= 0.12811004084423316 \n",
      "acc for optim= 0.1560144326448014\n",
      "Epoch:389/1000\n",
      "Loss on train= 0.015246243216097355\n",
      "Loss on test= 0.018657725304365158\n",
      "acc for Lsat= 0.07893088338946613 \n",
      "acc for Psat= 0.1348575903605903 \n",
      "acc for optim= 0.15247377560262387\n",
      "Epoch:390/1000\n",
      "Loss on train= 0.01564960554242134\n",
      "Loss on test= 0.018282447010278702\n",
      "acc for Lsat= 0.08507576472324208 \n",
      "acc for Psat= 0.1104901878479768 \n",
      "acc for optim= 0.1526867562095253\n",
      "Epoch:391/1000\n",
      "Loss on train= 0.016150254756212234\n",
      "Loss on test= 0.01808594912290573\n",
      "acc for Lsat= 0.08679048460050216 \n",
      "acc for Psat= 0.1101546532885973 \n",
      "acc for optim= 0.14997200479660994\n",
      "Epoch:392/1000\n",
      "Loss on train= 0.015794288367033005\n",
      "Loss on test= 0.01939232088625431\n",
      "acc for Lsat= 0.09915226936766841 \n",
      "acc for Psat= 0.15757173490652246 \n",
      "acc for optim= 0.15048640706244865\n",
      "Epoch:393/1000\n",
      "Loss on train= 0.016049033030867577\n",
      "Loss on test= 0.017813393846154213\n",
      "acc for Lsat= 0.07771616839338928 \n",
      "acc for Psat= 0.13086517232475212 \n",
      "acc for optim= 0.15059752382191435\n",
      "Epoch:394/1000\n",
      "Loss on train= 0.015475594438612461\n",
      "Loss on test= 0.017392579466104507\n",
      "acc for Lsat= 0.07580737225271507 \n",
      "acc for Psat= 0.10951602514400041 \n",
      "acc for optim= 0.15103144906927735\n",
      "Epoch:395/1000\n",
      "Loss on train= 0.015280541963875294\n",
      "Loss on test= 0.018571795895695686\n",
      "acc for Lsat= 0.07126340424651963 \n",
      "acc for Psat= 0.11125192873593098 \n",
      "acc for optim= 0.14876836991480719\n",
      "Epoch:396/1000\n",
      "Loss on train= 0.01567077822983265\n",
      "Loss on test= 0.017858199775218964\n",
      "acc for Lsat= 0.07543689718933141 \n",
      "acc for Psat= 0.12731950308641088 \n",
      "acc for optim= 0.1518571615112489\n",
      "Epoch:397/1000\n",
      "Loss on train= 0.016525885090231895\n",
      "Loss on test= 0.017963755875825882\n",
      "acc for Lsat= 0.07921886945239973 \n",
      "acc for Psat= 0.11087469284777564 \n",
      "acc for optim= 0.14948575543589582\n",
      "Epoch:398/1000\n",
      "Loss on train= 0.016024166718125343\n",
      "Loss on test= 0.01809603162109852\n",
      "acc for Lsat= 0.0832136145644623 \n",
      "acc for Psat= 0.10670722054454207 \n",
      "acc for optim= 0.15137392094821958\n",
      "Epoch:399/1000\n",
      "Loss on train= 0.01564047858119011\n",
      "Loss on test= 0.017159659415483475\n",
      "acc for Lsat= 0.07369486880963519 \n",
      "acc for Psat= 0.11027854102028932 \n",
      "acc for optim= 0.15571301846043584\n",
      "Epoch:400/1000\n",
      "Loss on train= 0.014958192594349384\n",
      "Loss on test= 0.01836451329290867\n",
      "acc for Lsat= 0.07715173932008965 \n",
      "acc for Psat= 0.11782172754113703 \n",
      "acc for optim= 0.14865236321800723\n",
      "Epoch:401/1000\n",
      "Loss on train= 0.015813563019037247\n",
      "Loss on test= 0.01837903819978237\n",
      "acc for Lsat= 0.08156558962435544 \n",
      "acc for Psat= 0.11171091174396933 \n",
      "acc for optim= 0.15144088138194758\n",
      "Epoch:402/1000\n",
      "Loss on train= 0.0156426839530468\n",
      "Loss on test= 0.017640259116888046\n",
      "acc for Lsat= 0.08118297675214638 \n",
      "acc for Psat= 0.12549149935701875 \n",
      "acc for optim= 0.15171020436158972\n",
      "Epoch:403/1000\n",
      "Loss on train= 0.014837478287518024\n",
      "Loss on test= 0.018456676974892616\n",
      "acc for Lsat= 0.08162011188556555 \n",
      "acc for Psat= 0.11039865319758706 \n",
      "acc for optim= 0.15035028285971694\n",
      "Epoch:404/1000\n",
      "Loss on train= 0.015170739032328129\n",
      "Loss on test= 0.01848648674786091\n",
      "acc for Lsat= 0.08302864744752804 \n",
      "acc for Psat= 0.11030085283016687 \n",
      "acc for optim= 0.14898703798838298\n",
      "Epoch:405/1000\n",
      "Loss on train= 0.01518318708986044\n",
      "Loss on test= 0.018944786861538887\n",
      "acc for Lsat= 0.08698914634840221 \n",
      "acc for Psat= 0.117675476157388 \n",
      "acc for optim= 0.1520930892772879\n",
      "Epoch:406/1000\n",
      "Loss on train= 0.014696932397782803\n",
      "Loss on test= 0.01817111298441887\n",
      "acc for Lsat= 0.07817756229735016 \n",
      "acc for Psat= 0.10741714523815298 \n",
      "acc for optim= 0.15299952520667334\n",
      "Epoch:407/1000\n",
      "Loss on train= 0.015423150733113289\n",
      "Loss on test= 0.018224535509943962\n",
      "acc for Lsat= 0.07290612339760194 \n",
      "acc for Psat= 0.1214851217854215 \n",
      "acc for optim= 0.15110865143629215\n",
      "Epoch:408/1000\n",
      "Loss on train= 0.014170331880450249\n",
      "Loss on test= 0.018162280321121216\n",
      "acc for Lsat= 0.08081799734363827 \n",
      "acc for Psat= 0.11078876688783196 \n",
      "acc for optim= 0.15795120626741316\n",
      "Epoch:409/1000\n",
      "Loss on train= 0.015897944569587708\n",
      "Loss on test= 0.017875485122203827\n",
      "acc for Lsat= 0.07073065502592404 \n",
      "acc for Psat= 0.10822170519658195 \n",
      "acc for optim= 0.15090404950442166\n",
      "Epoch:410/1000\n",
      "Loss on train= 0.015452404506504536\n",
      "Loss on test= 0.017552411183714867\n",
      "acc for Lsat= 0.07428035817653848 \n",
      "acc for Psat= 0.11204996766899054 \n",
      "acc for optim= 0.14971496210115323\n",
      "Epoch:411/1000\n",
      "Loss on train= 0.015014858916401863\n",
      "Loss on test= 0.018038218840956688\n",
      "acc for Lsat= 0.07699870451191883 \n",
      "acc for Psat= 0.12871964776451983 \n",
      "acc for optim= 0.15148294532874831\n",
      "Epoch:412/1000\n",
      "Loss on train= 0.015590682625770569\n",
      "Loss on test= 0.01841883361339569\n",
      "acc for Lsat= 0.08129354745202928 \n",
      "acc for Psat= 0.1147168084312637 \n",
      "acc for optim= 0.15091349418559954\n",
      "Epoch:413/1000\n",
      "Loss on train= 0.014694713056087494\n",
      "Loss on test= 0.01772775687277317\n",
      "acc for Lsat= 0.07660049741724521 \n",
      "acc for Psat= 0.10825417905459464 \n",
      "acc for optim= 0.15038228380445506\n",
      "Epoch:414/1000\n",
      "Loss on train= 0.014536214992403984\n",
      "Loss on test= 0.0181808453053236\n",
      "acc for Lsat= 0.09044609823679027 \n",
      "acc for Psat= 0.1410124551204961 \n",
      "acc for optim= 0.15107541874397631\n",
      "Epoch:415/1000\n",
      "Loss on train= 0.015004782006144524\n",
      "Loss on test= 0.017498619854450226\n",
      "acc for Lsat= 0.08069280309199435 \n",
      "acc for Psat= 0.11049962189841568 \n",
      "acc for optim= 0.1504943719703524\n",
      "Epoch:416/1000\n",
      "Loss on train= 0.01539283525198698\n",
      "Loss on test= 0.017999103292822838\n",
      "acc for Lsat= 0.07776515114925843 \n",
      "acc for Psat= 0.11325146181638847 \n",
      "acc for optim= 0.14916028145813987\n",
      "Epoch:417/1000\n",
      "Loss on train= 0.015367348678410053\n",
      "Loss on test= 0.017643002793192863\n",
      "acc for Lsat= 0.0817561377033138 \n",
      "acc for Psat= 0.11464612366688272 \n",
      "acc for optim= 0.14886316892712614\n",
      "Epoch:418/1000\n",
      "Loss on train= 0.015368468128144741\n",
      "Loss on test= 0.01794329471886158\n",
      "acc for Lsat= 0.07678771286616383 \n",
      "acc for Psat= 0.1163237640917408 \n",
      "acc for optim= 0.15631398064932372\n",
      "Epoch:419/1000\n",
      "Loss on train= 0.015408427454531193\n",
      "Loss on test= 0.018892571330070496\n",
      "acc for Lsat= 0.09173556494798131 \n",
      "acc for Psat= 0.11733162817886776 \n",
      "acc for optim= 0.15084466792389828\n",
      "Epoch:420/1000\n",
      "Loss on train= 0.015158562920987606\n",
      "Loss on test= 0.01803712174296379\n",
      "acc for Lsat= 0.0814831300317068 \n",
      "acc for Psat= 0.11066939165425854 \n",
      "acc for optim= 0.15034236062404724\n",
      "Epoch:421/1000\n",
      "Loss on train= 0.014934929087758064\n",
      "Loss on test= 0.01783776842057705\n",
      "acc for Lsat= 0.07225321846912501 \n",
      "acc for Psat= 0.10837379874185075 \n",
      "acc for optim= 0.152618010845935\n",
      "Epoch:422/1000\n",
      "Loss on train= 0.014806082472205162\n",
      "Loss on test= 0.017863992601633072\n",
      "acc for Lsat= 0.07335226406244985 \n",
      "acc for Psat= 0.11839399538227821 \n",
      "acc for optim= 0.14908547557104046\n",
      "Epoch:423/1000\n",
      "Loss on train= 0.015116220340132713\n",
      "Loss on test= 0.01764003559947014\n",
      "acc for Lsat= 0.0826509875133767 \n",
      "acc for Psat= 0.11313156681751732 \n",
      "acc for optim= 0.1498994429977294\n",
      "Epoch:424/1000\n",
      "Loss on train= 0.014540462754666805\n",
      "Loss on test= 0.01761714741587639\n",
      "acc for Lsat= 0.07634168028404974 \n",
      "acc for Psat= 0.11575364401174144 \n",
      "acc for optim= 0.15262029884326433\n",
      "Epoch:425/1000\n",
      "Loss on train= 0.015023122541606426\n",
      "Loss on test= 0.018001869320869446\n",
      "acc for Lsat= 0.08193088858946494 \n",
      "acc for Psat= 0.11636060168150286 \n",
      "acc for optim= 0.15208839768800408\n",
      "Epoch:426/1000\n",
      "Loss on train= 0.015021896921098232\n",
      "Loss on test= 0.018485773354768753\n",
      "acc for Lsat= 0.07763896329671625 \n",
      "acc for Psat= 0.11856558874169486 \n",
      "acc for optim= 0.1479022358525844\n",
      "Epoch:427/1000\n",
      "Loss on train= 0.015361243858933449\n",
      "Loss on test= 0.01824934594333172\n",
      "acc for Lsat= 0.07308481256202636 \n",
      "acc for Psat= 0.10634829709056451 \n",
      "acc for optim= 0.15402640158673733\n",
      "Epoch:428/1000\n",
      "Loss on train= 0.01544711273163557\n",
      "Loss on test= 0.0181263480335474\n",
      "acc for Lsat= 0.07418835569899497 \n",
      "acc for Psat= 0.117843416413169 \n",
      "acc for optim= 0.15406779808946924\n",
      "Epoch:429/1000\n",
      "Loss on train= 0.01530113909393549\n",
      "Loss on test= 0.017928943037986755\n",
      "acc for Lsat= 0.09297610720176387 \n",
      "acc for Psat= 0.1147756827965192 \n",
      "acc for optim= 0.14920275213466774\n",
      "Epoch:430/1000\n",
      "Loss on train= 0.01558355987071991\n",
      "Loss on test= 0.018232718110084534\n",
      "acc for Lsat= 0.07662572216689054 \n",
      "acc for Psat= 0.11910418293343887 \n",
      "acc for optim= 0.14970884666459933\n",
      "Epoch:431/1000\n",
      "Loss on train= 0.014388295821845531\n",
      "Loss on test= 0.018251044675707817\n",
      "acc for Lsat= 0.08082107292198326 \n",
      "acc for Psat= 0.11574832995377203 \n",
      "acc for optim= 0.1512891734115553\n",
      "Epoch:432/1000\n",
      "Loss on train= 0.015029089525341988\n",
      "Loss on test= 0.018497701734304428\n",
      "acc for Lsat= 0.09033113180631386 \n",
      "acc for Psat= 0.11043281264083328 \n",
      "acc for optim= 0.14792314143001714\n",
      "Epoch:433/1000\n",
      "Loss on train= 0.015402480959892273\n",
      "Loss on test= 0.018227409571409225\n",
      "acc for Lsat= 0.09219367862387505 \n",
      "acc for Psat= 0.11930067247056363 \n",
      "acc for optim= 0.15245056124620662\n",
      "Epoch:434/1000\n",
      "Loss on train= 0.015494328923523426\n",
      "Loss on test= 0.017485609278082848\n",
      "acc for Lsat= 0.08197923552158266 \n",
      "acc for Psat= 0.10578995063394468 \n",
      "acc for optim= 0.15004987354142083\n",
      "Epoch:435/1000\n",
      "Loss on train= 0.015309600159525871\n",
      "Loss on test= 0.017704101279377937\n",
      "acc for Lsat= 0.07894880780167146 \n",
      "acc for Psat= 0.10505142062635883 \n",
      "acc for optim= 0.1510397812015042\n",
      "Epoch:436/1000\n",
      "Loss on train= 0.014713677577674389\n",
      "Loss on test= 0.0176477562636137\n",
      "acc for Lsat= 0.08329276599269861 \n",
      "acc for Psat= 0.12132613013810038 \n",
      "acc for optim= 0.1479714291999933\n",
      "Epoch:437/1000\n",
      "Loss on train= 0.014886184595525265\n",
      "Loss on test= 0.016989175230264664\n",
      "acc for Lsat= 0.0697230798507206 \n",
      "acc for Psat= 0.11577497872129114 \n",
      "acc for optim= 0.14872970095900603\n",
      "Epoch:438/1000\n",
      "Loss on train= 0.014226379804313183\n",
      "Loss on test= 0.01834724098443985\n",
      "acc for Lsat= 0.07972050277193032 \n",
      "acc for Psat= 0.10914051586059 \n",
      "acc for optim= 0.1512329489899023\n",
      "Epoch:439/1000\n",
      "Loss on train= 0.015488128177821636\n",
      "Loss on test= 0.017421724274754524\n",
      "acc for Lsat= 0.08234996020474034 \n",
      "acc for Psat= 0.11623836313249383 \n",
      "acc for optim= 0.1506546301897182\n",
      "Epoch:440/1000\n",
      "Loss on train= 0.01504584215581417\n",
      "Loss on test= 0.017874283716082573\n",
      "acc for Lsat= 0.08020553275383008 \n",
      "acc for Psat= 0.11335838501056746 \n",
      "acc for optim= 0.14791104879916675\n",
      "Epoch:441/1000\n",
      "Loss on train= 0.014602599665522575\n",
      "Loss on test= 0.017792630940675735\n",
      "acc for Lsat= 0.07541084478281052 \n",
      "acc for Psat= 0.11545617626263548 \n",
      "acc for optim= 0.14696684493574982\n",
      "Epoch:442/1000\n",
      "Loss on train= 0.015348619781434536\n",
      "Loss on test= 0.017120352014899254\n",
      "acc for Lsat= 0.07447673521656042 \n",
      "acc for Psat= 0.12520657795912896 \n",
      "acc for optim= 0.1525067592989354\n",
      "Epoch:443/1000\n",
      "Loss on train= 0.015127652324736118\n",
      "Loss on test= 0.017703711986541748\n",
      "acc for Lsat= 0.07381413061311716 \n",
      "acc for Psat= 0.11258943874209003 \n",
      "acc for optim= 0.1495827825743312\n",
      "Epoch:444/1000\n",
      "Loss on train= 0.014915494248270988\n",
      "Loss on test= 0.0176851823925972\n",
      "acc for Lsat= 0.08423146531385683 \n",
      "acc for Psat= 0.12133945157361582 \n",
      "acc for optim= 0.14858298150286053\n",
      "Epoch:445/1000\n",
      "Loss on train= 0.01627991534769535\n",
      "Loss on test= 0.019186314195394516\n",
      "acc for Lsat= 0.11170832747210328 \n",
      "acc for Psat= 0.1477896061481005 \n",
      "acc for optim= 0.14828673850658328\n",
      "Epoch:446/1000\n",
      "Loss on train= 0.01606644317507744\n",
      "Loss on test= 0.018136508762836456\n",
      "acc for Lsat= 0.08028488267406367 \n",
      "acc for Psat= 0.1149567649914668 \n",
      "acc for optim= 0.1537315884088574\n",
      "Epoch:447/1000\n",
      "Loss on train= 0.015505609102547169\n",
      "Loss on test= 0.017874497920274734\n",
      "acc for Lsat= 0.08774508348197972 \n",
      "acc for Psat= 0.10851092896222643 \n",
      "acc for optim= 0.14973414290783016\n",
      "Epoch:448/1000\n",
      "Loss on train= 0.014677656814455986\n",
      "Loss on test= 0.017776960507035255\n",
      "acc for Lsat= 0.08092661777741154 \n",
      "acc for Psat= 0.10850002863752609 \n",
      "acc for optim= 0.14948710076284324\n",
      "Epoch:449/1000\n",
      "Loss on train= 0.015250570140779018\n",
      "Loss on test= 0.01769835688173771\n",
      "acc for Lsat= 0.08504973384476731 \n",
      "acc for Psat= 0.12515983233938066 \n",
      "acc for optim= 0.14772923838260565\n",
      "Epoch:450/1000\n",
      "Loss on train= 0.01500700879842043\n",
      "Loss on test= 0.017810136079788208\n",
      "acc for Lsat= 0.0811881249247382 \n",
      "acc for Psat= 0.11336006783415464 \n",
      "acc for optim= 0.14953916121894006\n",
      "Epoch:451/1000\n",
      "Loss on train= 0.014362411573529243\n",
      "Loss on test= 0.017761703580617905\n",
      "acc for Lsat= 0.07456635098124659 \n",
      "acc for Psat= 0.10403559129012081 \n",
      "acc for optim= 0.14830724137413692\n",
      "Epoch:452/1000\n",
      "Loss on train= 0.015132015570998192\n",
      "Loss on test= 0.01757783442735672\n",
      "acc for Lsat= 0.07436728508933073 \n",
      "acc for Psat= 0.10543299369607288 \n",
      "acc for optim= 0.1477860618895836\n",
      "Epoch:453/1000\n",
      "Loss on train= 0.014350234530866146\n",
      "Loss on test= 0.017410211265087128\n",
      "acc for Lsat= 0.07210552097966291 \n",
      "acc for Psat= 0.1022243738814202 \n",
      "acc for optim= 0.15061752168138468\n",
      "Epoch:454/1000\n",
      "Loss on train= 0.014163320884108543\n",
      "Loss on test= 0.017543282359838486\n",
      "acc for Lsat= 0.07899021629997144 \n",
      "acc for Psat= 0.11121194965084463 \n",
      "acc for optim= 0.14956256750444608\n",
      "Epoch:455/1000\n",
      "Loss on train= 0.01457024272531271\n",
      "Loss on test= 0.017823604866862297\n",
      "acc for Lsat= 0.08965522623445993 \n",
      "acc for Psat= 0.12074768837534676 \n",
      "acc for optim= 0.14938209956788423\n",
      "Epoch:456/1000\n",
      "Loss on train= 0.014981207437813282\n",
      "Loss on test= 0.01782286912202835\n",
      "acc for Lsat= 0.08446013282364725 \n",
      "acc for Psat= 0.11073670186808797 \n",
      "acc for optim= 0.1501494840667158\n",
      "Epoch:457/1000\n",
      "Loss on train= 0.01461638230830431\n",
      "Loss on test= 0.017983265221118927\n",
      "acc for Lsat= 0.07664358009699208 \n",
      "acc for Psat= 0.12527866201451943 \n",
      "acc for optim= 0.1477760302039506\n",
      "Epoch:458/1000\n",
      "Loss on train= 0.014666922390460968\n",
      "Loss on test= 0.017742428928613663\n",
      "acc for Lsat= 0.07500195105515141 \n",
      "acc for Psat= 0.10557554519880226 \n",
      "acc for optim= 0.1500663431471277\n",
      "Epoch:459/1000\n",
      "Loss on train= 0.015035836026072502\n",
      "Loss on test= 0.01867309771478176\n",
      "acc for Lsat= 0.09905603965727715 \n",
      "acc for Psat= 0.1183377114945959 \n",
      "acc for optim= 0.15327925847340143\n",
      "Epoch:460/1000\n",
      "Loss on train= 0.0143769895657897\n",
      "Loss on test= 0.01807321608066559\n",
      "acc for Lsat= 0.07960161883950444 \n",
      "acc for Psat= 0.11509243934440272 \n",
      "acc for optim= 0.14809047515788956\n",
      "Epoch:461/1000\n",
      "Loss on train= 0.014690072275698185\n",
      "Loss on test= 0.016392439603805542\n",
      "acc for Lsat= 0.07781141627453309 \n",
      "acc for Psat= 0.10588093510893888 \n",
      "acc for optim= 0.14819834480771865\n",
      "Epoch:462/1000\n",
      "Loss on train= 0.01487674005329609\n",
      "Loss on test= 0.017339296638965607\n",
      "acc for Lsat= 0.07640917310774434 \n",
      "acc for Psat= 0.11705533776812134 \n",
      "acc for optim= 0.15096495884049133\n",
      "Epoch:463/1000\n",
      "Loss on train= 0.014911453239619732\n",
      "Loss on test= 0.017740899696946144\n",
      "acc for Lsat= 0.0809301314232388 \n",
      "acc for Psat= 0.12314672370929416 \n",
      "acc for optim= 0.1507057281427605\n",
      "Epoch:464/1000\n",
      "Loss on train= 0.014428133144974709\n",
      "Loss on test= 0.01785002090036869\n",
      "acc for Lsat= 0.08285996016108285 \n",
      "acc for Psat= 0.12504316210107003 \n",
      "acc for optim= 0.14999514607069528\n",
      "Epoch:465/1000\n",
      "Loss on train= 0.014953847043216228\n",
      "Loss on test= 0.017257550731301308\n",
      "acc for Lsat= 0.08412361980977341 \n",
      "acc for Psat= 0.11902522389278855 \n",
      "acc for optim= 0.1469214127827203\n",
      "Epoch:466/1000\n",
      "Loss on train= 0.014010140672326088\n",
      "Loss on test= 0.017190389335155487\n",
      "acc for Lsat= 0.0765573726250575 \n",
      "acc for Psat= 0.11352727128482676 \n",
      "acc for optim= 0.14572472655495938\n",
      "Epoch:467/1000\n",
      "Loss on train= 0.013933234848082066\n",
      "Loss on test= 0.017891032621264458\n",
      "acc for Lsat= 0.08081265990763957 \n",
      "acc for Psat= 0.10429576586312171 \n",
      "acc for optim= 0.1462054774991515\n",
      "Epoch:468/1000\n",
      "Loss on train= 0.014329427853226662\n",
      "Loss on test= 0.016741639003157616\n",
      "acc for Lsat= 0.07400908913723261 \n",
      "acc for Psat= 0.10052644600808515 \n",
      "acc for optim= 0.14839251118183983\n",
      "Epoch:469/1000\n",
      "Loss on train= 0.014078938402235508\n",
      "Loss on test= 0.017729153856635094\n",
      "acc for Lsat= 0.0736734183455196 \n",
      "acc for Psat= 0.11895762096577339 \n",
      "acc for optim= 0.14758304076032688\n",
      "Epoch:470/1000\n",
      "Loss on train= 0.014387743547558784\n",
      "Loss on test= 0.017589950934052467\n",
      "acc for Lsat= 0.07755996838132041 \n",
      "acc for Psat= 0.10675938681754317 \n",
      "acc for optim= 0.14711261387374616\n",
      "Epoch:471/1000\n",
      "Loss on train= 0.014807685278356075\n",
      "Loss on test= 0.017438126727938652\n",
      "acc for Lsat= 0.0740270575598442 \n",
      "acc for Psat= 0.11066193767225165 \n",
      "acc for optim= 0.15416406452762418\n",
      "Epoch:472/1000\n",
      "Loss on train= 0.014282618649303913\n",
      "Loss on test= 0.017925843596458435\n",
      "acc for Lsat= 0.07294945418834686 \n",
      "acc for Psat= 0.10618593423012548 \n",
      "acc for optim= 0.1474551122602282\n",
      "Epoch:473/1000\n",
      "Loss on train= 0.014544880948960781\n",
      "Loss on test= 0.017276905477046967\n",
      "acc for Lsat= 0.07645673218161561 \n",
      "acc for Psat= 0.10450019116691696 \n",
      "acc for optim= 0.14899993188052785\n",
      "Epoch:474/1000\n",
      "Loss on train= 0.014467797242105007\n",
      "Loss on test= 0.017324376851320267\n",
      "acc for Lsat= 0.07549832267070285 \n",
      "acc for Psat= 0.10846560621944012 \n",
      "acc for optim= 0.15035847103446456\n",
      "Epoch:475/1000\n",
      "Loss on train= 0.014441504143178463\n",
      "Loss on test= 0.01719183474779129\n",
      "acc for Lsat= 0.07834078626897124 \n",
      "acc for Psat= 0.10789915395763991 \n",
      "acc for optim= 0.14698714886880307\n",
      "Epoch:476/1000\n",
      "Loss on train= 0.014184239320456982\n",
      "Loss on test= 0.017717523500323296\n",
      "acc for Lsat= 0.0769872314761491 \n",
      "acc for Psat= 0.11281685273634512 \n",
      "acc for optim= 0.14833525866003497\n",
      "Epoch:477/1000\n",
      "Loss on train= 0.014123484492301941\n",
      "Loss on test= 0.017848897725343704\n",
      "acc for Lsat= 0.07532261964033671 \n",
      "acc for Psat= 0.10529342253220957 \n",
      "acc for optim= 0.14957759804717116\n",
      "Epoch:478/1000\n",
      "Loss on train= 0.014118865132331848\n",
      "Loss on test= 0.017051158472895622\n",
      "acc for Lsat= 0.07544653176834845 \n",
      "acc for Psat= 0.10514482153123089 \n",
      "acc for optim= 0.14620426415544077\n",
      "Epoch:479/1000\n",
      "Loss on train= 0.014567865058779716\n",
      "Loss on test= 0.017881423234939575\n",
      "acc for Lsat= 0.07834550589056474 \n",
      "acc for Psat= 0.11862831592346561 \n",
      "acc for optim= 0.1494148028131461\n",
      "Epoch:480/1000\n",
      "Loss on train= 0.014463537372648716\n",
      "Loss on test= 0.016813281923532486\n",
      "acc for Lsat= 0.08413230547538172 \n",
      "acc for Psat= 0.1064613536560045 \n",
      "acc for optim= 0.14650270952734834\n",
      "Epoch:481/1000\n",
      "Loss on train= 0.014491423033177853\n",
      "Loss on test= 0.017611587420105934\n",
      "acc for Lsat= 0.08449274779219107 \n",
      "acc for Psat= 0.11264534142661392 \n",
      "acc for optim= 0.14725341025959823\n",
      "Epoch:482/1000\n",
      "Loss on train= 0.01449275016784668\n",
      "Loss on test= 0.017911583185195923\n",
      "acc for Lsat= 0.07083149187680006 \n",
      "acc for Psat= 0.11221662553990247 \n",
      "acc for optim= 0.1486689543788037\n",
      "Epoch:483/1000\n",
      "Loss on train= 0.014212192967534065\n",
      "Loss on test= 0.017369097098708153\n",
      "acc for Lsat= 0.07746991306170158 \n",
      "acc for Psat= 0.1143996147968688 \n",
      "acc for optim= 0.14542525387194163\n",
      "Epoch:484/1000\n",
      "Loss on train= 0.014175902120769024\n",
      "Loss on test= 0.017689816653728485\n",
      "acc for Lsat= 0.07674010367747494 \n",
      "acc for Psat= 0.10380428768227908 \n",
      "acc for optim= 0.15057829145028898\n",
      "Epoch:485/1000\n",
      "Loss on train= 0.014241330325603485\n",
      "Loss on test= 0.017214611172676086\n",
      "acc for Lsat= 0.07243496326193186 \n",
      "acc for Psat= 0.11150672598900219 \n",
      "acc for optim= 0.14719128364527162\n",
      "Epoch:486/1000\n",
      "Loss on train= 0.014015362598001957\n",
      "Loss on test= 0.017228741198778152\n",
      "acc for Lsat= 0.09343958617536242 \n",
      "acc for Psat= 0.1138859527695371 \n",
      "acc for optim= 0.14792558432478384\n",
      "Epoch:487/1000\n",
      "Loss on train= 0.014463337138295174\n",
      "Loss on test= 0.016930697485804558\n",
      "acc for Lsat= 0.07349351003152954 \n",
      "acc for Psat= 0.10853439512960812 \n",
      "acc for optim= 0.14542298012001573\n",
      "Epoch:488/1000\n",
      "Loss on train= 0.014940028078854084\n",
      "Loss on test= 0.017647355794906616\n",
      "acc for Lsat= 0.07265781580548808 \n",
      "acc for Psat= 0.10568108088428517 \n",
      "acc for optim= 0.14849323318981308\n",
      "Epoch:489/1000\n",
      "Loss on train= 0.013937006704509258\n",
      "Loss on test= 0.016835864633321762\n",
      "acc for Lsat= 0.07059770816660525 \n",
      "acc for Psat= 0.1118345805165593 \n",
      "acc for optim= 0.15043231035601048\n",
      "Epoch:490/1000\n",
      "Loss on train= 0.014540493488311768\n",
      "Loss on test= 0.017145896330475807\n",
      "acc for Lsat= 0.07691321379916614 \n",
      "acc for Psat= 0.10747043524530579 \n",
      "acc for optim= 0.14703652300007233\n",
      "Epoch:491/1000\n",
      "Loss on train= 0.014123047702014446\n",
      "Loss on test= 0.017007233574986458\n",
      "acc for Lsat= 0.07367010690765007 \n",
      "acc for Psat= 0.10940701485959706 \n",
      "acc for optim= 0.14773053197400088\n",
      "Epoch:492/1000\n",
      "Loss on train= 0.01443377137184143\n",
      "Loss on test= 0.017276842147111893\n",
      "acc for Lsat= 0.07426559363686763 \n",
      "acc for Psat= 0.11144324833250645 \n",
      "acc for optim= 0.1482567082150891\n",
      "Epoch:493/1000\n",
      "Loss on train= 0.014572777785360813\n",
      "Loss on test= 0.01722429320216179\n",
      "acc for Lsat= 0.07738560109210996 \n",
      "acc for Psat= 0.11248498059982476 \n",
      "acc for optim= 0.148046025692457\n",
      "Epoch:494/1000\n",
      "Loss on train= 0.014136908575892448\n",
      "Loss on test= 0.016645094379782677\n",
      "acc for Lsat= 0.07269788227695473 \n",
      "acc for Psat= 0.1104122446453422 \n",
      "acc for optim= 0.14754621854621738\n",
      "Epoch:495/1000\n",
      "Loss on train= 0.013822128996253014\n",
      "Loss on test= 0.01731298305094242\n",
      "acc for Lsat= 0.07553181673311804 \n",
      "acc for Psat= 0.10648140402940606 \n",
      "acc for optim= 0.14864810108072224\n",
      "Epoch:496/1000\n",
      "Loss on train= 0.013700908049941063\n",
      "Loss on test= 0.016944123432040215\n",
      "acc for Lsat= 0.0695581390574068 \n",
      "acc for Psat= 0.1028906719735782 \n",
      "acc for optim= 0.14944090769627866\n",
      "Epoch:497/1000\n",
      "Loss on train= 0.014346097595989704\n",
      "Loss on test= 0.01811853237450123\n",
      "acc for Lsat= 0.07661740595944665 \n",
      "acc for Psat= 0.10353805931394984 \n",
      "acc for optim= 0.14752537281867217\n",
      "Epoch:498/1000\n",
      "Loss on train= 0.01402380783110857\n",
      "Loss on test= 0.017509065568447113\n",
      "acc for Lsat= 0.074859648996261 \n",
      "acc for Psat= 0.10661798819019883 \n",
      "acc for optim= 0.147708235689054\n",
      "Epoch:499/1000\n",
      "Loss on train= 0.013877025805413723\n",
      "Loss on test= 0.017139868810772896\n",
      "acc for Lsat= 0.07631709596978528 \n",
      "acc for Psat= 0.10923246206025958 \n",
      "acc for optim= 0.1465549566984603\n",
      "Epoch:500/1000\n",
      "Loss on train= 0.014270173385739326\n",
      "Loss on test= 0.01715957187116146\n",
      "acc for Lsat= 0.0719348878201517 \n",
      "acc for Psat= 0.10782174390202556 \n",
      "acc for optim= 0.15557151127586638\n",
      "Epoch:501/1000\n",
      "Loss on train= 0.01467984914779663\n",
      "Loss on test= 0.017353437840938568\n",
      "acc for Lsat= 0.07200849495015116 \n",
      "acc for Psat= 0.1002713387362219 \n",
      "acc for optim= 0.1485907834736307\n",
      "Epoch:502/1000\n",
      "Loss on train= 0.013965742662549019\n",
      "Loss on test= 0.01674504764378071\n",
      "acc for Lsat= 0.0701796468020765 \n",
      "acc for Psat= 0.1067714515013857 \n",
      "acc for optim= 0.15063835708741002\n",
      "Epoch:503/1000\n",
      "Loss on train= 0.013931447640061378\n",
      "Loss on test= 0.01767640933394432\n",
      "acc for Lsat= 0.08788043200863069 \n",
      "acc for Psat= 0.11291846234171463 \n",
      "acc for optim= 0.1473844237212418\n",
      "Epoch:504/1000\n",
      "Loss on train= 0.013765336945652962\n",
      "Loss on test= 0.017093826085329056\n",
      "acc for Lsat= 0.07480965032773707 \n",
      "acc for Psat= 0.1053764222986157 \n",
      "acc for optim= 0.14810647302322186\n",
      "Epoch:505/1000\n",
      "Loss on train= 0.014235028065741062\n",
      "Loss on test= 0.017911776900291443\n",
      "acc for Lsat= 0.08069330781751541 \n",
      "acc for Psat= 0.10687455769514993 \n",
      "acc for optim= 0.14721524321968954\n",
      "Epoch:506/1000\n",
      "Loss on train= 0.013945247046649456\n",
      "Loss on test= 0.017049260437488556\n",
      "acc for Lsat= 0.06936301179030288 \n",
      "acc for Psat= 0.11240684674762873 \n",
      "acc for optim= 0.14721819477985498\n",
      "Epoch:507/1000\n",
      "Loss on train= 0.014407222159206867\n",
      "Loss on test= 0.01722702756524086\n",
      "acc for Lsat= 0.07180596270160303 \n",
      "acc for Psat= 0.10632156691312364 \n",
      "acc for optim= 0.14675072128742883\n",
      "Epoch:508/1000\n",
      "Loss on train= 0.013734492473304272\n",
      "Loss on test= 0.01791767030954361\n",
      "acc for Lsat= 0.08582093363797727 \n",
      "acc for Psat= 0.10320656622671698 \n",
      "acc for optim= 0.14632820933790663\n",
      "Epoch:509/1000\n",
      "Loss on train= 0.013897375203669071\n",
      "Loss on test= 0.018088126555085182\n",
      "acc for Lsat= 0.07461625109418348 \n",
      "acc for Psat= 0.1104816058987155 \n",
      "acc for optim= 0.14958766021754108\n",
      "Epoch:510/1000\n",
      "Loss on train= 0.014278529211878777\n",
      "Loss on test= 0.01724175736308098\n",
      "acc for Lsat= 0.07833545597166836 \n",
      "acc for Psat= 0.1089631998155113 \n",
      "acc for optim= 0.1493528933879088\n",
      "Epoch:511/1000\n",
      "Loss on train= 0.013804922811686993\n",
      "Loss on test= 0.01701076328754425\n",
      "acc for Lsat= 0.06842538130624563 \n",
      "acc for Psat= 0.1002876792589539 \n",
      "acc for optim= 0.14889073431598474\n",
      "Epoch:512/1000\n",
      "Loss on train= 0.01452293898910284\n",
      "Loss on test= 0.017056904733181\n",
      "acc for Lsat= 0.0744412025206844 \n",
      "acc for Psat= 0.1047382933615785 \n",
      "acc for optim= 0.15043642660586262\n",
      "Epoch:513/1000\n",
      "Loss on train= 0.014446807093918324\n",
      "Loss on test= 0.017265798524022102\n",
      "acc for Lsat= 0.07204955176505293 \n",
      "acc for Psat= 0.11453543608218482 \n",
      "acc for optim= 0.1480821046718331\n",
      "Epoch:514/1000\n",
      "Loss on train= 0.015282750129699707\n",
      "Loss on test= 0.017283491790294647\n",
      "acc for Lsat= 0.08159623598689897 \n",
      "acc for Psat= 0.11892999168585368 \n",
      "acc for optim= 0.1547640569521404\n",
      "Epoch:515/1000\n",
      "Loss on train= 0.014781628735363483\n",
      "Loss on test= 0.016883159056305885\n",
      "acc for Lsat= 0.07721046652904776 \n",
      "acc for Psat= 0.1133897759620107 \n",
      "acc for optim= 0.1479848403730205\n",
      "Epoch:516/1000\n",
      "Loss on train= 0.013636087067425251\n",
      "Loss on test= 0.017578601837158203\n",
      "acc for Lsat= 0.0783617000567039 \n",
      "acc for Psat= 0.10646712119975969 \n",
      "acc for optim= 0.15158709602193882\n",
      "Epoch:517/1000\n",
      "Loss on train= 0.014129316434264183\n",
      "Loss on test= 0.01743440330028534\n",
      "acc for Lsat= 0.0751510900653539 \n",
      "acc for Psat= 0.10322973824144478 \n",
      "acc for optim= 0.1478527400276102\n",
      "Epoch:518/1000\n",
      "Loss on train= 0.013881959952414036\n",
      "Loss on test= 0.017546769231557846\n",
      "acc for Lsat= 0.07957369136042587 \n",
      "acc for Psat= 0.10745680859988833 \n",
      "acc for optim= 0.14947467240536574\n",
      "Epoch:519/1000\n",
      "Loss on train= 0.013837226666510105\n",
      "Loss on test= 0.016502751037478447\n",
      "acc for Lsat= 0.07690920180412869 \n",
      "acc for Psat= 0.10172283498033853 \n",
      "acc for optim= 0.15098611549316027\n",
      "Epoch:520/1000\n",
      "Loss on train= 0.01354333758354187\n",
      "Loss on test= 0.01778014563024044\n",
      "acc for Lsat= 0.08172623399971637 \n",
      "acc for Psat= 0.10896136544258311 \n",
      "acc for optim= 0.14914723577141126\n",
      "Epoch:521/1000\n",
      "Loss on train= 0.013514026068150997\n",
      "Loss on test= 0.017819808796048164\n",
      "acc for Lsat= 0.07209802455467061 \n",
      "acc for Psat= 0.1062421587903939 \n",
      "acc for optim= 0.14847220075791548\n",
      "Epoch:522/1000\n",
      "Loss on train= 0.013399197719991207\n",
      "Loss on test= 0.0172431580722332\n",
      "acc for Lsat= 0.07911489910324485 \n",
      "acc for Psat= 0.10059687951596348 \n",
      "acc for optim= 0.14741149496733605\n",
      "Epoch:523/1000\n",
      "Loss on train= 0.013598362915217876\n",
      "Loss on test= 0.01756766252219677\n",
      "acc for Lsat= 0.0776746726846439 \n",
      "acc for Psat= 0.11626502961080273 \n",
      "acc for optim= 0.1491498844034983\n",
      "Epoch:524/1000\n",
      "Loss on train= 0.01388036459684372\n",
      "Loss on test= 0.017605658620595932\n",
      "acc for Lsat= 0.09516481675594139 \n",
      "acc for Psat= 0.11263265572421668 \n",
      "acc for optim= 0.1474546584225724\n",
      "Epoch:525/1000\n",
      "Loss on train= 0.014011712744832039\n",
      "Loss on test= 0.01729649491608143\n",
      "acc for Lsat= 0.07064630957536919 \n",
      "acc for Psat= 0.10057131631003297 \n",
      "acc for optim= 0.1493320840309259\n",
      "Epoch:526/1000\n",
      "Loss on train= 0.013225307688117027\n",
      "Loss on test= 0.017090555280447006\n",
      "acc for Lsat= 0.06947580055601907 \n",
      "acc for Psat= 0.10271634661021087 \n",
      "acc for optim= 0.14901306220798458\n",
      "Epoch:527/1000\n",
      "Loss on train= 0.014465666376054287\n",
      "Loss on test= 0.01681857369840145\n",
      "acc for Lsat= 0.0709766700463986 \n",
      "acc for Psat= 0.10763805544866863 \n",
      "acc for optim= 0.14624020280266492\n",
      "Epoch:528/1000\n",
      "Loss on train= 0.0147587601095438\n",
      "Loss on test= 0.017388779670000076\n",
      "acc for Lsat= 0.09081562408820887 \n",
      "acc for Psat= 0.11533413509136874 \n",
      "acc for optim= 0.14818758300250673\n",
      "Epoch:529/1000\n",
      "Loss on train= 0.014141128398478031\n",
      "Loss on test= 0.017491955310106277\n",
      "acc for Lsat= 0.07888170079809607 \n",
      "acc for Psat= 0.10466674920271464 \n",
      "acc for optim= 0.14835454697898975\n",
      "Epoch:530/1000\n",
      "Loss on train= 0.013843940570950508\n",
      "Loss on test= 0.017104482278227806\n",
      "acc for Lsat= 0.07280842041286886 \n",
      "acc for Psat= 0.10495136911199428 \n",
      "acc for optim= 0.14619734641264506\n",
      "Epoch:531/1000\n",
      "Loss on train= 0.0135654890909791\n",
      "Loss on test= 0.017203105613589287\n",
      "acc for Lsat= 0.08078864780862768 \n",
      "acc for Psat= 0.10680156609240073 \n",
      "acc for optim= 0.15004354998975827\n",
      "Epoch:532/1000\n",
      "Loss on train= 0.013804963789880276\n",
      "Loss on test= 0.016686351969838142\n",
      "acc for Lsat= 0.07403408881803318 \n",
      "acc for Psat= 0.10319666231254347 \n",
      "acc for optim= 0.14617919468709095\n",
      "Epoch:533/1000\n",
      "Loss on train= 0.013551819138228893\n",
      "Loss on test= 0.01647484488785267\n",
      "acc for Lsat= 0.07622662304445754 \n",
      "acc for Psat= 0.10804784054832935 \n",
      "acc for optim= 0.14817194923731847\n",
      "Epoch:534/1000\n",
      "Loss on train= 0.013619731180369854\n",
      "Loss on test= 0.017042936757206917\n",
      "acc for Lsat= 0.07013653445329136 \n",
      "acc for Psat= 0.09968272552080785 \n",
      "acc for optim= 0.14674781693115646\n",
      "Epoch:535/1000\n",
      "Loss on train= 0.013410037383437157\n",
      "Loss on test= 0.017084628343582153\n",
      "acc for Lsat= 0.07397416104464286 \n",
      "acc for Psat= 0.10499219241850276 \n",
      "acc for optim= 0.1475435095950827\n",
      "Epoch:536/1000\n",
      "Loss on train= 0.013652835972607136\n",
      "Loss on test= 0.016868004575371742\n",
      "acc for Lsat= 0.07034763684213054 \n",
      "acc for Psat= 0.11877601057131094 \n",
      "acc for optim= 0.14517283565456407\n",
      "Epoch:537/1000\n",
      "Loss on train= 0.013607501983642578\n",
      "Loss on test= 0.016903186216950417\n",
      "acc for Lsat= 0.07767996855733218 \n",
      "acc for Psat= 0.10519547067942474 \n",
      "acc for optim= 0.14553424929250336\n",
      "Epoch:538/1000\n",
      "Loss on train= 0.013320671394467354\n",
      "Loss on test= 0.017368244007229805\n",
      "acc for Lsat= 0.07948480751731955 \n",
      "acc for Psat= 0.10668136221564946 \n",
      "acc for optim= 0.1443547711282809\n",
      "Epoch:539/1000\n",
      "Loss on train= 0.012982362881302834\n",
      "Loss on test= 0.016854852437973022\n",
      "acc for Lsat= 0.06744366968467112 \n",
      "acc for Psat= 0.10869570993993277 \n",
      "acc for optim= 0.14382087968969604\n",
      "Epoch:540/1000\n",
      "Loss on train= 0.013559611514210701\n",
      "Loss on test= 0.018310321494936943\n",
      "acc for Lsat= 0.07948353023456546 \n",
      "acc for Psat= 0.12429254388766553 \n",
      "acc for optim= 0.1495455414536601\n",
      "Epoch:541/1000\n",
      "Loss on train= 0.014298344030976295\n",
      "Loss on test= 0.016670791432261467\n",
      "acc for Lsat= 0.06979686537880803 \n",
      "acc for Psat= 0.10658910251047615 \n",
      "acc for optim= 0.14663872134493586\n",
      "Epoch:542/1000\n",
      "Loss on train= 0.01397828757762909\n",
      "Loss on test= 0.017607614398002625\n",
      "acc for Lsat= 0.07863713796642045 \n",
      "acc for Psat= 0.10541749314033497 \n",
      "acc for optim= 0.14644278943645295\n",
      "Epoch:543/1000\n",
      "Loss on train= 0.013198799453675747\n",
      "Loss on test= 0.017271514981985092\n",
      "acc for Lsat= 0.07263940034270927 \n",
      "acc for Psat= 0.10146426301522843 \n",
      "acc for optim= 0.1487962544177641\n",
      "Epoch:544/1000\n",
      "Loss on train= 0.013514603488147259\n",
      "Loss on test= 0.017434012144804\n",
      "acc for Lsat= 0.06989811438673937 \n",
      "acc for Psat= 0.10388057308674713 \n",
      "acc for optim= 0.14514710564093003\n",
      "Epoch:545/1000\n",
      "Loss on train= 0.01388664823025465\n",
      "Loss on test= 0.017042558640241623\n",
      "acc for Lsat= 0.07811192643557123 \n",
      "acc for Psat= 0.10215438022169956 \n",
      "acc for optim= 0.148168252437827\n",
      "Epoch:546/1000\n",
      "Loss on train= 0.01348849106580019\n",
      "Loss on test= 0.016354046761989594\n",
      "acc for Lsat= 0.07376909161083173 \n",
      "acc for Psat= 0.10318635009055915 \n",
      "acc for optim= 0.1486499712910763\n",
      "Epoch:547/1000\n",
      "Loss on train= 0.01316339522600174\n",
      "Loss on test= 0.01706572249531746\n",
      "acc for Lsat= 0.067681029652227 \n",
      "acc for Psat= 0.10090736688143026 \n",
      "acc for optim= 0.145888275174634\n",
      "Epoch:548/1000\n",
      "Loss on train= 0.013687421567738056\n",
      "Loss on test= 0.016950249671936035\n",
      "acc for Lsat= 0.0710887452250943 \n",
      "acc for Psat= 0.10399553164813088 \n",
      "acc for optim= 0.14801150717121117\n",
      "Epoch:549/1000\n",
      "Loss on train= 0.013711960054934025\n",
      "Loss on test= 0.017832648009061813\n",
      "acc for Lsat= 0.07130114187708055 \n",
      "acc for Psat= 0.11902552550935151 \n",
      "acc for optim= 0.15156111469848846\n",
      "Epoch:550/1000\n",
      "Loss on train= 0.013421461917459965\n",
      "Loss on test= 0.017753837630152702\n",
      "acc for Lsat= 0.07878383679018891 \n",
      "acc for Psat= 0.1254905611649822 \n",
      "acc for optim= 0.1475394685067943\n",
      "Epoch:551/1000\n",
      "Loss on train= 0.013650103472173214\n",
      "Loss on test= 0.017358312383294106\n",
      "acc for Lsat= 0.07484840783215595 \n",
      "acc for Psat= 0.11885080533718592 \n",
      "acc for optim= 0.14633212021297118\n",
      "Epoch:552/1000\n",
      "Loss on train= 0.014355291612446308\n",
      "Loss on test= 0.017410701140761375\n",
      "acc for Lsat= 0.08536892239011039 \n",
      "acc for Psat= 0.11397221236621331 \n",
      "acc for optim= 0.14640983277015482\n",
      "Epoch:553/1000\n",
      "Loss on train= 0.013652150519192219\n",
      "Loss on test= 0.01682559959590435\n",
      "acc for Lsat= 0.07226366341540555 \n",
      "acc for Psat= 0.10533344394832261 \n",
      "acc for optim= 0.14677377454070156\n",
      "Epoch:554/1000\n",
      "Loss on train= 0.012808081693947315\n",
      "Loss on test= 0.0169063713401556\n",
      "acc for Lsat= 0.0699462284822157 \n",
      "acc for Psat= 0.10106780388274218 \n",
      "acc for optim= 0.14751721301744147\n",
      "Epoch:555/1000\n",
      "Loss on train= 0.013428336009383202\n",
      "Loss on test= 0.01765747182071209\n",
      "acc for Lsat= 0.07475416458783293 \n",
      "acc for Psat= 0.11083202536929272 \n",
      "acc for optim= 0.1471864781354109\n",
      "Epoch:556/1000\n",
      "Loss on train= 0.013546849600970745\n",
      "Loss on test= 0.01711469702422619\n",
      "acc for Lsat= 0.07756792267447932 \n",
      "acc for Psat= 0.10826115048422158 \n",
      "acc for optim= 0.14729624801117105\n",
      "Epoch:557/1000\n",
      "Loss on train= 0.013117730617523193\n",
      "Loss on test= 0.01706552505493164\n",
      "acc for Lsat= 0.0814416329421383 \n",
      "acc for Psat= 0.10545870772627897 \n",
      "acc for optim= 0.14833914215321622\n",
      "Epoch:558/1000\n",
      "Loss on train= 0.01369532011449337\n",
      "Loss on test= 0.01680937223136425\n",
      "acc for Lsat= 0.07740263300206453 \n",
      "acc for Psat= 0.11247033506685165 \n",
      "acc for optim= 0.14657281656384685\n",
      "Epoch:559/1000\n",
      "Loss on train= 0.013155333697795868\n",
      "Loss on test= 0.017186153680086136\n",
      "acc for Lsat= 0.07323283370257704 \n",
      "acc for Psat= 0.10980171440752334 \n",
      "acc for optim= 0.14734541008212268\n",
      "Epoch:560/1000\n",
      "Loss on train= 0.01304866373538971\n",
      "Loss on test= 0.016955433413386345\n",
      "acc for Lsat= 0.0910970864974109 \n",
      "acc for Psat= 0.10546691086083276 \n",
      "acc for optim= 0.1478522806559991\n",
      "Epoch:561/1000\n",
      "Loss on train= 0.01370802242308855\n",
      "Loss on test= 0.01685231924057007\n",
      "acc for Lsat= 0.07478740514391012 \n",
      "acc for Psat= 0.11041788614499973 \n",
      "acc for optim= 0.150005001831993\n",
      "Epoch:562/1000\n",
      "Loss on train= 0.013461166061460972\n",
      "Loss on test= 0.016846906393766403\n",
      "acc for Lsat= 0.07427457465468663 \n",
      "acc for Psat= 0.10353019805841668 \n",
      "acc for optim= 0.14616614935223235\n",
      "Epoch:563/1000\n",
      "Loss on train= 0.013311699964106083\n",
      "Loss on test= 0.017315227538347244\n",
      "acc for Lsat= 0.06965451709166408 \n",
      "acc for Psat= 0.10182235836982724 \n",
      "acc for optim= 0.14560348460840622\n",
      "Epoch:564/1000\n",
      "Loss on train= 0.012999767437577248\n",
      "Loss on test= 0.01717589609324932\n",
      "acc for Lsat= 0.07166969188637298 \n",
      "acc for Psat= 0.10463232484401233 \n",
      "acc for optim= 0.14645772795984274\n",
      "Epoch:565/1000\n",
      "Loss on train= 0.013259561732411385\n",
      "Loss on test= 0.017405666410923004\n",
      "acc for Lsat= 0.07113435216154737 \n",
      "acc for Psat= 0.09955483846672963 \n",
      "acc for optim= 0.14685080632540748\n",
      "Epoch:566/1000\n",
      "Loss on train= 0.013454967178404331\n",
      "Loss on test= 0.016738492995500565\n",
      "acc for Lsat= 0.07658912250522211 \n",
      "acc for Psat= 0.10650389121981982 \n",
      "acc for optim= 0.14727341978409203\n",
      "Epoch:567/1000\n",
      "Loss on train= 0.013581461273133755\n",
      "Loss on test= 0.01740242727100849\n",
      "acc for Lsat= 0.07526629579408438 \n",
      "acc for Psat= 0.10689468058575886 \n",
      "acc for optim= 0.14651285392867008\n",
      "Epoch:568/1000\n",
      "Loss on train= 0.013058244250714779\n",
      "Loss on test= 0.016710631549358368\n",
      "acc for Lsat= 0.07096018787466775 \n",
      "acc for Psat= 0.10042888055120684 \n",
      "acc for optim= 0.14639541337229905\n",
      "Epoch:569/1000\n",
      "Loss on train= 0.013613083399832249\n",
      "Loss on test= 0.016921890899538994\n",
      "acc for Lsat= 0.06880386643418258 \n",
      "acc for Psat= 0.09789643530768874 \n",
      "acc for optim= 0.14595033080718595\n",
      "Epoch:570/1000\n",
      "Loss on train= 0.01322415191680193\n",
      "Loss on test= 0.017521174624562263\n",
      "acc for Lsat= 0.07965610242167184 \n",
      "acc for Psat= 0.10490639885764214 \n",
      "acc for optim= 0.14830043551533723\n",
      "Epoch:571/1000\n",
      "Loss on train= 0.013542522676289082\n",
      "Loss on test= 0.016288137063384056\n",
      "acc for Lsat= 0.07780738875989623 \n",
      "acc for Psat= 0.0989193558266423 \n",
      "acc for optim= 0.1479605266574457\n",
      "Epoch:572/1000\n",
      "Loss on train= 0.012621053494513035\n",
      "Loss on test= 0.01699419692158699\n",
      "acc for Lsat= 0.07627741281802837 \n",
      "acc for Psat= 0.10429113700266175 \n",
      "acc for optim= 0.14902715863183488\n",
      "Epoch:573/1000\n",
      "Loss on train= 0.012955780141055584\n",
      "Loss on test= 0.017727315425872803\n",
      "acc for Lsat= 0.07183220331592083 \n",
      "acc for Psat= 0.10829011469492972 \n",
      "acc for optim= 0.1461108461795425\n",
      "Epoch:574/1000\n",
      "Loss on train= 0.012995855882763863\n",
      "Loss on test= 0.017066264525055885\n",
      "acc for Lsat= 0.07386615446940303 \n",
      "acc for Psat= 0.11601205395671253 \n",
      "acc for optim= 0.14966768786604376\n",
      "Epoch:575/1000\n",
      "Loss on train= 0.01318084541708231\n",
      "Loss on test= 0.01698390208184719\n",
      "acc for Lsat= 0.06827277966603398 \n",
      "acc for Psat= 0.10049881161005635 \n",
      "acc for optim= 0.1468553360651559\n",
      "Epoch:576/1000\n",
      "Loss on train= 0.013567768037319183\n",
      "Loss on test= 0.01665320433676243\n",
      "acc for Lsat= 0.07898307417074554 \n",
      "acc for Psat= 0.10025357642412612 \n",
      "acc for optim= 0.14776378416844474\n",
      "Epoch:577/1000\n",
      "Loss on train= 0.013410712592303753\n",
      "Loss on test= 0.01673094742000103\n",
      "acc for Lsat= 0.07905626225556797 \n",
      "acc for Psat= 0.10829865021867703 \n",
      "acc for optim= 0.14677804583726928\n",
      "Epoch:578/1000\n",
      "Loss on train= 0.013447188772261143\n",
      "Loss on test= 0.016993649303913116\n",
      "acc for Lsat= 0.07503649513495418 \n",
      "acc for Psat= 0.1016361578419298 \n",
      "acc for optim= 0.1486169255483556\n",
      "Epoch:579/1000\n",
      "Loss on train= 0.013537188060581684\n",
      "Loss on test= 0.01738157495856285\n",
      "acc for Lsat= 0.06914987712298823 \n",
      "acc for Psat= 0.09927616337970671 \n",
      "acc for optim= 0.1494666730248651\n",
      "Epoch:580/1000\n",
      "Loss on train= 0.013505743816494942\n",
      "Loss on test= 0.017255807295441628\n",
      "acc for Lsat= 0.0806875694736385 \n",
      "acc for Psat= 0.12148323327782755 \n",
      "acc for optim= 0.14747905677887535\n",
      "Epoch:581/1000\n",
      "Loss on train= 0.013405472971498966\n",
      "Loss on test= 0.017295558005571365\n",
      "acc for Lsat= 0.07341000820528416 \n",
      "acc for Psat= 0.11298832532971405 \n",
      "acc for optim= 0.1485423090952667\n",
      "Epoch:582/1000\n",
      "Loss on train= 0.013026071712374687\n",
      "Loss on test= 0.01737036742269993\n",
      "acc for Lsat= 0.07161625239844827 \n",
      "acc for Psat= 0.10096109237568533 \n",
      "acc for optim= 0.1488026960378043\n",
      "Epoch:583/1000\n",
      "Loss on train= 0.013173635117709637\n",
      "Loss on test= 0.01680740900337696\n",
      "acc for Lsat= 0.07246753268996801 \n",
      "acc for Psat= 0.10283423303918035 \n",
      "acc for optim= 0.14604207540667333\n",
      "Epoch:584/1000\n",
      "Loss on train= 0.012871847487986088\n",
      "Loss on test= 0.016418911516666412\n",
      "acc for Lsat= 0.07130834115852394 \n",
      "acc for Psat= 0.10288787260251735 \n",
      "acc for optim= 0.14672311977325056\n",
      "Epoch:585/1000\n",
      "Loss on train= 0.013006171211600304\n",
      "Loss on test= 0.017406797036528587\n",
      "acc for Lsat= 0.06952575135422949 \n",
      "acc for Psat= 0.10162339005785893 \n",
      "acc for optim= 0.14882922743002286\n",
      "Epoch:586/1000\n",
      "Loss on train= 0.013532419688999653\n",
      "Loss on test= 0.016877001151442528\n",
      "acc for Lsat= 0.08063628062046062 \n",
      "acc for Psat= 0.11185591825432342 \n",
      "acc for optim= 0.1485021131836238\n",
      "Epoch:587/1000\n",
      "Loss on train= 0.013036300428211689\n",
      "Loss on test= 0.017679138109087944\n",
      "acc for Lsat= 0.07465799755295185 \n",
      "acc for Psat= 0.099960371366767 \n",
      "acc for optim= 0.14622478533089697\n",
      "Epoch:588/1000\n",
      "Loss on train= 0.01341092400252819\n",
      "Loss on test= 0.017098309472203255\n",
      "acc for Lsat= 0.06995480291958571 \n",
      "acc for Psat= 0.09937418028578988 \n",
      "acc for optim= 0.14644427093708876\n",
      "Epoch:589/1000\n",
      "Loss on train= 0.012902960181236267\n",
      "Loss on test= 0.017399409785866737\n",
      "acc for Lsat= 0.08613068795694649 \n",
      "acc for Psat= 0.1413921798491094 \n",
      "acc for optim= 0.1464429114926907\n",
      "Epoch:590/1000\n",
      "Loss on train= 0.013990581966936588\n",
      "Loss on test= 0.01684345118701458\n",
      "acc for Lsat= 0.07377755100803852 \n",
      "acc for Psat= 0.1223483830103934 \n",
      "acc for optim= 0.15166714963418218\n",
      "Epoch:591/1000\n",
      "Loss on train= 0.012670373544096947\n",
      "Loss on test= 0.01708916574716568\n",
      "acc for Lsat= 0.07595842589952437 \n",
      "acc for Psat= 0.10444826679920681 \n",
      "acc for optim= 0.1484593157264969\n",
      "Epoch:592/1000\n",
      "Loss on train= 0.01304546743631363\n",
      "Loss on test= 0.017359236255288124\n",
      "acc for Lsat= 0.07001625814996593 \n",
      "acc for Psat= 0.09825761628492147 \n",
      "acc for optim= 0.14931808450993997\n",
      "Epoch:593/1000\n",
      "Loss on train= 0.01298901904374361\n",
      "Loss on test= 0.016992032527923584\n",
      "acc for Lsat= 0.07128409009180256 \n",
      "acc for Psat= 0.10019795620590713 \n",
      "acc for optim= 0.147597165056546\n",
      "Epoch:594/1000\n",
      "Loss on train= 0.013712371699512005\n",
      "Loss on test= 0.016734076663851738\n",
      "acc for Lsat= 0.07641920583832454 \n",
      "acc for Psat= 0.11558719819048434 \n",
      "acc for optim= 0.1500040042485662\n",
      "Epoch:595/1000\n",
      "Loss on train= 0.012791907414793968\n",
      "Loss on test= 0.017221393063664436\n",
      "acc for Lsat= 0.06964083648751589 \n",
      "acc for Psat= 0.10909043315911336 \n",
      "acc for optim= 0.14593372850383976\n",
      "Epoch:596/1000\n",
      "Loss on train= 0.01301342062652111\n",
      "Loss on test= 0.017517734318971634\n",
      "acc for Lsat= 0.07452168713635325 \n",
      "acc for Psat= 0.1074049282905667 \n",
      "acc for optim= 0.14660676012951912\n",
      "Epoch:597/1000\n",
      "Loss on train= 0.012753819115459919\n",
      "Loss on test= 0.01712167263031006\n",
      "acc for Lsat= 0.079213945357019 \n",
      "acc for Psat= 0.10900786005107146 \n",
      "acc for optim= 0.14909228998134727\n",
      "Epoch:598/1000\n",
      "Loss on train= 0.013546927832067013\n",
      "Loss on test= 0.01661399006843567\n",
      "acc for Lsat= 0.08059362225327808 \n",
      "acc for Psat= 0.10860711042910869 \n",
      "acc for optim= 0.14823721018161676\n",
      "Epoch:599/1000\n",
      "Loss on train= 0.013340105302631855\n",
      "Loss on test= 0.017024891451001167\n",
      "acc for Lsat= 0.0700297596096353 \n",
      "acc for Psat= 0.09641949082849703 \n",
      "acc for optim= 0.147871693867477\n",
      "Epoch:600/1000\n",
      "Loss on train= 0.013390660285949707\n",
      "Loss on test= 0.016826914623379707\n",
      "acc for Lsat= 0.0807885503832897 \n",
      "acc for Psat= 0.10149077164252457 \n",
      "acc for optim= 0.14926939096988207\n",
      "Epoch:601/1000\n",
      "Loss on train= 0.013073035515844822\n",
      "Loss on test= 0.01688823290169239\n",
      "acc for Lsat= 0.07230362211335752 \n",
      "acc for Psat= 0.09895876442916918 \n",
      "acc for optim= 0.14600164743143662\n",
      "Epoch:602/1000\n",
      "Loss on train= 0.012936333194375038\n",
      "Loss on test= 0.01720304787158966\n",
      "acc for Lsat= 0.06608150166347758 \n",
      "acc for Psat= 0.10282658530475844 \n",
      "acc for optim= 0.14867069523961474\n",
      "Epoch:603/1000\n",
      "Loss on train= 0.01312231458723545\n",
      "Loss on test= 0.017495157197117805\n",
      "acc for Lsat= 0.07521392982952922 \n",
      "acc for Psat= 0.11775429923760443 \n",
      "acc for optim= 0.1457139507922376\n",
      "Epoch:604/1000\n",
      "Loss on train= 0.012698614969849586\n",
      "Loss on test= 0.016917990520596504\n",
      "acc for Lsat= 0.0815793240699444 \n",
      "acc for Psat= 0.10087057078033951 \n",
      "acc for optim= 0.14646978057347812\n",
      "Epoch:605/1000\n",
      "Loss on train= 0.012883847579360008\n",
      "Loss on test= 0.01743442565202713\n",
      "acc for Lsat= 0.08083261053122859 \n",
      "acc for Psat= 0.1009231472164659 \n",
      "acc for optim= 0.1476936874961171\n",
      "Epoch:606/1000\n",
      "Loss on train= 0.013087734580039978\n",
      "Loss on test= 0.017006652429699898\n",
      "acc for Lsat= 0.07341364182278597 \n",
      "acc for Psat= 0.09933074736211295 \n",
      "acc for optim= 0.1495449168630917\n",
      "Epoch:607/1000\n",
      "Loss on train= 0.012719548307359219\n",
      "Loss on test= 0.016587715595960617\n",
      "acc for Lsat= 0.07172895312735773 \n",
      "acc for Psat= 0.1016397510636898 \n",
      "acc for optim= 0.1446958673234916\n",
      "Epoch:608/1000\n",
      "Loss on train= 0.012869181111454964\n",
      "Loss on test= 0.017438272014260292\n",
      "acc for Lsat= 0.07954566559232838 \n",
      "acc for Psat= 0.10776771835862844 \n",
      "acc for optim= 0.1505198939755052\n",
      "Epoch:609/1000\n",
      "Loss on train= 0.014119627885520458\n",
      "Loss on test= 0.017518503591418266\n",
      "acc for Lsat= 0.08465040878447735 \n",
      "acc for Psat= 0.11311951895945825 \n",
      "acc for optim= 0.14475622934177654\n",
      "Epoch:610/1000\n",
      "Loss on train= 0.013016595505177975\n",
      "Loss on test= 0.017311567440629005\n",
      "acc for Lsat= 0.07508924369948496 \n",
      "acc for Psat= 0.10446162915826908 \n",
      "acc for optim= 0.1486678158874375\n",
      "Epoch:611/1000\n",
      "Loss on train= 0.012345917522907257\n",
      "Loss on test= 0.017295004799962044\n",
      "acc for Lsat= 0.07086644868411403 \n",
      "acc for Psat= 0.11295489107986686 \n",
      "acc for optim= 0.14679730580403258\n",
      "Epoch:612/1000\n",
      "Loss on train= 0.013136642053723335\n",
      "Loss on test= 0.01741744950413704\n",
      "acc for Lsat= 0.07249975388932954 \n",
      "acc for Psat= 0.0988421323047768 \n",
      "acc for optim= 0.14843107616538861\n",
      "Epoch:613/1000\n",
      "Loss on train= 0.012450764887034893\n",
      "Loss on test= 0.016675502061843872\n",
      "acc for Lsat= 0.07418954652302594 \n",
      "acc for Psat= 0.0985977457119868 \n",
      "acc for optim= 0.14860715928998958\n",
      "Epoch:614/1000\n",
      "Loss on train= 0.013009493239223957\n",
      "Loss on test= 0.016501856967806816\n",
      "acc for Lsat= 0.07082717307245157 \n",
      "acc for Psat= 0.09822397128627211 \n",
      "acc for optim= 0.14768235763411613\n",
      "Epoch:615/1000\n",
      "Loss on train= 0.013421143405139446\n",
      "Loss on test= 0.01691357232630253\n",
      "acc for Lsat= 0.07644472711841194 \n",
      "acc for Psat= 0.09798689816207064 \n",
      "acc for optim= 0.14521621043438993\n",
      "Epoch:616/1000\n",
      "Loss on train= 0.012575730681419373\n",
      "Loss on test= 0.017740080133080482\n",
      "acc for Lsat= 0.1004251206550274 \n",
      "acc for Psat= 0.11464922156231558 \n",
      "acc for optim= 0.14844531919107457\n",
      "Epoch:617/1000\n",
      "Loss on train= 0.012770777568221092\n",
      "Loss on test= 0.01640835404396057\n",
      "acc for Lsat= 0.07275613338981417 \n",
      "acc for Psat= 0.1000644723609863 \n",
      "acc for optim= 0.14834218821713238\n",
      "Epoch:618/1000\n",
      "Loss on train= 0.01304845605045557\n",
      "Loss on test= 0.016760708764195442\n",
      "acc for Lsat= 0.06872086499905969 \n",
      "acc for Psat= 0.10394120883856348 \n",
      "acc for optim= 0.14956412525424376\n",
      "Epoch:619/1000\n",
      "Loss on train= 0.013436815701425076\n",
      "Loss on test= 0.016823727637529373\n",
      "acc for Lsat= 0.0725560365134787 \n",
      "acc for Psat= 0.10949616113374401 \n",
      "acc for optim= 0.14925121456011464\n",
      "Epoch:620/1000\n",
      "Loss on train= 0.013330223970115185\n",
      "Loss on test= 0.016529595479369164\n",
      "acc for Lsat= 0.07621832428656665 \n",
      "acc for Psat= 0.11130640386255565 \n",
      "acc for optim= 0.14798382308487387\n",
      "Epoch:621/1000\n",
      "Loss on train= 0.012883150950074196\n",
      "Loss on test= 0.01667916402220726\n",
      "acc for Lsat= 0.07424404820517266 \n",
      "acc for Psat= 0.1060485780558987 \n",
      "acc for optim= 0.14721718395332106\n",
      "Epoch:622/1000\n",
      "Loss on train= 0.012435749173164368\n",
      "Loss on test= 0.01747436448931694\n",
      "acc for Lsat= 0.07051223566579053 \n",
      "acc for Psat= 0.11561627414657304 \n",
      "acc for optim= 0.1474292861860852\n",
      "Epoch:623/1000\n",
      "Loss on train= 0.012656730599701405\n",
      "Loss on test= 0.016839195042848587\n",
      "acc for Lsat= 0.07847303747064524 \n",
      "acc for Psat= 0.11225170636006461 \n",
      "acc for optim= 0.14734241912318044\n",
      "Epoch:624/1000\n",
      "Loss on train= 0.013215921819210052\n",
      "Loss on test= 0.016975846141576767\n",
      "acc for Lsat= 0.08520337683356938 \n",
      "acc for Psat= 0.1048497514242776 \n",
      "acc for optim= 0.14715276978949957\n",
      "Epoch:625/1000\n",
      "Loss on train= 0.012954295612871647\n",
      "Loss on test= 0.01698148064315319\n",
      "acc for Lsat= 0.07253661843446585 \n",
      "acc for Psat= 0.09816830703099004 \n",
      "acc for optim= 0.14576209140591634\n",
      "Epoch:626/1000\n",
      "Loss on train= 0.012487211264669895\n",
      "Loss on test= 0.016709569841623306\n",
      "acc for Lsat= 0.0673251881422425 \n",
      "acc for Psat= 0.09766164119213763 \n",
      "acc for optim= 0.1456277277367273\n",
      "Epoch:627/1000\n",
      "Loss on train= 0.012634105049073696\n",
      "Loss on test= 0.01713893935084343\n",
      "acc for Lsat= 0.07629571195045394 \n",
      "acc for Psat= 0.11551183215833948 \n",
      "acc for optim= 0.14734283637701934\n",
      "Epoch:628/1000\n",
      "Loss on train= 0.013442152179777622\n",
      "Loss on test= 0.01691240817308426\n",
      "acc for Lsat= 0.07556008736008181 \n",
      "acc for Psat= 0.10387445095400052 \n",
      "acc for optim= 0.14902301452027666\n",
      "Epoch:629/1000\n",
      "Loss on train= 0.012550482526421547\n",
      "Loss on test= 0.016753917559981346\n",
      "acc for Lsat= 0.06982828827045046 \n",
      "acc for Psat= 0.0946002004398216 \n",
      "acc for optim= 0.1463907445159496\n",
      "Epoch:630/1000\n",
      "Loss on train= 0.012953700497746468\n",
      "Loss on test= 0.016313645988702774\n",
      "acc for Lsat= 0.0666479914794028 \n",
      "acc for Psat= 0.10142810606999132 \n",
      "acc for optim= 0.1468301403074657\n",
      "Epoch:631/1000\n",
      "Loss on train= 0.01293734647333622\n",
      "Loss on test= 0.016862167045474052\n",
      "acc for Lsat= 0.06807509106259013 \n",
      "acc for Psat= 0.10889131621939127 \n",
      "acc for optim= 0.1476872457481241\n",
      "Epoch:632/1000\n",
      "Loss on train= 0.012500299140810966\n",
      "Loss on test= 0.017051953822374344\n",
      "acc for Lsat= 0.07308506732656617 \n",
      "acc for Psat= 0.09919895132027286 \n",
      "acc for optim= 0.1464898605466102\n",
      "Epoch:633/1000\n",
      "Loss on train= 0.012551429681479931\n",
      "Loss on test= 0.017108209431171417\n",
      "acc for Lsat= 0.07415311873918784 \n",
      "acc for Psat= 0.10398697620640929 \n",
      "acc for optim= 0.14719449008726687\n",
      "Epoch:634/1000\n",
      "Loss on train= 0.012751862406730652\n",
      "Loss on test= 0.01646874099969864\n",
      "acc for Lsat= 0.0696023116118153 \n",
      "acc for Psat= 0.1187062433239386 \n",
      "acc for optim= 0.15038931033905586\n",
      "Epoch:635/1000\n",
      "Loss on train= 0.012956894002854824\n",
      "Loss on test= 0.01765584945678711\n",
      "acc for Lsat= 0.07699617684847128 \n",
      "acc for Psat= 0.1236379366228107 \n",
      "acc for optim= 0.1499341761383473\n",
      "Epoch:636/1000\n",
      "Loss on train= 0.012338506057858467\n",
      "Loss on test= 0.01767793856561184\n",
      "acc for Lsat= 0.08602828797159554 \n",
      "acc for Psat= 0.11517120982538612 \n",
      "acc for optim= 0.14646817349790459\n",
      "Epoch:637/1000\n",
      "Loss on train= 0.01286186370998621\n",
      "Loss on test= 0.017237966880202293\n",
      "acc for Lsat= 0.07812655048848054 \n",
      "acc for Psat= 0.1099653875998393 \n",
      "acc for optim= 0.14709488045764102\n",
      "Epoch:638/1000\n",
      "Loss on train= 0.012777840718626976\n",
      "Loss on test= 0.017888234928250313\n",
      "acc for Lsat= 0.0756702124379402 \n",
      "acc for Psat= 0.10825673214864646 \n",
      "acc for optim= 0.14535192731028168\n",
      "Epoch:639/1000\n",
      "Loss on train= 0.012854672968387604\n",
      "Loss on test= 0.016641590744256973\n",
      "acc for Lsat= 0.07440969976521562 \n",
      "acc for Psat= 0.10041530847762693 \n",
      "acc for optim= 0.1461016916844841\n",
      "Epoch:640/1000\n",
      "Loss on train= 0.012693934142589569\n",
      "Loss on test= 0.016712898388504982\n",
      "acc for Lsat= 0.07535181180948865 \n",
      "acc for Psat= 0.09893028340527324 \n",
      "acc for optim= 0.14800912197246108\n",
      "Epoch:641/1000\n",
      "Loss on train= 0.012583679519593716\n",
      "Loss on test= 0.01624751277267933\n",
      "acc for Lsat= 0.06979484090011746 \n",
      "acc for Psat= 0.09722906171955661 \n",
      "acc for optim= 0.145743158614273\n",
      "Epoch:642/1000\n",
      "Loss on train= 0.012948483228683472\n",
      "Loss on test= 0.017607124522328377\n",
      "acc for Lsat= 0.08255852760483842 \n",
      "acc for Psat= 0.1088371069998562 \n",
      "acc for optim= 0.14894584499446142\n",
      "Epoch:643/1000\n",
      "Loss on train= 0.012758426368236542\n",
      "Loss on test= 0.01697821170091629\n",
      "acc for Lsat= 0.07476165311174443 \n",
      "acc for Psat= 0.09613214457184344 \n",
      "acc for optim= 0.14640144341107134\n",
      "Epoch:644/1000\n",
      "Loss on train= 0.012437609024345875\n",
      "Loss on test= 0.01678168773651123\n",
      "acc for Lsat= 0.07150659830924225 \n",
      "acc for Psat= 0.09916468321317427 \n",
      "acc for optim= 0.14792262136616308\n",
      "Epoch:645/1000\n",
      "Loss on train= 0.012671380303800106\n",
      "Loss on test= 0.016405850648880005\n",
      "acc for Lsat= 0.07252460115710822 \n",
      "acc for Psat= 0.09811289391492047 \n",
      "acc for optim= 0.14814515263108746\n",
      "Epoch:646/1000\n",
      "Loss on train= 0.012354099191725254\n",
      "Loss on test= 0.01699073240160942\n",
      "acc for Lsat= 0.07079025110319817 \n",
      "acc for Psat= 0.10582318190811785 \n",
      "acc for optim= 0.14578594574987994\n",
      "Epoch:647/1000\n",
      "Loss on train= 0.011975109577178955\n",
      "Loss on test= 0.01726841926574707\n",
      "acc for Lsat= 0.06996594086849198 \n",
      "acc for Psat= 0.09984550427025676 \n",
      "acc for optim= 0.14829067382061842\n",
      "Epoch:648/1000\n",
      "Loss on train= 0.012216161005198956\n",
      "Loss on test= 0.016987992450594902\n",
      "acc for Lsat= 0.0696568300131182 \n",
      "acc for Psat= 0.09932400734565339 \n",
      "acc for optim= 0.14773414739555876\n",
      "Epoch:649/1000\n",
      "Loss on train= 0.013041282072663307\n",
      "Loss on test= 0.016386689618229866\n",
      "acc for Lsat= 0.0802862156109221 \n",
      "acc for Psat= 0.10014841172690897 \n",
      "acc for optim= 0.14708300596486265\n",
      "Epoch:650/1000\n",
      "Loss on train= 0.012232241220772266\n",
      "Loss on test= 0.0171468835324049\n",
      "acc for Lsat= 0.06766427551697746 \n",
      "acc for Psat= 0.09977428859802819 \n",
      "acc for optim= 0.14513458729003537\n",
      "Epoch:651/1000\n",
      "Loss on train= 0.012270445935428143\n",
      "Loss on test= 0.016586758196353912\n",
      "acc for Lsat= 0.07116469257846075 \n",
      "acc for Psat= 0.10158487381363598 \n",
      "acc for optim= 0.14891149383538096\n",
      "Epoch:652/1000\n",
      "Loss on train= 0.012520244345068932\n",
      "Loss on test= 0.01728270947933197\n",
      "acc for Lsat= 0.07371795824154122 \n",
      "acc for Psat= 0.09524381414508992 \n",
      "acc for optim= 0.1457823262444976\n",
      "Epoch:653/1000\n",
      "Loss on train= 0.012496834620833397\n",
      "Loss on test= 0.016752786934375763\n",
      "acc for Lsat= 0.0725129590211486 \n",
      "acc for Psat= 0.10831470322097138 \n",
      "acc for optim= 0.14802255044043172\n",
      "Epoch:654/1000\n",
      "Loss on train= 0.01250131893903017\n",
      "Loss on test= 0.017065061256289482\n",
      "acc for Lsat= 0.0708736150710442 \n",
      "acc for Psat= 0.10513963260036462 \n",
      "acc for optim= 0.14608227534882712\n",
      "Epoch:655/1000\n",
      "Loss on train= 0.012341218069195747\n",
      "Loss on test= 0.01687641814351082\n",
      "acc for Lsat= 0.08023682747949215 \n",
      "acc for Psat= 0.10284782525465185 \n",
      "acc for optim= 0.14615609540282506\n",
      "Epoch:656/1000\n",
      "Loss on train= 0.012769734486937523\n",
      "Loss on test= 0.017177242785692215\n",
      "acc for Lsat= 0.06801339177304816 \n",
      "acc for Psat= 0.1013913119308424 \n",
      "acc for optim= 0.15011121198828187\n",
      "Epoch:657/1000\n",
      "Loss on train= 0.01196968276053667\n",
      "Loss on test= 0.0169527567923069\n",
      "acc for Lsat= 0.0708369345219276 \n",
      "acc for Psat= 0.10247844196389529 \n",
      "acc for optim= 0.14651741021860903\n",
      "Epoch:658/1000\n",
      "Loss on train= 0.011744880117475986\n",
      "Loss on test= 0.01639925315976143\n",
      "acc for Lsat= 0.07091310057849065 \n",
      "acc for Psat= 0.0995491696171769 \n",
      "acc for optim= 0.14573207533850016\n",
      "Epoch:659/1000\n",
      "Loss on train= 0.011803584173321724\n",
      "Loss on test= 0.017164045944809914\n",
      "acc for Lsat= 0.0719307949163406 \n",
      "acc for Psat= 0.1052587491775028 \n",
      "acc for optim= 0.1472719042373684\n",
      "Epoch:660/1000\n",
      "Loss on train= 0.01194013375788927\n",
      "Loss on test= 0.017817527055740356\n",
      "acc for Lsat= 0.08382671856069823 \n",
      "acc for Psat= 0.10179521569412384 \n",
      "acc for optim= 0.1454636379943124\n",
      "Epoch:661/1000\n",
      "Loss on train= 0.013155011460185051\n",
      "Loss on test= 0.016927648335695267\n",
      "acc for Lsat= 0.07239780075221666 \n",
      "acc for Psat= 0.10765623604888779 \n",
      "acc for optim= 0.15324575410546046\n",
      "Epoch:662/1000\n",
      "Loss on train= 0.012738085351884365\n",
      "Loss on test= 0.01655633933842182\n",
      "acc for Lsat= 0.07621617765034244 \n",
      "acc for Psat= 0.09848512192531647 \n",
      "acc for optim= 0.14925561168752546\n",
      "Epoch:663/1000\n",
      "Loss on train= 0.012017271481454372\n",
      "Loss on test= 0.016935236752033234\n",
      "acc for Lsat= 0.07321822644879439 \n",
      "acc for Psat= 0.1032632016325253 \n",
      "acc for optim= 0.1490075954597623\n",
      "Epoch:664/1000\n",
      "Loss on train= 0.012824179604649544\n",
      "Loss on test= 0.016675779595971107\n",
      "acc for Lsat= 0.06812897602859251 \n",
      "acc for Psat= 0.09478958067825742 \n",
      "acc for optim= 0.14645145026856973\n",
      "Epoch:665/1000\n",
      "Loss on train= 0.01233400497585535\n",
      "Loss on test= 0.016931332647800446\n",
      "acc for Lsat= 0.07698922045328963 \n",
      "acc for Psat= 0.10038069867277401 \n",
      "acc for optim= 0.14785759204828677\n",
      "Epoch:666/1000\n",
      "Loss on train= 0.012236014008522034\n",
      "Loss on test= 0.017005374655127525\n",
      "acc for Lsat= 0.07253167449041853 \n",
      "acc for Psat= 0.09668638945052366 \n",
      "acc for optim= 0.1473200674347033\n",
      "Epoch:667/1000\n",
      "Loss on train= 0.011761456727981567\n",
      "Loss on test= 0.016889888793230057\n",
      "acc for Lsat= 0.07225870833413972 \n",
      "acc for Psat= 0.10118178722043795 \n",
      "acc for optim= 0.14758251778554834\n",
      "Epoch:668/1000\n",
      "Loss on train= 0.012423709966242313\n",
      "Loss on test= 0.01700042001903057\n",
      "acc for Lsat= 0.0741416624598938 \n",
      "acc for Psat= 0.09836774780839844 \n",
      "acc for optim= 0.1477816459211339\n",
      "Epoch:669/1000\n",
      "Loss on train= 0.012342953123152256\n",
      "Loss on test= 0.01666397601366043\n",
      "acc for Lsat= 0.07148850823024518 \n",
      "acc for Psat= 0.09899217712431349 \n",
      "acc for optim= 0.1467117666984926\n",
      "Epoch:670/1000\n",
      "Loss on train= 0.013127559795975685\n",
      "Loss on test= 0.01683923415839672\n",
      "acc for Lsat= 0.06979918374891571 \n",
      "acc for Psat= 0.09949475677154146 \n",
      "acc for optim= 0.14754880907710424\n",
      "Epoch:671/1000\n",
      "Loss on train= 0.012044703587889671\n",
      "Loss on test= 0.016968123614788055\n",
      "acc for Lsat= 0.07126529165585269 \n",
      "acc for Psat= 0.09714009354494126 \n",
      "acc for optim= 0.14772558035279001\n",
      "Epoch:672/1000\n",
      "Loss on train= 0.012492273934185505\n",
      "Loss on test= 0.017745167016983032\n",
      "acc for Lsat= 0.071752060174089 \n",
      "acc for Psat= 0.10178657274340049 \n",
      "acc for optim= 0.14804025386228545\n",
      "Epoch:673/1000\n",
      "Loss on train= 0.01209957990795374\n",
      "Loss on test= 0.0168826412409544\n",
      "acc for Lsat= 0.07398727395453478 \n",
      "acc for Psat= 0.10260319740699743 \n",
      "acc for optim= 0.14718120605659826\n",
      "Epoch:674/1000\n",
      "Loss on train= 0.012672794982790947\n",
      "Loss on test= 0.016734065487980843\n",
      "acc for Lsat= 0.07662662899451521 \n",
      "acc for Psat= 0.09667922975978614 \n",
      "acc for optim= 0.14555346576600253\n",
      "Epoch:675/1000\n",
      "Loss on train= 0.01275959424674511\n",
      "Loss on test= 0.01701464131474495\n",
      "acc for Lsat= 0.07229972157580696 \n",
      "acc for Psat= 0.09969984343098827 \n",
      "acc for optim= 0.14709623256608287\n",
      "Epoch:676/1000\n",
      "Loss on train= 0.012572542764246464\n",
      "Loss on test= 0.017229430377483368\n",
      "acc for Lsat= 0.07793334316269016 \n",
      "acc for Psat= 0.1034688895418307 \n",
      "acc for optim= 0.14824456625846286\n",
      "Epoch:677/1000\n",
      "Loss on train= 0.012126289308071136\n",
      "Loss on test= 0.01761944591999054\n",
      "acc for Lsat= 0.07253466415917084 \n",
      "acc for Psat= 0.10328958439699013 \n",
      "acc for optim= 0.14723711436037942\n",
      "Epoch:678/1000\n",
      "Loss on train= 0.012180404737591743\n",
      "Loss on test= 0.016232792288064957\n",
      "acc for Lsat= 0.07140852622887742 \n",
      "acc for Psat= 0.10180393973488717 \n",
      "acc for optim= 0.1462283698305461\n",
      "Epoch:679/1000\n",
      "Loss on train= 0.012718805111944675\n",
      "Loss on test= 0.016515051946043968\n",
      "acc for Lsat= 0.07256025733269605 \n",
      "acc for Psat= 0.1120270120862985 \n",
      "acc for optim= 0.1494604023070156\n",
      "Epoch:680/1000\n",
      "Loss on train= 0.012710949406027794\n",
      "Loss on test= 0.01705761067569256\n",
      "acc for Lsat= 0.07845378116652875 \n",
      "acc for Psat= 0.10934945367316652 \n",
      "acc for optim= 0.14693289994127212\n",
      "Epoch:681/1000\n",
      "Loss on train= 0.012240576557815075\n",
      "Loss on test= 0.016772663220763206\n",
      "acc for Lsat= 0.06599176322944689 \n",
      "acc for Psat= 0.09490674707458782 \n",
      "acc for optim= 0.14807936004321348\n",
      "Epoch:682/1000\n",
      "Loss on train= 0.012315979227423668\n",
      "Loss on test= 0.01741669327020645\n",
      "acc for Lsat= 0.08073284222422857 \n",
      "acc for Psat= 0.10738207934580891 \n",
      "acc for optim= 0.1474317561002025\n",
      "Epoch:683/1000\n",
      "Loss on train= 0.012375721707940102\n",
      "Loss on test= 0.016222499310970306\n",
      "acc for Lsat= 0.07121612091610385 \n",
      "acc for Psat= 0.10229662263329424 \n",
      "acc for optim= 0.14979965291210917\n",
      "Epoch:684/1000\n",
      "Loss on train= 0.012442454695701599\n",
      "Loss on test= 0.01696345955133438\n",
      "acc for Lsat= 0.06984092162952013 \n",
      "acc for Psat= 0.10152083735559836 \n",
      "acc for optim= 0.14736002465053627\n",
      "Epoch:685/1000\n",
      "Loss on train= 0.01222867053002119\n",
      "Loss on test= 0.017152180895209312\n",
      "acc for Lsat= 0.07876004677445818 \n",
      "acc for Psat= 0.10484090165929846 \n",
      "acc for optim= 0.14804064040747011\n",
      "Epoch:686/1000\n",
      "Loss on train= 0.012323418632149696\n",
      "Loss on test= 0.016822194680571556\n",
      "acc for Lsat= 0.06918181442723931 \n",
      "acc for Psat= 0.1022245361894529 \n",
      "acc for optim= 0.14733308042951043\n",
      "Epoch:687/1000\n",
      "Loss on train= 0.012008707039058208\n",
      "Loss on test= 0.016734905540943146\n",
      "acc for Lsat= 0.06909765733375958 \n",
      "acc for Psat= 0.09873593060403049 \n",
      "acc for optim= 0.14686113143862894\n",
      "Epoch:688/1000\n",
      "Loss on train= 0.011866247281432152\n",
      "Loss on test= 0.01613580621778965\n",
      "acc for Lsat= 0.07997922186134966 \n",
      "acc for Psat= 0.10220488834466405 \n",
      "acc for optim= 0.14702667751764356\n",
      "Epoch:689/1000\n",
      "Loss on train= 0.01257426105439663\n",
      "Loss on test= 0.0167196337133646\n",
      "acc for Lsat= 0.0749096404994323 \n",
      "acc for Psat= 0.10450407437220456 \n",
      "acc for optim= 0.14771732877746677\n",
      "Epoch:690/1000\n",
      "Loss on train= 0.011923464946448803\n",
      "Loss on test= 0.016945386305451393\n",
      "acc for Lsat= 0.07694331429405588 \n",
      "acc for Psat= 0.10447802181107414 \n",
      "acc for optim= 0.1467670171546595\n",
      "Epoch:691/1000\n",
      "Loss on train= 0.011940807104110718\n",
      "Loss on test= 0.01697480119764805\n",
      "acc for Lsat= 0.06850608540349014 \n",
      "acc for Psat= 0.10168099518538802 \n",
      "acc for optim= 0.1479520000690637\n",
      "Epoch:692/1000\n",
      "Loss on train= 0.012047739699482918\n",
      "Loss on test= 0.017322873696684837\n",
      "acc for Lsat= 0.07519454550337917 \n",
      "acc for Psat= 0.10106257397501542 \n",
      "acc for optim= 0.14975189776667958\n",
      "Epoch:693/1000\n",
      "Loss on train= 0.012122235260903835\n",
      "Loss on test= 0.017350075766444206\n",
      "acc for Lsat= 0.07153293166796082 \n",
      "acc for Psat= 0.09778695486099435 \n",
      "acc for optim= 0.14630759344544525\n",
      "Epoch:694/1000\n",
      "Loss on train= 0.012831700965762138\n",
      "Loss on test= 0.016950024291872978\n",
      "acc for Lsat= 0.06794170504819089 \n",
      "acc for Psat= 0.09624004567671758 \n",
      "acc for optim= 0.14572546096095457\n",
      "Epoch:695/1000\n",
      "Loss on train= 0.012072673067450523\n",
      "Loss on test= 0.016790086403489113\n",
      "acc for Lsat= 0.07753357447963709 \n",
      "acc for Psat= 0.09706344187579556 \n",
      "acc for optim= 0.1454592911203347\n",
      "Epoch:696/1000\n",
      "Loss on train= 0.011974942870438099\n",
      "Loss on test= 0.01707078330218792\n",
      "acc for Lsat= 0.06825477721866 \n",
      "acc for Psat= 0.09594659723194854 \n",
      "acc for optim= 0.14641179703642512\n",
      "Epoch:697/1000\n",
      "Loss on train= 0.011367058381438255\n",
      "Loss on test= 0.016966504976153374\n",
      "acc for Lsat= 0.07105318171926817 \n",
      "acc for Psat= 0.09747583517448208 \n",
      "acc for optim= 0.14702919785579757\n",
      "Epoch:698/1000\n",
      "Loss on train= 0.011521467007696629\n",
      "Loss on test= 0.017113542184233665\n",
      "acc for Lsat= 0.07046707043814103 \n",
      "acc for Psat= 0.09702993910726364 \n",
      "acc for optim= 0.14622806857651588\n",
      "Epoch:699/1000\n",
      "Loss on train= 0.01207411102950573\n",
      "Loss on test= 0.016571952030062675\n",
      "acc for Lsat= 0.0693984613488953 \n",
      "acc for Psat= 0.09837835938644751 \n",
      "acc for optim= 0.1479040546366055\n",
      "Epoch:700/1000\n",
      "Loss on train= 0.012419250793755054\n",
      "Loss on test= 0.016143543645739555\n",
      "acc for Lsat= 0.0667830544422266 \n",
      "acc for Psat= 0.09981650144981359 \n",
      "acc for optim= 0.14765704808380184\n",
      "Epoch:701/1000\n",
      "Loss on train= 0.012360543012619019\n",
      "Loss on test= 0.017021100968122482\n",
      "acc for Lsat= 0.07402226140119522 \n",
      "acc for Psat= 0.10230314262437905 \n",
      "acc for optim= 0.14873704863362325\n",
      "Epoch:702/1000\n",
      "Loss on train= 0.011897767893970013\n",
      "Loss on test= 0.01704174466431141\n",
      "acc for Lsat= 0.06855356877199863 \n",
      "acc for Psat= 0.09949426144521435 \n",
      "acc for optim= 0.14674725869473912\n",
      "Epoch:703/1000\n",
      "Loss on train= 0.012036412954330444\n",
      "Loss on test= 0.016475027427077293\n",
      "acc for Lsat= 0.07458320375738503 \n",
      "acc for Psat= 0.10466140712309822 \n",
      "acc for optim= 0.14546197699732769\n",
      "Epoch:704/1000\n",
      "Loss on train= 0.011781426146626472\n",
      "Loss on test= 0.01664625108242035\n",
      "acc for Lsat= 0.0760214906205858 \n",
      "acc for Psat= 0.10307462505449765 \n",
      "acc for optim= 0.1459701813184298\n",
      "Epoch:705/1000\n",
      "Loss on train= 0.011860840953886509\n",
      "Loss on test= 0.016454320400953293\n",
      "acc for Lsat= 0.07007480381000022 \n",
      "acc for Psat= 0.09517548629124391 \n",
      "acc for optim= 0.14626554068384534\n",
      "Epoch:706/1000\n",
      "Loss on train= 0.011895330622792244\n",
      "Loss on test= 0.016819944605231285\n",
      "acc for Lsat= 0.069451467684855 \n",
      "acc for Psat= 0.1015139979838472 \n",
      "acc for optim= 0.14789354337349353\n",
      "Epoch:707/1000\n",
      "Loss on train= 0.012076614424586296\n",
      "Loss on test= 0.017346203327178955\n",
      "acc for Lsat= 0.07574195935922359 \n",
      "acc for Psat= 0.09881705631083797 \n",
      "acc for optim= 0.14680024813027634\n",
      "Epoch:708/1000\n",
      "Loss on train= 0.01179205346852541\n",
      "Loss on test= 0.017027921974658966\n",
      "acc for Lsat= 0.06642319364923059 \n",
      "acc for Psat= 0.09557420260577809 \n",
      "acc for optim= 0.14694346067943814\n",
      "Epoch:709/1000\n",
      "Loss on train= 0.01208068709820509\n",
      "Loss on test= 0.01726297102868557\n",
      "acc for Lsat= 0.07624336334481864 \n",
      "acc for Psat= 0.10311406281425191 \n",
      "acc for optim= 0.14579362616556066\n",
      "Epoch:710/1000\n",
      "Loss on train= 0.012188553810119629\n",
      "Loss on test= 0.016816601157188416\n",
      "acc for Lsat= 0.07277782821697924 \n",
      "acc for Psat= 0.0994803129667033 \n",
      "acc for optim= 0.14710522788795036\n",
      "Epoch:711/1000\n",
      "Loss on train= 0.012012717314064503\n",
      "Loss on test= 0.016673307865858078\n",
      "acc for Lsat= 0.07425274673536127 \n",
      "acc for Psat= 0.09710788598853914 \n",
      "acc for optim= 0.1473002231504921\n",
      "Epoch:712/1000\n",
      "Loss on train= 0.011689819395542145\n",
      "Loss on test= 0.016264189034700394\n",
      "acc for Lsat= 0.06761351681992492 \n",
      "acc for Psat= 0.10409575972659431 \n",
      "acc for optim= 0.147370236184388\n",
      "Epoch:713/1000\n",
      "Loss on train= 0.012510840781033039\n",
      "Loss on test= 0.016785018146038055\n",
      "acc for Lsat= 0.06833406382361977 \n",
      "acc for Psat= 0.0970791314177095 \n",
      "acc for optim= 0.14845828357450863\n",
      "Epoch:714/1000\n",
      "Loss on train= 0.012447968125343323\n",
      "Loss on test= 0.016719425097107887\n",
      "acc for Lsat= 0.0725962577171104 \n",
      "acc for Psat= 0.10135209910131737 \n",
      "acc for optim= 0.14838718727151057\n",
      "Epoch:715/1000\n",
      "Loss on train= 0.012134484015405178\n",
      "Loss on test= 0.01704530417919159\n",
      "acc for Lsat= 0.06899043084896954 \n",
      "acc for Psat= 0.09588347963442319 \n",
      "acc for optim= 0.146315127739113\n",
      "Epoch:716/1000\n",
      "Loss on train= 0.01222720555961132\n",
      "Loss on test= 0.016832899302244186\n",
      "acc for Lsat= 0.06728161321236538 \n",
      "acc for Psat= 0.09526802205442317 \n",
      "acc for optim= 0.1461240111715252\n",
      "Epoch:717/1000\n",
      "Loss on train= 0.011623285710811615\n",
      "Loss on test= 0.01649041473865509\n",
      "acc for Lsat= 0.07184305096141769 \n",
      "acc for Psat= 0.10779258205766964 \n",
      "acc for optim= 0.1477363627477281\n",
      "Epoch:718/1000\n",
      "Loss on train= 0.011912462301552296\n",
      "Loss on test= 0.016567466780543327\n",
      "acc for Lsat= 0.06874025858578828 \n",
      "acc for Psat= 0.1075864555050947 \n",
      "acc for optim= 0.14800326582144332\n",
      "Epoch:719/1000\n",
      "Loss on train= 0.011645286343991756\n",
      "Loss on test= 0.01697520725429058\n",
      "acc for Lsat= 0.06925703182096771 \n",
      "acc for Psat= 0.10436705716607395 \n",
      "acc for optim= 0.14782763060602175\n",
      "Epoch:720/1000\n",
      "Loss on train= 0.012457599863409996\n",
      "Loss on test= 0.017417628318071365\n",
      "acc for Lsat= 0.07674039479125165 \n",
      "acc for Psat= 0.11111888347030324 \n",
      "acc for optim= 0.14699934822288954\n",
      "Epoch:721/1000\n",
      "Loss on train= 0.012297769077122211\n",
      "Loss on test= 0.016715101897716522\n",
      "acc for Lsat= 0.0696932787865347 \n",
      "acc for Psat= 0.10499961047778193 \n",
      "acc for optim= 0.14853756797761528\n",
      "Epoch:722/1000\n",
      "Loss on train= 0.012118970043957233\n",
      "Loss on test= 0.01695883460342884\n",
      "acc for Lsat= 0.06825901173948173 \n",
      "acc for Psat= 0.09392060989344056 \n",
      "acc for optim= 0.14802616448649783\n",
      "Epoch:723/1000\n",
      "Loss on train= 0.012470653280615807\n",
      "Loss on test= 0.01712307333946228\n",
      "acc for Lsat= 0.07299171346031487 \n",
      "acc for Psat= 0.09654385034854597 \n",
      "acc for optim= 0.14687577997849113\n",
      "Epoch:724/1000\n",
      "Loss on train= 0.011834673583507538\n",
      "Loss on test= 0.016512837260961533\n",
      "acc for Lsat= 0.06825076258992892 \n",
      "acc for Psat= 0.10194000276555318 \n",
      "acc for optim= 0.14824066166374467\n",
      "Epoch:725/1000\n",
      "Loss on train= 0.011869775131344795\n",
      "Loss on test= 0.016241878271102905\n",
      "acc for Lsat= 0.07199332837235309 \n",
      "acc for Psat= 0.10251376665129004 \n",
      "acc for optim= 0.14747924792958497\n",
      "Epoch:726/1000\n",
      "Loss on train= 0.011776985600590706\n",
      "Loss on test= 0.017049109563231468\n",
      "acc for Lsat= 0.06921637731609276 \n",
      "acc for Psat= 0.09891264099554427 \n",
      "acc for optim= 0.14796365963325092\n",
      "Epoch:727/1000\n",
      "Loss on train= 0.012380249798297882\n",
      "Loss on test= 0.017370199784636497\n",
      "acc for Lsat= 0.07157573201468677 \n",
      "acc for Psat= 0.10171526380429753 \n",
      "acc for optim= 0.14816431789150822\n",
      "Epoch:728/1000\n",
      "Loss on train= 0.01188638899475336\n",
      "Loss on test= 0.017462195828557014\n",
      "acc for Lsat= 0.07023553376974062 \n",
      "acc for Psat= 0.0979793221344888 \n",
      "acc for optim= 0.1471850662517207\n",
      "Epoch:729/1000\n",
      "Loss on train= 0.011611737310886383\n",
      "Loss on test= 0.017145060002803802\n",
      "acc for Lsat= 0.07564031925845445 \n",
      "acc for Psat= 0.10806321745908325 \n",
      "acc for optim= 0.1463372864322287\n",
      "Epoch:730/1000\n",
      "Loss on train= 0.01244518905878067\n",
      "Loss on test= 0.016533657908439636\n",
      "acc for Lsat= 0.07097818605695086 \n",
      "acc for Psat= 0.09710728623146238 \n",
      "acc for optim= 0.14703992966675802\n",
      "Epoch:731/1000\n",
      "Loss on train= 0.012509984895586967\n",
      "Loss on test= 0.016736358404159546\n",
      "acc for Lsat= 0.06895666809649288 \n",
      "acc for Psat= 0.11106857775788831 \n",
      "acc for optim= 0.14706634836461327\n",
      "Epoch:732/1000\n",
      "Loss on train= 0.01177230291068554\n",
      "Loss on test= 0.01590011455118656\n",
      "acc for Lsat= 0.07123675373564466 \n",
      "acc for Psat= 0.10046360524694482 \n",
      "acc for optim= 0.14630102908675266\n",
      "Epoch:733/1000\n",
      "Loss on train= 0.012245125137269497\n",
      "Loss on test= 0.01677522622048855\n",
      "acc for Lsat= 0.06933099568849814 \n",
      "acc for Psat= 0.09828900742829379 \n",
      "acc for optim= 0.14813388283222859\n",
      "Epoch:734/1000\n",
      "Loss on train= 0.012783827260136604\n",
      "Loss on test= 0.01724865287542343\n",
      "acc for Lsat= 0.0730880720830774 \n",
      "acc for Psat= 0.10471967812727516 \n",
      "acc for optim= 0.146129861969854\n",
      "Epoch:735/1000\n",
      "Loss on train= 0.011966227553784847\n",
      "Loss on test= 0.016781270503997803\n",
      "acc for Lsat= 0.07288969160725692 \n",
      "acc for Psat= 0.1039352073972255 \n",
      "acc for optim= 0.14861469675048739\n",
      "Epoch:736/1000\n",
      "Loss on train= 0.012052461504936218\n",
      "Loss on test= 0.016736799851059914\n",
      "acc for Lsat= 0.06930274609376365 \n",
      "acc for Psat= 0.09933505024167845 \n",
      "acc for optim= 0.14812752713244376\n",
      "Epoch:737/1000\n",
      "Loss on train= 0.011447861790657043\n",
      "Loss on test= 0.01657978817820549\n",
      "acc for Lsat= 0.07651565866095007 \n",
      "acc for Psat= 0.1089949369004033 \n",
      "acc for optim= 0.14731476797400733\n",
      "Epoch:738/1000\n",
      "Loss on train= 0.01191397663205862\n",
      "Loss on test= 0.016313983127474785\n",
      "acc for Lsat= 0.0689134825374231 \n",
      "acc for Psat= 0.10413178679981473 \n",
      "acc for optim= 0.14800155896406905\n",
      "Epoch:739/1000\n",
      "Loss on train= 0.011571021750569344\n",
      "Loss on test= 0.016616208478808403\n",
      "acc for Lsat= 0.07159683689981541 \n",
      "acc for Psat= 0.10066308802483549 \n",
      "acc for optim= 0.1486351937642038\n",
      "Epoch:740/1000\n",
      "Loss on train= 0.011916987597942352\n",
      "Loss on test= 0.017084291204810143\n",
      "acc for Lsat= 0.07517116746876874 \n",
      "acc for Psat= 0.10741090166760685 \n",
      "acc for optim= 0.1477472875433872\n",
      "Epoch:741/1000\n",
      "Loss on train= 0.011610439978539944\n",
      "Loss on test= 0.01697608083486557\n",
      "acc for Lsat= 0.0695439684689578 \n",
      "acc for Psat= 0.10512473763421525 \n",
      "acc for optim= 0.15041258598482884\n",
      "Epoch:742/1000\n",
      "Loss on train= 0.012058758176863194\n",
      "Loss on test= 0.016848288476467133\n",
      "acc for Lsat= 0.06983554189981414 \n",
      "acc for Psat= 0.0970785939096338 \n",
      "acc for optim= 0.14757827146108762\n",
      "Epoch:743/1000\n",
      "Loss on train= 0.011797795072197914\n",
      "Loss on test= 0.017395567148923874\n",
      "acc for Lsat= 0.07068912222155092 \n",
      "acc for Psat= 0.10231396134722852 \n",
      "acc for optim= 0.14690700949624536\n",
      "Epoch:744/1000\n",
      "Loss on train= 0.0112954406067729\n",
      "Loss on test= 0.01683064177632332\n",
      "acc for Lsat= 0.07385519575027742 \n",
      "acc for Psat= 0.10472956346697802 \n",
      "acc for optim= 0.1470506491622686\n",
      "Epoch:745/1000\n",
      "Loss on train= 0.011721246875822544\n",
      "Loss on test= 0.016841866075992584\n",
      "acc for Lsat= 0.07058635901039954 \n",
      "acc for Psat= 0.0976443029047125 \n",
      "acc for optim= 0.14770176500241955\n",
      "Epoch:746/1000\n",
      "Loss on train= 0.011811698786914349\n",
      "Loss on test= 0.016898570582270622\n",
      "acc for Lsat= 0.07291501080733934 \n",
      "acc for Psat= 0.09879060842909841 \n",
      "acc for optim= 0.14698586637730676\n",
      "Epoch:747/1000\n",
      "Loss on train= 0.012271889485418797\n",
      "Loss on test= 0.016907015815377235\n",
      "acc for Lsat= 0.07590090508751025 \n",
      "acc for Psat= 0.10296507313554315 \n",
      "acc for optim= 0.14678156480379737\n",
      "Epoch:748/1000\n",
      "Loss on train= 0.012382873333990574\n",
      "Loss on test= 0.016830898821353912\n",
      "acc for Lsat= 0.07179619579814203 \n",
      "acc for Psat= 0.0955248352453406 \n",
      "acc for optim= 0.14712593209978078\n",
      "Epoch:749/1000\n",
      "Loss on train= 0.012006021104753017\n",
      "Loss on test= 0.01683961972594261\n",
      "acc for Lsat= 0.06978996528815709 \n",
      "acc for Psat= 0.09953823837056781 \n",
      "acc for optim= 0.14651726195338802\n",
      "Epoch:750/1000\n",
      "Loss on train= 0.011612717993557453\n",
      "Loss on test= 0.01661057025194168\n",
      "acc for Lsat= 0.07628567474152834 \n",
      "acc for Psat= 0.10230819210809969 \n",
      "acc for optim= 0.14896931803930208\n",
      "Epoch:751/1000\n",
      "Loss on train= 0.011671354062855244\n",
      "Loss on test= 0.017390063032507896\n",
      "acc for Lsat= 0.06698511982326645 \n",
      "acc for Psat= 0.10151085259662758 \n",
      "acc for optim= 0.14848430259069945\n",
      "Epoch:752/1000\n",
      "Loss on train= 0.01174273993819952\n",
      "Loss on test= 0.01774270087480545\n",
      "acc for Lsat= 0.06820747553235945 \n",
      "acc for Psat= 0.10223125697035267 \n",
      "acc for optim= 0.14673103865016124\n",
      "Epoch:753/1000\n",
      "Loss on train= 0.012217432260513306\n",
      "Loss on test= 0.01710478402674198\n",
      "acc for Lsat= 0.07157699271156875 \n",
      "acc for Psat= 0.11225851759287879 \n",
      "acc for optim= 0.1473475900233751\n",
      "Epoch:754/1000\n",
      "Loss on train= 0.012034106068313122\n",
      "Loss on test= 0.016470354050397873\n",
      "acc for Lsat= 0.07055670071479886 \n",
      "acc for Psat= 0.09879502006848084 \n",
      "acc for optim= 0.14683148226499124\n",
      "Epoch:755/1000\n",
      "Loss on train= 0.011292776092886925\n",
      "Loss on test= 0.016545290127396584\n",
      "acc for Lsat= 0.07368824380135072 \n",
      "acc for Psat= 0.09669841536469025 \n",
      "acc for optim= 0.1465893705650391\n",
      "Epoch:756/1000\n",
      "Loss on train= 0.011945770122110844\n",
      "Loss on test= 0.016935549676418304\n",
      "acc for Lsat= 0.07094925471409916 \n",
      "acc for Psat= 0.09527374307669982 \n",
      "acc for optim= 0.14682397738978772\n",
      "Epoch:757/1000\n",
      "Loss on train= 0.011612795293331146\n",
      "Loss on test= 0.017297767102718353\n",
      "acc for Lsat= 0.06727893480461271 \n",
      "acc for Psat= 0.09885201131081965 \n",
      "acc for optim= 0.1486278246468423\n",
      "Epoch:758/1000\n",
      "Loss on train= 0.01173952966928482\n",
      "Loss on test= 0.016405154019594193\n",
      "acc for Lsat= 0.06706283320148004 \n",
      "acc for Psat= 0.10806694071280085 \n",
      "acc for optim= 0.14822759983364708\n",
      "Epoch:759/1000\n",
      "Loss on train= 0.012026660144329071\n",
      "Loss on test= 0.01712193712592125\n",
      "acc for Lsat= 0.07394179775377936 \n",
      "acc for Psat= 0.10286732963244691 \n",
      "acc for optim= 0.14769074076616703\n",
      "Epoch:760/1000\n",
      "Loss on train= 0.011444227769970894\n",
      "Loss on test= 0.01690862886607647\n",
      "acc for Lsat= 0.06832683289520214 \n",
      "acc for Psat= 0.0960803874703341 \n",
      "acc for optim= 0.14708125092262447\n",
      "Epoch:761/1000\n",
      "Loss on train= 0.012048746459186077\n",
      "Loss on test= 0.016944747418165207\n",
      "acc for Lsat= 0.07947223212616601 \n",
      "acc for Psat= 0.09954076722824083 \n",
      "acc for optim= 0.14662023541752467\n",
      "Epoch:762/1000\n",
      "Loss on train= 0.011954224668443203\n",
      "Loss on test= 0.016508905217051506\n",
      "acc for Lsat= 0.06621020947564692 \n",
      "acc for Psat= 0.09808040438269885 \n",
      "acc for optim= 0.14691441804437178\n",
      "Epoch:763/1000\n",
      "Loss on train= 0.011942535638809204\n",
      "Loss on test= 0.01700643263757229\n",
      "acc for Lsat= 0.07434834208279473 \n",
      "acc for Psat= 0.107295863231734 \n",
      "acc for optim= 0.15137491808381193\n",
      "Epoch:764/1000\n",
      "Loss on train= 0.0116093335673213\n",
      "Loss on test= 0.016047658398747444\n",
      "acc for Lsat= 0.06659763769408245 \n",
      "acc for Psat= 0.09932050135566421 \n",
      "acc for optim= 0.14847292592998906\n",
      "Epoch:765/1000\n",
      "Loss on train= 0.012360903434455395\n",
      "Loss on test= 0.016401683911681175\n",
      "acc for Lsat= 0.0727912383983728 \n",
      "acc for Psat= 0.1048621985908058 \n",
      "acc for optim= 0.14950266518831679\n",
      "Epoch:766/1000\n",
      "Loss on train= 0.011675548739731312\n",
      "Loss on test= 0.016185486689209938\n",
      "acc for Lsat= 0.06927255863900261 \n",
      "acc for Psat= 0.09985212541863402 \n",
      "acc for optim= 0.14713180355181207\n",
      "Epoch:767/1000\n",
      "Loss on train= 0.011243486776947975\n",
      "Loss on test= 0.016266586259007454\n",
      "acc for Lsat= 0.06883875186520101 \n",
      "acc for Psat= 0.10147195604491537 \n",
      "acc for optim= 0.14676707861461882\n",
      "Epoch:768/1000\n",
      "Loss on train= 0.011893024668097496\n",
      "Loss on test= 0.01643355004489422\n",
      "acc for Lsat= 0.07600539939774596 \n",
      "acc for Psat= 0.09634538952907638 \n",
      "acc for optim= 0.14733184545112638\n",
      "Epoch:769/1000\n",
      "Loss on train= 0.012026362121105194\n",
      "Loss on test= 0.016868172213435173\n",
      "acc for Lsat= 0.07118451460422899 \n",
      "acc for Psat= 0.09986544209857319 \n",
      "acc for optim= 0.14710108234758668\n",
      "Epoch:770/1000\n",
      "Loss on train= 0.011929498054087162\n",
      "Loss on test= 0.016451016068458557\n",
      "acc for Lsat= 0.0757758372105085 \n",
      "acc for Psat= 0.10349663244377096 \n",
      "acc for optim= 0.14785995865124069\n",
      "Epoch:771/1000\n",
      "Loss on train= 0.01179066114127636\n",
      "Loss on test= 0.0159300547093153\n",
      "acc for Lsat= 0.06636990566799596 \n",
      "acc for Psat= 0.10079473209082548 \n",
      "acc for optim= 0.14574449722370222\n",
      "Epoch:772/1000\n",
      "Loss on train= 0.011136844754219055\n",
      "Loss on test= 0.016663050279021263\n",
      "acc for Lsat= 0.07559009941298121 \n",
      "acc for Psat= 0.10150593439240364 \n",
      "acc for optim= 0.14646031163459597\n",
      "Epoch:773/1000\n",
      "Loss on train= 0.011546782217919827\n",
      "Loss on test= 0.01637330837547779\n",
      "acc for Lsat= 0.06706509521374335 \n",
      "acc for Psat= 0.10018143515254173 \n",
      "acc for optim= 0.14731118535099813\n",
      "Epoch:774/1000\n",
      "Loss on train= 0.011381599120795727\n",
      "Loss on test= 0.016741245985031128\n",
      "acc for Lsat= 0.06805080629631957 \n",
      "acc for Psat= 0.09765064461499934 \n",
      "acc for optim= 0.14578068080443177\n",
      "Epoch:775/1000\n",
      "Loss on train= 0.011532088741660118\n",
      "Loss on test= 0.0165858156979084\n",
      "acc for Lsat= 0.06827927669493582 \n",
      "acc for Psat= 0.10275425235999082 \n",
      "acc for optim= 0.14769736073098158\n",
      "Epoch:776/1000\n",
      "Loss on train= 0.012450171634554863\n",
      "Loss on test= 0.016779176890850067\n",
      "acc for Lsat= 0.08171364531747344 \n",
      "acc for Psat= 0.10573222512422606 \n",
      "acc for optim= 0.1480653086587227\n",
      "Epoch:777/1000\n",
      "Loss on train= 0.011193818412721157\n",
      "Loss on test= 0.01687241904437542\n",
      "acc for Lsat= 0.06723229291187414 \n",
      "acc for Psat= 0.09592996568074166 \n",
      "acc for optim= 0.14773525533821166\n",
      "Epoch:778/1000\n",
      "Loss on train= 0.01151591632515192\n",
      "Loss on test= 0.01722949557006359\n",
      "acc for Lsat= 0.07657684942904014 \n",
      "acc for Psat= 0.09588583081907368 \n",
      "acc for optim= 0.14800074224182022\n",
      "Epoch:779/1000\n",
      "Loss on train= 0.011895881965756416\n",
      "Loss on test= 0.017167478799819946\n",
      "acc for Lsat= 0.07129816702418765 \n",
      "acc for Psat= 0.09763822760266354 \n",
      "acc for optim= 0.14668663942749896\n",
      "Epoch:780/1000\n",
      "Loss on train= 0.011801728047430515\n",
      "Loss on test= 0.01633802428841591\n",
      "acc for Lsat= 0.08127738571657477 \n",
      "acc for Psat= 0.10123892310267055 \n",
      "acc for optim= 0.14724694646108552\n",
      "Epoch:781/1000\n",
      "Loss on train= 0.012267475947737694\n",
      "Loss on test= 0.01742858812212944\n",
      "acc for Lsat= 0.0702318806021286 \n",
      "acc for Psat= 0.10860852508510796 \n",
      "acc for optim= 0.14735061519900894\n",
      "Epoch:782/1000\n",
      "Loss on train= 0.01218398381024599\n",
      "Loss on test= 0.0168725848197937\n",
      "acc for Lsat= 0.07400146973367666 \n",
      "acc for Psat= 0.10030249599267417 \n",
      "acc for optim= 0.14835845222413435\n",
      "Epoch:783/1000\n",
      "Loss on train= 0.011941986158490181\n",
      "Loss on test= 0.0175361055880785\n",
      "acc for Lsat= 0.07230022262268715 \n",
      "acc for Psat= 0.11079118816925075 \n",
      "acc for optim= 0.1487121648993176\n",
      "Epoch:784/1000\n",
      "Loss on train= 0.011193772777915001\n",
      "Loss on test= 0.016761958599090576\n",
      "acc for Lsat= 0.0660713473351569 \n",
      "acc for Psat= 0.10452010842683278 \n",
      "acc for optim= 0.1478372414658023\n",
      "Epoch:785/1000\n",
      "Loss on train= 0.011313932947814465\n",
      "Loss on test= 0.01709364354610443\n",
      "acc for Lsat= 0.07847013608394239 \n",
      "acc for Psat= 0.10215105809977744 \n",
      "acc for optim= 0.14628533179303613\n",
      "Epoch:786/1000\n",
      "Loss on train= 0.011162671260535717\n",
      "Loss on test= 0.016238072887063026\n",
      "acc for Lsat= 0.06808640819862619 \n",
      "acc for Psat= 0.1123150815904034 \n",
      "acc for optim= 0.14909428595856816\n",
      "Epoch:787/1000\n",
      "Loss on train= 0.011516019701957703\n",
      "Loss on test= 0.017041051760315895\n",
      "acc for Lsat= 0.07187573005560256 \n",
      "acc for Psat= 0.10335489114836416 \n",
      "acc for optim= 0.1484136340016756\n",
      "Epoch:788/1000\n",
      "Loss on train= 0.011463982053101063\n",
      "Loss on test= 0.016825934872031212\n",
      "acc for Lsat= 0.06850468460370476 \n",
      "acc for Psat= 0.09872340361632687 \n",
      "acc for optim= 0.14903893044255076\n",
      "Epoch:789/1000\n",
      "Loss on train= 0.01193175371736288\n",
      "Loss on test= 0.016729457303881645\n",
      "acc for Lsat= 0.07215604812493263 \n",
      "acc for Psat= 0.10272328506002276 \n",
      "acc for optim= 0.14794055688359867\n",
      "Epoch:790/1000\n",
      "Loss on train= 0.01119829248636961\n",
      "Loss on test= 0.01650666631758213\n",
      "acc for Lsat= 0.06808542988807868 \n",
      "acc for Psat= 0.09725846698331063 \n",
      "acc for optim= 0.14661476952232058\n",
      "Epoch:791/1000\n",
      "Loss on train= 0.011663801036775112\n",
      "Loss on test= 0.016978999599814415\n",
      "acc for Lsat= 0.06949574774620997 \n",
      "acc for Psat= 0.10083402306747778 \n",
      "acc for optim= 0.14868874523208905\n",
      "Epoch:792/1000\n",
      "Loss on train= 0.011608505621552467\n",
      "Loss on test= 0.01671353541314602\n",
      "acc for Lsat= 0.06742643195209434 \n",
      "acc for Psat= 0.09823616799600221 \n",
      "acc for optim= 0.14765322417393995\n",
      "Epoch:793/1000\n",
      "Loss on train= 0.011924759484827518\n",
      "Loss on test= 0.016941437497735023\n",
      "acc for Lsat= 0.07442207164222737 \n",
      "acc for Psat= 0.09723322247563194 \n",
      "acc for optim= 0.14618726158398\n",
      "Epoch:794/1000\n",
      "Loss on train= 0.012590145692229271\n",
      "Loss on test= 0.016917139291763306\n",
      "acc for Lsat= 0.06995730095664589 \n",
      "acc for Psat= 0.10662864892342011 \n",
      "acc for optim= 0.14565280829431332\n",
      "Epoch:795/1000\n",
      "Loss on train= 0.011626632884144783\n",
      "Loss on test= 0.016489621251821518\n",
      "acc for Lsat= 0.06949434588015188 \n",
      "acc for Psat= 0.09914586750893771 \n",
      "acc for optim= 0.14822117248033587\n",
      "Epoch:796/1000\n",
      "Loss on train= 0.011562489904463291\n",
      "Loss on test= 0.016690637916326523\n",
      "acc for Lsat= 0.07411709779703554 \n",
      "acc for Psat= 0.09748958891960717 \n",
      "acc for optim= 0.14636720488237775\n",
      "Epoch:797/1000\n",
      "Loss on train= 0.01141945831477642\n",
      "Loss on test= 0.01686934195458889\n",
      "acc for Lsat= 0.0665122642393402 \n",
      "acc for Psat= 0.09408200519669245 \n",
      "acc for optim= 0.14813288705080271\n",
      "Epoch:798/1000\n",
      "Loss on train= 0.011998004280030727\n",
      "Loss on test= 0.01691160723567009\n",
      "acc for Lsat= 0.07283264573443553 \n",
      "acc for Psat= 0.09832192861969867 \n",
      "acc for optim= 0.14783119552890386\n",
      "Epoch:799/1000\n",
      "Loss on train= 0.011684893630445004\n",
      "Loss on test= 0.016904808580875397\n",
      "acc for Lsat= 0.07164643247993345 \n",
      "acc for Psat= 0.09948037346701712 \n",
      "acc for optim= 0.14776694754368508\n",
      "Epoch:800/1000\n",
      "Loss on train= 0.011325179599225521\n",
      "Loss on test= 0.016669251024723053\n",
      "acc for Lsat= 0.07307509611459026 \n",
      "acc for Psat= 0.09560455433584497 \n",
      "acc for optim= 0.1469752981965572\n",
      "Epoch:801/1000\n",
      "Loss on train= 0.01163535751402378\n",
      "Loss on test= 0.016966236755251884\n",
      "acc for Lsat= 0.06786208523833902 \n",
      "acc for Psat= 0.09599654500727578 \n",
      "acc for optim= 0.14797790177393041\n",
      "Epoch:802/1000\n",
      "Loss on train= 0.011699718423187733\n",
      "Loss on test= 0.016705060377717018\n",
      "acc for Lsat= 0.08600098895898659 \n",
      "acc for Psat= 0.09969738052228268 \n",
      "acc for optim= 0.14853230416241614\n",
      "Epoch:803/1000\n",
      "Loss on train= 0.010974309407174587\n",
      "Loss on test= 0.016261326149106026\n",
      "acc for Lsat= 0.06684197847979655 \n",
      "acc for Psat= 0.09711903694064117 \n",
      "acc for optim= 0.14633665855754038\n",
      "Epoch:804/1000\n",
      "Loss on train= 0.011884779669344425\n",
      "Loss on test= 0.016180891543626785\n",
      "acc for Lsat= 0.07250544348321789 \n",
      "acc for Psat= 0.09560924596777969 \n",
      "acc for optim= 0.14708590285722603\n",
      "Epoch:805/1000\n",
      "Loss on train= 0.011156504973769188\n",
      "Loss on test= 0.016205135732889175\n",
      "acc for Lsat= 0.06885695677110676 \n",
      "acc for Psat= 0.10168889708723707 \n",
      "acc for optim= 0.14843203459528137\n",
      "Epoch:806/1000\n",
      "Loss on train= 0.011264883913099766\n",
      "Loss on test= 0.01657918095588684\n",
      "acc for Lsat= 0.07623066618532101 \n",
      "acc for Psat= 0.10338279535604075 \n",
      "acc for optim= 0.14784667631594564\n",
      "Epoch:807/1000\n",
      "Loss on train= 0.011097018606960773\n",
      "Loss on test= 0.01652851328253746\n",
      "acc for Lsat= 0.06806397128297093 \n",
      "acc for Psat= 0.10045600751005046 \n",
      "acc for optim= 0.14680626266544325\n",
      "Epoch:808/1000\n",
      "Loss on train= 0.011697689071297646\n",
      "Loss on test= 0.0168649572879076\n",
      "acc for Lsat= 0.06827588468416859 \n",
      "acc for Psat= 0.11008612519939809 \n",
      "acc for optim= 0.14741929157475456\n",
      "Epoch:809/1000\n",
      "Loss on train= 0.011273237876594067\n",
      "Loss on test= 0.016458479687571526\n",
      "acc for Lsat= 0.06870983048926951 \n",
      "acc for Psat= 0.10633308594256691 \n",
      "acc for optim= 0.14825396229627946\n",
      "Epoch:810/1000\n",
      "Loss on train= 0.011333173140883446\n",
      "Loss on test= 0.01647624932229519\n",
      "acc for Lsat= 0.07038731065866985 \n",
      "acc for Psat= 0.0970571589491235 \n",
      "acc for optim= 0.14765421602081952\n",
      "Epoch:811/1000\n",
      "Loss on train= 0.011379214003682137\n",
      "Loss on test= 0.01662490889430046\n",
      "acc for Lsat= 0.06661871424301365 \n",
      "acc for Psat= 0.10011862438565292 \n",
      "acc for optim= 0.14721182005776487\n",
      "Epoch:812/1000\n",
      "Loss on train= 0.01091803703457117\n",
      "Loss on test= 0.016774388030171394\n",
      "acc for Lsat= 0.06552174983809375 \n",
      "acc for Psat= 0.09765750781794566 \n",
      "acc for optim= 0.1471213500380303\n",
      "Epoch:813/1000\n",
      "Loss on train= 0.011171698570251465\n",
      "Loss on test= 0.017040226608514786\n",
      "acc for Lsat= 0.07459806800954882 \n",
      "acc for Psat= 0.10078795364589725 \n",
      "acc for optim= 0.14802540527899913\n",
      "Epoch:814/1000\n",
      "Loss on train= 0.01157871913164854\n",
      "Loss on test= 0.016481244936585426\n",
      "acc for Lsat= 0.0667501333361235 \n",
      "acc for Psat= 0.10833692108156003 \n",
      "acc for optim= 0.14745281637248073\n",
      "Epoch:815/1000\n",
      "Loss on train= 0.011484099552035332\n",
      "Loss on test= 0.017093447968363762\n",
      "acc for Lsat= 0.07740530342343455 \n",
      "acc for Psat= 0.10512282997636334 \n",
      "acc for optim= 0.1484371507317095\n",
      "Epoch:816/1000\n",
      "Loss on train= 0.011616088449954987\n",
      "Loss on test= 0.016376720741391182\n",
      "acc for Lsat= 0.06515326891794186 \n",
      "acc for Psat= 0.09364706784326833 \n",
      "acc for optim= 0.14688989539265848\n",
      "Epoch:817/1000\n",
      "Loss on train= 0.011480231769382954\n",
      "Loss on test= 0.01639881171286106\n",
      "acc for Lsat= 0.06761089846465158 \n",
      "acc for Psat= 0.10273225515387781 \n",
      "acc for optim= 0.14744095597582768\n",
      "Epoch:818/1000\n",
      "Loss on train= 0.011374376714229584\n",
      "Loss on test= 0.0163816437125206\n",
      "acc for Lsat= 0.07401999939426324 \n",
      "acc for Psat= 0.10086391094758813 \n",
      "acc for optim= 0.14802249410924417\n",
      "Epoch:819/1000\n",
      "Loss on train= 0.011246669106185436\n",
      "Loss on test= 0.016456836834549904\n",
      "acc for Lsat= 0.06763718793025383 \n",
      "acc for Psat= 0.09612745537101047 \n",
      "acc for optim= 0.14779207794952057\n",
      "Epoch:820/1000\n",
      "Loss on train= 0.011479958891868591\n",
      "Loss on test= 0.01664164662361145\n",
      "acc for Lsat= 0.06879133538931129 \n",
      "acc for Psat= 0.09901107965513717 \n",
      "acc for optim= 0.14759485421965501\n",
      "Epoch:821/1000\n",
      "Loss on train= 0.011210063472390175\n",
      "Loss on test= 0.016343068331480026\n",
      "acc for Lsat= 0.06983986346047767 \n",
      "acc for Psat= 0.09941987407015561 \n",
      "acc for optim= 0.14689566756403724\n",
      "Epoch:822/1000\n",
      "Loss on train= 0.011580387130379677\n",
      "Loss on test= 0.016535762697458267\n",
      "acc for Lsat= 0.06529934219149656 \n",
      "acc for Psat= 0.09927422053485523 \n",
      "acc for optim= 0.14765481318685364\n",
      "Epoch:823/1000\n",
      "Loss on train= 0.010864253155887127\n",
      "Loss on test= 0.01720317266881466\n",
      "acc for Lsat= 0.07171201553988757 \n",
      "acc for Psat= 0.10288921525312024 \n",
      "acc for optim= 0.148187865458149\n",
      "Epoch:824/1000\n",
      "Loss on train= 0.011687894351780415\n",
      "Loss on test= 0.01694810390472412\n",
      "acc for Lsat= 0.07121661323021052 \n",
      "acc for Psat= 0.10505587449014081 \n",
      "acc for optim= 0.14791397298811157\n",
      "Epoch:825/1000\n",
      "Loss on train= 0.01123221218585968\n",
      "Loss on test= 0.01689329743385315\n",
      "acc for Lsat= 0.07349228947981526 \n",
      "acc for Psat= 0.0983360159908936 \n",
      "acc for optim= 0.14791325041774347\n",
      "Epoch:826/1000\n",
      "Loss on train= 0.011176764033734798\n",
      "Loss on test= 0.016397740691900253\n",
      "acc for Lsat= 0.07022077528649877 \n",
      "acc for Psat= 0.09778279416889539 \n",
      "acc for optim= 0.1468714617552612\n",
      "Epoch:827/1000\n",
      "Loss on train= 0.011217738501727581\n",
      "Loss on test= 0.017282407730817795\n",
      "acc for Lsat= 0.07257346213930195 \n",
      "acc for Psat= 0.099471475986334 \n",
      "acc for optim= 0.1465207446453183\n",
      "Epoch:828/1000\n",
      "Loss on train= 0.011652635410428047\n",
      "Loss on test= 0.016887957230210304\n",
      "acc for Lsat= 0.06720026517170273 \n",
      "acc for Psat= 0.09500734546956525 \n",
      "acc for optim= 0.14763227760898406\n",
      "Epoch:829/1000\n",
      "Loss on train= 0.011385192163288593\n",
      "Loss on test= 0.01676996424794197\n",
      "acc for Lsat= 0.07614192217854995 \n",
      "acc for Psat= 0.10274379639378177 \n",
      "acc for optim= 0.1479944757037598\n",
      "Epoch:830/1000\n",
      "Loss on train= 0.01085678581148386\n",
      "Loss on test= 0.015882650390267372\n",
      "acc for Lsat= 0.071544804310756 \n",
      "acc for Psat= 0.1016278276716447 \n",
      "acc for optim= 0.14742273369500808\n",
      "Epoch:831/1000\n",
      "Loss on train= 0.011103073135018349\n",
      "Loss on test= 0.017248637974262238\n",
      "acc for Lsat= 0.06623909119949785 \n",
      "acc for Psat= 0.10246507707776666 \n",
      "acc for optim= 0.14785059617968918\n",
      "Epoch:832/1000\n",
      "Loss on train= 0.01101718284189701\n",
      "Loss on test= 0.01714584045112133\n",
      "acc for Lsat= 0.06840994287155609 \n",
      "acc for Psat= 0.09579406931916375 \n",
      "acc for optim= 0.14693006989354523\n",
      "Epoch:833/1000\n",
      "Loss on train= 0.011330782435834408\n",
      "Loss on test= 0.016474315896630287\n",
      "acc for Lsat= 0.06966877780148298 \n",
      "acc for Psat= 0.10045106020724413 \n",
      "acc for optim= 0.14941550551885358\n",
      "Epoch:834/1000\n",
      "Loss on train= 0.01173561904579401\n",
      "Loss on test= 0.016444921493530273\n",
      "acc for Lsat= 0.06740449481978611 \n",
      "acc for Psat= 0.0962921757177718 \n",
      "acc for optim= 0.14674876666026376\n",
      "Epoch:835/1000\n",
      "Loss on train= 0.01084011048078537\n",
      "Loss on test= 0.017002103850245476\n",
      "acc for Lsat= 0.06704461608142034 \n",
      "acc for Psat= 0.10023833898184337 \n",
      "acc for optim= 0.1466919773273263\n",
      "Epoch:836/1000\n",
      "Loss on train= 0.011325506493449211\n",
      "Loss on test= 0.016877567395567894\n",
      "acc for Lsat= 0.07024902422547553 \n",
      "acc for Psat= 0.0957473062018802 \n",
      "acc for optim= 0.14619308747417814\n",
      "Epoch:837/1000\n",
      "Loss on train= 0.011175212450325489\n",
      "Loss on test= 0.016947248950600624\n",
      "acc for Lsat= 0.07221478901416968 \n",
      "acc for Psat= 0.09688978936249967 \n",
      "acc for optim= 0.1473164337905447\n",
      "Epoch:838/1000\n",
      "Loss on train= 0.011260109953582287\n",
      "Loss on test= 0.01671627163887024\n",
      "acc for Lsat= 0.07312343656910125 \n",
      "acc for Psat= 0.09819245333108578 \n",
      "acc for optim= 0.14723664499779285\n",
      "Epoch:839/1000\n",
      "Loss on train= 0.011319334618747234\n",
      "Loss on test= 0.016778483986854553\n",
      "acc for Lsat= 0.07109338754724731 \n",
      "acc for Psat= 0.09900647863505779 \n",
      "acc for optim= 0.1486891422907227\n",
      "Epoch:840/1000\n",
      "Loss on train= 0.011523169465363026\n",
      "Loss on test= 0.0166388638317585\n",
      "acc for Lsat= 0.07246993486377974 \n",
      "acc for Psat= 0.10405503527422924 \n",
      "acc for optim= 0.14760087384094273\n",
      "Epoch:841/1000\n",
      "Loss on train= 0.011476775631308556\n",
      "Loss on test= 0.01640179194509983\n",
      "acc for Lsat= 0.06651279965219004 \n",
      "acc for Psat= 0.10370086036982393 \n",
      "acc for optim= 0.148562713183316\n",
      "Epoch:842/1000\n",
      "Loss on train= 0.011384710669517517\n",
      "Loss on test= 0.016942787915468216\n",
      "acc for Lsat= 0.06991685684870308 \n",
      "acc for Psat= 0.09556088308955349 \n",
      "acc for optim= 0.14764302019996164\n",
      "Epoch:843/1000\n",
      "Loss on train= 0.01094637718051672\n",
      "Loss on test= 0.01666567288339138\n",
      "acc for Lsat= 0.0682598786831755 \n",
      "acc for Psat= 0.10510040010877074 \n",
      "acc for optim= 0.14731707676365466\n",
      "Epoch:844/1000\n",
      "Loss on train= 0.010744248516857624\n",
      "Loss on test= 0.016412390395998955\n",
      "acc for Lsat= 0.06904821181020072 \n",
      "acc for Psat= 0.0984074676186113 \n",
      "acc for optim= 0.1474622458592723\n",
      "Epoch:845/1000\n",
      "Loss on train= 0.010980835184454918\n",
      "Loss on test= 0.01688433438539505\n",
      "acc for Lsat= 0.07338127811607606 \n",
      "acc for Psat= 0.10151599182852267 \n",
      "acc for optim= 0.14834294442840462\n",
      "Epoch:846/1000\n",
      "Loss on train= 0.011224566958844662\n",
      "Loss on test= 0.01663072779774666\n",
      "acc for Lsat= 0.06848301162446759 \n",
      "acc for Psat= 0.09526154180332246 \n",
      "acc for optim= 0.14802311923294884\n",
      "Epoch:847/1000\n",
      "Loss on train= 0.010630014352500439\n",
      "Loss on test= 0.017354007810354233\n",
      "acc for Lsat= 0.07587291155816832 \n",
      "acc for Psat= 0.09620450284058803 \n",
      "acc for optim= 0.1474174280925812\n",
      "Epoch:848/1000\n",
      "Loss on train= 0.011763498187065125\n",
      "Loss on test= 0.01730785332620144\n",
      "acc for Lsat= 0.06706187359868736 \n",
      "acc for Psat= 0.0960457897356883 \n",
      "acc for optim= 0.14727836379211579\n",
      "Epoch:849/1000\n",
      "Loss on train= 0.011129389517009258\n",
      "Loss on test= 0.01657438836991787\n",
      "acc for Lsat= 0.06981603975052911 \n",
      "acc for Psat= 0.09700003925291072 \n",
      "acc for optim= 0.1478591661547079\n",
      "Epoch:850/1000\n",
      "Loss on train= 0.010868937708437443\n",
      "Loss on test= 0.016874980181455612\n",
      "acc for Lsat= 0.06966316129525786 \n",
      "acc for Psat= 0.10133858934071495 \n",
      "acc for optim= 0.1463238761015569\n",
      "Epoch:851/1000\n",
      "Loss on train= 0.011277471669018269\n",
      "Loss on test= 0.016943836584687233\n",
      "acc for Lsat= 0.06834968139212544 \n",
      "acc for Psat= 0.1014515961645328 \n",
      "acc for optim= 0.14766090181731154\n",
      "Epoch:852/1000\n",
      "Loss on train= 0.011258506216108799\n",
      "Loss on test= 0.01683138497173786\n",
      "acc for Lsat= 0.06471224459744522 \n",
      "acc for Psat= 0.09407077431678772 \n",
      "acc for optim= 0.14699975439815485\n",
      "Epoch:853/1000\n",
      "Loss on train= 0.011106431484222412\n",
      "Loss on test= 0.016623614355921745\n",
      "acc for Lsat= 0.0680100809376227 \n",
      "acc for Psat= 0.09476854275505532 \n",
      "acc for optim= 0.14870328202657068\n",
      "Epoch:854/1000\n",
      "Loss on train= 0.010641887784004211\n",
      "Loss on test= 0.017248915508389473\n",
      "acc for Lsat= 0.06754025372814833 \n",
      "acc for Psat= 0.09578363037066724 \n",
      "acc for optim= 0.14959307162620938\n",
      "Epoch:855/1000\n",
      "Loss on train= 0.011573383584618568\n",
      "Loss on test= 0.016957160085439682\n",
      "acc for Lsat= 0.0720320253124817 \n",
      "acc for Psat= 0.09952494986155376 \n",
      "acc for optim= 0.14733400187040271\n",
      "Epoch:856/1000\n",
      "Loss on train= 0.011478195898234844\n",
      "Loss on test= 0.016514115035533905\n",
      "acc for Lsat= 0.07062406746240761 \n",
      "acc for Psat= 0.10059715823965122 \n",
      "acc for optim= 0.14634428943419073\n",
      "Epoch:857/1000\n",
      "Loss on train= 0.011325770057737827\n",
      "Loss on test= 0.016418134793639183\n",
      "acc for Lsat= 0.07306453884714191 \n",
      "acc for Psat= 0.10402740752974221 \n",
      "acc for optim= 0.14846296169676806\n",
      "Epoch:858/1000\n",
      "Loss on train= 0.011326909065246582\n",
      "Loss on test= 0.016726434230804443\n",
      "acc for Lsat= 0.06615326164127036 \n",
      "acc for Psat= 0.09718546666911336 \n",
      "acc for optim= 0.14829513650247578\n",
      "Epoch:859/1000\n",
      "Loss on train= 0.011723301373422146\n",
      "Loss on test= 0.016862142831087112\n",
      "acc for Lsat= 0.077484500002989 \n",
      "acc for Psat= 0.11626130949192696 \n",
      "acc for optim= 0.14615579626001482\n",
      "Epoch:860/1000\n",
      "Loss on train= 0.01132415235042572\n",
      "Loss on test= 0.016540052369236946\n",
      "acc for Lsat= 0.0719425358104791 \n",
      "acc for Psat= 0.1026615910858502 \n",
      "acc for optim= 0.14776092098096189\n",
      "Epoch:861/1000\n",
      "Loss on train= 0.011313299648463726\n",
      "Loss on test= 0.016852207481861115\n",
      "acc for Lsat= 0.07086868497254808 \n",
      "acc for Psat= 0.10895574673983618 \n",
      "acc for optim= 0.1480696971599872\n",
      "Epoch:862/1000\n",
      "Loss on train= 0.0107098538428545\n",
      "Loss on test= 0.016982214525341988\n",
      "acc for Lsat= 0.06718902089941267 \n",
      "acc for Psat= 0.1004985252625187 \n",
      "acc for optim= 0.14784077169217025\n",
      "Epoch:863/1000\n",
      "Loss on train= 0.011272026225924492\n",
      "Loss on test= 0.01736956648528576\n",
      "acc for Lsat= 0.07099096865154976 \n",
      "acc for Psat= 0.10068712382708979 \n",
      "acc for optim= 0.14699577696634747\n",
      "Epoch:864/1000\n",
      "Loss on train= 0.011430546641349792\n",
      "Loss on test= 0.01606050878763199\n",
      "acc for Lsat= 0.0696347515890125 \n",
      "acc for Psat= 0.10843954316618616 \n",
      "acc for optim= 0.14811991201530417\n",
      "Epoch:865/1000\n",
      "Loss on train= 0.011132567189633846\n",
      "Loss on test= 0.016524285078048706\n",
      "acc for Lsat= 0.0711650741388205 \n",
      "acc for Psat= 0.0986354480489209 \n",
      "acc for optim= 0.1482702344815931\n",
      "Epoch:866/1000\n",
      "Loss on train= 0.01130389142781496\n",
      "Loss on test= 0.017197616398334503\n",
      "acc for Lsat= 0.06653645219337963 \n",
      "acc for Psat= 0.09892497409221737 \n",
      "acc for optim= 0.14688869002466765\n",
      "Epoch:867/1000\n",
      "Loss on train= 0.011126372031867504\n",
      "Loss on test= 0.017243454232811928\n",
      "acc for Lsat= 0.07645802284076089 \n",
      "acc for Psat= 0.10328798297266202 \n",
      "acc for optim= 0.15151629736896918\n",
      "Epoch:868/1000\n",
      "Loss on train= 0.010748175904154778\n",
      "Loss on test= 0.016715528443455696\n",
      "acc for Lsat= 0.06910300620553317 \n",
      "acc for Psat= 0.09450673823493108 \n",
      "acc for optim= 0.14969144373972212\n",
      "Epoch:869/1000\n",
      "Loss on train= 0.011116518639028072\n",
      "Loss on test= 0.017034953460097313\n",
      "acc for Lsat= 0.06953695285832094 \n",
      "acc for Psat= 0.10434419488224447 \n",
      "acc for optim= 0.14886880665217825\n",
      "Epoch:870/1000\n",
      "Loss on train= 0.01099941972643137\n",
      "Loss on test= 0.017015628516674042\n",
      "acc for Lsat= 0.07399275638762869 \n",
      "acc for Psat= 0.0980857002180676 \n",
      "acc for optim= 0.1485314865338354\n",
      "Epoch:871/1000\n",
      "Loss on train= 0.0117123331874609\n",
      "Loss on test= 0.016299081966280937\n",
      "acc for Lsat= 0.06666807249215077 \n",
      "acc for Psat= 0.09825898725572768 \n",
      "acc for optim= 0.14750061013830792\n",
      "Epoch:872/1000\n",
      "Loss on train= 0.010815157555043697\n",
      "Loss on test= 0.016688978299498558\n",
      "acc for Lsat= 0.07502923019243692 \n",
      "acc for Psat= 0.10100823946843636 \n",
      "acc for optim= 0.14887326541015197\n",
      "Epoch:873/1000\n",
      "Loss on train= 0.010952658019959927\n",
      "Loss on test= 0.0175249632447958\n",
      "acc for Lsat= 0.07158760591035239 \n",
      "acc for Psat= 0.09779293156480533 \n",
      "acc for optim= 0.1486251170285486\n",
      "Epoch:874/1000\n",
      "Loss on train= 0.01068153977394104\n",
      "Loss on test= 0.016761943697929382\n",
      "acc for Lsat= 0.0669791235853393 \n",
      "acc for Psat= 0.09731120572746975 \n",
      "acc for optim= 0.1477438199392158\n",
      "Epoch:875/1000\n",
      "Loss on train= 0.010980898514389992\n",
      "Loss on test= 0.01708390563726425\n",
      "acc for Lsat= 0.07362683628667448 \n",
      "acc for Psat= 0.10391534259792731 \n",
      "acc for optim= 0.1470604753238357\n",
      "Epoch:876/1000\n",
      "Loss on train= 0.011048182845115662\n",
      "Loss on test= 0.017012761905789375\n",
      "acc for Lsat= 0.06522952626877479 \n",
      "acc for Psat= 0.09609588155379661 \n",
      "acc for optim= 0.14740769876990206\n",
      "Epoch:877/1000\n",
      "Loss on train= 0.010915512219071388\n",
      "Loss on test= 0.01704913191497326\n",
      "acc for Lsat= 0.06844230435508733 \n",
      "acc for Psat= 0.10093855712836032 \n",
      "acc for optim= 0.147969239171801\n",
      "Epoch:878/1000\n",
      "Loss on train= 0.010860839858651161\n",
      "Loss on test= 0.016781510785222054\n",
      "acc for Lsat= 0.06871778908483885 \n",
      "acc for Psat= 0.09696926990647226 \n",
      "acc for optim= 0.1473719365161732\n",
      "Epoch:879/1000\n",
      "Loss on train= 0.010902046225965023\n",
      "Loss on test= 0.01699027605354786\n",
      "acc for Lsat= 0.06752606631924726 \n",
      "acc for Psat= 0.09987014960197729 \n",
      "acc for optim= 0.14747442291759638\n",
      "Epoch:880/1000\n",
      "Loss on train= 0.011204630136489868\n",
      "Loss on test= 0.01684829778969288\n",
      "acc for Lsat= 0.0715318683648152 \n",
      "acc for Psat= 0.09603509191750201 \n",
      "acc for optim= 0.14649450244758552\n",
      "Epoch:881/1000\n",
      "Loss on train= 0.011232764460146427\n",
      "Loss on test= 0.01696847379207611\n",
      "acc for Lsat= 0.06755267834300857 \n",
      "acc for Psat= 0.09572337945799919 \n",
      "acc for optim= 0.1475134430823045\n",
      "Epoch:882/1000\n",
      "Loss on train= 0.011258297599852085\n",
      "Loss on test= 0.017170382663607597\n",
      "acc for Lsat= 0.06984986088143691 \n",
      "acc for Psat= 0.10678418193392337 \n",
      "acc for optim= 0.14878961436437152\n",
      "Epoch:883/1000\n",
      "Loss on train= 0.011505192145705223\n",
      "Loss on test= 0.016570953652262688\n",
      "acc for Lsat= 0.0733434937134625 \n",
      "acc for Psat= 0.10774652792643988 \n",
      "acc for optim= 0.14855547053656126\n",
      "Epoch:884/1000\n",
      "Loss on train= 0.010843784548342228\n",
      "Loss on test= 0.016455844044685364\n",
      "acc for Lsat= 0.06947096732839915 \n",
      "acc for Psat= 0.09832567035192241 \n",
      "acc for optim= 0.14689034235285517\n",
      "Epoch:885/1000\n",
      "Loss on train= 0.011165318079292774\n",
      "Loss on test= 0.016701487824320793\n",
      "acc for Lsat= 0.06907999977762669 \n",
      "acc for Psat= 0.09876866493114206 \n",
      "acc for optim= 0.1493490216343902\n",
      "Epoch:886/1000\n",
      "Loss on train= 0.01123438123613596\n",
      "Loss on test= 0.01660834811627865\n",
      "acc for Lsat= 0.06659665438271593 \n",
      "acc for Psat= 0.09909710413867971 \n",
      "acc for optim= 0.14779064546544143\n",
      "Epoch:887/1000\n",
      "Loss on train= 0.011072552762925625\n",
      "Loss on test= 0.016772788017988205\n",
      "acc for Lsat= 0.0731283257227251 \n",
      "acc for Psat= 0.10488023496697754 \n",
      "acc for optim= 0.14685727338671464\n",
      "Epoch:888/1000\n",
      "Loss on train= 0.010793725959956646\n",
      "Loss on test= 0.01701267994940281\n",
      "acc for Lsat= 0.06873997990475143 \n",
      "acc for Psat= 0.10684873205817878 \n",
      "acc for optim= 0.1481041823602106\n",
      "Epoch:889/1000\n",
      "Loss on train= 0.011732572689652443\n",
      "Loss on test= 0.016861597076058388\n",
      "acc for Lsat= 0.07076692168529217 \n",
      "acc for Psat= 0.09912919409586411 \n",
      "acc for optim= 0.14752274938260934\n",
      "Epoch:890/1000\n",
      "Loss on train= 0.010409513488411903\n",
      "Loss on test= 0.016343945637345314\n",
      "acc for Lsat= 0.06660535949713854 \n",
      "acc for Psat= 0.09473531115033756 \n",
      "acc for optim= 0.14720987162564442\n",
      "Epoch:891/1000\n",
      "Loss on train= 0.011083760298788548\n",
      "Loss on test= 0.016042102128267288\n",
      "acc for Lsat= 0.06941461380777718 \n",
      "acc for Psat= 0.09774345673474089 \n",
      "acc for optim= 0.14816921844891864\n",
      "Epoch:892/1000\n",
      "Loss on train= 0.01128371898084879\n",
      "Loss on test= 0.01650228351354599\n",
      "acc for Lsat= 0.06579748280146466 \n",
      "acc for Psat= 0.10104326997118901 \n",
      "acc for optim= 0.14762437574978593\n",
      "Epoch:893/1000\n",
      "Loss on train= 0.010411703959107399\n",
      "Loss on test= 0.01679421216249466\n",
      "acc for Lsat= 0.06830587734156732 \n",
      "acc for Psat= 0.09812503610079533 \n",
      "acc for optim= 0.14623326171915946\n",
      "Epoch:894/1000\n",
      "Loss on train= 0.010805459693074226\n",
      "Loss on test= 0.016462378203868866\n",
      "acc for Lsat= 0.06916651494921001 \n",
      "acc for Psat= 0.10059888801122607 \n",
      "acc for optim= 0.1460722339494497\n",
      "Epoch:895/1000\n",
      "Loss on train= 0.011600206606090069\n",
      "Loss on test= 0.016484912484884262\n",
      "acc for Lsat= 0.06804565100635741 \n",
      "acc for Psat= 0.09682002397044188 \n",
      "acc for optim= 0.14631624946867205\n",
      "Epoch:896/1000\n",
      "Loss on train= 0.011218824423849583\n",
      "Loss on test= 0.016452837735414505\n",
      "acc for Lsat= 0.0712148026618207 \n",
      "acc for Psat= 0.09878830169735742 \n",
      "acc for optim= 0.1480528974788987\n",
      "Epoch:897/1000\n",
      "Loss on train= 0.011225035414099693\n",
      "Loss on test= 0.01657917909324169\n",
      "acc for Lsat= 0.07060675396049172 \n",
      "acc for Psat= 0.09262216832215542 \n",
      "acc for optim= 0.1483108720647201\n",
      "Epoch:898/1000\n",
      "Loss on train= 0.011132561601698399\n",
      "Loss on test= 0.016367601230740547\n",
      "acc for Lsat= 0.06544962393149921 \n",
      "acc for Psat= 0.09303986847080786 \n",
      "acc for optim= 0.1467095774380167\n",
      "Epoch:899/1000\n",
      "Loss on train= 0.01123027317225933\n",
      "Loss on test= 0.016623415052890778\n",
      "acc for Lsat= 0.07000669958553075 \n",
      "acc for Psat= 0.10187268073742205 \n",
      "acc for optim= 0.14841776481681304\n",
      "Epoch:900/1000\n",
      "Loss on train= 0.011056585237383842\n",
      "Loss on test= 0.015932612121105194\n",
      "acc for Lsat= 0.06625599076901437 \n",
      "acc for Psat= 0.09414987608870366 \n",
      "acc for optim= 0.14704994929284654\n",
      "Epoch:901/1000\n",
      "Loss on train= 0.011402060277760029\n",
      "Loss on test= 0.016703231260180473\n",
      "acc for Lsat= 0.06805604349735171 \n",
      "acc for Psat= 0.10161435847205642 \n",
      "acc for optim= 0.14754690093942954\n",
      "Epoch:902/1000\n",
      "Loss on train= 0.010928607545793056\n",
      "Loss on test= 0.01627187244594097\n",
      "acc for Lsat= 0.06641134076446882 \n",
      "acc for Psat= 0.09844040640351596 \n",
      "acc for optim= 0.14948502972002317\n",
      "Epoch:903/1000\n",
      "Loss on train= 0.011129617691040039\n",
      "Loss on test= 0.016707560047507286\n",
      "acc for Lsat= 0.06837885046581037 \n",
      "acc for Psat= 0.09782403774679453 \n",
      "acc for optim= 0.1483997983454805\n",
      "Epoch:904/1000\n",
      "Loss on train= 0.010555691085755825\n",
      "Loss on test= 0.017153114080429077\n",
      "acc for Lsat= 0.0677569335283235 \n",
      "acc for Psat= 0.10448313349048227 \n",
      "acc for optim= 0.1493650788591245\n",
      "Epoch:905/1000\n",
      "Loss on train= 0.011488971300423145\n",
      "Loss on test= 0.01692737266421318\n",
      "acc for Lsat= 0.07412604485939996 \n",
      "acc for Psat= 0.10655035613261309 \n",
      "acc for optim= 0.14827428898359243\n",
      "Epoch:906/1000\n",
      "Loss on train= 0.010739488527178764\n",
      "Loss on test= 0.016293339431285858\n",
      "acc for Lsat= 0.0675301283171437 \n",
      "acc for Psat= 0.09860098438740629 \n",
      "acc for optim= 0.14668943920374344\n",
      "Epoch:907/1000\n",
      "Loss on train= 0.011422686278820038\n",
      "Loss on test= 0.016957959160208702\n",
      "acc for Lsat= 0.07176782872147978 \n",
      "acc for Psat= 0.10903415285410736 \n",
      "acc for optim= 0.14893250562423888\n",
      "Epoch:908/1000\n",
      "Loss on train= 0.010937613435089588\n",
      "Loss on test= 0.01690705306828022\n",
      "acc for Lsat= 0.0666112376133743 \n",
      "acc for Psat= 0.09494165140529015 \n",
      "acc for optim= 0.14875513611624622\n",
      "Epoch:909/1000\n",
      "Loss on train= 0.011209332384169102\n",
      "Loss on test= 0.016875337809324265\n",
      "acc for Lsat= 0.07234276480879469 \n",
      "acc for Psat= 0.10036070431067819 \n",
      "acc for optim= 0.1487826852764341\n",
      "Epoch:910/1000\n",
      "Loss on train= 0.01059145200997591\n",
      "Loss on test= 0.016509275883436203\n",
      "acc for Lsat= 0.07084996641108732 \n",
      "acc for Psat= 0.10553913351035074 \n",
      "acc for optim= 0.1477704660197277\n",
      "Epoch:911/1000\n",
      "Loss on train= 0.010574628598988056\n",
      "Loss on test= 0.016699815168976784\n",
      "acc for Lsat= 0.06721150861230006 \n",
      "acc for Psat= 0.09806488936830292 \n",
      "acc for optim= 0.1486541799761102\n",
      "Epoch:912/1000\n",
      "Loss on train= 0.010975259356200695\n",
      "Loss on test= 0.016335168853402138\n",
      "acc for Lsat= 0.0672816088726354 \n",
      "acc for Psat= 0.10091800210940816 \n",
      "acc for optim= 0.14863781855443298\n",
      "Epoch:913/1000\n",
      "Loss on train= 0.010533045046031475\n",
      "Loss on test= 0.016844041645526886\n",
      "acc for Lsat= 0.06767236367001303 \n",
      "acc for Psat= 0.09861579931912569 \n",
      "acc for optim= 0.14888234937127034\n",
      "Epoch:914/1000\n",
      "Loss on train= 0.010618067346513271\n",
      "Loss on test= 0.016781900078058243\n",
      "acc for Lsat= 0.07159648678383801 \n",
      "acc for Psat= 0.10076521660646093 \n",
      "acc for optim= 0.14827420279676887\n",
      "Epoch:915/1000\n",
      "Loss on train= 0.010887845419347286\n",
      "Loss on test= 0.016765834763646126\n",
      "acc for Lsat= 0.06513241306613723 \n",
      "acc for Psat= 0.09447776324420584 \n",
      "acc for optim= 0.14691976650029898\n",
      "Epoch:916/1000\n",
      "Loss on train= 0.011018279008567333\n",
      "Loss on test= 0.016142643988132477\n",
      "acc for Lsat= 0.07515519656414209 \n",
      "acc for Psat= 0.10518251985897958 \n",
      "acc for optim= 0.14886525156033062\n",
      "Epoch:917/1000\n",
      "Loss on train= 0.010706654749810696\n",
      "Loss on test= 0.016194351017475128\n",
      "acc for Lsat= 0.06639262849401699 \n",
      "acc for Psat= 0.09397927018312308 \n",
      "acc for optim= 0.14808792990094222\n",
      "Epoch:918/1000\n",
      "Loss on train= 0.010929818265140057\n",
      "Loss on test= 0.016488971188664436\n",
      "acc for Lsat= 0.07147321976148163 \n",
      "acc for Psat= 0.09704683844220019 \n",
      "acc for optim= 0.14765314438475263\n",
      "Epoch:919/1000\n",
      "Loss on train= 0.010885395109653473\n",
      "Loss on test= 0.016643265262246132\n",
      "acc for Lsat= 0.06694914613832088 \n",
      "acc for Psat= 0.09897863177152783 \n",
      "acc for optim= 0.14731224695343875\n",
      "Epoch:920/1000\n",
      "Loss on train= 0.01026354730129242\n",
      "Loss on test= 0.01640992797911167\n",
      "acc for Lsat= 0.06951130213272592 \n",
      "acc for Psat= 0.09894650174381484 \n",
      "acc for optim= 0.14817820100750184\n",
      "Epoch:921/1000\n",
      "Loss on train= 0.010599588975310326\n",
      "Loss on test= 0.016155702993273735\n",
      "acc for Lsat= 0.06720136510664747 \n",
      "acc for Psat= 0.10518080127260555 \n",
      "acc for optim= 0.14804433116119534\n",
      "Epoch:922/1000\n",
      "Loss on train= 0.01096438616514206\n",
      "Loss on test= 0.017254387959837914\n",
      "acc for Lsat= 0.07305003022358539 \n",
      "acc for Psat= 0.10110258396068496 \n",
      "acc for optim= 0.14962278689809264\n",
      "Epoch:923/1000\n",
      "Loss on train= 0.01047008391469717\n",
      "Loss on test= 0.01678386703133583\n",
      "acc for Lsat= 0.07002162471120388 \n",
      "acc for Psat= 0.10374251726701565 \n",
      "acc for optim= 0.14883489037242473\n",
      "Epoch:924/1000\n",
      "Loss on train= 0.0107439449056983\n",
      "Loss on test= 0.01678067445755005\n",
      "acc for Lsat= 0.06471502279766131 \n",
      "acc for Psat= 0.09692711234625849 \n",
      "acc for optim= 0.14952628393932407\n",
      "Epoch:925/1000\n",
      "Loss on train= 0.010552339255809784\n",
      "Loss on test= 0.016367437317967415\n",
      "acc for Lsat= 0.06964011544191773 \n",
      "acc for Psat= 0.09650613510971205 \n",
      "acc for optim= 0.1480562231834971\n",
      "Epoch:926/1000\n",
      "Loss on train= 0.010918465442955494\n",
      "Loss on test= 0.016682742163538933\n",
      "acc for Lsat= 0.07092546997641835 \n",
      "acc for Psat= 0.09733886729412726 \n",
      "acc for optim= 0.14848930909296695\n",
      "Epoch:927/1000\n",
      "Loss on train= 0.010630668140947819\n",
      "Loss on test= 0.01690797321498394\n",
      "acc for Lsat= 0.06788225689493055 \n",
      "acc for Psat= 0.09589364825612959 \n",
      "acc for optim= 0.14773751184850775\n",
      "Epoch:928/1000\n",
      "Loss on train= 0.010695059783756733\n",
      "Loss on test= 0.0165102481842041\n",
      "acc for Lsat= 0.06718035475725778 \n",
      "acc for Psat= 0.09524845649389972 \n",
      "acc for optim= 0.14772138985836866\n",
      "Epoch:929/1000\n",
      "Loss on train= 0.01081323716789484\n",
      "Loss on test= 0.016278201714158058\n",
      "acc for Lsat= 0.06501047677878619 \n",
      "acc for Psat= 0.09387136235006807 \n",
      "acc for optim= 0.1466118067876171\n",
      "Epoch:930/1000\n",
      "Loss on train= 0.010470731183886528\n",
      "Loss on test= 0.016728222370147705\n",
      "acc for Lsat= 0.07451339625821769 \n",
      "acc for Psat= 0.09673617692241088 \n",
      "acc for optim= 0.1467088796904773\n",
      "Epoch:931/1000\n",
      "Loss on train= 0.010949870571494102\n",
      "Loss on test= 0.01636413298547268\n",
      "acc for Lsat= 0.06741969261804932 \n",
      "acc for Psat= 0.0993280579877454 \n",
      "acc for optim= 0.14952912605725802\n",
      "Epoch:932/1000\n",
      "Loss on train= 0.01101860124617815\n",
      "Loss on test= 0.016475258395075798\n",
      "acc for Lsat= 0.06916936432846116 \n",
      "acc for Psat= 0.09890750670049188 \n",
      "acc for optim= 0.1475738103359885\n",
      "Epoch:933/1000\n",
      "Loss on train= 0.010364619083702564\n",
      "Loss on test= 0.01635906659066677\n",
      "acc for Lsat= 0.06706657033806836 \n",
      "acc for Psat= 0.09549992647495167 \n",
      "acc for optim= 0.14806136220213767\n",
      "Epoch:934/1000\n",
      "Loss on train= 0.01041733380407095\n",
      "Loss on test= 0.016782281920313835\n",
      "acc for Lsat= 0.06759748431139215 \n",
      "acc for Psat= 0.11059589431836053 \n",
      "acc for optim= 0.14692711192508076\n",
      "Epoch:935/1000\n",
      "Loss on train= 0.011139145120978355\n",
      "Loss on test= 0.01637526974081993\n",
      "acc for Lsat= 0.06555361844239381 \n",
      "acc for Psat= 0.09977937645157249 \n",
      "acc for optim= 0.14801055447786568\n",
      "Epoch:936/1000\n",
      "Loss on train= 0.011740601621568203\n",
      "Loss on test= 0.016583435237407684\n",
      "acc for Lsat= 0.07350492579246036 \n",
      "acc for Psat= 0.0945703736038242 \n",
      "acc for optim= 0.14849795639408295\n",
      "Epoch:937/1000\n",
      "Loss on train= 0.010415472090244293\n",
      "Loss on test= 0.016600150614976883\n",
      "acc for Lsat= 0.06666498256177508 \n",
      "acc for Psat= 0.09845360423669831 \n",
      "acc for optim= 0.148196021452786\n",
      "Epoch:938/1000\n",
      "Loss on train= 0.010782920755445957\n",
      "Loss on test= 0.016756461933255196\n",
      "acc for Lsat= 0.07145685075327408 \n",
      "acc for Psat= 0.10284667982185038 \n",
      "acc for optim= 0.14735979413303796\n",
      "Epoch:939/1000\n",
      "Loss on train= 0.010545183904469013\n",
      "Loss on test= 0.016550986096262932\n",
      "acc for Lsat= 0.06670775881606905 \n",
      "acc for Psat= 0.10141208408557026 \n",
      "acc for optim= 0.14741283603985536\n",
      "Epoch:940/1000\n",
      "Loss on train= 0.01012519933283329\n",
      "Loss on test= 0.01678605005145073\n",
      "acc for Lsat= 0.06735516027922281 \n",
      "acc for Psat= 0.09993560041212651 \n",
      "acc for optim= 0.14867703530784157\n",
      "Epoch:941/1000\n",
      "Loss on train= 0.010606296360492706\n",
      "Loss on test= 0.01709216833114624\n",
      "acc for Lsat= 0.0682004239670066 \n",
      "acc for Psat= 0.10301168679338024 \n",
      "acc for optim= 0.14788469075090338\n",
      "Epoch:942/1000\n",
      "Loss on train= 0.011122802272439003\n",
      "Loss on test= 0.016546573489904404\n",
      "acc for Lsat= 0.06711272444302792 \n",
      "acc for Psat= 0.098724857084653 \n",
      "acc for optim= 0.1491380736738283\n",
      "Epoch:943/1000\n",
      "Loss on train= 0.010989702306687832\n",
      "Loss on test= 0.01620337925851345\n",
      "acc for Lsat= 0.06827742826853328 \n",
      "acc for Psat= 0.09982383774730089 \n",
      "acc for optim= 0.1488750204523902\n",
      "Epoch:944/1000\n",
      "Loss on train= 0.010737568140029907\n",
      "Loss on test= 0.01658882014453411\n",
      "acc for Lsat= 0.06937146729028287 \n",
      "acc for Psat= 0.1028684977022181 \n",
      "acc for optim= 0.14990901563163309\n",
      "Epoch:945/1000\n",
      "Loss on train= 0.010729939676821232\n",
      "Loss on test= 0.016787568107247353\n",
      "acc for Lsat= 0.06884450232726731 \n",
      "acc for Psat= 0.09634661626517238 \n",
      "acc for optim= 0.14812824414753106\n",
      "Epoch:946/1000\n",
      "Loss on train= 0.010497694835066795\n",
      "Loss on test= 0.01639280840754509\n",
      "acc for Lsat= 0.06584627392680145 \n",
      "acc for Psat= 0.09958742646923645 \n",
      "acc for optim= 0.14787973951355074\n",
      "Epoch:947/1000\n",
      "Loss on train= 0.010872299782931805\n",
      "Loss on test= 0.016301944851875305\n",
      "acc for Lsat= 0.07377398602118003 \n",
      "acc for Psat= 0.09621384424259068 \n",
      "acc for optim= 0.14923584735244247\n",
      "Epoch:948/1000\n",
      "Loss on train= 0.010170440189540386\n",
      "Loss on test= 0.016886290162801743\n",
      "acc for Lsat= 0.06797946155711021 \n",
      "acc for Psat= 0.10389172621404547 \n",
      "acc for optim= 0.14893813939341918\n",
      "Epoch:949/1000\n",
      "Loss on train= 0.011053355410695076\n",
      "Loss on test= 0.016319744288921356\n",
      "acc for Lsat= 0.06677101308843106 \n",
      "acc for Psat= 0.09602426514429355 \n",
      "acc for optim= 0.14737209132830867\n",
      "Epoch:950/1000\n",
      "Loss on train= 0.010726126842200756\n",
      "Loss on test= 0.016628462821245193\n",
      "acc for Lsat= 0.06542270927821588 \n",
      "acc for Psat= 0.10026752475869037 \n",
      "acc for optim= 0.14689601276982875\n",
      "Epoch:951/1000\n",
      "Loss on train= 0.010808257386088371\n",
      "Loss on test= 0.016005508601665497\n",
      "acc for Lsat= 0.06749573568005468 \n",
      "acc for Psat= 0.09530909540188337 \n",
      "acc for optim= 0.1489661172165641\n",
      "Epoch:952/1000\n",
      "Loss on train= 0.010675610043108463\n",
      "Loss on test= 0.017055297270417213\n",
      "acc for Lsat= 0.07074193951802092 \n",
      "acc for Psat= 0.10022903717587797 \n",
      "acc for optim= 0.1483296273112937\n",
      "Epoch:953/1000\n",
      "Loss on train= 0.01077287457883358\n",
      "Loss on test= 0.016606217250227928\n",
      "acc for Lsat= 0.0763204133734507 \n",
      "acc for Psat= 0.11009699081159022 \n",
      "acc for optim= 0.14766796462651016\n",
      "Epoch:954/1000\n",
      "Loss on train= 0.010745031759142876\n",
      "Loss on test= 0.01630459539592266\n",
      "acc for Lsat= 0.06462202165975554 \n",
      "acc for Psat= 0.10110752751021136 \n",
      "acc for optim= 0.1487447614213435\n",
      "Epoch:955/1000\n",
      "Loss on train= 0.010833443142473698\n",
      "Loss on test= 0.017032349482178688\n",
      "acc for Lsat= 0.0673941807851382 \n",
      "acc for Psat= 0.10164302125813068 \n",
      "acc for optim= 0.14740165061515648\n",
      "Epoch:956/1000\n",
      "Loss on train= 0.010408018715679646\n",
      "Loss on test= 0.017020050436258316\n",
      "acc for Lsat= 0.06743579032915864 \n",
      "acc for Psat= 0.09701324006099397 \n",
      "acc for optim= 0.14799141490395468\n",
      "Epoch:957/1000\n",
      "Loss on train= 0.010792790912091732\n",
      "Loss on test= 0.016523759812116623\n",
      "acc for Lsat= 0.06689007539228804 \n",
      "acc for Psat= 0.0972448522908103 \n",
      "acc for optim= 0.1476148891747531\n",
      "Epoch:958/1000\n",
      "Loss on train= 0.011068015359342098\n",
      "Loss on test= 0.016722720116376877\n",
      "acc for Lsat= 0.06610189073840705 \n",
      "acc for Psat= 0.09834100368944179 \n",
      "acc for optim= 0.14712187953839792\n",
      "Epoch:959/1000\n",
      "Loss on train= 0.010442912578582764\n",
      "Loss on test= 0.01671256497502327\n",
      "acc for Lsat= 0.07115946340646215 \n",
      "acc for Psat= 0.09759712329064371 \n",
      "acc for optim= 0.14846226324548872\n",
      "Epoch:960/1000\n",
      "Loss on train= 0.010521483607590199\n",
      "Loss on test= 0.01668565906584263\n",
      "acc for Lsat= 0.07013215453552217 \n",
      "acc for Psat= 0.09531808369701365 \n",
      "acc for optim= 0.14742513004697075\n",
      "Epoch:961/1000\n",
      "Loss on train= 0.010331767611205578\n",
      "Loss on test= 0.01652677170932293\n",
      "acc for Lsat= 0.06951736670274002 \n",
      "acc for Psat= 0.10028279171540187 \n",
      "acc for optim= 0.1497303172706065\n",
      "Epoch:962/1000\n",
      "Loss on train= 0.01082246657460928\n",
      "Loss on test= 0.016772190108895302\n",
      "acc for Lsat= 0.06747407828972464 \n",
      "acc for Psat= 0.09624847593802244 \n",
      "acc for optim= 0.1476106494611832\n",
      "Epoch:963/1000\n",
      "Loss on train= 0.0108019495382905\n",
      "Loss on test= 0.016820721328258514\n",
      "acc for Lsat= 0.06705863843663647 \n",
      "acc for Psat= 0.10778516870065326 \n",
      "acc for optim= 0.14961871215823724\n",
      "Epoch:964/1000\n",
      "Loss on train= 0.010224065743386745\n",
      "Loss on test= 0.01702742837369442\n",
      "acc for Lsat= 0.06731839979697213 \n",
      "acc for Psat= 0.09740936913942391 \n",
      "acc for optim= 0.1481099383882205\n",
      "Epoch:965/1000\n",
      "Loss on train= 0.010606667026877403\n",
      "Loss on test= 0.01690412312746048\n",
      "acc for Lsat= 0.06776309015499245 \n",
      "acc for Psat= 0.0943083415730908 \n",
      "acc for optim= 0.1485344027470604\n",
      "Epoch:966/1000\n",
      "Loss on train= 0.010981102474033833\n",
      "Loss on test= 0.01737312600016594\n",
      "acc for Lsat= 0.07281843070907115 \n",
      "acc for Psat= 0.101091162127331 \n",
      "acc for optim= 0.14833671285342656\n",
      "Epoch:967/1000\n",
      "Loss on train= 0.010934730060398579\n",
      "Loss on test= 0.016193998977541924\n",
      "acc for Lsat= 0.0676255150749773 \n",
      "acc for Psat= 0.09703270565311896 \n",
      "acc for optim= 0.1479729719366711\n",
      "Epoch:968/1000\n",
      "Loss on train= 0.010358684696257114\n",
      "Loss on test= 0.016327621415257454\n",
      "acc for Lsat= 0.07043539368829062 \n",
      "acc for Psat= 0.09741251492116451 \n",
      "acc for optim= 0.14742845946219824\n",
      "Epoch:969/1000\n",
      "Loss on train= 0.010609866119921207\n",
      "Loss on test= 0.01651770807802677\n",
      "acc for Lsat= 0.06933800743922777 \n",
      "acc for Psat= 0.10332115913119852 \n",
      "acc for optim= 0.14906529680774125\n",
      "Epoch:970/1000\n",
      "Loss on train= 0.011129180900752544\n",
      "Loss on test= 0.016490083187818527\n",
      "acc for Lsat= 0.07067655635754409 \n",
      "acc for Psat= 0.10679599511602059 \n",
      "acc for optim= 0.1485682705860445\n",
      "Epoch:971/1000\n",
      "Loss on train= 0.01031471136957407\n",
      "Loss on test= 0.016424080356955528\n",
      "acc for Lsat= 0.07018709955253839 \n",
      "acc for Psat= 0.09674522958842503 \n",
      "acc for optim= 0.14781376936994425\n",
      "Epoch:972/1000\n",
      "Loss on train= 0.00999782420694828\n",
      "Loss on test= 0.01716354675590992\n",
      "acc for Lsat= 0.06664407832251466 \n",
      "acc for Psat= 0.09849930915295116 \n",
      "acc for optim= 0.14918073383768038\n",
      "Epoch:973/1000\n",
      "Loss on train= 0.010796098969876766\n",
      "Loss on test= 0.017541199922561646\n",
      "acc for Lsat= 0.06847518404182681 \n",
      "acc for Psat= 0.09430574808862854 \n",
      "acc for optim= 0.14917247998053354\n",
      "Epoch:974/1000\n",
      "Loss on train= 0.011214555241167545\n",
      "Loss on test= 0.01679091341793537\n",
      "acc for Lsat= 0.06796417486369077 \n",
      "acc for Psat= 0.09610924067672122 \n",
      "acc for optim= 0.1491826951290499\n",
      "Epoch:975/1000\n",
      "Loss on train= 0.010954420082271099\n",
      "Loss on test= 0.016254659742116928\n",
      "acc for Lsat= 0.06683321016205869 \n",
      "acc for Psat= 0.09508455153121505 \n",
      "acc for optim= 0.14771357608395952\n",
      "Epoch:976/1000\n",
      "Loss on train= 0.010735402815043926\n",
      "Loss on test= 0.016562219709157944\n",
      "acc for Lsat= 0.0725668048229542 \n",
      "acc for Psat= 0.09668801968554051 \n",
      "acc for optim= 0.14745429566593204\n",
      "Epoch:977/1000\n",
      "Loss on train= 0.011027531698346138\n",
      "Loss on test= 0.015689663589000702\n",
      "acc for Lsat= 0.06823384888696757 \n",
      "acc for Psat= 0.09968238048561993 \n",
      "acc for optim= 0.14889730347290447\n",
      "Epoch:978/1000\n",
      "Loss on train= 0.010527526028454304\n",
      "Loss on test= 0.01670953817665577\n",
      "acc for Lsat= 0.07011766020535143 \n",
      "acc for Psat= 0.09965526879580162 \n",
      "acc for optim= 0.1486101560707809\n",
      "Epoch:979/1000\n",
      "Loss on train= 0.01076124794781208\n",
      "Loss on test= 0.016547752544283867\n",
      "acc for Lsat= 0.06452093738029595 \n",
      "acc for Psat= 0.09418724322148428 \n",
      "acc for optim= 0.14891548103424643\n",
      "Epoch:980/1000\n",
      "Loss on train= 0.01049792766571045\n",
      "Loss on test= 0.017259497195482254\n",
      "acc for Lsat= 0.06968457297370345 \n",
      "acc for Psat= 0.10613539304204403 \n",
      "acc for optim= 0.14966523491632533\n",
      "Epoch:981/1000\n",
      "Loss on train= 0.010865433141589165\n",
      "Loss on test= 0.016467003151774406\n",
      "acc for Lsat= 0.06681501853124826 \n",
      "acc for Psat= 0.09655646501585494 \n",
      "acc for optim= 0.14908338648688602\n",
      "Epoch:982/1000\n",
      "Loss on train= 0.010457429103553295\n",
      "Loss on test= 0.0174860879778862\n",
      "acc for Lsat= 0.07092817468378751 \n",
      "acc for Psat= 0.10141123541779082 \n",
      "acc for optim= 0.14796606103934626\n",
      "Epoch:983/1000\n",
      "Loss on train= 0.010658184997737408\n",
      "Loss on test= 0.0171439778059721\n",
      "acc for Lsat= 0.06611054556740845 \n",
      "acc for Psat= 0.09890904025656168 \n",
      "acc for optim= 0.1502751351149223\n",
      "Epoch:984/1000\n",
      "Loss on train= 0.010767982341349125\n",
      "Loss on test= 0.016581596806645393\n",
      "acc for Lsat= 0.0727609651246523 \n",
      "acc for Psat= 0.09469665095929808 \n",
      "acc for optim= 0.14838773237144798\n",
      "Epoch:985/1000\n",
      "Loss on train= 0.010361509397625923\n",
      "Loss on test= 0.0162575114518404\n",
      "acc for Lsat= 0.06625183739154626 \n",
      "acc for Psat= 0.09929121208105611 \n",
      "acc for optim= 0.14861426628552957\n",
      "Epoch:986/1000\n",
      "Loss on train= 0.010429914109408855\n",
      "Loss on test= 0.016261380165815353\n",
      "acc for Lsat= 0.07029062674809014 \n",
      "acc for Psat= 0.09624663085971621 \n",
      "acc for optim= 0.14841317358511721\n",
      "Epoch:987/1000\n",
      "Loss on train= 0.010380307212471962\n",
      "Loss on test= 0.01660071313381195\n",
      "acc for Lsat= 0.0710663125745299 \n",
      "acc for Psat= 0.09518204609695188 \n",
      "acc for optim= 0.14797952674156012\n",
      "Epoch:988/1000\n",
      "Loss on train= 0.010458714328706264\n",
      "Loss on test= 0.01612081751227379\n",
      "acc for Lsat= 0.06679934486080366 \n",
      "acc for Psat= 0.09427988985570042 \n",
      "acc for optim= 0.14887532298594766\n",
      "Epoch:989/1000\n",
      "Loss on train= 0.010413824580609798\n",
      "Loss on test= 0.016909990459680557\n",
      "acc for Lsat= 0.06515867493127028 \n",
      "acc for Psat= 0.09545065699834941 \n",
      "acc for optim= 0.147723949808554\n",
      "Epoch:990/1000\n",
      "Loss on train= 0.010433079674839973\n",
      "Loss on test= 0.017415551468729973\n",
      "acc for Lsat= 0.06718414321888325 \n",
      "acc for Psat= 0.09800927980214838 \n",
      "acc for optim= 0.14825673973411055\n",
      "Epoch:991/1000\n",
      "Loss on train= 0.01061178743839264\n",
      "Loss on test= 0.016660260036587715\n",
      "acc for Lsat= 0.07183712600382154 \n",
      "acc for Psat= 0.09673231282473033 \n",
      "acc for optim= 0.14911069985152575\n",
      "Epoch:992/1000\n",
      "Loss on train= 0.009999409317970276\n",
      "Loss on test= 0.016869351267814636\n",
      "acc for Lsat= 0.06713053638478725 \n",
      "acc for Psat= 0.09587166305091599 \n",
      "acc for optim= 0.14797781261220602\n",
      "Epoch:993/1000\n",
      "Loss on train= 0.010190342552959919\n",
      "Loss on test= 0.01648971252143383\n",
      "acc for Lsat= 0.07086543918508961 \n",
      "acc for Psat= 0.09547503341075987 \n",
      "acc for optim= 0.14802770391773026\n",
      "Epoch:994/1000\n",
      "Loss on train= 0.010576008819043636\n",
      "Loss on test= 0.017344698309898376\n",
      "acc for Lsat= 0.0663940330310883 \n",
      "acc for Psat= 0.09992195432855744 \n",
      "acc for optim= 0.14750982113515756\n",
      "Epoch:995/1000\n",
      "Loss on train= 0.010817260481417179\n",
      "Loss on test= 0.016801444813609123\n",
      "acc for Lsat= 0.06662722573830532 \n",
      "acc for Psat= 0.09473987332610195 \n",
      "acc for optim= 0.147994870747138\n",
      "Epoch:996/1000\n",
      "Loss on train= 0.010450701229274273\n",
      "Loss on test= 0.01672961376607418\n",
      "acc for Lsat= 0.0687072102306994 \n",
      "acc for Psat= 0.10103741557952114 \n",
      "acc for optim= 0.1487303011532547\n",
      "Epoch:997/1000\n",
      "Loss on train= 0.010472939349710941\n",
      "Loss on test= 0.016775762662291527\n",
      "acc for Lsat= 0.07071428070767834 \n",
      "acc for Psat= 0.10159002885835539 \n",
      "acc for optim= 0.15004930132616828\n",
      "Epoch:998/1000\n",
      "Loss on train= 0.010824127122759819\n",
      "Loss on test= 0.016555415466427803\n",
      "acc for Lsat= 0.07201531271388575 \n",
      "acc for Psat= 0.10399450674679712 \n",
      "acc for optim= 0.14850411849073092\n",
      "Epoch:999/1000\n",
      "Loss on train= 0.010004940442740917\n",
      "Loss on test= 0.016560202464461327\n",
      "acc for Lsat= 0.06830127149127252 \n",
      "acc for Psat= 0.09874682318759095 \n",
      "acc for optim= 0.14803687658207576\n",
      "Epoch:1000/1000\n",
      "Loss on train= 0.010075323283672333\n",
      "Loss on test= 0.016716938465833664\n",
      "acc for Lsat= 0.07153394043125705 \n",
      "acc for Psat= 0.09779013699197171 \n",
      "acc for optim= 0.147485147531216\n"
     ]
    }
   ],
   "source": [
    "history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc1':[], 'test_acc2':[], \n",
    "          'test_acc3':[]}\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(df1)))):\n",
    "\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    \n",
    "    X_train = df1.iloc[train_idx]\n",
    "    X_test = df1.iloc[val_idx]\n",
    "    y_train = df2.iloc[train_idx]\n",
    "    y_test = df2.iloc[val_idx]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    scaler2 = MinMaxScaler()\n",
    "    scaler2.fit(y_train)\n",
    "\n",
    "    y_train = scaler2.transform(y_train)\n",
    "    y_test = scaler2.transform(y_test)\n",
    "    \n",
    "    \n",
    "    X_train = torch.Tensor(X_train) \n",
    "    y_train = torch.Tensor(y_train)\n",
    "\n",
    "    X_test = torch.Tensor(X_test)\n",
    "    y_test = torch.Tensor(y_test)\n",
    "\n",
    "    train_set = TensorDataset(X_train, y_train) \n",
    "    test_set = TensorDataset(X_test, y_test) \n",
    "\n",
    "\n",
    "    # Create Dataloader to read the data within batch sizes and put into memory. \n",
    "    train_loader = DataLoader(train_set, batch_size = 64, shuffle = True) \n",
    "    test_loader = DataLoader(test_set, batch_size = 20)\n",
    "\n",
    "    \n",
    "    model = Network(input_size,output_size).to(device) \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "    \n",
    "    lambda1 = lambda epoch: 0.998 ** epoch\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1, last_epoch = -1)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loss_on_train = train_epoch(model, optimizer, criterion, train_loader)\n",
    "        train_loss = float(np.mean(loss_on_train))\n",
    "        _, loss_on_test = validate(model, criterion, test_loader)\n",
    "        test_loss = float(np.mean(loss_on_test))\n",
    "        model.eval()\n",
    "        train_acc = test(model, train_loader)\n",
    "        test_acc = test(model, test_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(\"Epoch:{}/{}\".format(epoch + 1, num_epochs))\n",
    "        print('Loss on train=', train_loss)\n",
    "        print('Loss on test=', test_loss)\n",
    "        print('acc for Lsat=', test_acc[0],'\\n' 'acc for Psat=', test_acc[1], '\\n' 'acc for optim=', test_acc[2])\n",
    "        \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    #history['train_acc'].append(train_acc)\n",
    "    history['test_acc1'].append(test_acc[0])  \n",
    "    history['test_acc2'].append(test_acc[1])  \n",
    "    history['test_acc3'].append(test_acc[2])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1cd2aea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of 2 fold cross validation\n",
      "Average Training Loss: 0.0105 \t Average Test Loss: 0.0196 \t Average Lsat Acc: 0.070 +- 0.002 \t Average Psat Acc: 0.098 +- 0.000 \t Average Loptim Acc: 0.150 +- 0.002\n"
     ]
    }
   ],
   "source": [
    "avg_train_loss = np.mean(history['train_loss'])\n",
    "avg_test_loss = np.mean(history['test_loss'])\n",
    "avg_test_acc1 = np.mean(history['test_acc1'])\n",
    "std1 = np.std(history['test_acc1'])\n",
    "avg_test_acc2 = np.mean(history['test_acc2'])\n",
    "std2 = np.std(history['test_acc2'])\n",
    "avg_test_acc3 = np.mean(history['test_acc3'])\n",
    "std3 = np.std(history['test_acc3'])\n",
    "\n",
    "print('Performance of {} fold cross validation'.format(k))\n",
    "print(\"Average Training Loss: {:.4f} \\t Average Test Loss: {:.4f} \\t Average Lsat Acc: {:.3f} +- {:.3f} \\t Average Psat Acc: {:.3f} +- {:.3f} \\t Average Loptim Acc: {:.3f} +- {:.3f}\".\n",
    "      format(avg_train_loss,avg_test_loss,avg_test_acc1,std1, avg_test_acc2, std2,\n",
    "avg_test_acc3, std3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6341f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
