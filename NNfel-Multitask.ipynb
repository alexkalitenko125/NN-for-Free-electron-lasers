{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "1d4cf661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset \n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import torch.optim as optim \n",
    "from torch.optim import AdamW\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "21fc117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Данные.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "a3c142df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>lu</th>\n",
       "      <th>P0</th>\n",
       "      <th>I</th>\n",
       "      <th>gamma</th>\n",
       "      <th>sgamma</th>\n",
       "      <th>r</th>\n",
       "      <th>Psat</th>\n",
       "      <th>Lsat</th>\n",
       "      <th>loptim</th>\n",
       "      <th>lres</th>\n",
       "      <th>loptim/lres</th>\n",
       "      <th>Psat/Pall</th>\n",
       "      <th>Lsat/lu</th>\n",
       "      <th>f</th>\n",
       "      <th>rho</th>\n",
       "      <th>P0/Pall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.89</td>\n",
       "      <td>3.69</td>\n",
       "      <td>12800000.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2650.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.430000e+08</td>\n",
       "      <td>18.5</td>\n",
       "      <td>2.261250e-08</td>\n",
       "      <td>2.257770e-08</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>5.01</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>1.180000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.89</td>\n",
       "      <td>3.69</td>\n",
       "      <td>12800.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>9.800000e+07</td>\n",
       "      <td>16.7</td>\n",
       "      <td>1.585280e-07</td>\n",
       "      <td>1.580440e-07</td>\n",
       "      <td>0.003065</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>4.52</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>3.130000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.98</td>\n",
       "      <td>3.69</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2830.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.130000e+09</td>\n",
       "      <td>20.3</td>\n",
       "      <td>2.061380e-08</td>\n",
       "      <td>2.057220e-08</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.00156</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>2.070000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.98</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1500000.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>3160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.100000e+09</td>\n",
       "      <td>18.3</td>\n",
       "      <td>1.648760e-08</td>\n",
       "      <td>1.645770e-08</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>4.96</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>1.850000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.98</td>\n",
       "      <td>3.69</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2650.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.180000e+09</td>\n",
       "      <td>19.4</td>\n",
       "      <td>2.356200e-08</td>\n",
       "      <td>2.351110e-08</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.00175</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>2.210000e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      k    lu          P0      I   gamma  sgamma       r          Psat  Lsat  \\\n",
       "0  3.89  3.69  12800000.0   80.0  2650.0     0.0  0.0001  1.430000e+08  18.5   \n",
       "1  3.89  3.69     12800.0   80.0  1000.0     0.0  0.0001  9.800000e+07  16.7   \n",
       "2  3.98  3.69    150000.0  500.0  2830.0     0.0  0.0001  1.130000e+09  20.3   \n",
       "3  3.98  3.69   1500000.0  500.0  3160.0     0.0  0.0001  1.100000e+09  18.3   \n",
       "4  3.98  3.69    150000.0  500.0  2650.0     0.0  0.0001  1.180000e+09  19.4   \n",
       "\n",
       "         loptim          lres  loptim/lres  Psat/Pall  Lsat/lu      f  \\\n",
       "0  2.261250e-08  2.257770e-08     0.001542    0.00132     5.01  0.736   \n",
       "1  1.585280e-07  1.580440e-07     0.003065    0.00240     4.52  0.736   \n",
       "2  2.061380e-08  2.057220e-08     0.002024    0.00156     5.49  0.735   \n",
       "3  1.648760e-08  1.645770e-08     0.001814    0.00136     4.96  0.735   \n",
       "4  2.356200e-08  2.351110e-08     0.002164    0.00175     5.25  0.735   \n",
       "\n",
       "        rho       P0/Pall  \n",
       "0  0.000965  1.180000e-04  \n",
       "1  0.002550  3.130000e-07  \n",
       "2  0.001690  2.070000e-07  \n",
       "3  0.001510  1.850000e-06  \n",
       "4  0.001800  2.210000e-07  "
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "140b811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "52e1ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "243cc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Psat/Pall']<0.008]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "2f6103f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log'] = np.log(df[['Psat/Pall']])\n",
    "df['log0'] = np.log(df['P0']/df['I']/df['gamma']/511000)\n",
    "df['optim'] = df['loptim/lres']/df['Psat/Pall']\n",
    "df['Lsat/lu'] = np.log(df['Lsat/lu'])\n",
    "df['sqgamma'] = np.log(df['gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "ee8f0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['optim']<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "294bf40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9070138150903294"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)/l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "611d767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['optim'] = df['loptim/lres']/df['rho']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "cf41df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['optim'] = (df['optim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "f418fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(columns=['gamma', 'P0', 'optim', 'P0/Pall', 'Psat', 'Lsat', 'loptim','lres', 'loptim/lres', 'Psat/Pall', 'Lsat/lu', 'f', 'rho', 'log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "3642dca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>lu</th>\n",
       "      <th>I</th>\n",
       "      <th>sgamma</th>\n",
       "      <th>r</th>\n",
       "      <th>log0</th>\n",
       "      <th>sqgamma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.89</td>\n",
       "      <td>3.69</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>-9.043511</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.89</td>\n",
       "      <td>3.69</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>-14.976706</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.98</td>\n",
       "      <td>3.69</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>-15.388374</td>\n",
       "      <td>7.948032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.98</td>\n",
       "      <td>3.69</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>-13.196085</td>\n",
       "      <td>8.058327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.98</td>\n",
       "      <td>3.69</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>-15.322657</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>1.72</td>\n",
       "      <td>1.99</td>\n",
       "      <td>7600.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-16.247034</td>\n",
       "      <td>10.275051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>1.72</td>\n",
       "      <td>1.99</td>\n",
       "      <td>7600.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-11.532665</td>\n",
       "      <td>10.165852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>3.29</td>\n",
       "      <td>1.99</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-11.512708</td>\n",
       "      <td>10.404263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>3.29</td>\n",
       "      <td>1.99</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-11.653787</td>\n",
       "      <td>10.545341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>3.29</td>\n",
       "      <td>1.99</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-15.988667</td>\n",
       "      <td>10.275051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1707 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         k    lu       I   sgamma        r       log0    sqgamma\n",
       "0     3.89  3.69    80.0  0.00000  0.00010  -9.043511   7.882315\n",
       "1     3.89  3.69    80.0  0.00000  0.00010 -14.976706   6.907755\n",
       "2     3.98  3.69   500.0  0.00000  0.00010 -15.388374   7.948032\n",
       "3     3.98  3.69   500.0  0.00000  0.00010 -13.196085   8.058327\n",
       "4     3.98  3.69   500.0  0.00000  0.00010 -15.322657   7.882315\n",
       "...    ...   ...     ...      ...      ...        ...        ...\n",
       "1877  1.72  1.99  7600.0  0.00005  0.00001 -16.247034  10.275051\n",
       "1878  1.72  1.99  7600.0  0.00005  0.00001 -11.532665  10.165852\n",
       "1879  3.29  1.99  4500.0  0.00005  0.00001 -11.512708  10.404263\n",
       "1880  3.29  1.99  4500.0  0.00005  0.00001 -11.653787  10.545341\n",
       "1881  3.29  1.99  4500.0  0.00005  0.00001 -15.988667  10.275051\n",
       "\n",
       "[1707 rows x 7 columns]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "17404835",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(columns=['sqgamma', 'k', 'lu', 'I', 'gamma', 'sgamma', 'r', 'log0', 'P0', 'P0/Pall', 'loptim','lres', 'loptim/lres', 'Psat/Pall', 'Lsat', 'f', 'rho', 'Psat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "01cfb8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lsat/lu</th>\n",
       "      <th>log</th>\n",
       "      <th>optim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.611436</td>\n",
       "      <td>-6.630124</td>\n",
       "      <td>1.598041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.508512</td>\n",
       "      <td>-6.032287</td>\n",
       "      <td>1.201894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.702928</td>\n",
       "      <td>-6.463069</td>\n",
       "      <td>1.197420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.601406</td>\n",
       "      <td>-6.600271</td>\n",
       "      <td>1.201364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.658228</td>\n",
       "      <td>-6.348139</td>\n",
       "      <td>1.202222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>2.360854</td>\n",
       "      <td>-7.606921</td>\n",
       "      <td>0.798766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>1.829376</td>\n",
       "      <td>-7.086882</td>\n",
       "      <td>1.198454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>1.906575</td>\n",
       "      <td>-7.127156</td>\n",
       "      <td>1.200737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>2.034706</td>\n",
       "      <td>-7.268725</td>\n",
       "      <td>1.197812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>2.202765</td>\n",
       "      <td>-7.360312</td>\n",
       "      <td>0.799514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1707 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Lsat/lu       log     optim\n",
       "0     1.611436 -6.630124  1.598041\n",
       "1     1.508512 -6.032287  1.201894\n",
       "2     1.702928 -6.463069  1.197420\n",
       "3     1.601406 -6.600271  1.201364\n",
       "4     1.658228 -6.348139  1.202222\n",
       "...        ...       ...       ...\n",
       "1877  2.360854 -7.606921  0.798766\n",
       "1878  1.829376 -7.086882  1.198454\n",
       "1879  1.906575 -7.127156  1.200737\n",
       "1880  2.034706 -7.268725  1.197812\n",
       "1881  2.202765 -7.360312  0.799514\n",
       "\n",
       "[1707 rows x 3 columns]"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "2a77acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df1, df2, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "515fa4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler2.fit(y_train)\n",
    "\n",
    "y_train = scaler2.transform(y_train)\n",
    "y_test = scaler2.transform(y_test)\n",
    "y_val = scaler2.transform(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "dcc3a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = torch.Tensor(X_train) \n",
    "y_train = torch.Tensor(y_train)\n",
    "X_val = torch.Tensor(X_val)\n",
    "y_val = torch.Tensor(y_val)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "train_set = TensorDataset(X_train, y_train) \n",
    "validate_set = TensorDataset(X_val, y_val) \n",
    "test_set = TensorDataset(X_test, y_test) \n",
    "\n",
    "\n",
    "# Create Dataloader to read the data within batch sizes and put into memory. \n",
    "train_loader = DataLoader(train_set, batch_size = 64, shuffle = True) \n",
    "validate_loader = DataLoader(validate_set, batch_size = 20) \n",
    "test_loader = DataLoader(test_set, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "541c3e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41137123, 0.77591974, 0.51269037, 0.05      , 0.        ,\n",
       "        0.32778853, 0.97307026],\n",
       "       [0.24414715, 0.4715719 , 0.00203046, 0.1       , 0.0768116 ,\n",
       "        0.3207677 , 0.56033206],\n",
       "       [0.67892975, 0.68561876, 0.07614213, 0.        , 0.2753623 ,\n",
       "        0.        , 0.58063394],\n",
       "       [0.24414715, 0.8762542 , 0.04568528, 0.        , 0.01449275,\n",
       "        0.5015955 , 0.79025006],\n",
       "       [0.04347826, 0.        , 0.00304569, 0.2       , 0.04347826,\n",
       "        0.3579892 , 0.4193661 ],\n",
       "       [0.14715719, 0.7993311 , 0.04568528, 0.1       , 0.20289855,\n",
       "        0.41221565, 0.4193661 ],\n",
       "       [0.4381271 , 0.22408026, 0.8071066 , 0.1       , 0.02898551,\n",
       "        0.70393956, 0.79025006],\n",
       "       [0.8160535 , 0.5852843 , 0.5025381 , 0.2       , 0.04347826,\n",
       "        0.67376107, 0.88021713],\n",
       "       [0.26086956, 0.76254183, 0.00101523, 0.1       , 0.13043478,\n",
       "        0.5432396 , 0.45869353],\n",
       "       [0.66220737, 0.89966553, 0.70558375, 0.        , 0.07246377,\n",
       "        0.6216226 , 0.6446678 ],\n",
       "       [0.7458194 , 0.78595316, 0.07614213, 0.1       , 0.02753623,\n",
       "        0.52758884, 0.78102744],\n",
       "       [0.31438127, 0.4949833 , 0.00203046, 0.1       , 0.04637681,\n",
       "        0.3290904 , 0.59649855],\n",
       "       [0.50501674, 0.73913044, 0.07614213, 0.1       , 0.1594203 ,\n",
       "        0.48789194, 0.6932179 ],\n",
       "       [0.7792642 , 0.78595316, 0.70558375, 0.2       , 0.71014494,\n",
       "        0.54904   , 0.58063394],\n",
       "       [0.80936456, 0.9632107 , 0.70558375, 0.        , 0.2753623 ,\n",
       "        0.7660533 , 0.6090432 ],\n",
       "       [0.40802675, 0.07023411, 0.        , 0.        , 0.42028984,\n",
       "        0.44868228, 0.125586  ],\n",
       "       [0.84615386, 0.6555184 , 0.70558375, 1.        , 0.20289855,\n",
       "        0.6741735 , 0.78102744],\n",
       "       [0.6220736 , 0.14715719, 0.70558375, 0.2       , 0.71014494,\n",
       "        0.46271518, 0.3223339 ],\n",
       "       [0.6688963 , 0.24414715, 0.04568528, 0.        , 0.10144927,\n",
       "        0.6245359 , 0.4193661 ],\n",
       "       [0.89966553, 0.24749164, 0.70558375, 0.1       , 0.08695652,\n",
       "        0.6786592 , 0.79860777],\n",
       "       [0.19732441, 0.7692308 , 0.        , 0.        , 0.01449275,\n",
       "        0.526768  , 0.62002826],\n",
       "       [0.10702341, 0.65217394, 0.06598984, 0.        , 0.85507244,\n",
       "        0.15264311, 0.4350645 ],\n",
       "       [0.03010033, 0.33444816, 0.5025381 , 0.        , 0.07246377,\n",
       "        0.5034328 , 0.6932179 ],\n",
       "       [0.6187291 , 0.8729097 , 0.07614213, 0.1       , 0.5652174 ,\n",
       "        0.734933  , 0.47627386],\n",
       "       [0.22408026, 0.23411371, 0.5025381 , 0.2       , 0.5652174 ,\n",
       "        0.7118966 , 0.48360172],\n",
       "       [0.46822742, 0.8561873 , 0.06598984, 0.1       , 0.07246377,\n",
       "        0.48092234, 0.77025384],\n",
       "       [0.916388  , 0.9531773 , 0.07614213, 0.1       , 0.04637681,\n",
       "        0.36796317, 0.7417    ],\n",
       "       [0.71237457, 0.4949833 , 0.07614213, 0.1       , 0.04637681,\n",
       "        0.3009063 , 0.8059356 ],\n",
       "       [0.02675585, 0.3779264 , 0.00304569, 0.2       , 0.71014494,\n",
       "        0.718441  , 0.23799816],\n",
       "       [0.41137123, 0.14381272, 0.70558375, 0.        , 0.10144927,\n",
       "        0.8975539 , 0.6446678 ],\n",
       "       [0.01337793, 0.46488294, 0.5025381 , 0.        , 0.11594203,\n",
       "        0.5426431 , 0.59649855],\n",
       "       [0.46153846, 0.9531773 , 0.00203046, 0.        , 0.2753623 ,\n",
       "        0.1756821 , 0.46791616],\n",
       "       [0.5150502 , 0.53177255, 0.06598984, 0.1       , 0.01449275,\n",
       "        0.33110702, 0.79860777],\n",
       "       [0.09364548, 0.03010033, 0.        , 0.        , 0.11594203,\n",
       "        0.6690996 , 0.4193661 ],\n",
       "       [0.916388  , 0.8528428 , 0.07614213, 0.1       , 0.11014493,\n",
       "        0.47264245, 0.79860777],\n",
       "       [0.13712375, 0.5719063 , 0.        , 0.1       , 0.07246377,\n",
       "        0.47606534, 0.58063394],\n",
       "       [0.84949833, 0.19063546, 0.6040609 , 0.        , 0.01449275,\n",
       "        0.9969816 , 0.6373941 ],\n",
       "       [0.5852843 , 0.06354515, 0.6040609 , 0.        , 0.07246377,\n",
       "        0.29399303, 0.79856104],\n",
       "       [0.458194  , 0.05685619, 0.        , 0.        , 0.10144927,\n",
       "        0.7103189 , 0.4193661 ],\n",
       "       [0.91973245, 0.04347826, 0.8071066 , 1.        , 0.10144927,\n",
       "        0.7736825 , 0.58063394],\n",
       "       [0.66220737, 0.8929766 , 0.17766498, 0.        , 0.00724638,\n",
       "        0.67251885, 0.93969566],\n",
       "       [0.5852843 , 0.01672241, 0.00304569, 0.1       , 0.10144927,\n",
       "        0.6978572 , 0.46791616],\n",
       "       [0.48160535, 0.909699  , 0.        , 0.1       , 0.04347826,\n",
       "        0.69647497, 0.62002826],\n",
       "       [0.8361204 , 0.1270903 , 0.8172589 , 0.1       , 0.0173913 ,\n",
       "        0.64394957, 0.95006984],\n",
       "       [0.02341137, 0.13712375, 0.04568528, 0.1       , 0.07246377,\n",
       "        0.72621715, 0.46791616],\n",
       "       [0.15050167, 0.22073579, 0.04568528, 0.2       , 0.2753623 ,\n",
       "        0.5068814 , 0.47627386],\n",
       "       [0.9598662 , 0.4347826 , 0.06598984, 0.2       , 0.13043478,\n",
       "        0.45678008, 0.4479199 ],\n",
       "       [0.9799331 , 0.9632107 , 0.06598984, 0.1       , 0.11594203,\n",
       "        0.32845676, 0.7417    ],\n",
       "       [0.8361204 , 0.71571904, 0.07614213, 0.1       , 0.1594203 ,\n",
       "        0.35741192, 0.6373941 ],\n",
       "       [0.        , 0.77591974, 0.6852792 , 0.05      , 0.        ,\n",
       "        0.64352787, 0.9813072 ],\n",
       "       [0.4347826 , 0.8561873 , 0.42131978, 0.2       , 0.04347826,\n",
       "        0.15162516, 0.84266347],\n",
       "       [0.6187291 , 0.8729097 , 0.07614213, 0.1       , 0.5652174 ,\n",
       "        0.64647263, 0.6373941 ],\n",
       "       [0.40802675, 0.77257526, 0.05583756, 0.5       , 0.14492753,\n",
       "        0.7725013 , 0.48360172],\n",
       "       [0.74916387, 0.9866221 , 0.8071066 , 0.        , 0.10144927,\n",
       "        0.4763304 , 0.58063394],\n",
       "       [0.32441473, 0.26086956, 0.        , 1.        , 0.2753623 ,\n",
       "        0.6486341 , 0.14558226],\n",
       "       [0.61204016, 0.8695652 , 0.00203046, 0.1       , 0.13043478,\n",
       "        0.7655985 , 0.4350645 ],\n",
       "       [0.09030101, 0.48494983, 0.05583756, 0.1       , 0.10144927,\n",
       "        0.45264006, 0.370884  ],\n",
       "       [0.32441473, 0.9498328 , 0.5025381 , 0.1       , 0.07246377,\n",
       "        0.5671586 , 0.6932179 ],\n",
       "       [0.48160535, 0.23411371, 0.70558375, 0.2       , 0.5652174 ,\n",
       "        0.5011147 , 0.59625274],\n",
       "       [0.4347826 , 0.8561873 , 0.42131978, 0.2       , 0.04347826,\n",
       "        0.6421068 , 0.9163101 ],\n",
       "       [0.01337793, 0.5150502 , 0.06598984, 0.        , 0.11594203,\n",
       "        0.5308262 , 0.53170013],\n",
       "       [0.84615386, 0.6555184 , 0.70558375, 1.        , 0.20289855,\n",
       "        0.483526  , 0.48360172],\n",
       "       [0.8394649 , 0.72240806, 0.04568528, 0.        , 0.10144927,\n",
       "        0.57490057, 0.58063394],\n",
       "       [0.5585284 , 0.26755852, 0.04568528, 0.5       , 0.02898551,\n",
       "        0.8213743 , 0.46791616]], dtype=float32)"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
    "train_dataset_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "db94f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters \n",
    "input_size = list(X_train.shape)[1]   \n",
    "output_size = list(y_train.shape)[1]  \n",
    "\n",
    "\n",
    "\n",
    "# Define neural network \n",
    "class Network(nn.Module): \n",
    "    def __init__(self, input_size, output_size, init_form=\"normal\"): \n",
    "        super().__init__() \n",
    "        self.conv_stack = nn.Sequential(\n",
    "        nn.Linear(input_size, 300), \n",
    "        nn.Tanh(),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(300, 100),\n",
    "        nn.Tanh(),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(100, 100),\n",
    "        nn.Tanh(),\n",
    "        nn.Dropout(p=0.1))\n",
    "        \n",
    "        self.conv_stack1 = nn.Sequential(\n",
    "        nn.Linear(100, 100), \n",
    "        nn.Tanh(),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(100, 30),\n",
    "        nn.Tanh(), \n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(30, 1))\n",
    "                \n",
    "        self.conv_stack2 = nn.Sequential(\n",
    "        nn.Linear(100, 100), \n",
    "        nn.Tanh(),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(100, 30),\n",
    "        nn.Tanh(), \n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(30, 1))\n",
    "        \n",
    "        self.conv_stack3 = nn.Sequential(\n",
    "        nn.Linear(100, 100), \n",
    "        nn.Tanh(),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(100, 30),\n",
    "        nn.Tanh(), \n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(30, 1))\n",
    "            \n",
    "        \n",
    "        self.init_form = init_form\n",
    "        if self.init_form is not None:\n",
    "            self.init()\n",
    "\n",
    "    def forward(self, x): \n",
    "        s = self.conv_stack(x)\n",
    "        out1 = self.conv_stack1(s)\n",
    "        out2 = self.conv_stack2(s)\n",
    "        out3 = self.conv_stack3(s)\n",
    "        return out1, out2, out3\n",
    "    \n",
    "    \n",
    "        # xavier weight initialization\n",
    "    def init(self):\n",
    "        sigmoid_gain = torch.nn.init.calculate_gain(\"tanh\")\n",
    "        for child in self.conv_stack.children():\n",
    "            if isinstance(child, nn.Linear):\n",
    "                if self.init_form == \"normal\":\n",
    "                    torch.nn.init.xavier_normal_(child.weight,\n",
    "                                                 gain=sigmoid_gain)\n",
    "                    if child.bias is not None:\n",
    "                        torch.nn.init.zeros_(child.bias)\n",
    "                elif self.init_form == \"uniform\":\n",
    "                    torch.nn.init.xavier_uniform_(child.weight,\n",
    "                                                  gain=sigmoid_gain)\n",
    "                    if child.bias is not None:\n",
    "                        torch.nn.init.zeros_(child.bias)\n",
    "                else:\n",
    "                    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "a50d04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_epoch(model,\n",
    "                optimizer,\n",
    "                criterion,\n",
    "                train_loader):\n",
    "    loss_history = []\n",
    "    for batch in train_loader: \n",
    "        optimizer.zero_grad()\n",
    "        x_train, y_train = batch # parse data\n",
    "        x_train, y_train = x_train.to(device), y_train.to(device) # compute on gpu\n",
    "        y1, y2, y3 = model(x_train) # get predictions\n",
    "        loss = criterion(y1, y_train[:, 0:1]) + criterion(y2, y_train[:,1:2]) + criterion(y3, y_train[:,2:]) # compute loss\n",
    "        loss_history.append(loss.cpu().detach().numpy()) # write loss to log\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "8a6c7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,\n",
    "             criterion,\n",
    "             val_loader):\n",
    "    cumloss = 0\n",
    "    loss_history = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x_train, y_train = batch # parse data\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device) # compute on gpu\n",
    "            y1, y2, y3 = model(x_train) # get predictions\n",
    "            loss = criterion(y1, y_train[:, 0:1]) + criterion(y2, y_train[:,1:2]) + criterion(y3, y_train[:,2:]) # compute loss\n",
    "            loss_history.append(loss.cpu().detach().numpy()) # write loss to log\n",
    "            cumloss += loss\n",
    "    return cumloss / len(val_loader), loss_history # mean loss and history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "f3fed850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, optimizer, model_name=None, n_epochs=5):\n",
    "  \n",
    "    criterion = nn.MSELoss().to(device)\n",
    "\n",
    "    train_history = {}\n",
    "    train_history['model_name'] = model_name\n",
    "    train_history['loss_on_train'] = []\n",
    "    train_history['loss_on_test'] = []\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        loss_on_train = train_epoch(model,\n",
    "                                    optimizer,\n",
    "                                    criterion,\n",
    "                                    train_loader)\n",
    "        _, loss_on_test = validate(model,\n",
    "                                   criterion,\n",
    "                                   validate_loader)\n",
    "        train_history['loss_on_train'].append(np.mean(loss_on_train))\n",
    "        train_history['loss_on_test'].append(np.mean(loss_on_test))\n",
    "        scheduler.step()\n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "a2e8bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smoothing(scalars, weight):  \n",
    "    last = scalars[0]  \n",
    "    smoothed = []\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  \n",
    "        smoothed.append(smoothed_val)                        \n",
    "        last = smoothed_val                                 \n",
    "\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "def plot_history(history, n_epochs=5, smooth_val=0.9):\n",
    "    fig, ax =  plt.subplots(3, 1, figsize=(12, 14))\n",
    "    for stage_idx, (stage_lbl, stage_title) in enumerate(\n",
    "        zip(['loss_on_train', 'loss_on_test'],\n",
    "            ['train loss', 'val loss'])):\n",
    "        # plot history on each learning step\n",
    "        epoch_len = len(history[stage_lbl])//n_epochs\n",
    "        full_stage_len = len(history[stage_lbl])\n",
    "        ax[stage_idx].plot(exponential_smoothing(history[stage_lbl], smooth_val),\n",
    "                           label='smoothed',\n",
    "                           color='m')\n",
    "        ax[stage_idx].plot(history[stage_lbl],\n",
    "                           label='raw',\n",
    "                           alpha=0.2,\n",
    "                           color='c')\n",
    "        ax[stage_idx].set_title(stage_title)\n",
    "        ax[stage_idx].set_xlabel('epochs')\n",
    "        ax[stage_idx].set_ylabel('loss')\n",
    "        epochs_ticks_positions = np.arange(stop=full_stage_len+1,\n",
    "                                           step=epoch_len)\n",
    "        ax[stage_idx].set_xticks(np.arange(0,1000,100))\n",
    "        ax[stage_idx].set_xticklabels(np.arange(0,1000,100))\n",
    "        ax[stage_idx].legend()\n",
    "\n",
    "        # plot mean train and test loss combined\n",
    "        mean_loss_on_epoch = [np.mean(history[stage_lbl][i:i+epoch_len]) \\\n",
    "                              for i in range(0, full_stage_len, epoch_len)]\n",
    "        std_loss_on_epoch = [np.std(history[stage_lbl][i:i+epoch_len]) \\\n",
    "                              for i in range(0, full_stage_len, epoch_len)]\n",
    "\n",
    "        ax[2].set_title('\\nAverage loss per epoch')\n",
    "        ax[2].errorbar(np.arange(n_epochs) + stage_idx / 30.,\n",
    "                       mean_loss_on_epoch,\n",
    "                       yerr=std_loss_on_epoch,\n",
    "                       capsize=5,\n",
    "                       fmt=\"X--\",\n",
    "                       label=stage_title)\n",
    "        ax[2].set_xticks(np.arange(0,1000,100))\n",
    "        ax[2].set_xticklabels(np.arange(0,1000,100))\n",
    "        ax[2].set_xlabel('epochs')\n",
    "        ax[2].set_ylabel('loss')\n",
    "        ax[2].legend()\n",
    "\n",
    "    fig.suptitle(history['model_name'], fontsize=24)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "f030e212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:51<00:00,  8.94it/s]\n"
     ]
    }
   ],
   "source": [
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "model = Network(input_size,output_size).to(device) \n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "lambda1 = lambda epoch: 0.998 ** epoch\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1, last_epoch = -1)\n",
    "\n",
    "n_epochs = 1000\n",
    "history = train_model(model, optimizer, model_name='model', n_epochs=n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "6971052a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAt0AAAOLCAYAAACMlz+sAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAADnrElEQVR4nOzde5xddX3v/9dnrX2bWya3yT0hAcIlARJj5CKKtagF9Yi2WqEHFesp5Shear2g7flpW3sOWqrWlsKhLVVaFayXI9WIilZFBUmAgIGAhEvIJCEJCbnMbd/W5/fHWpPsDDPJJJk1O7P3+/l4zGPvtdZ37f1dkxDe+7s/6/s1d0dERERERNIT1LsDIiIiIiKNTqFbRERERCRlCt0iIiIiIilT6BYRERERSZlCt4iIiIhIyhS6RURERERSptAtIiLDMjNPfhaO4Wv+JHnNK8bqNUVEJgKFbhERERGRlCl0i4iIiIikTKFbRERERCRlCt0iIiIiIilT6BYRSZmZPZ3cPPhbZjbbzG40s01m1m9m683sT8wsqGn/FjO7y8x2m9leM/uumZ1xiNd/kZn9e/KaRTN7zsy+b2a/d5h+BWb2XjN7MOnLDjP7TzM7b5TX1WVm/8fMfm1mPWbWa2brzOyvzWzq6H9DIiKNL1PvDoiINJFFwFeBWcBeIAucBnwWOBF4r5ldC3wUqAJ9QAfwWuClZna2uz9e+4JmdiVwAwcGUXYDk4HXAK8xs38HrnD36pDzMsDXgUuSXRXi/ye8HrjIzN56qAsxs5cB3wYGw3Up6fPS5OdtZvZqd39sNL8YEZFGp5FuEZHx8zngKWCZu3cCk4D/lRx7j5l9HPgg8AGg090nAWcCjxEH6b+ufTEzeykHAvfXgfnuPiVp+2eAA5cDHxumLx8lDtwR8OHk/aYQh/87gZtHuggzOwH4T+LA/c/EHxxagDbgDOAOYD7wTTMLR/OLERFpdObu9e6DiEhDM7OngROA54ET3X33kOM/An472fyEu//lkOMvB34GFIFJ7l4act4vgFcMM5r9v4kDdw8w1933JvvbgC3Eof8v3P2TQ87LA/cDS5Jdi9z96Zrj/w78d+AL7v7+Ya43B9wLLAPe4u5frzn2E+AVwDvd/YvD/LpERBqSRrpFRMbPjUMDd+LO5LFEXGoy1C+AASAPnAyQ1Ey/Mjn+f4YG7sSnk/PaiUtUBr2GOHAXiUffD+LuReC64S7AzFqAtySbw/WV5EPBYNB+9XBtRESajWq6RUTGz69H2L89eXza3XuGHnT3yMyeA+YBU5LdLwKMuITkp8O9qLvvMbP7gPOBFcCtyaEVyeNad98zQp+GfU1gJZBLnv/KzEZoRkvyOH+kBiIizUShW0Rk/GwdYX/1MMdr22STx67kcc9wQb1G95D2tc+3HOK8zSPsn13zfOYhzh/UOoo2IiINT6FbRGRiy4/z+w2WJT7v7poWUERklFTTLSIyMe1IHlvMrOsQ7eYNaV/7fM4hzhvp2LbkcYqZzTp0F0VEZJBCt4jIxPQAcT03HLih8iBm1gm8ONm8v+bQ4PPlZjZphNd/xQj71xDP6Q3wu6PrqoiIKHSLiExA7r4L+K9k86O1K1rW+ChQIJ4ycFXN/u8TL86TB0aa8u9PR3jffcA3ks0/N7MR67rNLGNm7Ye5FBGRpqDQLSIycf0v4sVtVgC3mtk8ADNrTxbauSZpd+3gHN0A7t4HfCbZ/ISZfTCZChAzWwh8i0PPOnINsIv4pspfmtmbkrm9SV7jZDP7ALCeeLYTEZGmp9AtIjJBufsvgXcTB++3AM+Y2S7ipeD/mnhKwS8D1w5z+qeJl3EPgb8F9prZ88QrZr4G+MNDvO/TwEXEs5+cCHwT6DGz58xsAHiceP7vkzlQAiMi0tQUukVEJjB3/7/AS4CvEE852A7sAX5IvBrk5cMtnOPuFeD3gPcBDxHXaVeB7xKvbvnNw7zvauLl3z8K/BLYR7z8fD9x3fengZe4+0jzfYuINBUtAy8iIiIikjKNdIuIiIiIpEyhW0REREQkZQrdIiIiIiIpU+gWEREREUmZQreIiIiISMoUukVEREREUqbQLSIiIiKSMoVuEREREZGUKXSLiIiIiKRMoVtEREREJGUK3SIiIiIiKVPoFhERERFJmUK3iIiIiEjKFLpFRERERFKm0C0iIiIikjKFbhERERGRlCl0i4iIiIikTKFbRERERCRlCt0iIiIiIilT6BYRERERSZlCt4iIiIhIyhS6RURERERSptAtIiIiIpIyhW4RERERkZQpdIuIiIiIpEyhW0REREQkZQrdIiIiIiIpU+gWEREREUmZQreIiIiISMoUukVEREREUqbQLSIiIiKSMoVuEREREZGUKXSLiIiIiKRMoVtEREREJGUK3SIiIiIiKVPoFhERERFJmUK3iIiIiEjKFLpFRERERFKm0C0iIiIikjKFbhERERGRlCl0i4iIiIikTKFbRERERCRlCt0iIiIiIilT6BYRERERSZlCt4iIiIhIyhS6RURERERSptAtIiIiIpIyhW4RERERkZQpdIuIiIiIpEyhW0REREQkZQrdIiIiIiIpU+gWEREREUmZQreIiIiISMoUukVEREREUqbQLSIiIiKSMoVuEREREZGUKXSLiIiIiKRMoVtEREREJGUK3SIiIiIiKVPoFhERERFJmUK3iIiIiEjKFLpFRERERFKm0C0iIiIikjKFbhERERGRlCl0i4iIiIikTKFbRKTJmNmNZva/jvLcn5jZ/xjrPomINLpMvTsgIiKjZ2ZPA//D3e882tdw96vGrkciIjIaGukWEWkgZqbBFBGR45BCt4jIBGFm/wYsAP7TzHrM7CNmttDM3MzeZWbPAD9O2v6HmT1rZnvM7GdmtrTmdb5oZp9Knv+WmXWb2Z+a2XYz22pm7xxlfwIz+3Mz25ice4uZdSbHCmb272a208x2m9lqM5uZHLvCzJ40s31m9pSZ/fcx/lWJiBx3FLpFRCYId38b8Azw39y93d0/U3P4FcDpwO8k298DFgMzgPuBLx/ipWcBncBc4F3A9WY2ZRRduiL5eSVwItAO/ENy7B3Ja84HpgFXAf1m1gZ8AbjY3TuAlwJrR/FeIiITmkK3iEhj+KS797p7P4C73+zu+9y9CHwSWDY4Cj2MMvCX7l5291VAD3DqKN7zvwOfdfcn3b0H+BhwaVLiUiYO2ye7e9Xd73P3vcl5EXCGmbW4+1Z3f/hoL1pEZKJQ6BYRaQybBp+YWWhm15rZE2a2F3g6OTR9hHN3unulZruPeNT6cOYAG2u2NxLfoD8T+Dfg+8CtZrbFzD5jZll37wXeSjzyvdXMvmtmp43ivUREJjSFbhGRicVHsf8PgEuAVxGXeCxM9tsY92ULcELN9gKgAmxLRs3/wt2XEJeQvB54O4C7f9/dXw3MBh4F/mmM+yUictxR6BYRmVi2EddPH0oHUAR2Aq3A/06pL18F/sTMFplZe/I+t7l7xcxeaWZnmlkI7CUuN6ma2Uwze0NS210kLmWpptQ/EZHjhkK3iMjE8n+AP09mBPnQCG1uIS712Aw8AtyTUl9uJi4j+RnwFDAAvDc5Ngv4OnHgXg/8FPh34v/v/CnxKPku4htA351S/0REjhvmPtI3lSIiIiIiMhY00i0iIiIikjKFbhERERGRlCl0i4iIiIikTKFbRERERCRlCt0iIiIiIinL1LsD42H69Om+cOHCendDRERERBrcfffd95y7dw3d3xShe+HChaxZs6be3RARERGRBmdmG4fbr/ISEREREZGUKXSLiIiIiKRMoVtEREREJGVNUdMtIiIi0szK5TLd3d0MDAzUuysNo1AoMG/ePLLZ7KjaK3SLiIiINLju7m46OjpYuHAhZlbv7kx47s7OnTvp7u5m0aJFozpH5SUiIiIiDW5gYIBp06YpcI8RM2PatGlH9M2BQreIiIhIE1DgHltH+vtU6E7Jk/+5jdte/Sv2be6vd1dEREREmsbTTz/NV77ylf3bX/ziF7n66quP+vV+8pOf8PrXv/6Y+6XQnZJSf5Xdz/RT3l2pd1dEREREmsbQ0H28UOhOSaY9BKC6r1rnnoiIiIjUX29vL6973etYtmwZZ5xxBrfddhsLFy7k4x//OOeddx4rV67k/vvv53d+53c46aSTuPHGG4H4psUPf/jDnHHGGZx55pncdttth9x/zTXXcNddd7F8+XI+97nPAbBlyxYuuugiFi9ezEc+8pH9ffrBD37Aeeedx4oVK3jLW95CT08PAHfccQennXYaL3vZy/jmN785Jtev2UtSEiahu9yjkW4RERE5fjz+gcfpWdszpq/ZvrydxZ9ffMg2d9xxB3PmzOG73/0uAHv27OGjH/0o8+fP5+677+ZP/uRPuOKKK/jFL37BwMAAS5cu5aqrruKb3/wma9eu5cEHH+S5557jJS95CRdccAG//OUvh91/7bXXct111/Gd73wHiMtL1q5dywMPPEA+n+fUU0/lve99Ly0tLXzqU5/izjvvpK2tjU9/+tN89rOf5SMf+Qh/9Ed/xI9//GNOPvlk3vrWt47J7yjVkW4zu8jMHjOzDWZ2zTDHTzOzu82saGYfqtl/qpmtrfnZa2YfSI590sw21xx7bZrXcLQCjXSLiIiI7HfmmWdy55138tGPfpS77rqLzs5OAN7whjfsP37OOefQ0dFBV1cXhUKB3bt38/Of/5zLLruMMAyZOXMmr3jFK1i9evWI+4dz4YUX0tnZSaFQYMmSJWzcuJF77rmHRx55hPPPP5/ly5fzpS99iY0bN/Loo4+yaNEiFi9ejJlx+eWXj8n1pzbSbWYhcD3waqAbWG1mt7v7IzXNdgHvA95Ye667PwYsr3mdzcC3app8zt2vS6vvYyHTHv9qFbpFRETkeHK4Eem0nHLKKdx3332sWrWKj33sY7zmNa8BIJ/PAxAEwf7ng9uVSgV3H/b1Rto/nNrXDcNw/+u++tWv5qtf/epBbdeuXZvKTC9pjnSfDWxw9yfdvQTcClxS28Ddt7v7aqB8iNe5EHjC3Tem19Wxl+lIRrp7FLpFREREtmzZQmtrK5dffjkf+tCHuP/++0d13gUXXMBtt91GtVplx44d/OxnP+Pss88ecX9HRwf79u077Ouee+65/OIXv2DDhg0A9PX18Zvf/IbTTjuNp556iieeeALgBaH8aKVZ0z0X2FSz3Q2ccxSvcykw9GqvNrO3A2uAP3X354eeZGZXAlcCLFiw4Cje9tiEbfHnGYVuEREREfj1r3/Nhz/8YYIgIJvNcsMNN/DmN7/5sOe96U1v4u6772bZsmWYGZ/5zGeYNWvWiPunTZtGJpNh2bJlXHHFFUyZMmXY1+3q6uKLX/wil112GcViEYBPfepTnHLKKdx000287nWvY/r06bzsZS9j3bp1x3z9diRD80f0wmZvAX7H3f9Hsv024Gx3f+8wbT8J9AwtGTGzHLAFWOru25J9M4HnAAf+Cpjt7n94qL6sXLnS16xZc+wXdQR2lUr8x1m/5OW/P48lf3nyuL63iIiISK3169dz+umn17sbDWe436uZ3efuK4e2TbO8pBuYX7M9jzhAH4mLgfsHAzeAu29z96q7R8A/EZexHHfMjLAtoLovqndXRERERKTO0gzdq4HFZrYoGbG+FLj9CF/jMoaUlpjZ7JrNNwHHPt6fAgPC1lDlJSIiIiKSXk23u1fM7Grg+0AI3OzuD5vZVcnxG81sFnFd9iQgSqYFXOLue82slXjmkz8e8tKfMbPlxOUlTw9z/LgRtIZUFLpFREREml6qi+O4+ypg1ZB9N9Y8f5a47GS4c/uAacPsf9sYdzM1YVtAZZ8WxxERERFpdloGPkVBS0C1VyPdIiIiIs1OoTsl8Y2UIZEWxxERERFpegrdKQpV0y0iIiIiKHSnKmgLNHuJiIiIyBDuThQ117TKCt0pCltD3UgpIiIiAjz99NOcfvrpvPvd72bFihW8613vYuXKlSxdupRPfOITANx777387u/+LgDf/va3aWlpoVQqMTAwwIknnljP7h+zVGcvaWYGBK0BXoKoHBFk9flGRERE6m/TwAD9YzzK3BIEzC8UDtvuscce41//9V/5x3/8R3bt2sXUqVOpVqtceOGFPPTQQ6xYsYIHHngAgLvuuoszzjiD1atXU6lUOOecc8a0z+NNoTtFQWsIQHVflWCqQreIiIg0txNOOIFzzz0XgK997WvcdNNNVCoVtm7dyiOPPMJZZ53FySefzPr167n33nv54Ac/yM9+9jOq1Sovf/nL69z7Y6PQnaJMa0gEVHuqZKdm690dERERkVGNSKelra0NgKeeeorrrruO1atXM2XKFK644goGBgYAePnLX873vvc9stksr3rVq7jiiiuoVqtcd911dev3WNDwa4qCtvjXW9W0gSIiIiL77d27l7a2Njo7O9m2bRvf+9739h+74IIL+PznP895551HV1cXO3fu5NFHH2Xp0qV17PGx00h3SuKa7ri8RDdTioiIiBywbNkyXvSiF7F06VJOPPFEzj///P3HzjnnHLZt28YFF1wAwFlnncWMGTMws3p1d0wodKcobAtw00i3iIiIyMKFC1m3bt3+7S9+8YvDtmtpaaFYLO7fvummm9Lu2rhQeUmKghaVl4iIiIiIQneqwrb4iwSFbhEREZHmptCdEgPC1mSkW6tSioiIiDQ1he4UBUlNt26kFBERkXpz93p3oaEc6e9ToTtFQS6AUOUlIiIiUl+FQoGdO3cqeI8Rd2fnzp0UjmDOc81ekiIzI9MeKnSLiIhIXc2bN4/u7m527NhR7640jEKhwLx580bdXqE7JYNzSYZtCt0iIiJSX9lslkWLFtW7G01N5SUpCzpC1XSLiIiINDmF7pSFHRrpFhEREWl2qYZuM7vIzB4zsw1mds0wx08zs7vNrGhmHxpy7Gkz+7WZrTWzNTX7p5rZD83s8eRxSprXcKyC9lBTBoqIiIg0udRCt5mFwPXAxcAS4DIzWzKk2S7gfcB1I7zMK919ubuvrNl3DfAjd18M/CjZPu5Y8qgbKUVEREQkzZHus4EN7v6ku5eAW4FLahu4+3Z3Xw2Uj+B1LwG+lDz/EvDGMehragKFbhEREZGml2bongtsqtnuTvaNlgM/MLP7zOzKmv0z3X0rQPI445h7mqKwXTdSioiIiDS7NKcMtGH2HcmM7Oe7+xYzmwH80MwedfefjfrN46B+JcCCBQuO4G3HVqiRbhEREZGml+ZIdzcwv2Z7HrBltCe7+5bkcTvwLeJyFYBtZjYbIHncPsL5N7n7Sndf2dXVdRTdPzaDnzjC9gAvOVEpGvc+iIiIiMjxIc3QvRpYbGaLzCwHXArcPpoTzazNzDoGnwOvAdYlh28H3pE8fwfw7THt9RgLOuIvEzSDiYiIiEjzSq28xN0rZnY18H0gBG5294fN7Krk+I1mNgtYA0wCIjP7APFMJ9OBbyWrOmaAr7j7HclLXwt8zczeBTwDvCWtaxgLYXsIQHVflezUbJ17IyIiIiL1kOoy8O6+Clg1ZN+NNc+fJS47GWovsGyE19wJXDiG3UxV2BaHbt1MKSIiItK8tCJlSg7UdB8Y6RYRERGR5qTQnbKgPf4VK3SLiIiINC+F7pSF7cmNlArdIiIiIk1LoTtlg+UlqukWERERaV4K3SlJZl4hHCwv0ZSBIiIiIk1LoTtlQZtupBQRERFpdgrdKQsKAYQK3SIiIiLNTKE7ZWZGpiOj0C0iIiLSxBS6U2I1z8POkMoe3UgpIiIi0qwUulPm7mQmZxS6RURERJqYQvc4yHRmqOxW6BYRERFpVgrd4yDTqZFuERERkWam0J2S2pruzOQM1T26kVJERESkWSl0p8xReYmIiIhIs1PoHgdhZ0hlbwV3r3dXRERERKQOFLrHQWZyBiItBS8iIiLSrBS6U3JQTXdnBkAlJiIiIiJNSqE7ZYM13YBmMBERERFpUgrd4yAzOQ7dmsFEREREpDkpdI8DlZeIiIiINLdUQ7eZXWRmj5nZBjO7Zpjjp5nZ3WZWNLMP1eyfb2b/ZWbrzexhM3t/zbFPmtlmM1ub/Lw2zWs4WmYHqrrDzhBQeYmIiIhIs8qk9cJmFgLXA68GuoHVZna7uz9S02wX8D7gjUNOrwB/6u73m1kHcJ+Z/bDm3M+5+3Vp9X0sOQfKSxS6RURERJpTmiPdZwMb3P1Jdy8BtwKX1DZw9+3uvhooD9m/1d3vT57vA9YDc1Psa6p0I6WIiIhIc0szdM8FNtVsd3MUwdnMFgIvAn5Vs/tqM3vIzG42synH1MtxEBQCLGuq6RYRERFpUmmGbhtm3xEtyWhm7cA3gA+4+95k9w3AScByYCvwtyOce6WZrTGzNTt27DiStx0TtRdvZmQmZzR7iYiIiEiTSjN0dwPza7bnAVtGe7KZZYkD95fd/ZuD+919m7tX3T0C/om4jOUF3P0md1/p7iu7urqO6gLGwuDS75nOjMpLRERERJpUmqF7NbDYzBaZWQ64FLh9NCdaPPXHvwDr3f2zQ47Nrtl8E7BujPqbqrAzVHmJiIiISJNKbfYSd6+Y2dXA94EQuNndHzazq5LjN5rZLGANMAmIzOwDwBLgLOBtwK/NbG3ykh9391XAZ8xsOXGpytPAH6d1DWMpM1kj3SIiIiLNKrXQDZCE5FVD9t1Y8/xZ4rKToX7O8DXhuPvbxrKPaRna+eyULL1beuvSFxERERGpL61ImbLBO0czUzOUd5YP2VZEREREGpNC9zjJTstS2VXZf2OliIiIiDQPhe5xkpmawStOdZ+mDRQRERFpNgrdKYknYDkgOy0LQHmXSkxEREREmo1Cd8oGi0myU+PQXdmpGUxEREREmo1C9zjJTIsnitFIt4iIiEjzUegeJ4Mj3ZrBRERERKT5KHSPk8Ga7soulZeIiIiINBuF7hQZNfN0T0nKSzTSLSIiItJ0FLrHSZALCDtCjXSLiIiINCGF7nGkVSlFREREmpNC9zjKTstq9hIRERGRJqTQnSKDg5Z9z07Nap5uERERkSak0D2OMtMyGukWERERaUIK3eMoOzWrmm4RERGRJqTQPY4yUzNUnq/gkR++sYiIiIg0DIXuFNXO0w2Qm5GDCCrPq65bREREpJkodI+j7Ix4VcrStlKdeyIiIiIi40mhexzlZuQAKG1X6BYRERFpJgrd42hwpLu8XTdTioiIiDQThe4UmdkLa7rRSLeIiIhIs0k1dJvZRWb2mJltMLNrhjl+mpndbWZFM/vQaM41s6lm9kMzezx5nJLmNYyl7LQsBBrpFhEREWk2qYVuMwuB64GLgSXAZWa2ZEizXcD7gOuO4NxrgB+5+2LgR8n2hGChkZ2e1Ui3iIiISJNJc6T7bGCDuz/p7iXgVuCS2gbuvt3dVwNDh34Pde4lwJeS518C3phS/1ORm5HTSLeIiIhIk0kzdM8FNtVsdyf7jvXcme6+FSB5nHGM/UzN0Hm6Ib6ZUiPdIiIiIs1lVKHbzN5vZpMs9i9mdr+ZveZwpw2zb7RLMR7LufELmF1pZmvMbM2OHTuO5NRU5WbkKG/TSLeIiIhIMxntSPcfuvte4DVAF/BO4NrDnNMNzK/ZngdsGeX7HercbWY2GyB53D7cC7j7Te6+0t1XdnV1jfJt06eRbhEREZHmM9rQPTjy/FrgX939QYYfja61GlhsZovMLAdcCtw+yvc71Lm3A+9Inr8D+PYoX/O4kJuZo7q3SnWgWu+uiIiIiMg4yYyy3X1m9gNgEfAxM+sAokOd4O4VM7sa+D4QAje7+8NmdlVy/EYzmwWsASYBkZl9AFji7nuHOzd56WuBr5nZu4BngLccwfWOKwPcD66K2b9Azo4y4fywDr0SERERkfE22tD9LmA58KS795nZVOISk0Ny91XAqiH7bqx5/ixx6ciozk327wQuHGW/jzv7F8jZVqIwv1Dn3oiIiIjIeBhtecl5wGPuvtvMLgf+HNiTXrcaV25WErqfVV23iIiISLMYbei+Aegzs2XAR4CNwC2p9aqB5eYkoXuLQreIiIhIsxht6K54XJx8CfB37v53QEd63WoMw83TnZuZA4PilmI9uiQiIiIidTDamu59ZvYx4G3Ay5Nl2rPpdatxBdkgnjZQI90iIiIiTWO0I91vBYrE83U/S7w65N+k1qsGl5+T10i3iIiISBMZVehOgvaXgU4zez0w4O6q6T5KuTk5jXSLiIiINJHRLgP/+8C9xHNi/z7wKzN7c5odawRmNuza9RrpFhEREWkuo63p/jPgJe6+HcDMuoA7ga+n1bFGlpuTo7y9TFSOCLKjrfARERERkYlqtIkvGAzciZ1HcK4MkZ+TB48XyBERERGRxjfake47zOz7wFeT7bcyzGqRMjq1c3UX5mlVShEREZFGN6rQ7e4fNrPfA84nnn76Jnf/Vqo9awDDzdMNyUg3mqtbREREpFmMdqQbd/8G8I0U+9I09o90b1Z5iYiIiEgzOGToNrN9DD9Ya4C7+6RUetXgcjNyWNYY2DRQ766IiIiIyDg4ZOh2dy31ngILjPyCPANPK3SLiIiINAPNQJKi5OuAYY8VFhYoblRNt4iIiEgzUOiuk8IJBY10i4iIiDQJhe46KSwsUHq2RHWgWu+uiIiIiEjKFLrrpHBCPD938RmVmIiIiIg0OoXuFI00TzfEI90AAxtVYiIiIiLS6BS662R/6FZdt4iIiEjDSzV0m9lFZvaYmW0ws2uGOW5m9oXk+ENmtiLZf6qZra352WtmH0iOfdLMNtcce22a15CW3JwchBrpFhEREWkGo16R8kiZWQhcD7wa6AZWm9nt7v5ITbOLgcXJzznADcA57v4YsLzmdTYDtcvOf87dr0ur7+MhyAQU5hcYeEqhW0RERKTRpTnSfTawwd2fdPcScCtwyZA2lwC3eOweYLKZzR7S5kLgCXffmGJfU2FmI9Z0A7Sc3EL/hv5x64+IiIiI1EeaoXsusKlmuzvZd6RtLgW+OmTf1Uk5ys1mNmUsOlsPLae00PdY34gL6IiIiIhIY0gzdNsw+4amy0O2MbMc8AbgP2qO3wCcRFx+shX422Hf3OxKM1tjZmt27NhxBN0eW4eK062ntlLdU6W8vTxu/RERERGR8Zdm6O4G5tdszwO2HGGbi4H73X3b4A533+buVXePgH8iLmN5AXe/yd1XuvvKrq6uY7iMoxcC0SFGsVtPbQWg7zd949QjEREREamHNEP3amCxmS1KRqwvBW4f0uZ24O3JLCbnAnvcfWvN8csYUloypOb7TcC6se/62AjMiA5xvOWUFgD6HlPoFhEREWlkqc1e4u4VM7sa+D7xoO/N7v6wmV2VHL8RWAW8FtgA9AHvHDzfzFqJZz754yEv/RkzW05cufH0MMePGwGHHukuLChgeaP/N7qZUkRERKSRpRa6Adx9FXGwrt13Y81zB94zwrl9wLRh9r9tjLuZmtCM6iFCt4VGy8ktGukWERERaXBakTJFhysvgbiuW6FbREREpLEpdKcoIK6BOdSUgG1L2uh/vJ/qQHXc+iUiIiIi40uhO0WhxTMiHqrEpG1ZG0TQ94hGu0VEREQalUJ3igZ/uYcqMWk/qx2Angd7Uu+PiIiIiNSHQneKBke6DzWDSctJLQStAb0P9Y5Xt0RERERknCl0pygYLC85RBsLjbYz2uh5SCPdIiIiIo1KoTtF+8tLDjHSDXGJSc+DPYe84VJEREREJi6F7hSN5kZKiG+mrOysUNxcHI9uiYiIiMg4U+hO0WhupASYdM4kAPbeszfV/oiIiIhIfSh0pyiTjHRXDldesrydoCVg7y8VukVEREQakUJ3ikZbXhJkAzpWdrDnl3vGo1siIiIiMs4UulNkZoQcfqQbYNJLJ9Fzf49WphQRERFpQArdKcuYjSp0d760Ey87Pfdp6kARERGRRqPQnbJwlKF70rnxzZQqMRERERFpPArdKcuYHbamGyA3I0fLyS26mVJERESkASl0p2y05SUQ13Xv+eUeLZIjIiIi0mAUulN2JKF78gWTKW8v07e+L+VeiYiIiMh4UuhOWcaMKoxq9Hryb08GYPd/7U61TyIiIiIyvhS6U5ZN5uoujyJ0tyxqIX9Cnud//Hza3RIRERGRcaTQnbJsEP+KRxO6Aab89hR2/2Q3HqmuW0RERKRRKHSnbP9IdxSNqv3k355MZVeFfWv2pdktERERERlHqYZuM7vIzB4zsw1mds0wx83MvpAcf8jMVtQce9rMfm1ma81sTc3+qWb2QzN7PHmckuY1HKsjKS8BmPbaaVjG2PHNHWl2S0RERETGUWqh28xC4HrgYmAJcJmZLRnS7GJgcfJzJXDDkOOvdPfl7r6yZt81wI/cfTHwo2T7uJU5wtCdnZpl8oWT2fH1HZo6UERERKRBpDnSfTawwd2fdPcScCtwyZA2lwC3eOweYLKZzT7M614CfCl5/iXgjWPY5zFnZmTNKI2yvASg681dDDwxQM+DWhJeREREpBGkGbrnAptqtruTfaNt48APzOw+M7uyps1Md98KkDzOGNNep6AQBAwcQeiefsl0CGDH11ViIiIiItII0gzdNsy+ofUSh2pzvruvIC5BeY+ZXXBEb252pZmtMbM1O3bUN7wWgoDeKGJHqTSq9rmuHJN/azI7/kMlJiIiIiKNIM3Q3Q3Mr9meB2wZbRt3H3zcDnyLuFwFYNtgCUryuH24N3f3m9x9pbuv7OrqOsZLOTa5pK77mWJx1OfMuHQG/b/pZ99qzWIiIiIiMtGlGbpXA4vNbJGZ5YBLgduHtLkdeHsyi8m5wB5332pmbWbWAWBmbcBrgHU157wjef4O4NspXsOYmJLN7n8+UK2O6pwZb51B0BKw9V+2ptUtERERERknqYVud68AVwPfB9YDX3P3h83sKjO7Kmm2CngS2AD8E/DuZP9M4Odm9iBwL/Bdd78jOXYt8Gozexx4dbJ9XMsHAae3tgLwcF8f0ShKRjKTMnS9pYvtX91OtXd0QV1EREREjk/WDDXDK1eu9DVr1hy+Ycoe7u3df0Pl6a2ttIbhIdvv+cUeHnjZAyz+h8XMfc/Qe1BFRERE5HhjZvcNme4a0IqU42phobD/+b5RlJlMeukkJr10Es/8zTNE5dHPfiIiIiIixxeF7nFUCA78uiuj+IbBzFhwzQKKG4ts+7dtaXZNRERERFKk0D2OQjswQ2L/KG+onPa6aXSc08GTH3+Syt5KWl0TERERkRQpdI+zZW1tdIYhe6pVni+XKUYRPZWRw7QFxuK/X0x5W5mNf7VxHHsqIiIiImNFoXucZYKAWbkcAE8ODLCut5fH+vupHqLcZNJLJjHrD2fR/flueh/tHa+uioiIiMgYUeiug/ZMhpNbWqidu+Q3fX3sLpdHPOfE/30iQWvA4+95HK82/owzIiIiIo1EobtOOpPgHQAG9EURTwwMUImGn6UkNzPHyZ89md0/3s2GP90wrn0VERERkWOTqXcHmll7JsPy9naq7jxbKrGtXGZHuczsfH7Y9rPfNZvedb10f76btqVtzPmjOePcYxERERE5GhrprjMzIxMEzCsU6AhDdpbLuDsjLVp04t+cyJTfmcLj736c3T/bPb6dFREREZGjotB9HJmWzVJ0Z21PD4/39w/bJsgELLl1CYWTCqx74zr23bdvnHspIiIiIkdKofs4MiUTV/tExCtWjjSVYHZylrPuOItMZ4a1F65lzy/3jGMvRURERORIKXQfRwIzFre00BHG85psLZXYV6mwq1zmif5+3J3+apXInZaFLSz/6XKy07OsfcVanvrEU1QHRrfgjoiIiIiML4Xu48ykTIZTWluZm8uxt1rlN/39PDUwwO5KhX3VKo/09bFxYACAwoICL179YmZcOoONf7mRta9YS2lHqc5XICIiIiJDKXQfp2bl88zN5Ziby5FLlo8frPPeVVN2kp2S5fR/O52l31hK70O9rFm+hh3f2DHijZgiIiIiMv4Uuo9js/J5ZuXznNne/oJjGwcG9q9iWY4iWt4whRf9/EXkZuR4+M0P8+vX/5r+p4a/GVNERERExpdC9wRxRlsbJxUKnJDPUwgCniuXWd/by6aBAR7t6+Ox/n6KZ+ZZsXoFJ3/+ZPb8bA+rl6zmiWueoPScSk5ERERE6kmhe4LIBwGTs1mm53Kc3trKwkKBwIwd5TKWtNlZqVAJYPrVs3nR+pVMess0Nn1mE/csvIcnP/YkA1sHiEZY8VJERERE0mPNUPu7cuVKX7NmTb27karNxSLPluIRbQMG/1RP6w7Y+Fcb2X7rdp46AVqmZDnvLfOY+965ZNq1IKmIiIjIWDKz+9x95dD9GuluEFMzGcLkee3HqO4TjNO/fDpn/frFdH1wDpzVwoY/f4pfnfgrNv3tJqp9mmZQREREJG0a6W4gg3+W9/f0vOBY3oxicnzKuhL7PtHN8z98ntysHAs+toDZV84mLIQvOE9ERERERq8uI91mdpGZPWZmG8zsmmGOm5l9ITn+kJmtSPbPN7P/MrP1Zvawmb2/5pxPmtlmM1ub/Lw2zWuYSMwMM+PMtjaWt7ezor2dliAgZ0YuCFhUKFAIAvrPKrD0jjNZ/rPltJ7Wyob3b+BXJ/+KzTdsJiqp5ltERERkrKUWus0sBK4HLgaWAJeZ2ZIhzS4GFic/VwI3JPsrwJ+6++nAucB7hpz7OXdfnvysSusaJqpcEBAmAfz01lbObG/nlNZWpmazzM7lKEYR6/v66D+7hZl3nM6yHy2jcEKBx9/9+IHwXVT4FhERERkraY50nw1scPcn3b0E3ApcMqTNJcAtHrsHmGxms919q7vfD+Du+4D1wNwU+9qwzOyg7anZLKe2tlJyZ0upxJZSiSdfkuG0n57FGXecSX5ensff/Tj3nHQP3X/fraXlRURERMZAmqF7LrCpZrubFwbnw7Yxs4XAi4Bf1ey+OilHudnMpoxZj5tEWxhycksLJxYKzMxmAVjf38/Gl+aY/V9LOf3OM2k5sYUN79vAvYvvZfONmynvLNe51yIiIiITV5qh24bZN/SuzUO2MbN24BvAB9x9b7L7BuAkYDmwFfjbYd/c7EozW2Nma3bs2HGEXW98nZkMU7JZ5hUKnNzSsn//llKJLWfnmP3DJZxy55nk5uZ4/H8+zi9m/IIHLniAZ/7mGXof7dUy8yIiIiJHIM3Q3Q3Mr9meB2wZbRszyxIH7i+7+zcHG7j7NnevunsE/BNxGcsLuPtN7r7S3Vd2dXUd88U0ss5MhmVtbby4o4NTW1qIgM2lElvPzrH8ly/i1HuW0frJeWyLyjzxkSdZffpqfjn7lzzx4SfYt3YfHimAi4iIiBxKmqujrAYWm9kiYDNwKfAHQ9rcTlwqcitwDrDH3bdaXIj8L8B6d/9s7QmDNd/J5puAdSleQ9PIBPHnr/ZMhrm5HJuThXbW9fVRXZIhXDKTrg/MhG1l9vxiDzt/uofy5zex6bpNZKdnmfzKyUz+rfin9fTWF9SSi4iIiDSzVOfpTqbz+zwQAje7+1+b2VUA7n5jEq7/AbgI6APe6e5rzOxlwF3Ar4HBaTQ+7u6rzOzfiEtLHHga+OOaED6sZpmneyxV3emrVtleLtNbrTI/n6ea3HxZTv7O5HdH7LjrecKf9lD68V76ny3R3gO5GVk6z++k7aw22s6If1pPVRAXERGRxjfSPN1aHEeOWDGKeLyvb/9iOxAvzFPcXKK4poeeX+0lc08fvc8MEFYgrMLMaQWmvGYKHS/uoOPFHbQuadViPCIiItJwRgrdaZaXSIPKBwFL2tqouhOa0VetUnFn98IsO+fl6XzjNACigYj+jf30retj4Lt7eeI/t1G6dStusK8DZgUZZp7aQetprbSc1ELLyfFPYWGBIJfquk0iIiIi40qhW45KYEaQlIu0Z+K/Rp2ZDDOiiEIQUHYn3xFQnNbBI6f2Ev1eF+2RU9xcpG99H5lnBti3ucS2x/so/XQPrV+LiALIlWD6LghPytN5QiuFkwsUTm6hb3GOyQtamL6glcyUjEpVREREZEJR6JYxY2a0hnHJSD4Jxfkg4LTWVgaiiMCMnpPy7Jhf4MSWFjb09+NJ256dJYqbigxsKrKve4Bid5HnNpTouXsv/T8+sEBP1w6odhqTpxaY0pWnZX6e1nkFCgsK5OfnKcyPH8NWla6IiIjI8UOhW1LXEoa0JGG8M5Nhbj4PwJltbYTJiPlAS5XMfGNHuczOcpm2MKS3WqXoTmVPhYHuIqWtRUrPlilvK7FrW4ltz5YoPdyD/7hCphK/VxBBYQAKHSHZ+XlaZ+WZ0VWgdX4L+QV5Wk5sofW0VjKd+qsvIiIi40fJQ+omGxyo2y4koXx2Ps/sJJRDfIPm9nyZztmTyAUB/VFETzUe+Y7cGYgi9g1UiLaXqW4ts+fZfvq2lSg+W6J3W4nt2/r5zYY9hN+NaOmH9h6ohtA/CYqTA6odAdlsQC4T4B0BbfkMk1qzZGZkmTW5QDA5QzA5ZMvkiBOntDF5fgu5WTk8YH95jYiIiMjhKHTLcc3MmJnL7d9uC0PawiGlIy3AFODUeKrDYhThxIG9p1qlP4rYu6/Evi3FuIRl4wCt+6q090cUSxE+EJHtiegrVdnbW2XXpiLltWXW9UUHvc3j/ZAvwp7JEGaMaZUQpoVkJ2fJTcowpTWLTc2wdwrMyeRomZYj15WDaRnaO7IEhQArGL15p7M1Cy0B5YD9JTkiIiLSuBS6paGENXXlcOAmT1paKE6P6FlapSUIaAkCLJl5JWdGJggoRhHPDAwQmpE1o7+/QrS3ysC+CqV9FfY9X6K6vUzLjhJecqp9Vap7KvTvrVLZU2Tztj4qaytU+yJ+M2QmzkwF3OLpE0u5mgMBtBGQbQ9pb80wqS1HdWbI3snG5GpIVy5L2JmhOjkkMymkpxNmTirQ0ZkjnBTSm3HKodOey9A+NfeCWV/2Virkg4B8oNlgRERE6kmhW5rGcOGzNqDng4DFra0HDhaIR9ATg1MkDlSrFMKQqjsG8U2iwPZymdYgIBsEPLevyI7nBmB3lYHny7T0OcGA01+qEg1E9FaqdPRCtRixp1Kluq9Kz54Ku/dUqHQPED0Z8VzFeaynSjRwcII3P1C73tt2YF+mAtHkgKAloDUbkm/J0FtwCGBWmKOzLUNuchamZqi0GoVMQKXFaG/L4u0hU9qzhO0h2UkZwo6QTEeGsD3EQiNyT62cpr9apZB8CBIREWlUCt0ioxQmoXCw/nxwezC4L6gJ8J1TMpw0pW3E16pEEZnkA4C7U3WnN4qoutMSBOSCgL5qlb3VKpmqE+2LeHbPANN6jF17S5T3VdjXWyYsw/RiyGZKFPdVqOyuEPVFVPuqFPsiwmoc2LfuKdL9XB/Vh6pUeqrD9imsxvXuuVK8Xc5Cax9kM0bvrJDWTECpYOQqUAkgACYREhYCip0BLbmQfD6gLZehNZ9hYBIErSFRa0ChJaQv52Qx5uTy0BmSmxTS32I8YUVaMyELCwXyuQDLGFHGCLNGLhdiWcMyNupQXnUnAIV4ERE5rih0i9RBpmbE3czImNE5ZBS+I5OhY7A8phVmz4xH4RcM83qnu1NORt4ryUqhoRn5IKA3ufG0v1olY4ZHEJUiisUqNhBR7qlS7amyo6dEtbeK9UVEvVWsJ2LPQIVif5VMXxXvrdJZPBBkqzi9lYjqQJWwp0LPnhKVgYhqf1wnX+2PYJgFbzMVqAzzL8/qYa4rW44/BJRzUG01yMXXZDmjIwrwQsDAJCMIDc8YHdWAvR1ONgzoiAIymYBWC8i0hlQ74se2lgy0GMUcWGi0hAHZMCCfD8m0h1TaA/oyEZ3ZDIW2DEF7SCYfULT4foE2QsKsYTnDsvF7iIiIHI5Ct0gDCMz2z42eG3Js8MbTg25AzQMdB7dbNMzrujsRB0b1h6q6U3EnHwTxaH21SgDsrlToCEMqxYg9PWXC/ogpHrKrWqWvv4Ltq1LcVyEYcFrKRlRxdkRl+qpVphVDcmUoVSL6o4h9UZVsEbwUke13nqdKtRxRKTpecgp9Tlh2+izieavi+5xypcLAgFOtOtVKRDTgRH1VotIwnwJGwTyuyR9OrgxhzvDW+MNASzWAvEEhIMwHBBmwbDyCnw8CMqHFo/fZgOc7nFYCspmAXGDkM/EHgFIeSnnIh/FIfyUXT73ZngkoZEKCtoBqe0BvxmnNhbRnM1jOyGUDqjn2f0MQ5pNvCnKGhaP/tmCQJ3++2ZpvZfQNgojI0VHoFpERmRmHmlslNNsfyEMzJiUj8/tvYM3CtPYDU0BOOsRrnXKYgD9opPKRwTKdTBDsr0F3d/qjaH//BspVSr1VSn0V2j3EKxGlckR/pUrvQBV6q1hvRKYCz1bLVPqrDPRXaS0aHSWjGkAYGH1RxJ6owtS+uAyor1Il1wtWihhwJ6o4xUpE2BPRR0RYdMLeKvuIPwh4xfFy/Ph8xSF5HpUjPJlz/lBBfzQGa//DpJrIDaICtEQBUd4gG98wTC5+Ti4ga3HAz5hRzcXfHngWchiVZDuP0eoBFhrZwKhm45uByzmjGjpBaJRycemRZyAIjUImhHxANQ+TMxl6WmFqNkuUN7xg9GWdQhDQGgT0B05HEBIGxr4gwg08gJI5rRYyKRuyL4yoZqA1G1LIhVQDyGfiPlnWqITx+1bdiapOzo0wN/x9A/3VKsaBsrGjMfTDiD6ciMhwFLpF5LhwuIA/aKRQPlimAwfmUK9dJRUgnw8gn4Wph3+fxYc5Pnhj7UgG6/b7qlXyQbC/7eD+wcfd5TKZpBTIgLI7QRSX4TzTO0C1HDE1CslU4PmBMjuL8YeB9n6jrWzsrVTxUkSmBD3VKkHJ6atGFEpOpeJUy46XI6wMlCJ6PKIYRQQlJyhBWHIoOZVKRF/otPXBgDkVIrzqWL9TrDhhCYoWERVhjzlejX+s7FQjoJLsi5LHCsOWF43G4IeFcvaFx0b6MBJWD3zAKA39ugcIQmjLhFRaDM8a+SguS+pvNSwDWYyWKKCtavS0QdaMKGuQiX8yGWOShUQFI5MNyOQDMOgPnd1BlbDfaanG3yjsa3XyFjCrkqGUg30FyAXG3GoWzxo9OceyUAhC9uYipuWy5DozPG9VWgho8wBz2OsVks9g+2/cbg9CwmxAJRt/AOz1KoFDGARMzWboL0CRCCJos5AwNPotoi0Tkg0DeiyiGDhdmQwDgZMJA6qB05oJqWTgyb4BukohUztyeGtArjXc/wFiuA8XzgvXLKj9hqwcRfGHuOQDsSflb8Oddyhp3swtMl4UukVEjsLhRuQH6/aHzsM+uH/wcXL24GSZAQiBLCxqOfhm3EnACUfd48Or/SBRjCIi9/2rycKB0NWbzDgTEIenvmqV1jAkMKMURVSShataggCrOKViRLlYJSw5z/eXoejsK1bIlyAqOZNKFn8YqFaZ5CF7vEoxipgShVgE/VFENoJ+HKs6VoGBapVq1aEM2SqUPYq/YYgiwhK0lIAAgsDoycLeUoVwb5WWIlgZei2iGjnmEZN7jCJOkYjncKg4/RUnU4qgL75folJxtpciolKEl8GL0f7fR/sAlNsDtmfAIyfX5xTNeSZyvApBFaIAHknxzw4O3Aw9GiPdWzHc8cAg61ApGPlq/CEkyhlkwHMBYRB/4C1ERqVglHMQRU6mBMXAoepQBapOlPzOgqwxdSAuuyq2Gq2RkQvj7UrBqOaNTGgMFMByAcUcdJbjD0v9GacUOpjRVQypTA7wrFEOnHIY3+TdGgW0RQZmeABVg4HQweIPbe0eUAri7QCjSvxhtxo5gUMhCCgX4pu52zIB5bzRmgup5CGTCwlzAdUs9AcR2SAgGxhlg9YwIAgMC+L/xgcsIgogFwZUzGkPQwYsnlWqNQwZsIhsGBBmAyxr7Ioq9O4tQ19EWIbOSkAuGxDkjFLWCHLxtzYtLXFJ2eBN5rXf9A39MFT7YakURWTN9n8e3lkuk43iD3MGYHF5YKsH5DMhljH6PV77oi0MKSUfoopRtP/fhqEfiKrulKPomL49Opza651IFLpFRAQ4+IPEcHO7D/6Pu/b+AKOmnAjIBQE5aj5shFA4UGHE5LHs8BEY6ZuJofsHQ8nQ8pByMrtQNgk1EfEHjpCDb4we/AZjoFqlN4qYFIZkMHqKFXYXy4RVaPWAoALFcpWwApt6B8juczqJb9itEgfd0OIwWAhCWglw4PlqmZ5ylY5KgEe+v51HsL1cIirG909Uk2AVRRGtVaMvcgY8olABItjrVfIViDz+gJWvGKVKRCEMqLYF9A9UKA1EZPviDyfFKMIqEJadSuQEZahWnSD5hqMSOeXIqVYjOvviEf9izslmjc5KQMbisqO9LQ7Jzdx9FeKyqkrEQMXxShT/9Du+L/5AFVUjvBx/o7Ivij9UuMcLlVVDeBan2lvFq/G3INly/FjM8wLHWrKVFksS8JH0LVuOvxEKIiAH5UJc/pVxqOaSb/yqcbuBjO9vWxn8TzfyuHSr4vtL2oZ7j7BK/MEnA1mPPwxZaBCAYQQGUQZyUfy8pWL0tcZlagTxB7EMRinnWBB/m5SxpNwrB6W8ETjko7gsrC/neGAUMLD4Q1ARp61oBEApE3/jE4VGOeNkkw9W+SoUIqOY9NMDyE3L8vK/OvXo/lBSotAtIiINb6RvJobuz40wcpYNAvZ/J3GIbzkGA3ghDA8a6etoydLRMky9DDCVkacXfWHbkaX5Lcho1U6HOpLBtQ4ij0e+K+77S8OceAamwVHUoaOm5Sgujxr8oLelWCRyZ16hsL90BYfeSpWeSoW2ICTE4kBOvK5CnoD+akQIhBg4hAF4EAdHN9hbrJAtQ3+xSrlYJVMyeosVwpJjpfgmbipOS2QUI6c/qpKLjF1UqEROGEEhCshXgQiKHpGpGr1Uaa0GFD1iLxFTywF9RPRWq1jJyVahpT3LzEKOUj4Oq/3VKD5WgqgYxTeZV6pUKk6lHMXlZRVjgIgiEe1FI1c1ogAGsk7W4+BbCaC1avSH8X0xGTfCwAjbAjw0iJxykATgDJRxSlFEterkyka1EtFLRLYMZYtvgvfIGTAnUwarOv0WMTnZTzX+s6zi5CtOGShWnd7IoVjBeyJyO6GYcSIDi+LZqgYy0BM5YYX4W5UslCIIIiNbiftYqvnwkC3DngAii59Xgvi1OqcOU2dWZwrdIiIiMiZG83X/YIAOzAjMXhBEaj+ahGaENR9esskCZIPm5A8Mae//dsKIV+nNvTDitA/zHsOZ1hKfe6ibv4cz3CxQcrCh9wZEyYclI/4zrP32aaT7BiD+QOY1N89HE6DcRKFbRERERMbF0NKtoYG69tsnM2Ok75VCs/3fOg1+gDveHd8fCUREREREGoBCt4iIiIhIyhS6RURERERSlmroNrOLzOwxM9tgZtcMc9zM7AvJ8YfMbMXhzjWzqWb2QzN7PHmckuY1iIiIiIgcq9RCt5mFwPXAxcAS4DIzWzKk2cXEC78tBq4EbhjFudcAP3L3xcCPkm0RERERkeNWmiPdZwMb3P1Jdy8BtwKXDGlzCXCLx+4BJpvZ7MOcewnwpeT5l4A3pngNIiIiIiLHLM3QPRfYVLPdnewbTZtDnTvT3bcCJI8zhntzM7vSzNaY2ZodO3Yc9UWIiIiIiByrNEP3cBMm+ijbjObcQ3L3m9x9pbuv7OrqOpJTRURERETGVJqL43QD82u25wFbRtkmd4hzt5nZbHffmpSibD9cR+67777nzGzjEfZ/LEwHnqvD+9aTrrk56Jqbg665Oeiam0MzXnO9nDDczjRD92pgsZktAjYDlwJ/MKTN7cDVZnYrcA6wJwnTOw5x7u3AO4Brk8dvH64j7l6XoW4zW+PuK+vx3vWia24OuubmoGtuDrrm5tCM13y8SS10u3vFzK4Gvg+EwM3u/rCZXZUcvxFYBbwW2AD0Ae881LnJS18LfM3M3gU8A7wlrWsQERERERkLaY504+6riIN17b4ba5478J7Rnpvs3wlcOLY9FRERERFJj1akTNdN9e5AHeiam4OuuTnompuDrrk5NOM1H1csHmwWEREREZG0aKRbRERERCRlCt0pMbOLzOwxM9tgZg2zVL2Z3Wxm281sXc2+qWb2QzN7PHmcUnPsY8nv4DEz+5369Promdl8M/svM1tvZg+b2fuT/Y18zQUzu9fMHkyu+S+S/Q17zYPMLDSzB8zsO8l2Q1+zmT1tZr82s7VmtibZ1+jXPNnMvm5mjyb/XZ/XyNdsZqcmf76DP3vN7AONfM0AZvYnyb9f68zsq8m/a41+ze9PrvdhM/tAsq+hr3nCcXf9jPEP8YwrTwAnEs85/iCwpN79GqNruwBYAayr2fcZ4Jrk+TXAp5PnS5JrzwOLkt9JWO9rOMLrnQ2sSJ53AL9JrquRr9mA9uR5FvgVcG4jX3PNtX8Q+ArwnWS7oa8ZeBqYPmRfo1/zl4D/kTzPAZMb/Zprrj0EniWeQ7hhr5l4BeungJZk+2vAFQ1+zWcA64BW4kky7gQWN/I1T8QfjXSn42xgg7s/6e4l4Fbgkjr3aUy4+8+AXUN2X0L8PzKSxzfW7L/V3Yvu/hTx1JBnj0c/x4q7b3X3+5Pn+4D1xP+gN/I1u7v3JJvZ5Mdp4GsGMLN5wOuAf67Z3dDXPIKGvWYzm0Q8cPAvAO5ecvfdNPA1D3Eh8IS7b6TxrzkDtJhZhjiIbqGxr/l04B5373P3CvBT4E009jVPOArd6ZgLbKrZ7k72NaqZ7r4V4pAKzEj2N9TvwcwWAi8iHvlt6GtOyizWEq/4+kN3b/hrBj4PfASIavY1+jU78AMzu8/Mrkz2NfI1nwjsAP41KSP6ZzNro7GvudalwFeT5w17ze6+GbiOeC2PrcQL7/2ABr5m4lHuC8xsmpm1Eq+BMp/GvuYJR6E7HTbMvmacJqZhfg9m1g58A/iAu+89VNNh9k24a3b3qrsvB+YBZ5vZGYdoPuGv2cxeD2x39/tGe8ow+ybUNSfOd/cVwMXAe8zsgkO0bYRrzhCXx93g7i8Ceom/ch9JI1wzAGaWA94A/Mfhmg6zb0Jdc1K3fAlx2cQcoM3MLj/UKcPsm1DX7O7rgU8DPwTuIC4dqRzilAl/zRORQnc6uok/YQ6aR/zVVqPaZmazAZLH7cn+hvg9mFmWOHB/2d2/mexu6GselHz1/hPgIhr7ms8H3mBmTxOXg/22mf07jX3NuPuW5HE78C3ir5cb+Zq7ge7kmxuArxOH8Ea+5kEXA/e7+7Zku5Gv+VXAU+6+w93LwDeBl9LY14y7/4u7r3D3C4jLQB+nwa95olHoTsdqYLGZLUpGFy4Fbq9zn9J0O/CO5Pk7gG/X7L/UzPJmtoj4po5769C/o2ZmRlz/ud7dP1tzqJGvucvMJifPW4j/B/YoDXzN7v4xd5/n7guJ/3v9sbtfTgNfs5m1mVnH4HPgNcRfUTfsNbv7s8AmMzs12XUh8AgNfM01LuNAaQk09jU/A5xrZq3Jv+EXEt+P08jXjJnNSB4XAL9L/Ofd0Nc84dT7Ts5G/SGup/oN8R3Bf1bv/ozhdX2VuEauTPxJ+V3ANOBHxJ+qfwRMrWn/Z8nv4DHg4nr3/yiu92XEX7k9BKxNfl7b4Nd8FvBAcs3rgP8v2d+w1zzk+n+LA7OXNOw1E9c3P5j8PDz471QjX3NyDcuBNcnf7/8HTGmCa24FdgKdNfsa/Zr/gniwYB3wb8SzdDT6Nd9F/CHyQeDCZvhznmg/WpFSRERERCRlKi8REREREUmZQreIiIiISMoUukVEREREUqbQLSIiIiKSMoVuEREREZGUKXSLiMiomdlvmdl36t0PEZGJRqFbRERERCRlCt0iIg3IzC43s3vNbK2Z/V8zC82sx8z+1szuN7MfmVlX0na5md1jZg+Z2bfMbEqy/2Qzu9PMHkzOOSl5+XYz+7qZPWpmX05W/cPMrjWzR5LXua5Oly4iclxS6BYRaTBmdjrwVuB8d18OVIH/DrQB97v7CuCnwCeSU24BPuruZwG/rtn/ZeB6d18GvJR4NVqAFwEfAJYQr2x5vplNBd4ELE1e51NpXqOIyESj0C0i0nguBF4MrDaztcn2iUAE3Ja0+XfgZWbWCUx2958m+78EXGBmHcBcd/8WgLsPuHtf0uZed+929whYCywE9gIDwD+b2e8Cg21FRASFbhGRRmTAl9x9efJzqrt/cph2fpjXGEmx5nkVyLh7BTgb+AbwRuCOI+uyiEhjU+gWEWk8PwLebGYzAMxsqpmdQPxv/puTNn8A/Nzd9wDPm9nLk/1vA37q7nuBbjN7Y/IaeTNrHekNzawd6HT3VcSlJ8vH/KpERCawTL07ICIiY8vdHzGzPwd+YGYBUAbeA/QCS83sPmAPcd03wDuAG5NQ/STwzmT/24D/a2Z/mbzGWw7xth3At82sQDxK/idjfFkiIhOauR/q20UREWkUZtbj7u01278F/Lu7zxuhvQOL3X3D+PRQRKRxqbxERERERCRlCt0iIk2idpRbRETGl0K3iMgEZmbXmNnXh+z7OzP7QvL8nWa23sz2mdmTZvbHR/k+nWZ2i5ntMLONZvbnSb344CI6PzWzPWb2nJndluw3M/ucmW1Pjj1kZmcc6zWLiExEupFSRGRi+yrw/5nZJHffa2Yh8PvEC9UAbAdeT3yD5AXA98xstbvff4Tv8/dAJ/F839OAHxAvlvMvwF8l268EcsDK5JzXJO95CvGNm6cBu4/iGkVEJjyNdIuITGDuvhG4n3hubIDfBvrc/Z7k+Hfd/QmP/ZQ4HL982BcbQRLk3wp8zN33ufvTwN8Sz24C8cwmJwBzkkV0fl6zv4M4bJu7r3f3rYiINCGFbhGRie8rwGXJ8z9ItgEws4vN7B4z22Vmu4HXAtOP8PWnE49gb6zZtxGYmzz/CPE0gfea2cNm9ocA7v5j4B+A64FtZnaTmU06wvcWEWkICt0iIhPffwC/ZWbziMtKvgLxgjbEK0ReB8x098nAKg692uRwnuPAaPagBcBmAHd/1t3/yN3nAH8M/KOZnZwc+4K7vxhYSlxm8uGjukIRkQlOoVtEZIJz9x3AT4B/BZ5y9/XJoRyQB3YAFTO7mLjO+khfvwp8DfhrM+tIVrf8IPDvAGb2liTwAzxPvLx81cxeYmbnmFmWeGGeAeJl40VEmo5Ct4hIY/gK8CpqSkvcfR/wPuLA/Dxx6cntR/n67yUOzk8CP0/e5+bk2EuAX5lZT/L673f3p4BJwD8l770R2Ek86i4i0nS0IqWIiIiISMo00i0iIiIikjKFbhERERGRlCl0i4iIiIikTKFbRERERCRlTbEM/PTp033hwoX17oaIiIiINLj77rvvOXfvGrq/KUL3woULWbNmTb27ISIiIiINzsw2Drdf5SUiIiIiIilT6BYRERERSZlCt4iIiIhIypqipltERESkmZXLZbq7uxkYGKh3VxpGoVBg3rx5ZLPZUbVX6BYRERFpcN3d3XR0dLBw4ULMrN7dmfDcnZ07d9Ld3c2iRYtGdY7KS0REREQa3MDAANOmTVPgHiNmxrRp047omwOFbhEREZEmoMA9to7096nQnZL+nSW2/HwXlWK13l0RERERkTpT6E7JU6t28J/veoiebt2wICIiIjJenn76ab7yla/s3/7iF7/I1VdffdSv95Of/ITXv/71x9yvuoRuM7vIzB4zsw1mds0wx83MvpAcf8jMVtQce7+ZrTOzh83sA+Pa8SOQmx7fo1raXqpzT0RERESax9DQfbwY99BtZiFwPXAxsAS4zMyWDGl2MbA4+bkSuCE59wzgj4CzgWXA681s8Th1/Yhku3IAlLaX69wTERERkfrr7e3lda97HcuWLeOMM87gtttuY+HChXz84x/nvPPOY+XKldx///38zu/8DieddBI33ngjEM8U8uEPf5gzzjiDM888k9tuu+2Q+6+55hruuusuli9fzuc+9zkAtmzZwkUXXcTixYv5yEc+sr9PP/jBDzjvvPNYsWIFb3nLW+jp6QHgjjvu4LTTTuNlL3sZ3/zmN8fk+usxZeDZwAZ3fxLAzG4FLgEeqWlzCXCLuztwj5lNNrPZwOnAPe7el5z7U+BNwGfG8wJGI9cVz9lY2qGRbhERETl+PP6Bx+lZ2zOmr9m+vJ3Fnz/0OOgdd9zBnDlz+O53vwvAnj17+OhHP8r8+fO5++67+ZM/+ROuuOIKfvGLXzAwMMDSpUu56qqr+OY3v8natWt58MEHee6553jJS17CBRdcwC9/+cth91977bVcd911fOc73wHi8pK1a9fywAMPkM/nOfXUU3nve99LS0sLn/rUp7jzzjtpa2vj05/+NJ/97Gf5yEc+wh/90R/x4x//mJNPPpm3vvWtY/I7qkd5yVxgU812d7JvNG3WAReY2TQzawVeC8wf7k3M7EozW2Nma3bs2DFmnR+t3PQkdKu8RERERIQzzzyTO++8k49+9KPcdddddHZ2AvCGN7xh//FzzjmHjo4Ourq6KBQK7N69m5///OdcdtllhGHIzJkzecUrXsHq1atH3D+cCy+8kM7OTgqFAkuWLGHjxo3cc889PPLII5x//vksX76cL33pS2zcuJFHH32URYsWsXjxYsyMyy+/fEyuvx4j3cPNr+KjaePu683s08APgR7gQaAy3Ju4+03ATQArV64c+vqpC3IBmfaQ8vZhuyciIiJSF4cbkU7LKaecwn333ceqVav42Mc+xmte8xoA8vk8AEEQ7H8+uF2pVIgLH15opP3DqX3dMAz3v+6rX/1qvvrVrx7Udu3atalMr1iPke5uDh6dngdsGW0bd/8Xd1/h7hcAu4DHU+zrUTMgMzVD8TmNdIuIiIhs2bKF1tZWLr/8cj70oQ9x//33j+q8Cy64gNtuu41qtcqOHTv42c9+xtlnnz3i/o6ODvbt23fY1z333HP5xS9+wYYNGwDo6+vjN7/5DaeddhpPPfUUTzzxBMALQvnRqsdI92pgsZktAjYDlwJ/MKTN7cDVSb33OcAed98KYGYz3H27mS0Afhc4b/y6fmQyU7OUt+lGShEREZFf//rXfPjDHyYIArLZLDfccANvfvObD3vem970Ju6++26WLVuGmfGZz3yGWbNmjbh/2rRpZDIZli1bxhVXXMGUKVOGfd2uri6++MUvctlll1EsFgH41Kc+xSmnnMJNN93E6173OqZPn87LXvYy1q1bd8zXb0cyND9WzOy1wOeBELjZ3f/azK4CcPcbLR7T/wfgIqAPeKe7r0nOvQuYBpSBD7r7jw73fitXrvQ1a9akci0j2VOpcMd71zH77iIXrD1nXN9bREREpNb69es5/fTT692NhjPc79XM7nP3lUPb1mOkG3dfBawasu/GmucOvGeEc1+ebu/GTnZqhtJzY3t3sIiIiIhMPFqRMiUGZKdmKe+qEFWiendHREREROpIoTtFmakZcCg/p7puERERkWam0J0SA7JT4rm6y1qVUkRERKSpKXSnKDMtgxuUtmnaQBEREZFmptCdEo10i4iIiMgghe4UZSbHI93lnQrdIiIiIs1MoTtFYXsICt0iIiIiB3F3oqi5ZndT6E6JmWGhEU4JFbpFRESk6T399NOcfvrpvPvd72bFihW8613vYuXKlSxdupRPfOITANx777387u/+LgDf/va3aWlpoVQqMTAwwIknnljP7h+zuiyO00yyU7NUdlbq3Q0RERERADYNDNA/xqPMLUHA/ELhsO0ee+wx/vVf/5V//Md/ZNeuXUydOpVqtcqFF17IQw89xIoVK3jggQcAuOuuuzjjjDNYvXo1lUqFc86Z2Ct8K3SnxJLHzJSsRrpFREREgBNOOIFzzz0XgK997WvcdNNNVCoVtm7dyiOPPMJZZ53FySefzPr167n33nv54Ac/yM9+9jOq1Sovf/mEWZR8WArdKQunhZSfVugWERGR48NoRqTT0tbWBsBTTz3Fddddx+rVq5kyZQpXXHEFAwMDALz85S/ne9/7Htlslle96lVcccUVVKtVrrvuurr1eyyopjslgyPd2aka6RYRERGptXfvXtra2ujs7GTbtm1873vf23/sggsu4POf/zznnXceXV1d7Ny5k0cffZSlS5fWscfHTiPdKctMzdKrZeBFRERE9lu2bBkvetGLWLp0KSeeeCLnn3/+/mPnnHMO27Zt44ILLgDgrLPOYsaMGZjZSC83ISh0p2T/SPeUDFFvRFSMCPL6YkFERESa08KFC1m3bt3+7S9+8YvDtmtpaaFYLO7fvummm9Lu2rhQCkxZMDX+XKMSExEREZHmpdCdksGvQLJTk6XgFbpFREREmlZdQreZXWRmj5nZBjO7ZpjjZmZfSI4/ZGYrao79iZk9bGbrzOyrZla/W3BHIaORbhERETkOuHu9u9BQjvT3Oe6h28xC4HrgYmAJcJmZLRnS7GJgcfJzJXBDcu5c4H3ASnc/AwiBS8ep60clMyUe6dYCOSIiIlIvhUKBnTt3KniPEXdn586dFI5g+sV63Eh5NrDB3Z8EMLNbgUuAR2raXALc4vHfjHvMbLKZzU6OZYAWMysDrcCW8ev66A3eSBlODQGNdIuIiEj9zJs3j+7ubnbs2FHvrjSMQqHAvHnzRt2+HqF7LrCpZrsbGLqu53Bt5rr7GjO7DngG6Ad+4O4/SLOzxyo7RTXdIiIiUl/ZbJZFixbVuxtNrR413cNNsjj0u45h25jZFOJR8EXAHKDNzC4f9k3MrjSzNWa2ph6f6vaPdBcCgpZAoVtERESkidUjdHcD82u25/HCEpGR2rwKeMrdd7h7Gfgm8NLh3sTdb3L3le6+squra8w6f6QcyM3KUd6m0C0iIiLSrOoRulcDi81skZnliG+EvH1Im9uBtyezmJwL7HH3rcRlJeeaWavFc/JdCKwfz86PVu1QfW5OjuKW4ohtRURERKSxjXtNt7tXzOxq4PvEs4/c7O4Pm9lVyfEbgVXAa4ENQB/wzuTYr8zs68D9QAV4ADiulylyID87T++63np3RURERETqpC7LwLv7KuJgXbvvxprnDrxnhHM/AXwi1Q6OgcHFcSAe6d71g1117I2IiIiI1JNWpEyZA/k5eap7q1R6NFe3iIiISDNS6E7J0JpugNLWUn06IyIiIiJ1pdCdMncnPycPQGmLQreIiIhIM1LoHgeDI92awURERESkOSl0p2SwvGSwphs00i0iIiLSrBS6x0E4KSRoDTTSLSIiItKkFLpTUjtloJmRn5PXSLeIiIhIk1LoTpknj1qVUkRERKR5KXSnqHbaQI10i4iIiDQvhe6UDR3pjhfbFBEREZFmotA9TvJz8kR9EdW91Xp3RURERETGmUJ3igz2j2xrrm4RERGR5qXQPU40V7eIiIhI81LoTpFxcE03aKRbREREpBkpdI+T3Ow4dGukW0RERKT5KHSnyMz2j3Rn2jOEk0KNdIuIiIg0IYXucaS5ukVERESaU11Ct5ldZGaPmdkGM7tmmONmZl9Ijj9kZiuS/aea2dqan71m9oFxv4BRsiHbWpVSREREpDmNe+g2sxC4HrgYWAJcZmZLhjS7GFic/FwJ3ADg7o+5+3J3Xw68GOgDvjVOXT8qtUvhaKRbREREpDnVY6T7bGCDuz/p7iXgVuCSIW0uAW7x2D3AZDObPaTNhcAT7r4x/S6PDa1KKSIiItKc6hG65wKbara7k31H2uZS4Ktj3rsxVLs4DkB+bh4vOeXnyvXrlIiIiIiMu3qE7qGlznBwFcZh25hZDngD8B8jvonZlWa2xszW7Nix46g6OtbyC+IFcorPqK5bREREpJnUI3R3A/NrtucBW46wzcXA/e6+baQ3cfeb3H2lu6/s6uo6xi4fndrFcQAKCwoADDwzUJf+iIiIiEh91CN0rwYWm9miZMT6UuD2IW1uB96ezGJyLrDH3bfWHL+M47y0ZDj5+clI9yaNdIuIiIg0k8x4v6G7V8zsauD7QAjc7O4Pm9lVyfEbgVXAa4ENxDOUvHPwfDNrBV4N/PF49/1I1S6OA5CdniUoBBrpFhEREWky4x66Adx9FXGwrt13Y81zB94zwrl9wLRUO5gSMyO/IK+abhEREZEmoxUpUzS0phvium6NdIuIiIg0F4XucZafn1dNt4iIiEiTUehO0XDzHuYX5CltLRGVonHvj4iIiIjUh0J3yoauPllYUACH4maNdouIiIg0C4XucbZ/gRyVmIiIiIg0DYXuFA17I+V8LZAjIiIi0mwUusfZ/gVyNG2giIiISNNQ6E7R0MVxAMLWkOz0rMpLRERERJqIQneKhisvgXi0W+UlIiIiIs1DoTtFoRlVf2Hs1qqUIiIiIs1FoTtFGTMqw4RurUopIiIi0lwUulMUwogj3dW9VSp7KuPfKREREREZdwrdKcqYEQHR0AVyBqcN3KTRbhEREZFmoNCdotDiheCHjnbvXyBHdd0iIiIiTUGhO0WZJHQPresuLNACOSIiIiLNRKE7RZkRRrpzs3JYxjTSLSIiItIkFLpTFI4w0m2hkV+QZ+BpjXSLiIiINIO6hG4zu8jMHjOzDWZ2zTDHzcy+kBx/yMxW1BybbGZfN7NHzWy9mZ03vr0fvf013cMcKywsKHSLiIiINIlxD91mFgLXAxcDS4DLzGzJkGYXA4uTnyuBG2qO/R1wh7ufBiwD1qfe6aNkyaMPN1f3CQUGNip0i4iIiDSDeox0nw1scPcn3b0E3ApcMqTNJcAtHrsHmGxms81sEnAB8C8A7l5y993j2PcjMvjLjYY5VlhYoLSlRFQc7qiIiIiINJJ6hO65wKaa7e5k32janAjsAP7VzB4ws382s7bh3sTMrjSzNWa2ZseOHWPX+yNgSXnJC8e549ANmsFEREREpBnUI3TbMPuG5tKR2mSAFcAN7v4ioBd4QU04gLvf5O4r3X1lV1fXsfT3qO0f6R6hvARQiYmIiIhIE6hH6O4G5tdszwO2jLJNN9Dt7r9K9n+dOIQfl0Y10q2bKUVEREQaXj1C92pgsZktMrMccClw+5A2twNvT2YxORfY4+5b3f1ZYJOZnZq0uxB4ZNx6fhQChh/pzs3NQajQLSIiItIMMuP9hu5eMbOrge8DIXCzuz9sZlclx28EVgGvBTYAfcA7a17ivcCXk8D+5JBjxx1j+JHuIBOQn6e5ukVERESawbiHbgB3X0UcrGv33Vjz3IH3jHDuWmBlmv0bS4HZsLOXALSe0krf+r5x7Y+IiIiIjD+tSJkyY/h5ugHalrbRt74Pj4Y/LiIiIiKNQaE7ZYcc6V7aStQfMfCUSkxEREREGplCd8pGqumGeKQboPfh3nHrj4iIiIiMP4XulI00ewlA2xKFbhEREZFmoNCdMjMbcaQ705khPy+v0C0iIiLS4BS6UxYA+6pV7tu3j2L0wuru1qWt9D2sGUxEREREGplCd8pq17PvrVZfcLx9eTu9D/cSFUe63VJEREREJjqF7pQFZoc8PunsSXjZ6XmwZ5x6JCIiIiLj7ZhDt5m938wmJUu2/4uZ3W9mrxmLzjWCQ0du6Di7A4C99+xNvzMiIiIiUhdjMdL9h+6+F3gN0EW8LPu1Y/C6DaF2pHu4WUzyc/MUFhV4/s7nx7NbIiIiIjKOxiJ0D6bK1wL/6u4PcvgB3qZR+4t4YUV3PLvJ1Iun8vyPnqc6MFwLEREREZnoxiJ032dmPyAO3d83sw4YcRHGplP7Cx5pvu6pr5lK1Bexb82+8emUiIiIiIyrsQjd7wKuAV7i7n1AlrjERIBccOBXvKVUYk+l8oI2k146CYC9v1Bdt4iIiEgjGovQfR7wmLvvNrPLgT8H9ozB6zaEQnDwr3hLsfiCNrmuHC2ntrDnF/q1iYiIiDSisQjdNwB9ZrYM+AiwEbhlDF63IQwN3SMVu3ee38meX+zBo5HWrxQRERGRiWosQnfF3R24BPg7d/87oGMMXrch5IaG7hHm7e48v5PKrgp9j2l1ShEREZFGMxahe5+ZfQx4G/BdMwuJ67pHZGYXmdljZrbBzK4Z5riZ2ReS4w+Z2YqaY0+b2a/NbK2ZrRmD/qduaWvr/uflYZaCB+h8WSeASkxEREREGtBYhO63AkXi+bqfBeYCfzNS4ySUXw9cDCwBLjOzJUOaXQwsTn6uJC5hqfVKd1/u7ivHoP+pK4QhCwsFDCi548PMYtKyuIVsV5Y9P1foFhEREWk0xxy6k6D9ZaDTzF4PDLj7oWq6zwY2uPuT7l4CbiUuTal1CXCLx+4BJpvZ7GPtaz1Ny2aZn8/jQHmY0G1mTHrpJM1gIiIiItKAxmIZ+N8H7gXeAvw+8Csze/MhTpkLbKrZ7k72jbaNAz8ws/vM7Mpj6ft4yyf13aWRSkzO76R/Qz+l7aXx7JaIiIiIpCwzBq/xZ8RzdG8HMLMu4E7g6yO0H+5OwqFDv4dqc767bzGzGcAPzexRd//ZC94kDuRXAixYsODwVzEOcslNlKURFsnpfGlc17337r1Mv2T6uPVLRERERNI1FjXdwWDgTuw8zOt2A/NrtucBW0bbxt0HH7cD3yIuV3kBd7/J3Ve6+8qurq7RXEfqBmcyKY4w0t3+4nYsa+z5peq6RURERBrJWITuO8zs+2Z2hZldAXwXWHWI9quBxWa2yMxywKXA7UPa3A68PZnF5Fxgj7tvNbO2ZJl5zKwNeA2wbgyuYVwEZmTMRhzpDgshHS/uYO8vVdctIiIi0kiOubzE3T9sZr8HnE9cFnKTu3/rEO0rZnY18H0gBG5294fN7Krk+I3Eof21wAagjwPLys8EvpXMdZ0BvuLudxzrNYynvNmINd0QLwm/+frNRKWIIDcWn4lEREREpN7GoqYbd/8G8I0jaL+KIaPhSdgefO7Ae4Y570lg2dH3tP5yQUD/oUL3eZPo/mw3PQ/0MOmcSePYMxERERFJy1GHbjPbxwtvgIR4tNvdXYlxGDkzdh8idA/eTLnn7j0K3SIiIiIN4qjrF9y9w90nDfPTocA9snwQxHN1jxC883Py5E/Is/u/do9rv0REREQkPSoaHmeDM5jsrVZHbDPj0hnsvH2nZjERERERaRAK3eNsUhjSGgQ8Wxp5AZyF/2shljWeu/25ceyZiIiIiKRFoXucmRntYXhQeUkliniuJoSHbSHtK9q1JLyIiIhIg1DoroOsGVUgSubrfmpggI3FIgM1JSedL+tk7+q9VHoqdeqliIiIiIwVhe46yCZ13eUkdA8ko961t1ZO/2/T8aKza9Wu8e6eiIiIiIwxhe46yMaL++wvMakk4btSs1Jl58s6yc7IsuPrO8a/gyIiIiIyphS662B/6E5C9uAId7UmdFtodP1uFzu/u5PKXpWYiIiIiExkCt11UBu6vSZo1450A8x8x0yigYhHLntkXPsnIiIiImNLobsOMkGAAcUo4jf9/fv3D450uztP9/dTOLuDhZ9cyK5Vu+jb0Fen3oqIiIjIsVLorpOMGc+Vy/TUzFiyuVSiGEX0VKvsrFTYODDA7D+cDQFs+9K2OvZWRERERI6FQnedZM0YbiH4neXyQdv5uXmmvHoKz37pWaLK8EvHi4iIiMjxTaG7TgbrugFObWnZ/3zoH0jVnZn/cw7FTUW2f3n7OPVORERERMaSQnedDM7V3RIEtGcynFgoAC+8mfKhnh66X5mndWkrW27cMu79FBEREZFjp9BdJ5lkpDufhO8p2Sw5Myru1MbuiHjp+Flvm8Xee/bS9xvdUCkiIiIy0Sh010lnGNIRhszJ5fbvy5hRdt+/PHytmW+fSdgRsuEDG8azmyIiIiIyBuoSus3sIjN7zMw2mNk1wxw3M/tCcvwhM1sx5HhoZg+Y2XfGr9djqz2T4ZTWVlrCcP++bDLSPdztkvnZeeZ9cB677thFcUtx/DoqIiIiIsds3EO3mYXA9cDFwBLgMjNbMqTZxcDi5OdK4IYhx98PrE+5q+PuUCPdADPeOgMctn1F0weKiIiITCT1GOk+G9jg7k+6ewm4FbhkSJtLgFs8dg8w2cxmA5jZPOB1wD+PZ6fHQ0sYUnZnY/GFI9mRO22ntzH5lZPp/ttuqv3VYV5BRERERI5H9Qjdc4FNNdvdyb7Rtvk88BEYtgpjQpuayRy0fdANlcno98JPLqT0bEkzmYiIiIhMIPUI3TbMvqH1FMO2MbPXA9vd/b7DvonZlWa2xszW7Nix42j6Oe6yQcCsmhsra8tMBse1J18wmcm/PZlnPv0M1T6NdouIiIhMBPUI3d3A/JrtecDQYduR2pwPvMHMniYuS/ltM/v34d7E3W9y95XuvrKrq2us+p662tHuvujAYH5tAF/4Fwspbyuz6W82ISIiIiLHv3qE7tXAYjNbZGY54FLg9iFtbgfensxici6wx923uvvH3H2euy9Mzvuxu18+rr1PWWjDDfLHK1MOmvyyyUz/velsum6TartFREREJoBxD93uXgGuBr5PPAPJ19z9YTO7ysyuSpqtAp4ENgD/BLx7vPtZLyOF7u3lMjtKJfZWKgDM+eM5VHuq7PzuzvHsnoiIiIgcBfMRpqdrJCtXrvQ1a9bUuxuj4u7c39NzyDYv7uggqkTce+q9RAMRL3noJWSnZcephyIiIiIyEjO7z91XDt2vFSmPMzbCSPdQQSZg6deXUtpWYu0nNvDMwEDKPRMRERGRo6XQfZybWzObyVAdL+pg3vvmcf8Pt3HX23+NR43/rYWIiIjIRKTQfZzryuU4o62NGdkD5SPVITOZtJxUoOfBXnauUn23iIiIyPFIofs4F5qRDwKCmrKT2tCd6chw2i2nkZ2WYd1bH6b/qf56dFNEREREDkGh+zh0aksLp7a08OKOjmGP76lUDgreYT5k8fWLqZacrf+8dby6KSIiIiKjpNB9HGrPZGgfsiR8pmak+5likSf7+ylHEZVkAZ3Wk1vpfMNUuv+um+e+89y49ldEREREDk2he4KYkc0ypSaI761Weai3lwd7e/fvK107h+rUkHVvWEfv+t7hXkZERERE6kChe4IwM+YcYiYTgPysPCfevYw9s40nPvQEzTAHu4iIiMhEoNA9gdSuVjnSVIL7OiHz/81l56pdbPjABorPFsereyIiIiIyAoXuCWSwrntBPn9QjXetsjszL53BjD+exeYvbGb10tWUny+PZzdFREREZAiF7gnEzHhxRwddudxBo961IsAC4+R/PIWz/msZPb0Vtn91+/h2VEREREQOotA9QWUOU2pSdefpFSHbLyyw5YuaRlBERESknhS6J6jake4pNatVDuqtVnEzpr1hOnvv62H7d5/j+bLKTERERETqQaF7gqod6R7uD3FftQrA9DdMp7C8lTv/aB0/uPphdq7bB0Apirhv3z4FcREREZFxoNA9QdWOdAfD1HcPhu5Me8iZd5xF+4Wd7L1nD7962X3s/vluepLjz1cq49NhERERkSam0D1BDYbuGdnssH+Ildpl4qdmWfJ/T+XM/zwTn5Nlw3s3UNxVPuh1RERERCQ9dQndZnaRmT1mZhvM7JphjpuZfSE5/pCZrUj2F8zsXjN70MweNrO/GP/eHz9WtLczv1DARgjOXUmtd5QE8OyULPOuXUTv+l7u+50HWX/Fep7//q5x66+IiIhIsxr30G1mIXA9cDGwBLjMzJYMaXYxsDj5uRK4IdlfBH7b3ZcBy4GLzOzc8ej38WiksD1oarJsfG+1yp7BcpMLOznzP8+kFDq96/r4zfsep/RcKfW+ioiIiDSzeox0nw1scPcn3b0E3ApcMqTNJcAtHrsHmGxms5PtnqRNNvnRWufAWW1ttAQB2SSIBxy42XJz6UCoHogicq+cxJl3LWfJradTKTtb/0lTCoqIiIikqR6hey6wqWa7O9k3qjZmFprZWmA78EN3/1V6XZ04skFAzox8ELC0tZUz29pecIPl4B920Z1iFNF6civtF3ay8VMbKW7WcvEiIiIiaalH6B6uJmLoaPWIbdy96u7LgXnA2WZ2xrBvYnalma0xszU7duw4lv5OGAsKBRYWChTCkEwQvOAPd04+D8DTAwMUkzrv+X9+AtFAxObrN49zb0VERESaRz1Cdzcwv2Z7HrDlSNu4+27gJ8BFw72Ju9/k7ivdfWVXV9cxdnliyAUB+eDAH2ntSPfUTIaZw6xcmZ2bY9rrpvHsl57FI1XqiIiIiKShHqF7NbDYzBaZWQ64FLh9SJvbgbcns5icC+xx961m1mVmkwHMrAV4FfDoOPZ9QqkN3cNNDdgaBFTc6fr9LkpbSuy9d+94dk9ERESkaWTG+w3dvWJmVwPfB0LgZnd/2MyuSo7fCKwCXgtsAPqAdyanzwa+lMyAEgBfc/fvjPc1TCQBEPHC0D05k6ElCNhaKjHt9dMIWgKevflZOs/trEs/RURERBrZuIduAHdfRRysa/fdWPPcgfcMc95DwItS72ADygwJ3fPyeXYnq1EGnRlmvm0m227ZxqL/vYjc9BeWoYiIiIjI0dOKlA1usEo7HLI/a7Z/X9Wdee+bF99Q+fe6oVJERERkrCl0N7hCcmPlYH137fbg6HfFnbalbXS9tYuNf7mRhy5+SAvmiIiIiIwhhe4GNy+ZJnAwbJ/W2soZbW3AgTrv7mIRd+fUfzqVzCfn8ODju9jw/g316bCIiIhIA1LobnCTMhlWtLfTGsbFJGGygM7gc4B91Sp7KhUyHRlaPjibzitnsf0r29lz95669VtERESkkSh0NwEbZrpAOPjmytoZume+YxY2P8uvX/drNl67MeXeiYiIiDQ+he4mVjuNYMUPxO5Ma8gp3zuT/lNy3Pu3T7Hvvn316J6IiIhIw1DobmK1obvkTiWK9m9nFheY9K1T2HtihnVvXMeuO3dpxUoRERGRo6TQLQCUo4hSzWh3yZ1Me8gp/7iY8kCFH1z+EPe9/kEqeyt17KWIiIjIxKTQ3eRe1N5OaxBQdqdUM9LdV60C0HpKK4sfXEHHJ+ax4aHdrHvTOqJyNNLLiYiIiMgwFLqbXGBGLgnd5WSkO2PGviR0A9i0DLMun8kJnz6R3T/ezRN/+kS9uisiIiIyISl0C1mzeKTbHQNaguCg2Uz6khHwqW+azrwPzmPz32/mkT94hJ4He+rSXxEREZGJRqFbyJpRcefZUomsGdkhUwwOlpoMRBGLrl3E3PfPZe1d2/nJa+9n8/WbcdcNliIiIiKHotAtB4XskvtB83cDFJMR8Io7feac+LmTWfijM+DsNh6/+nF+c+VvKG3XsvEiIiIiI1HoFrLBgb8GAQdCeKFm/xltbQTA7kqFvmqV/Ow8C245hRkfncfWf97KmuVr2PH/doxzz0VEREQmBoVuOWik+9TWVqZlsyzI51lYKOzfnwsC2sOQvdXq/hrvMtD/ZzNZcf8KstOzPPzmh9l+23aVm4iIiIgModAt5JLQfUI+T2sYkg0CunK5F9R2d2YyDEQR3cXiQfs3nGx0/vh0Ol7cwcOXPsKquXex+f9uBqAUReytxHN776lUiNzZXS6Pw1WJiIiIHD8y9e6A1F8mCHhxR8cL9odDQvf0bJad5fL+ke5BDuzLO/N+vJRd/28nD3x3Kw986HG87HRf3IKHsHz+ZDb09+8/5yRgcjabxuWIiIiIHHfqErrN7CLg74AQ+Gd3v3bIcUuOvxboA65w9/vNbD5wCzALiICb3P3vxrXzTWTwa5BpmfivSWDG9GyWZ4aMdA/qjspMetMUTn5NG3v2PMFj79/Ahr+PX6iybAp7zmuh5dRWgpzxyIIiZ5zQSUcmgw0J9yIiIiKNZtxDt5mFwPXAq4FuYLWZ3e7uj9Q0uxhYnPycA9yQPFaAP00CeAdwn5n9cMi5MkbMjOXt7QfVIE3NZtlVqTA3l6OnWqXoTjmK2JNMK7i3WiUshJz89SX43T0Un9hDsbvItq/uYucDzx/0+s+8dBKz3jGLF58/g46svnQRERGRxlWPpHM2sMHdnwQws1uBS4Da4HwJcIvHd+TdY2aTzWy2u28FtgK4+z4zWw/MHXKujKGhJSahGae2tgLQnjnw12dHqbR/BDyXrGhZfkmBmS+Jb8asXDWX2bsrDDzZT2Vflf7f9LH9tu3s+eVeNq/YwkvfNI+Fb5m5f9S7HEUMRBEdGYVxERERmfjqkWjmAptqtruJR7EP12YuSeAGMLOFwIuAX6XSSzkiLTXTC07KZHguuVmyK5tlR7lMpj0k0x5SmJePG71uGl2/P4OtN25h909284P7H+WlX95B9W/nQQjWlaXizor29heUn0TJvOFjUZayt1KhEATkAt1TLCIiIumpR+geLikNnWPukG3MrB34BvABd9877JuYXQlcCbBgwYKj66mM2mBoDYF8TRheUCjQFoY8PTAAwPL2draVSmwtlSjMzbPorxbhkbP9tu1sf183Gx7dCUDL4gL52XkW/s8Tmbai86D3eqCnh7Yg4LS2tmPu9+P9/WTMWNbefsyvJSIiIjKSeoTubmB+zfY8YMto25hZljhwf9ndvznSm7j7TcBNACtXrtTE0SnLBQGzcjmmZjIUh8xuMi2bpeqOE5enzMnnqbqzPRkNt8CYedlMul48jYF7niMqRez+r93su38fq1+9lsV/NI8FH1lAdmp2/xzgvUPe42hUk9eqaF5xERERSVk9QvdqYLGZLQI2A5cCfzCkze3A1Um99znAHnffmsxq8i/Aenf/7Hh2Wg5vbj4/4rEZudxB24Vhyjn6T88z65RZAMz5H3Mo7yxT+Xg3mz69icd+vINJyzuYelobz58ckJuVg7MPnubw2WIRM2PmkPcayZGE7f5qlYEoYoqmORQREZGjMO6h290rZnY18H3iaoSb3f1hM7sqOX4jsIp4usANxFMGvjM5/XzgbcCvzWxtsu/j7r5qHC9BDiOfBOr2MByxTVcuRyEI+E0yd3chCOhJZkCZlKx8mZ2WZc7Np7DpD55j5z9tZtv63ZR+emCp+bxtpG1lO+U3T6bv1Dz5ubn9oTty54n+fmblciPejHkkofuRvj4AXqzQLSIiIkfBmmHJ7pUrV/qaNWvq3Y2m0lutUgiCF8x+MtR9+/YB8WqYG5PZT1a0t7OrUmFLsUhpyN/P/if6Gdg4QHFLicrd+9i1vpfS3njFy0xHSGZ6lle+ag7+3zp5pn8AM+Nly2YM+957KpX9C/YMtzjQcP1c3t5+2GsSERGR5mVm97n7yqH7NR+bpKLtEKPcw5mWzdIbRbg7Zsa0bJZ9lQo7kyXkIV6kZ+dJLbSc1BLvuHwmc0oRe9fsY+/P91DeVaZ/wwAbPvIkm/8O+uKZDZn+uj0suHou4cICz5fL7CiXOaml5aCR7sH3rbrj7mRqyl9qP5iWo4jwCK9NRJpH1Z21PT2cWCioHE1EDqLQLXW1pLWVchJ4TygUDjrWmckcFLonJdsdYci+apW8GcVcwOSXdjL5pQdmOBnYXGTqT3Yze3KGnf+1m8e+uJnHbtnM3pe2kJ2WpdoXMfBb05nz1hk887lnKHYXyXa2Me+1XXSfnaVkzsrOSftfr3bZ+7I7g70cDOrHane5THsYHhT0d5bLTMlkCGpefzD8awVPkeNXKfn3YmuppNCdgq3FIh1heNA6EY2qGEU82tfHaa2t+8s25YV2lstMzmQmxLfQKi+R41oxiljX2wvA6a2ttAQBTvw/tJnZLFtLJYo1K2LWmpLJ8HylQvHZIlu+sJnyngr/f3t3HidXVeZ//PPU1tVburN09pAFQiBACBAgiIKKIosj4MKiLCqK/tQZcWZQXAdHZ8ZdB3dREEfZFBFGEEUU0JEtYICEEBKydrbuLN3prbZ7z++Pe6tT3enudCddqXT19/16dVJ16y7n1HLruaeec05mUwYv7VGxNkdHNTiDqnmVuJUppqxxrJkDkaRxwrhapl0xBe/t9WyzHKkNKdqWtLHwjZOYPLWa9akUO7JZJicSNGezzKyooCYaZVMmQ2UkslfH0f5kfZ/nOzqojUY5Mpx0KJ/2MjmR6NE5dUVHBynf54Q+UmF2ZLNUmJF1jrpewbqUv7Zcjk3pNEdWVR3yr31beCFdrhNfdXkeL3Z2koxEOGYYhjXtj+8cad+nchT98uY7x9/b24F9pwQeirZnMqxPpwedprg5nWZLJrPXd8GhojWXI+37g/6+K4YOz+Olzk4mxON7NdyVktJLZEQqvLqPmWFmGHtGSpmRTPY4ERcaE42yK5ejYnIFs/9zTvdy5zta/9JK15oups2oZsp5E9jQ1kXb421M+nsbfsZn+ZJ21l+3kt1fNWiIkWvN4accmS9uZvYFE1k72Se9IU3jjAoyWzOsjkfAc8TGxak+uorXnDGZinHBiWhZezsNiQS10SjtntfjBJUKW8U6Cy4a8i1lO7NZJsTj3c9BZz/DJPrOdY+DDuzzBN2ay1EdifRoWS+mZ9ramJpIMOUgf2m05XLURKMl+WUg5XnsyOWG5Yuy0/P2et/0tjaVIuscKd+nqo8gLOP7tHke4w+BlteXB9mPAqApkyEappsNZLh+dRqMtO+Tc67fFLr8UKQp3yfleSR7rZf1ffKf5P1pvczX9ZWuLnZ7HifU1PR5oZXz/b0+4x2eN+TUP+ccLbkc27NZpiQS1MRiZH2/+3y8oqODhnicCQch8MoMw1CxxZD2/UG9llszGSCoRzlcLOX7RDXE4zRls7R7HodXVg5q2zVdXUyIxxlzgBff2fA9cai+N3pT0C2HvO7RTPr5Uo2YUR+LMS4WI25GdTRKKvxSIOycCcGkPWZGJAJ2Zj0TXjuW42tqSPs+myoy1L+2nvrX1gPgvCAw5y8tZLZliFRFaXjDONp/s4Olv9xMVyXEG+Ls/P0uYvUx8B0Y5FqD4PkV1nB8vJLESTVse22SHUdXU3N0Fc7BhHiciBntuRwtYaufRzA75phYrDvXPOMcyzo6WFBdTbzXCX1r2Ml0aiLRHbhD8AXZvD3FlMlxUjhyznWf1JwL7q/u6qImGmVe2LK+v3znyDo34JdN/oS4OZMZVNCd832i4es0FG25HOtTKY6qqiIWidDpebzc1cXEeJwZ+9n6sSmdJuv7HJZMDrn1+JVUipTv97ho2l8rwpFzGuLxfp+XfKDX4XlkfJ/dnseMioru9Vd1dZHy/RHzE2zexvDzWxuN9jtrbP6ie3pFRfdwoTuyWTal08xOJoOhPmMxUr6/V0rCuq4uomZML3iuIOy7YbbX6+45x6rOTnxgQT8TahV+9S/v7GRyIsH4WIxkNErK81gevp4QdBpvTKepiUYHlYriOceLYZC7O7xQXx0GL1syGeZXVZFzjo3pNLtyuR555dsyGRrTaeZWVhI3G3TQtzGdpjmcUyGVSjG/uprnwzJMq6ig0/dZn04POehe2tbGxESCqQOcFzzn2JxOM7WigqgZ6XyK3ZCO1Lf8xc+ubJZJicR+X7Q1ZzJsCF/D2cnkgLMbmxmE5+HGVIqaaJTGdJo5lZUkIxFWd3UxvaKi+8LZD+vbnMnQEI9373tnNkvMrEfAuiWdpjISoX4IF9YZ32d32DiRc45kJMLmTIapiUT3BVtX+N070Hk54xyN4Wc1f1G4srOTcbEYyUiEnblcj1borO+zK5djVy7XffHdlMnggEmJxJAuDvODLYyU85qCbjnkzamsJOP7A54Ue19dV0aj9E6dMjOOqa5mYypFp+9TF36oKyIRjq+uJhV+ecxJJtmayWAFQThAVSRC59ljmdTpUReL0ZVwZNMeE6sr2JHL4ZzD6/Rpe2I3qfUp1i1tp/25HXhPhF/DEYgkjM2zqplUn6T5hASRqggtj7aQa82xIh5hUipKZGEVW9d0EBsXI7M5TUtdkuMumkLu5DheymONRWnaniK7K8sru3JUzKgguzNH1U07aHp0F42RHCujUdKz40QqIrzhsplMettE/t7R3j2ta6fn0ZrL0e55e7XG+s6R8X2yYVA9ruAk7sJJjWJmPVrXD6uooCGRYE1XF9XRKHXRKFEzloepQRBcVKxLpWiIx5mYSOx1ksynEk1LJJhcUKZOz6MyEul+/ftq1dyYTpN2jlbPY3wkQlcY7Lf3kXa0IZWiLhZjSzrNrGSSZPheyTpHIhIh4/t0eF53q9SO9naOrqrq0YKc8jw2ZTLM7hWQP9vWxrhYrDsIzvRqAXPO0e55/aZWOOfYlctRFYns1UKacY4KM7o8j5hZjwuxfKC3oeAisyYa7X7t8q1A6X5awvPH3u0F7+3e2nO5PnNo23I5KiMROn2f2miUjvD5jvYK6nZls0TNqO31HK5Pp4mbcVhFBZsyGSrM2JTJcEJNTY+Ozi90dOzVMt6SzVIbi3W/xo1hrm9VNBpcMDnX3aqef17qolFmJZPEIhG6wl8jIJjcq3B8/94pX3kbU6nuwM9zrs8veq/XeWdrJkNrLse8qqoeATfArlyOpmyWpmyWE2IxjKDfyPZslonx+F4t1ZvCi+3CdLrWdJa28P66VIq073dPHtbhed1Bd/61WRU+J9MSCSYmEsH73ozWMPiKRSK05XK83NUVnPMKLup99qQHNYfB6v7I+T4eQZpgZSTCjmyWI/poBNiSTtOUzZKMRGhIJLrfx32FtetTKTznmJVMknOOlZ2dzKmspDoaJeP7pMJfKFpzOVpzOTyChph0eD7J18V3rsdnOh024BS+1k2ZDPWxGIlIpLvhpN3zggu9AVp683tI+z7bslm2hRczm9JpJicStHkeG9Pp7gaRbP69RvAZmBKeN/MB7ok1NZgZWd9nc3i+yg9r6zlHc9ivIBruq9P32Raud2RlJWu6unpMNJd/vWuiUarDcubfL1FgYfgZbMlmewTFhefZtO+zI/xuKVw+OZHoPhf2HpUM9lxgZ8Pn5qiqqh7H8MJfdCME837kH8tPxrcrl6PT87rPbzuyWdK+z5QDuKAqBgXdcsjr/QU+WL0/aOPCoCHfWlAYXMQiEWoiEY4Jl/WevMfC7Tp9n1hVlOp4jGmxGLmkY2w8ziTPC06YlmbsWWO7t8vtztG1povM1gwdyzrw233aNqXZvrqF7BPByTo5JkbNgmqyvmNba46OX28LylQfo+bISja93IZ3yS7Wzg72+UI/9Z23Pcrss8cz9lVJtqzvgNYcqQ1pll6xgsqPv8KGBVHi4+NktmWIVkRoPn4MmVqj+pRJ1J0edERt8zx2ZLPsLOjAujmdZn51NY7gRJY/4Wd2ZNj9RBsu67OjMsKJ88axsa0DzEjOThKtjOBnfdoeb8P5jqfjLVjcaJ0Q5+WMY97MWiZP3pPzmg9yN2Uy7PY8jqyq6s5vzwf1m9NptmYyHF9Tw4ZUqvvkm28R2p3LMT4ep9PzyHV6pPBxVY5N6TRjwl9CmsMRbAC2ZbPMjEbZkE6zPZtlVjLJxlSK3qH6hrDlOt8ytzmToSX88q6Pxbq/+BywI5fr/lWm8Mslf9GRdY6jqqqoikRo87zu1qqt6TSbwucgGYngOdfjgigfwL/c1UUynAE230JVyDmH8113ysmkRKL7y35VV1cQePYKDLK+z/pUilbP4+iqKowgEJiVTLI9m2VTJsMR4Yg/UcJOztls9zCfQHcH57zCIHlNeIF2bEGO89rw4hcgYcb28DWB8KLQ8/DSHtnmHLndOZ5JpMh2eBx/0gTSMccrqRQT43EMyLXm6Hipg0fWbWN8ZYLdlY6WR1uIVkeJJCJY1Mh1ePhdHusrE0w8qoaOGmje2En7/+1m16I6Em+ZgjXEcXXBuabN8+j0PJqzWSIE6SI7d2dI3buTHRs7mfDGKew+Nk4lEY4YH7R658LALs/P+GS2Z4k1JGiv8Mi15mh/oYP259pJrekic9pYuqbF6FrdxYMxI9cafO4i8QgVvnHSceOpPbmWdI2xNZNh+44ULX/YxZZ2j44VnbQ/3Uau3SM5K4nFgnMGBpnNGXItOY46s4HU6fXEjqpkq2VJb82QmJQguzPLqjFRNs2uhIgxIR7vfv6rIxFyzuFnfJpWdZDbkWX3E7vxOjxiY2I0jU+w66V2ss1ZXmmIY1EjUhll6jHjqXxjPbGIUVkfh8TeQ8Z2eR5bMpngV8he742dYSBXEYnQFF6o5C8U8ueefIDlhZ+H/Lk86/vd5d/V3k51JEImDNKOqa7mpc7O7gC2UP4CakN7FzsiGebUVnb/OjE1DNaWdXRQE43SEI+zOQyqN6bTtOZyzKms7PGe7yvBYXPYCl4TjXZ/DnvPqJx1rvv8l18n5/vd549MU4bUxhTp6RVUTKogvS1N1+oUW6c5cjNirN/QTmpdisy2DBvnpNk1Blq2dNHRnGHKjGq8xVUQPud+zsfv9Pl7KkckET5/O7N0ruxk+64c6Y0pxl00lTUNPb9Dc86xYVcn42sTvJJKUWFGan2Klv9rpbElR2pVF6lNaVZPTJDr8sjuyBKtieKnHNltGTpn1jL79Q10HZWg2WVp2dhFtDLK02N3M/eoelyDAx82d6XoWpui/cgY0TEJ1qRSTE0kcEBLLkeu06PTZTlm8hh852jzPDI7gvf7sok5Tpk6Ft8F5/yKSISph1DADepIKWXupY4OqqNRplZUECE4iXR4Ho3pNEdUVvb7k9SObJZ14Yd9fDxOhKAVqTUcf3xeZWWfOdFNmQwbw5NsbTTKlvBEmjDrEYA53+G1efhZn/mTa6mrTNCWy7EhnaajI0ukIoJFjKmJBJu60uy6q5nWFzuI1ESI1cWYWVtJ+8QIfm2E1hc7sJjxpitmExsT6/4J2wDfc1T/ro3GP+2kqS1NtjlDrD6Gyzg6X+7qLs8Ru6Pkjqhg15ExYmNi5FpyWCJCZksar8MjWR/HrzSqj6shuzVDan2K2j93srVgCPREBjIFDV+RhOFn+j+/RD04IlrBlhPjHLFgLDvHQ8tTu4nWRak8oopjJ9Wywc+w85UOYmNiVKeh3XzSmzNBgDE2RtXkCk5YMJ5lXZ1sv2c7uV05kmaknKP1ry04HyqmJIhURrCYUX1cNfgQHRPDomBpxymnTWTdMVHantqN1+GTbkyBg2htjHRjiq7VKXI7s0Sro1hFhDPfMIX2V1fRUumIAdkdOeZNq8XGx3ilM2wVcrB7RScu61PrRZg5dwwrtrfRubKL7NYMs6fUED8y+EXlyOm1TJpSxbPt7Xhpj7bH20htSJHZmmHCMTUk31BPan2K2eOqmDizmhc6Osi15chuzzK+KsHEqZUs/VMTW27ajO85crtyZLfnqJiaYMqiemad30BjW4q2Z9rIteTwunxOnDqGhtePw1VGaK3wWd8afGFmNqeZUZWk4+RKsu05xnYa2zszdK5PMS4dhQvqwTnGH1bF7lSW9qXtdK1KkVrdRa7dI7M1jZ9yuIzP4fEkk+bXcNinZrK8MvgczEkmef7J7ex6eBdeh0eu1cNPeUw/vo7dCUdyZpLO1UFrcGZThpa/tJJryfV434wbk8A/sYrUhhSJCXH8jMOeaGfiJmiaCG1hrF9TGaMjk8OiwfswWh0hNiZGdlcWPxW8L2M5qKmJ0ZLac4zkjArik+K4jGPM4jH4XT6pjWlSq7uoXZqmpg3WzYXqFmivAYvBornjSLx1LJviHuQcbc+00bmqk67VQUBpcUhWxOjqyJH/uSk+IUauOYcbICao7IKuyiCY9js9/Ixj7C7YNRbGWRT3hlri4+N0vtQFXrBj5zniExNkmzPEn+ikZYDU+VhdlNiYGDULa0itS+F1ehABixqZdWlyqZ7BYaTC8NOO2soYbm4FXWu68Dp8YjVRKrZ6dITXVBaD+IQ4J8+op+qISnYcFad+VhVtkyKkch6JiQl2P7WbrtUpKqYl6FjeSXJ6BcSMBJCNQ3p9mkxTlrandjPjX2dQe3ItFckojbdvI9eUxXmO08+eQu1xNSwnhe/5tD/bTueqLsyga02KbFOGmkkVMDlOtDZK1ZFV5HblSG9Jk96UJlodZfcjLXS8kiJSYcTqYvhZB1nHjCNraJhXy4Z4ltjYGIkpCSKVUaJVETpe7KS2KsbMObW8uKqFXX/cRXpDmuiYKJMjCU54zwwa3trQ3am2kNflUVMVo2VzirZng2AZB15HjkxjhvjkBIfXJNk6M0IkGWHLT7aSWrvnV8XYuBi5XTkiHvgFX0HVHXQ//wARPxgowFkwOEC0Omh0Sa1P4aeD90rFtATOc2Sasj2uGKJRIBl8XixmOM+R3Rl8buITYnjtHi7ncDmob4GWsTCmJo4/O4HLOSwZ6T7/j43F6JoSxX+6A5al2Dmu7/d5qpbgWOF+K1Pg5lXgp3wiUWPWsXVs3tVF/KF2dtTDyUePI3p2HWubO9h2W1P3+25Rwxji/zCW1bs7OawpwknfP7r/D0AR9deRUkG3SB/ynYfyrZgAjakU27LZvVINCu3O5VjV1dWdS1k4qU5rmMNdH4vRHHY66Z2vvaqzsztXM7/dy52dPX7ijUD3CCb5i4PeIyXkwlbXFzo6iBC0DEUIzquVYfqFc45MU4a2J9tof6GDbHOGbFOW7M5s8CXf4VMxs4JIVRS/3SPTlCW1LsXYRIxpU6uZemo9zedXkak0XNbRtaaLSbEEWzMZMpvT5FpyRKqi1M+shFkV+F0eXpuH1+oRrYuS3pim48UOUmtTdL0SfKlEayJ47QV19Xt+sUAQbLuUj9cVrNfQDM0NcFhLhO0nJvC9oMXk8Llj2HFYlHRjCj/lk27MkN4UfMG5MMayGNTuhN1jggAs18dvf5Wzk1TMqMDr9Eg3pqlekSWVDIKhvOoOqJ9TybZUBiJGtDpCujHTYz/m6DPAqmmHWRan8Zwqcn9to2GNz7ZJwTjzVZ2QSu55DmZXJdk81ie9ac++K9LBxU5DXQXtkyMkpgat4157Dv/JDjqiYR5sFOLj40SqoqTWpYhng+Am/zxP2QLbJgUXQ+mCjKOoFxwjXQFePz84xcYEwVt8SpxIPEK0JkpmWwae6mBii7HhyAgWN6KVUVKb0jS0GdvrHLH6GNGaWBB49DLNxZgwr4Yd59WQmBi0qEYzjs33NOO15oiNDb78I1VRFk4cw9QLGvDmVbBiaxtem8cxp45nTWsX0YQFgUcsEox41Jkmsy2LS/vMra+ic1KUdX/bSXZnlvTGNDt/vwucw0/7ZLZmsRhMqk8ycXY1U46sZezZY1l1BDT/b3CR53V4xH+2k61+rsf7J1YfY8JbxhNJRvDaPbyUz4zKCsYfV8vUM8exviLL5r/twnX6VB9fQ0U8woJJY3hm524AnAcdy9vpeKGDTFOWWHWUaF2U086czM7pEWbXV+ERpFUUtrYeX11Nzjle7Owkl/ZIrUmRWp/Ca/ewiOF8F3ymOz3anmnD2+3R8UIHlUdUEh0TxeWCz0/ttCQNJ9fRUQvVx1R3v3651hyzx1exOZchk/LwOjxqxlfQ9LcW0huDCckyWzOk1qboXNVJZmuW/RH3IFYdpSu1p27J6RWkGtM918uBV2PgoGqHoz1Mta+ORojMqKBjy54gs4fwhHjYtComnjWODak0fsqHmBFLO3Y+s3uvYLQvUQ8iE2NUHlFJpCJCam2KCU+mmXHGOHKvqWajlyG9OU18QnBx1L60HYvtOQcBYMHnM5IM0uhy7V73xVlDLMZx75zGjpkRNj23m9xuj+TsJA2vqqNtc5rMljTzJtUyZV4tjQ0+jc+0kIxEWDCtjt0NEZ67Z3MQaGd8ci05Zs2oZft4h8s6Wv+vlYrpSY6dWkvta+vZNQ46fI8ttzfhd3i0P9dOYlICHMw5vJb0pBjbN3QSqY4QrY6CGWdfPJPYrAqiEese0GBhTQ2vdHXR5nlMSyRoD3+5Sq1P4bXk8LI+kaooseooud052v/eTseLHUTHxKhOxqg+pppd6zroWpUiWhXBa/No/3s71bUxjju9gdVVWewXO9niZYn5MPGkMcTPqye9MR1cAG3OEI0bp8wZy3H3HleS9BIF3Qq65QC58KesffW2LuzJ/nx7O1nn9spH9Zyjy/P2ypNN+z5NmQyJMM9xfnU17bkcK7v2tErXx2LdOewZ3+fFjg4Or6zsM094TVcXu3I5kpEIs5NJVnR2cmRlZXeua6EIQSeWVJjT1+Z5zKyoYGsmQ9Y5dmSzZHdkmT25hsmVQVSWDfOf28M840mJBM+GJ96GeJxxsVj3aAf5zl0AU8ITcT5YyLXm8Do9qqdUkHUuaM3u8DkmXsn6qY6ulMfU2goiPkyqS/L31jb8jCPXnKV9WQeVnXDaBdOIjIt153s66P4SyHfGdV4w+VFuZw6LGVW1MTb/djuZbRmOP3kCTePh8Nm1rFi3Gy/lUXl4JYlYhPz3o/McqY0pmn/VTHxsnPqaOLVjE2zc3EHrI61E66JEkhGyzVmOPKuBSUdUs9Zl6FjXxfSaJFOPHcP2yUbzhk661nZhUSPbnKXtyTa61nVx9FFjmXvRJMacOoaN0Szrft/Mtnu3M+b0YNz43Y/vxmUdx50wntykKNv9HF1NGeqr4sy5bApbo7keebi51hxtz7bhPMesV41jzsQaVnZ2smtjF+mNKWpSRnWXUV0bo2F2Ndsmw+Z1HXSu7qSuLoFfFeHommric5IsXd/Crj/vIloVJdOcJVoZoWpWkgWnTKBqQoLV6RRTEglacjka4nHWplK0r+qk8+4dtDSlsYRhEWN2dSXzrp/JS14Ki1rQqtqcwWv3SG/OcNTJ46hNxhhTHfxs0uV53R2rGtNpdnseCTNiZnT6fo+L4HynyonxOJMTCZ7v6GB8LMakRIJk2C+gJZulw/e7U3c2hSlL1ZEIHkEaSf5XIi/lc+yEGqp7fbbWdnWxM5fDCEZVSmc8MlsyTIonWPPcLixmjHvDOGYnk6wN0ycOTyZ7dHLbmErRlM0SN2NeVVV33nBzJkNFJELCrEcO+JGVlcT6SLXL+j6bwj4N9WFdAZ5rb+9OdZmVTOKFHSz7M72iAiPIra2PxZga5uBmww7dEAT07Z5HfTzePQkQBPnhmzIZkpEIR1ZW8nxBXw4v7ZHZnCG9MU16c5rcrhwu7UhMTzBmcR2ZbWnGH1tL6/ou6qvj7M55eF0eRx9ZT0NdkiVbW9h2z3Yym9N0renisFePp/Jd48nuytH611bSm9PBULAdHiefM5kxp9axyUsza0I1TRakgKU3Z/BTHp3LOolPilMxpYL45OC1OG5sLTGz7jLPqKigzfNoyeXwPR88yO7Kkt6QxoW/Jhx++Bga0xlSG1JMrkvSNT9IRamLRtnVnmXXNzbR+uRudm1K4aLBySgSDy54xr1xHJGaIOVp7OvGkpieYN5hY1ibSnUHh7lOj9S6FKk1KRac0cCUadXdwwcCxMN5LfK55RPCTtb5dLyqSISjq6u7G4EKnVRb290YBMF5+rCCTo7Lw6Fpe5udTJLxfTZlMsxKJoPhKiORHh2At2Uy3f0q0mHK2pxkkmx4EQhBGtqURIIu3ycRvucLv49mJZN0hGldddEokxOJ7u+/Wckk42Ixlra34zlHtjnLuLoER4yvYX0q1f39ktudY2JtBbPHHNhgAQdCQbeCbimBbJiXN9RhugoVfrkdGXYMGuxoGi7sPJOMBPmV+U6InZ5HhJ55tfsawm1FRwedvr9X8NDbtkyGlO/vNWaqc45MmLs4o6KCiBk532d5Zyc555gX5krmy1MXjXJEVRUpz6PD93sMG/dyZydtnsekeJwduRyHJ5N9dvRrDjtqVUWj3QH41ESiu9PR9IoKGtPpHr8e5Hyf5wqChukVFdTHYmR8v8eXQ4UZx9bUkA0nsMiEHdLyaUT5Tk75L8tjqqpIhp261oQjyIyPx9mWyXR36Cv85WNLOt1dzpkVFd3504VDQmbDjlGTEgl8YFlHB0dVVfFSr5+0J8TjPZ7zfP16v+b5L+mEGcfV1PTotFr4RQ1wXHU1Eeh36Mn8GPSFxsZizKmsxDnXfXHWu7zHV1f3u8/CAHlmMsmuXG6v0S8KO8KtT6Woj8X67Byalx/VY2wsxpholPVh0JnvINfXkHz5C+H6WIz6WKy7U/Fx1UH6T96JNTXsDHP5e4/NvCubZU04gkV/IwllfZ92zyPt+z06Fw9G/vUqHK5zUzpN2vcZF4vhoPtXMgg6rPc30s5LHR2kneP4XiO25I8xt7KSVeHzkW8QaEylqIpGqQs7u7bmcmTDXxB7P1fjw4nP8kHVrlyOseGvjC+0t/dIzct3sOv9fgS6P2N5qXAM58I+GrOSSarC82GsYISaHdls9yg5+caKOWEAuK2gv8H0igomxuN0+T7N2SxTEwki4VC2EbMev1bm2nJURqKkK/ZMbDahIt79eZ8QdnKcHg5925LL0ZzNMj4W6/68z6uspCYW6x4lpa965rkwjz3f16Vwnoup4bC1NbEYnZ7H7lyOTWHQXjgQQf48H4Uez9vhySR14edifyd9as5kqAs7oBbaEnZ8npJIEI9E8MNRaxoSCRJmrOrqoiEe7z7uK11d3b8az6io6O4Av7yjA58gnfPY6uqSdqDUON0iJRCPRDjQkZGjZkyKx6mORoc8oYiFQygW3ge6WwaPrq7u7py0L/nOgQMNiQX0O6KBmVFhPWcejUUizKioYEP4BT0pkWBtKsWYaLS79SUZje71BTM3/JIwM6YPUJaGXmWpjkSYUlHRfWIeG4t1d7gpLFNePnCGvcdUzrcixiMR5lRW8lJnJ+Pj8e4JnPLbTa2oYHzB0IGJSISjClKBZlVWsrOtjZpotEeqUWXB7epolBkVFeSc6xFkxiMRphc8n/kgujbsXDomFuueXCMfXMQiEWJmVPfxOo6JxZiVTDImfL4Lv7SM4NeDwyoqiJnt831QWJfDk0naPI8p4ethZswMh0arika7J7LKl68/k8MRLMbGYlRGo312sC4MkAczWUZ+D8ae17gyEqGlj/3l1cRiLKiuDlq6w22i9PxsnBC+d8bH432OM14fiwUzDQ4QGMQjEcbu55CT+Y6thUN19h6paLDB01H9TPJzdFUVqXDkmsmJBBML9lf4vqwruPDJB8uJSIRZ4Tq10SgOutP5CkdMiobD7EHw+e3dgDE3HIXDg72GVk1GoyysraUlmw1aViORfsd8L1w+LRyisC68qKoJGzry/0NwDp3Zx/uvIhKBMOiO1caYFV6Q5MtaFYkwPfwsF57XImG983Xfls2SKugsmn+X5Ief7IuZ9Rg9JVHw3ip8H1RFoyQjEdo9j8m9zpH5Ro/x4djbNdEoVZEIdeFrcyCzrPY+H/dVNgiei8L3T+9RhOaEI9QUvt6JSISF4RCcAw2vWmoKukVGgOmDCB7212DHkZ6ZTLIzl+s3n31/FX7RFN4eyP6cUBfW1HSnnhQGH9PDTraF6sILnN7HObqqCgd7tSRXR6PdM6b2VbZ9PcfH19TsNfZwPkjJj6k8lBF8Cr+kCgPuvAUDzJTYX1ASs2DG0zGx2JDHHh8Ti+3160jhuM5zKitpC1tCBxLtFVQMJyMYNSZCEATGKyr2Gv6vUOEXfl/P2b5+jep9QTzcDq+sHLD8wyF/0QR7B/T9mZ1Mdrf4Fz5v/b2u+ffErGSyx/r5vik10SjHhGkv/XWMr4/HqR9U6QIV4S8phdsPZVsI3kOzk8nu90nCeo6rva/g6/Dwl5zeF7dDmTNgoPNkxKzPYRpnJZPdc0Y0ZbNURiL7PddBsZhZn/N2mNkhV9beFHSLyKDEe41lPNL094Xc10yPfX0ZwZ5fCKYlEnvl9h/IxUhfZTMzjh+GacT7+pLen4uWIyorg/4GQ9h2Zjh5ymAChVJNC5+/mBkTixGPRLrTjPZ3kvHxsdheQ06WQrTX2NKHisFcVBfK/6JU20cLd1f43oqY7fcvAsOt8PORD7iPq64e8muRjEaZUlDncfE4Hb7P1CGegw8f4uRehb/MHBb+SifDR0G3iMgQDTW/dn8NlGpxsFX1Mb73vhyMqcEPVHU02uesr/trqM+RDKwuGqXV8/Zq8Y1HIsP2mg2nfHBdGObuKxVrMCK9UvMGayit9L31lw4i+09Bt4iIjGqHYvAmgcMrK/c1at8hpTYaZXwstleesggo6BYREZFDlJlRvMz34Wdm+rVD+lWSy3szO8fMVprZajO7vo/HzcxuDB9/3sxOLHjsZjNrMrNlB7fUIiIiIiL756AH3WYWBb4LnAvMBy4zs/m9VjsXmBv+XQN8v+CxnwLnFL+kIiIiIiLDoxQt3acAq51za5xzGeAO4IJe61wA/MwFngDqzWwKgHPuMWDnQS2xiIiIiMgBKEXQPQ3YWHC/MVw21HVEREREREaEUgTdfQ0Y2XsE/8GsM/BBzK4xsyVmtqS5uXkom4qIiIiIDKtSBN2NwIyC+9OBzfuxzoCccz9yzi1yzi1qaGjYr4KKiIiIiAyHUgTdTwNzzWy2mSWAS4H7eq1zH3BlOIrJYqDVObflYBdURERERGQ4HPSg2zmXAz4C/B5YAdzlnFtuZh80sw+Gqz0ArAFWAzcBH8pvb2a3A48D88ys0cyuPqgVEBEREREZInNuSKnSI5KZNQPrS3DoCcD2Ehy3lFTn0UF1Hh1U59FBdR4dRmOdS2Wmc26v3OZREXSXipktcc4tKnU5DibVeXRQnUcH1Xl0UJ1Hh9FY50NNSWakFBEREREZTRR0i4iIiIgUmYLu4vpRqQtQAqrz6KA6jw6q8+igOo8Oo7HOhxTldIuIiIiIFJlaukVEREREikxBd5GY2TlmttLMVpvZ9aUuz3Axs5vNrMnMlhUsG2dmD5nZqvD/sQWPfTJ8Dlaa2ZtKU+r9Z2YzzOzPZrbCzJab2UfD5eVc56SZPWVmz4V1/ny4vGzrnGdmUTP7u5n9Nrxf1nU2s3Vm9oKZLTWzJeGycq9zvZn9ysxeCj/Xp5Vznc1sXvj65v92m9m15VxnADP7WHj+WmZmt4fntXKv80fD+i43s2vDZWVd5xHHOae/Yf4DosArwBwgATwHzC91uYapbmcAJwLLCpZ9Bbg+vH098OXw9vyw7hXA7PA5iZa6DkOs7xTgxPB2LfByWK9yrrMBNeHtOPAksLic61xQ938GbgN+G94v6zoD64AJvZaVe51vBd4X3k4A9eVe54K6R4GtwMxyrjMwDVgLVIb37wLeXeZ1PhZYBlQBMeCPwNxyrvNI/FNLd3GcAqx2zq1xzmWAO4ALSlymYeGcewzY2WvxBQRfZIT/X1iw/A7nXNo5t5ZghtFTDkY5h4tzbotz7tnwdhvBLKrTKO86O+dce3g3Hv45yrjOAGY2HTgf+HHB4rKucz/Kts5mNoag4eAnAM65jHOuhTKucy9nAa8459ZT/nWOAZVmFiMIRDdT3nU+GnjCOdfpgpm/HwUuorzrPOIo6C6OacDGgvuN4bJyNck5twWCIBWYGC4vq+fBzGYBJxC0/JZ1ncM0i6VAE/CQc67s6wx8C/g44BcsK/c6O+APZvaMmV0TLivnOs8BmoFbwjSiH5tZNeVd50KXAreHt8u2zs65TcDXgA3AFqDVOfcHyrjOBK3cZ5jZeDOrAs4DZlDedR5xFHQXh/WxbDQOE1M2z4OZ1QB3A9c653YPtGofy0ZcnZ1znnNuITAdOMXMjh1g9RFfZzN7M9DknHtmsJv0sWxE1Tl0unPuROBc4MNmdsYA65ZDnWME6XHfd86dAHQQ/OTen3KoMwBmlgDeAvxyX6v2sWxE1TnMW76AIG1iKlBtZpcPtEkfy0ZUnZ1zK4AvAw8BDxKkjuQG2GTE13kkUtBdHI0EV5h50wl+2ipX28xsCkD4f1O4vCyeBzOLEwTcv3DO/TpcXNZ1zgt/en8EOIfyrvPpwFvMbB1BOtjrzeznlHedcc5tDv9vAu4h+Hm5nOvcCDSGv9wA/IogCC/nOuedCzzrnNsW3i/nOr8BWOuca3bOZYFfA6+ivOuMc+4nzrkTnXNnEKSBrqLM6zzSKOgujqeBuWY2O2xduBS4r8RlKqb7gKvC21cB9xYsv9TMKsxsNkGnjqdKUL79ZmZGkP+5wjn3jYKHyrnODWZWH96uJPgCe4kyrrNz7pPOuenOuVkEn9c/Oecup4zrbGbVZlabvw2cTfATddnW2Tm3FdhoZvPCRWcBL1LGdS5wGXtSS6C867wBWGxmVeE5/CyC/jjlXGfMbGL4/2HAWwle77Ku84hT6p6c5fpHkE/1MkGP4E+XujzDWK/bCXLksgRXylcD44GHCa6qHwbGFaz/6fA5WAmcW+ry70d9X03wk9vzwNLw77wyr/MC4O9hnZcBnwuXl22de9X/tewZvaRs60yQ3/xc+Lc8f54q5zqHdVgILAnf378Bxo6COlcBO4C6gmXlXufPEzQWLAP+h2CUjnKv818ILiKfA84aDa/zSPvTjJQiIiIiIkWm9BIRERERkSJT0C0iIiIiUmQKukVEREREikxBt4iIiIhIkSnoFhEREREpMgXdIiIyaGb2WjP7banLISIy0ijoFhEREREpMgXdIiJlyMwuN7OnzGypmf3QzKJm1m5mXzezZ83sYTNrCNddaGZPmNnzZnaPmY0Nlx9hZn80s+fCbQ4Pd19jZr8ys5fM7BfhrH+Y2ZfM7MVwP18rUdVFRA5JCrpFRMqMmR0NXAKc7pxbCHjAu4Bq4Fnn3InAo8C/hZv8DPiEc24B8ELB8l8A33XOHQ+8imA2WoATgGuB+QQzW55uZuOAi4Bjwv18sZh1FBEZaRR0i4iUn7OAk4CnzWxpeH8O4AN3huv8HHi1mdUB9c65R8PltwJnmFktMM05dw+Acy7lnOsM13nKOdfonPOBpcAsYDeQAn5sZm8F8uuKiAgKukVEypEBtzrnFoZ/85xzN/SxntvHPvqTLrjtATHnXA44BbgbuBB4cGhFFhEpbwq6RUTKz8PA281sIoCZjTOzmQTn/LeH67wT+KtzrhXYZWavCZdfATzqnNsNNJrZheE+Ksysqr8DmlkNUOece4Ag9WThsNdKRGQEi5W6ACIiMryccy+a2WeAP5hZBMgCHwY6gGPM7BmglSDvG+Aq4AdhUL0GeE+4/Argh2b27+E+3jHAYWuBe80sSdBK/rFhrpaIyIhmzg3066KIiIxEZnYDcIRz7vKCZe3OuZrSlWrkMLOfAo3Ouc+UuiwiUh6UXiIiEjKzR8xsl5lVlLosIiJSXhR0i4gAZjYLeA1B58K3FGH/JU/nGwmt3IfC8yQiUgwKukVEAlcCTwA/JchxzncebDGzY/MrmVmDmXUVdFJ8czgBTYuZ/c3MFhSsu87MPmFmzwMdZhYzs+vN7BUzawsnkrmoYP1oOHnNdjNba2YfMTOXD0TNrM7MfmJmW8xsk5l90cyig6mcmb3FzJaH5XwkHMs7/9gnwv21mdlKMzsrXH6KmS0xs91mts3MvtHPvl9rZo1m9qmw7OvM7F0Fj1eY2dfMbEO4nx+YWWWvbT9hZluBW/o5xnvNbEX4S8Tvw46h+cecmf2Tma0Jj//VMJcdM4uY2WfMbL2ZNZnZz8JhEvPbvjp83VrMbKOZvbvgsGPN7P7weXnS9kwOJCIyZAq6RUQCVxJMBvML4E1mNsk5lwZ+DVxWsN7FBKN7NJnZicDNwAeA8cAPgft6padcBpxPMBZ2DniFoEW9Dvg88HMzmxKu+37gXIKRP04kGHqv0K1ADjiCYIKas4H37atiZnYkcDvBqCINwAPA/5pZwszmAR8BTnbO1QJvAtaFm/438N/OuTHA4cBdAxxmMjABmEZw0fKjcN8AXwaODOt1RLjO53ptOw6YCVzTR/kvBD4FvDUs/1/C+hS6CFhE8LxdALw3XP7u8O91BGOV1wDfCfd7GPA74NvhfhcSjDuedxnBazQWWA38xwD1FxEZkIJuERn1zOzVBAHfXc65ZwgC43eGD99Gz6D7neEyCILkHzrnnnTOec65WwnGsF5csP6NzrmNzrkuAOfcL51zm51zvnPuTmAVwfjWEAT0/x1OPLML+FJBGScRBOTXOuc6nHNNwDeBSwdRxUuA+51zDznnssDXgEqCWSY9oAKYb2Zx59w659wr4XZZ4Agzm+Cca3fOPbGP43zWOZcOJ9q5H7jYzCx8nj7mnNvpnGsD/rNXuX3g38Jtu/rY7weA/3LOrQgvXP4TWFjY2g18Odz/BuBb7HnN3gV8wzm3xjnXDnwSuDT89eBdwB+dc7c757LOuR3OuaUF+/y1c+6p8Ji/QMMgisgBUNAtIhK0zP7BObc9vH9buAzgT0ClmZ0aBnkLgXvCx2YC/xKmJrSYWQswA5hasO+NhQcysysL0lFagGMJWogJt9vYz7YzgTiwpWDbHwITB1G/qcD6/J1wJsmNBDNOriZoAb8BaDKzO8wsX/6rCVqoXzKzp83szQMcY5dzrqPg/vrwuA1AFfBMQbkfDJfnNTvnUgPseybw3wXb7yQYlnBawTqFz1X+2HvVPbwdAyYRvFav0L+tBbc7CVrJRUT2izqsiMioFuYWXwxEw5xiCFp+683seOfcc2Z2F0HL6Tbgt2FrLQSB3n845wZKO+gelzUM2m8imJb9ceecZ8E07fnZH7cA0wu2nVFweyNBK/qEsOV1KDYDxxWUw8J9bwJwzt0G3GZmYwgC+S8DVzjnVgGXhfnRbwV+ZWbjewXXeWPNrLrgscOAZcB2oAs4xjm3qZ/y7Wvs2vzz/IsB1pkBLC849ubw9maCoJ2Cx3IEr+VG9vzKICJSVGrpFpHR7kKCFIv5BK3YC4GjCfKGrwzXuY0gReNd7EktgSCA/mDYCm5mVm1m55tZbT/HqiYIMJsBzOw9BC3deXcBHzWzaWZWD3wi/4BzbgvwB+DrZjYm7CB4uJmdOYg63gWcb2ZnmVkc+BeCAP5vZjbPzF4f5qGnCAJkLyzf5WbWELaMt4T78gY4zufDPPHXAG8GfhluexPwTdvT+XSamb1pEOXO+wHwSTM7Jty+zsx6T9RznZmNNbMZwEeBO8PltwMfM7PZFsya+Z/AnQUpI28ws4st6OQ63swWDqFcIiKDpqBbREa7q4BbnHMbnHNb838Ene3eZWYx59yTBLM5TiXoeAeAc24JQb7yd4BdBJ3t3t3fgZxzLwJfBx4naGk9Dvi/glVuIgisnwf+TtDhMceeQPdKIAG8GB7vV8AU9sE5txK4nKDD4HbgH4B/cM5lCFr1vxQu30qQrvKpcNNzgOVm1k7QqfLSAdJAtoZl2kwQzH7QOfdS+NgnCJ6bJ8xsN/BHYF6fe+m7/PcQtL7fEW6/jCC/vdC9wDMEHSHvB34SLr8Z+B/gMWAtwYXFP4b73QCcR3ARsjPc9vjBlktEZCg0I6WIyCHKzM4FfuCcm7nPlUvIzF4L/Nw5N30fqxbr+A6YG+ani4gcktTSLSJyiDCzSjM7L0x1mAb8G3s6bYqIyAimoFtE5NBhBONC7yJIL1lBz/GsRURkhFJ6iYiIiIhIkamlW0RERESkyBR0i4iIiIgU2aiYHGfChAlu1qxZpS6GiIiIiJS5Z555ZrtzrqH38lERdM+aNYslS5aUuhgiIiIiUubMbH1fy5VeIiIiIiJSZAq6RURERESKTEG3iIiIiEiRjYqcbhERERHZI5vN0tjYSCqVKnVRRqxkMsn06dOJx+ODWl9Bt4iIiMgo09jYSG1tLbNmzcLMSl2cEcc5x44dO2hsbGT27NmD2kbpJUXwt1e2c/Y3H6WpLdXjtoiIiMihIJVKMX78eAXc+8nMGD9+/JB+KVBL9zD72yvbufqnS8h4Ph+9YylLN7SQ8XxufHg1X7zw2FIXT0RERARgnwH3Nx96mf9+eNU+9/PRs+bysTceOVzFGjGGesGioHuY3XDfcjKej+c7/r5hF6msD8ADL2xR0C0iIiIjxsfeeGSPYPqSHz4OwJ0fOO2A993S0sJtt93Ghz70oSFve95553HbbbdRX18/qPVvuOEGampq+Nd//dchH2s4Kb1kmFzyw8eZdf39vLytHc93AN0BN8DOjgyzrr+/+w0rIiIiMlL87ZXtPN/YQibnD0vqbEtLC9/73vf6fMzzvAG3feCBBwYdcB9KFHQPkzs/cBrrvnQ+t73/VCrj0R6PxaPG5acexrovnT8sV4ciIiIiB0s+dbYr6/NKcztX/3QJrzR3cOPDq/d7n9dffz2vvPIKCxcu5LrrruORRx7hda97He985zs57rjjALjwwgs56aSTOOaYY/jRj37Uve2sWbPYvn0769at4+ijj+b9738/xxxzDGeffTZdXV0DHnfp0qUsXryYBQsWcNFFF7Fr1y4AbrzxRubPn8+CBQu49NJLAXj00UdZuHAhCxcu5IQTTqCtrW2/6wtKLxl2+fSSvFjUyHqOB5Zt5YsXHVfCkomIiIj0ra9f4t+8YApXnDaLf7t3OV3ZoPV5dyrX/fg9zzbyxQuPZWdHhv/382d6bLuvRsYvfelLLFu2jKVLlwLwyCOP8NRTT7Fs2bLu0UBuvvlmxo0bR1dXFyeffDJve9vbGD9+fI/9rFq1ittvv52bbrqJiy++mLvvvpvLL7+83+NeeeWVfPvb3+bMM8/kc5/7HJ///Of51re+xZe+9CXWrl1LRUUFLS0tAHzta1/ju9/9Lqeffjrt7e0kk8kB67QvRW3pNrNzzGylma02s+v7ePwoM3vczNJm9q8Fy+eZ2dKCv91mdm342A1mtqngsfOKWYeh+vn7TuWyUw6juiJo7T732MmMq07wnXeeUOKSiYiIiAzdj69axJhkz3baiMFbT5w2rMc55ZRTegy/d+ONN3L88cezePFiNm7cyKpVe3fqnD17NgsXLgTgpJNOYt26df3uv7W1lZaWFs4880wArrrqKh577DEAFixYwLve9S5+/vOfE4sFdT399NP553/+Z2688UZaWlq6l++vorV0m1kU+C7wRqAReNrM7nPOvViw2k7gn4ALC7d1zq0EFhbsZxNwT8Eq33TOfa1YZT8QE2uTfPHCYzll9jj+6fa/89GzjuTbl9WUulgiIiIi/RqoZXpTSxdZz/VYFo0YLlw0rjoxLOmz1dXV3bcfeeQR/vjHP/L4449TVVXFa1/72j6H56uoqNhTpmh0n+kl/bn//vt57LHHuO+++/jCF77A8uXLuf766zn//PN54IEHWLx4MX/84x856qij9mv/UNyW7lOA1c65Nc65DHAHcEHhCs65Jufc00B2gP2cBbzinFtfvKIOvyl1Sd44fxJViei+VxYRERE5RBWmzkYs6KuWT53dX7W1tQPmSLe2tjJ27Fiqqqp46aWXeOKJJ/b7WHl1dXWMHTuWv/zlLwD8z//8D2eeeSa+77Nx40Ze97rX8ZWvfIWWlhba29t55ZVXOO644/jEJz7BokWLeOmllw7o+MXM6Z4GbCy43wicuh/7uRS4vdeyj5jZlcAS4F+cc7v2r4jFc/KscZw8a1ypiyEiIiJyQH7+vlO58eHV3PHUBmZNqGbx7HE8sGzrAaXOjh8/ntNPP51jjz2Wc889l/PPP7/H4+eccw4/+MEPWLBgAfPmzWPx4sUHWg0Abr31Vj74wQ/S2dnJnDlzuOWWW/A8j8svv5zW1lacc3zsYx+jvr6ez372s/z5z38mGo0yf/58zj333AM6tjnn9r3W/uzY7B3Am5xz7wvvXwGc4pz7xz7WvQFo750yYmYJYDNwjHNuW7hsErAdcMAXgCnOuff2sc9rgGsADjvssJPWrx9RDeUiIiIiRbNixQqOPvroIW0znON0l4u+nkcze8Y5t6j3usVs6W4EZhTcn04QQA/FucCz+YAboPC2md0E/LavDZ1zPwJ+BLBo0aLiXFkM4E8vbeO6Xz7PHdcsZu6k2oN9eBEREZED0t+MlLOuv7/H/dE6I+VQFTPofhqYa2azCTpCXgq8c4j7uIxeqSVmNsU5tyW8exGw7EALWgyZnM+OjsxeHQ9ERERERoLeM1LKgSla0O2cy5nZR4DfA1HgZufccjP7YPj4D8xsMkFe9hjAD4cFnO+c221mVQQjn3yg166/YmYLCdJL1vXx+CHCAHAo6BYREREZ7Yo6OY5z7gHggV7LflBweytB2klf23YC4/tYfsUwF7MoLIi5KVLKvIiIiIiMIJoGvkgi+ahbREREREY9TQNfJFPqklx0wjTqKuOlLoqIiIjI0P35v+DRL+17vTOvh9d9svjlGeEUdBfJsdPq+OYlC0tdDBEREZH987pP9gymbwnH0n7P/X2vX2Q1NTW0t7cPevmhRuklIiIiIjKwtY/B5mfBywS3v7cY2rbtezvppqC7SB57uZl5n/kdSze2lLooIiIiIvtv7WNw28WQ7YTtK4Pbzavg0S/v9y4/8YlP8L3vfa/7/g033MDXv/512tvbOeusszjxxBM57rjjuPfeewe9T+cc1113HcceeyzHHXccd955JwBbtmzhjDPOYOHChRx77LH85S9/wfM83v3ud3ev+81vfnO/6zJYSi8pEt850jkfz9fwJSIiInKIu+X8vZcdcyGc8n544DrIdgXLUq17Hn/+TnjzN6BjB9x1Zc9t95GCcumll3LttdfyoQ99CIC77rqLBx98kGQyyT333MOYMWPYvn07ixcv5i1veQs2iAEqfv3rX7N06VKee+45tm/fzsknn8wZZ5zBbbfdxpve9CY+/elP43kenZ2dLF26lE2bNrFsWTDdS0tLyz73f6DU0l0ke94cCrpFRERkBLvsDkjW9VxmETj+0v3e5QknnEBTUxObN2/mueeeY+zYsRx22GE45/jUpz7FggULeMMb3sCmTZvYtm1waSx//etfueyyy4hGo0yaNIkzzzyTp59+mpNPPplbbrmFG264gRdeeIHa2lrmzJnDmjVr+Md//EcefPBBxowZs991GSy1dBdJd8itmFtEREQOdQO1TLduDHK5C0Vie4Kc6vH71bny7W9/O7/61a/YunUrl14aBPC/+MUvaG5u5plnniEejzNr1ixSqdSg9uf6CbrOOOMMHnvsMe6//36uuOIKrrvuOq688kqee+45fv/73/Pd736Xu+66i5tvvnnIdRgKtXQXSX6cbmWXiIiIyIj2u49DLhvctghEE0EQ/uJvDmi3l156KXfccQe/+tWvePvb3w5Aa2srEydOJB6P8+c//5n169cPen9nnHEGd955J57n0dzczGOPPcYpp5zC+vXrmThxIu9///u5+uqrefbZZ9m+fTu+7/O2t72NL3zhCzz77LMHVJfBUEt3kUyuS/KuUw+jobai1EURERER2X9X3Bt0mnz2Vhh3BMw6PQi43/HTA9rtMcccQ1tbG9OmTWPKlCkAvOtd7+If/uEfWLRoEQsXLuSoo44a9P4uuugiHn/8cY4//njMjK985StMnjyZW2+9la9+9avE43Fqamr42c9+xqZNm3jPe96D7/sA/Nd//dcB1WUwrL+m+HKyaNEit2TJklIXQ0REROSQsGLFCo4++uihbVTicboPRX09j2b2jHNuUe911dJdJM45PN8RMSMS0ZTwIiIiMsL0NyPlDb06VWpGykFR0F0kT67dyaU/eoLb3ncqrzpiQqmLIyIiIjI0vWeklAOijpRFogEDRURERCRPQXeR5MfpHgUp8yIiIjICjYZ+fcU01OdPQXeR5NO4ndq6RURE5BCTTCbZsWOHAu/95Jxjx44dJJPJQW+jnO4iyU9IqXG6RURE5FAzffp0GhsbaW5uLnVRRqxkMsn06dMHvb6C7iKZNCbJB86Yw/SxlaUuioiIiEgP8Xic2bNnl7oYo4qC7iKZPraKT543xPEvRURERKQsKae7SHKeT2tnlqznl7ooIiIiIlJiRQ26zewcM1tpZqvN7Po+Hj/KzB43s7SZ/Wuvx9aZ2QtmttTMlhQsH2dmD5nZqvD/scWsw/5avnk3x//7H3jsZeVKiYiIiIx2RQu6zSwKfBc4F5gPXGZm83utthP4J+Br/ezmdc65hb2m0rweeNg5Nxd4OLx/yMl3pFSnYBEREREpZkv3KcBq59wa51wGuAO4oHAF51yTc+5pIDuE/V4A3BrevhW4cBjKOuwsnB5HMbeIiIiIFDPongZsLLjfGC4bLAf8wcyeMbNrCpZPcs5tAQj/n3jAJS2CPS3dCrtFRERERrtijl5ifSwbSgR6unNus5lNBB4ys5ecc48N+uBBoH4NwGGHHTaEww4PjdMtIiIiInnFbOluBGYU3J8ObB7sxs65zeH/TcA9BOkqANvMbApA+H9TP9v/yDm3yDm3qKGhYT+Kf2Am1ib55zceyRETaw76sUVERETk0FLMoPtpYK6ZzTazBHApcN9gNjSzajOrzd8GzgaWhQ/fB1wV3r4KuHdYSz1MGmor+Kez5iroFhEREZHipZc453Jm9hHg90AUuNk5t9zMPhg+/gMzmwwsAcYAvpldSzDSyQTgHgtyNGLAbc65B8Ndfwm4y8yuBjYA7yhWHQ5E1vNpbksztipBZSJa6uKIiIiISAnZaOjot2jRIrdkyZJ9rziMXt7WxtnffIzvvPME3rxg6kE9toiIiIiUhpk902u4a0AzUhZNvhfpKLimEREREZF9UNBdJN1DBpa2GCIiIiJyCFDQXSRhPrrG6RYRERERBd3FovQSEREREclT0F0k46sr+Oyb53PstLpSF0VERERESqyYM1KOanVVca5+9exSF0NEREREDgFq6S6STM5n1bY2WruypS6KiIiIiJSYgu4i2bY7xRu/+Rh/WL611EURERERkRJT0F1k6kcpIiIiIgq6i8S6hy8paTFERERE5BCgoLtIIvlxuhV1i4iIiIx6CrqLJN/S7SvmFhERERn1NGRgkdRVxvny245j0axxpS6KiIiIiJSYgu4iqUrEuOTkw0pdDBERERE5BCi9pEiyns/fN+yiuS1d6qKIiIiISIkp6C6Sls4sF33vbzy4bEupiyIiIiIiJaagu0jyHSnVj1JEREREFHQXSfeQgYq6RUREREY9Bd1Fkp8bx1fULSIiIjLqKeguku70EsXcIiIiIqOehgwskqpEjG9fdgLHTB1T6qKIiIiISIkVtaXbzM4xs5VmttrMru/j8aPM7HEzS5vZvxYsn2FmfzazFWa23Mw+WvDYDWa2ycyWhn/nFbMO+ysRi/APx09lTkNNqYsiIiIiIiVWtJZuM4sC3wXeCDQCT5vZfc65FwtW2wn8E3Bhr81zwL845541s1rgGTN7qGDbbzrnvlassg+HnOfzxJqdzJpQxfSxVaUujoiIiIiUUDFbuk8BVjvn1jjnMsAdwAWFKzjnmpxzTwPZXsu3OOeeDW+3ASuAaUUs67BL5Xwu/8mTPPCCxukWERERGe2KGXRPAzYW3G9kPwJnM5sFnAA8WbD4I2b2vJndbGZj+9nuGjNbYmZLmpubh3rYA5YfvUQdKUVERESkmEG39bFsSCGomdUAdwPXOud2h4u/DxwOLAS2AF/va1vn3I+cc4ucc4saGhqGcthhoclxRERERCSvmEF3IzCj4P50YPNgNzazOEHA/Qvn3K/zy51z25xznnPOB24iSGM55BiaHEdEREREAsUMup8G5prZbDNLAJcC9w1mQzMz4CfACufcN3o9NqXg7kXAsmEq77Da09KtqFtERERktCva6CXOuZyZfQT4PRAFbnbOLTezD4aP/8DMJgNLgDGAb2bXAvOBBcAVwAtmtjTc5aeccw8AXzGzhQSZG+uADxSrDgciHo1wy7tPZk5DdamLIiIiIiIlZm4U5D8sWrTILVmypNTFEBEREZEyZ2bPOOcW9V6uaeCLxDnHg8u2srqpvdRFEREREZESU9BdJL6DD/78GY3TLSIiIiIKuotF43SLiIiISJ6C7iLR6CUiIiIikqegu0gsjLp9xdwiIiIio56C7mJTfomIiIjIqFe0cboF7vrAaUytT5a6GCIiIiJSYgq6i+iU2eNKXQQREREROQQovaSIfvP3TSzb1FrqYoiIiIhIiSnoLqLrfvUc92ucbhEREZFRT0F3ERmmfpQiIiIioqC7mMw0TreIiIiIKOguKjONGCgiIiIiCrqLKkgvUdQtIiIiMtppyMAi+vWHXsW46kSpiyEiIiIiJaagu4iOnjKm1EUQERERkUOA0kuK6M6nN/D0up2lLoaIiIiIlJiC7iL64v0ruP95jdMtIiIiMtop6C6iiFmpiyAiIiIihwAF3UVkBr5GLxEREREZ9YoadJvZOWa20sxWm9n1fTx+lJk9bmZpM/vXwWxrZuPM7CEzWxX+P7aYdTgQhsbpFhEREZEiBt1mFgW+C5wLzAcuM7P5vVbbCfwT8LUhbHs98LBzbi7wcHj/kGRmmpFSRERERIo6ZOApwGrn3BoAM7sDuAB4Mb+Cc64JaDKz84ew7QXAa8P1bgUeAT5RtFocgHs/fDrVFRqVUURERGS0K2Z6yTRgY8H9xnDZgW47yTm3BSD8f+IBlrNoZoyr0uQ4IiIiIjK4oNvMPmpmYyzwEzN71szO3tdmfSwbbK7FgWwb7MDsGjNbYmZLmpubh7LpsPnZ4+t4ZGVTSY4tIiIiIoeOwbZ0v9c5txs4G2gA3gN8aR/bNAIzCu5PBzYP8ngDbbvNzKYAhP/3GdU6537knFvknFvU0NAwyMMOr+/8aTUPLttakmOLiIiIyKFjsEF3vuX5POAW59xz9N0aXehpYK6ZzTazBHApcN8gjzfQtvcBV4W3rwLuHeQ+D7qImUYvEREREZFBd6R8xsz+AMwGPmlmtYA/0AbOuZyZfQT4PRAFbnbOLTezD4aP/8DMJgNLgDGAb2bXAvOdc7v72jbc9ZeAu8zsamAD8I4h1Peg0jjdIiIiIgKDD7qvBhYCa5xznWY2jiDFZEDOuQeAB3ot+0HB7a0EqSOD2jZcvgM4a5DlLiljiInoIiIiIlKWBptechqw0jnXYmaXA58BWotXrPJgSi8REREREQbf0v194HgzOx74OPAT4GfAmcUqWDn43398NfHovlLfRURERKTcDbalO+eccwQT0/y3c+6/gdriFWuEW/sYfG8x4/xd1G55HL63GNq2lbpUIiIiIlIig23pbjOzTwJXAK8Jp2mPF69YI9jax+C2iyGXZfPN72Li7heI+Tl49Mvw5m+UunQiIiIiUgKDbem+BEgTjNe9lWB2yK8WrVQj2e8+DrksuBzjdj1HzEuBy8GLvyl1yURERESkRAYVdIeB9i+AOjN7M5Byzv2sqCUbaW45D26og6YVQZANJMnsebxzR/D4LeeVqIAiIiIiUiqDSi8xs4sJWrYfIRgJ79tmdp1z7ldFLNvI8p5wdMN8ekm2a89j0QSccIXSS0RERERGqcGml3waONk5d5Vz7krgFOCzxSvWCJZPLwnliIKXUXqJiIiIyCg22KA74pxrKri/Ywjbji5X3AsnXQUVYwB4ofo0qBoP7/hpacslIiIiIiUz2NFLHjSz3wO3h/cvoY/ZIgWonRSkkcy/AH72Fo596/Vw+GtKXSoRERERKaFBBd3OuevM7G3A6QQ53T9yzt1T1JKNdOPmwFmfIz5+ZqlLIiIiIiIlNtiWbpxzdwN3F7Es5aV+Bt/3LmTcKrjk5FIXRkRERERKacCg28zaANfXQ4Bzzo0pSqnKQS7DE888Q+34KVxy8mGlLo2IiIiIlNCAnSGdc7XOuTF9/NUq4N6H7Su5te39zO9aUuqSiIiIiEiJaQSSYonEAYj62X2sKCIiIiLlTkF3sUSDoDvivBIXRERERERKTUF3sUSCdPkYuRIXRERERERKTUF3sYQt3e9ZPL3EBRERERGRUlPQXSzJejj3qzDj1FKXRERERERKTEF3sSSq+E7H6/jxy8lSl0RERERESkxBd7H4PquXPcnfV7xc6pKIiIiISIkVNeg2s3PMbKWZrTaz6/t43MzsxvDx583sxHD5PDNbWvC328yuDR+7wcw2FTx2XjHrsN/8LN/a+WHObH+w1CURERERkRIrWtBtZlHgu8C5wHzgMjOb32u1c4G54d81wPcBnHMrnXMLnXMLgZOATuCegu2+mX/cOfdAsepwIP62rhUA52f52yvbOfubj9LUlipxqURERESkFIrZ0n0KsNo5t8Y5lwHuAC7otc4FwM9c4Amg3sym9FrnLOAV59z6IpZ1WP3tle1cfeuzeM5obe/k6p8u4ZXmDm58eHWpiyYiIiIiJVDMoHsasLHgfmO4bKjrXArc3mvZR8J0lJvNbGxfBzeza8xsiZktaW5uHnrpD8AN9y0n4/nkiOFyWbqyHp7veOCFLQe1HCIiIiJyaChm0G19LHNDWcfMEsBbgF8WPP594HBgIbAF+HpfB3fO/cg5t8g5t6ihoWEIxd5/l/zwcWZdfz8vb2vH8x1ZokQLJsfZ2ZFh1vX3c8kPHz8o5RERERGRQ0OsiPtuBGYU3J8ObB7iOucCzzrntuUXFN42s5uA3w5XgQ/UnR84DQjTS366hE9lr2admwxAPGpcsmgGX7zouFIWUURERERKoJgt3U8Dc81sdthifSlwX6917gOuDEcxWQy0OucKczAuo1dqSa+c74uAZcNf9AOTTy+5zz+d5RxBPGpkPccDy7aWumgiIiIiUgJFC7qdczngI8DvgRXAXc655Wb2QTP7YLjaA8AaYDVwE/Ch/PZmVgW8Efh1r11/xcxeMLPngdcBHytWHfbXz993KpedchgnxtZxVHwrlyyawbjqBN955wmlLpqIiIiIlIA51zvNuvwsWrTILVmy5KAft+W/juKRriO48N8PmQwYERERESkiM3vGObeo93LNSFlMFsOcR9bzS10SERERESkhBd1F5CIxYuTozHilLoqIiIiIlJCC7iKKxOLUxiGTU0u3iIiIyGhWzCEDR7266irOmFQPtRWlLoqIiIiIlJBauotl7WPQtQtOem9w+3uLoW3bvrcTERERkbKjoLsY1j4Gt12M39rIE7d/Ae/n78Bvfpn7vv1RmtpSpS6diIiIiBxkCrqL4Xcfx89liDiPE1hJ1EsRcR6np//KjQ+vLnXpREREROQgU9A9nG45D26og6YVPOfNIuciVFgOgC6X4DPZ9/LAEy8E64mIiIjIqKGgezi95wEumfIgl2U+zTzbSMz2jFoSJcd/xn9MFJ9ZKz/MJT98vIQFFREREZGDSaOXDLM7P3AafO9j+E257mW+g4T5xF0H3572EIv/8aelK6CIiIiIHHRq6S6GK+6lnSqcC+46DAAzOHLHwyUsmIiIiIiUgoLu4XbLefD1I8k4Y52bCEDUgug75eI0e2OCvG/ldYuIiIiMGkovGW7veQCACWsfY8JtF0N2z0PJmDHvhDfBm79RosKJiIiISCmopbtYwmEDATJWgR+Jg5fBX/6b0pZLRERERA46Bd1F8vQZt3CHdxadLkGbn+De3Gm0ukpacgnNTCkiIiIyyijoLpJPP9TE/d4pJMkyljYmuu3E8RiT3QaPfrnUxRMRERGRg0g53cPskh8+zpNrdwLw7cRPAUfE4OTIShLmAbDj6bv40KZ3BMMLioiIiEjZU0v3MLvzA6exbt53uS3+Rb6bewt+OFxgPuDOugjjrY07t5wDP35DKYsqIiIiIgeJgu5ieM8D3DPmnXwl/iOiuB4PxcyHSAwsBpMXlKiAIiIiInIwKegukv9K/pwKy2HWc7kB+DlwOXjxNyUomYiIiIgcbEUNus3sHDNbaWarzez6Ph43M7sxfPx5Mzux4LF1ZvaCmS01syUFy8eZ2UNmtir8f2wx6zBkt5wHN9QR2/ESnS7RPStlnzp3aKIcERERkVGgaEG3mUWB7wLnAvOBy8xsfq/VzgXmhn/XAN/v9fjrnHMLnXOLCpZdDzzsnJsLPBzeP3S85wG4oRWu+l8qLbtXS3cPDUcF64YT6oiIiIhIeSpmS/cpwGrn3BrnXAa4A7ig1zoXAD9zgSeAejObso/9XgDcGt6+FbhwGMs8LJb/x6vh1n/Awmbuflu7m19SZ0oRERGRUaCYQfc0YGPB/cZw2WDXccAfzOwZM7umYJ1JzrktAOH/E4e11MPgmE//Ff7lZXYmpuIcfbZ2dwfijU8rvURERESkzBVznO6+Eit6t/kOtM7pzrnNZjYReMjMXnLOPTbogweB+jUAhx122GA3Gz61k2jJxRjXz8PdgXjlWKWXiIiIiJS5YrZ0NwIzCu5PBzYPdh3nXP7/JuAegnQVgG35FJTw/6a+Du6c+5FzbpFzblFDQ8MBVmXoLvnh41zadT2/9F6N7/a0bBemmjgHdO0KOlP++A2w9jH43mJNEy8iIiJSZooZdD8NzDWz2WaWAC4F7uu1zn3AleEoJouBVufcFjOrNrNaADOrBs4GlhVsc1V4+yrg3iLWYb/kZ6Vspp6P5z7Eqenv0U5yr1QTs3yzvkGsEm67GJpXaZp4ERERkTJTtPQS51zOzD4C/B6IAjc755ab2QfDx38APACcB6wGOoH3hJtPAu6xIEKNAbc55x4MH/sScJeZXQ1sAN5RrDrsr/z07n97ZTvvuulxmqlnmxtLjW3Za90gBnew7i90Z9a8+Bt48zcOUmlFREREpNjMDTiQdHlYtGiRW7Jkyb5XHGZzPnk/fvj0nhN5ku/EbyRmPZ/v/jpaAjD9ZDjrc/C7j8MV90LtpOIWWEREREQOiJk902u4a0AzUhbVE586i4m1FQB8LHZ3n71GB2RRpZyIiIiIlAEF3UU0sTbJU59+A4mocXnmU+ymaq+OlANOnrPxKch2BVPGv/BLdbIUERERGaEUdB8Ev/nI6TRTx9npr/CofxzODSLgBsDfczO9G5pWwA/PKGZRRURERKQIijlOt7BnJBMwmhnLHNsKDCbg7oefHbayiYiIiMjBoZbuIrvzA6dx6uxxfDb2M5KkyRDba4ag/vTZx7VzRzCut2axFBERERkxFHQfBHd+4DT+w7uSFBV8LXcxbpBdKvdqDY/GYdHVcEOrZrEUERERGUEUdB8kT3zqLKIR42Oxu4kUtHXvNUPlQLwsLLkZvjEfvr0o6FRZOIulZrQUEREROSQp6D5IJtYmefyTr+fyzKf4pfcaUi5Om6vkMf84Mi7ab8fKvQNxB7s3wY7V8L8f3TOk4N3v0/CCIiIiIocoTY5zkL24pZULv/N/ZLw9z/vvEx/nSGsceudKiwIWDCkYS0IuFSyvGg8fXzNsZRYRERGRwdHkOIeI+VPq+Ov1r6euMt697PLMp2ilsrtVe9DXQc4LAm7YE3CDOluKiIiIHGIUdJfAxNokD/3zGbztxOlUxCI0U88b01+nhepBjt9N91jf/cbnR54LXTuV3y0iIiJyCFDQXSITa5N8/eLjSUSNGWxjOzWcnf4KT/tzB9XSbRb+9bfCy78LJtP547+pg6WIiIhIiSnoLrEXPn8Od3/6nVQlKjgispkFkbX9dqjcr/T7526H/3lrEID/7hN7AvDl92rUExEREZGDRDNSHgIm1ibJej43xG6lglz38t6pJgc8i+WLv4FVD0I2DXe/N8hNuft9sOlpyGWDUU/e/I39rYaIiIiI9EMt3YeI33zkdD5X9x97DSeYcnE8t7/Rdm8Osl2AD34u6ITZ+FSwzOWCoFxEREREhp2GDDzENLWlWPyfD+MXvCwNtPBQxb9QRxdme9JMhtLyXdhqvs/tZ56uGS9FRERE9oOGDBwhJtYm+e0/vZo5E6q6l+VHN/kf7w3scLVcn30fOSJDyvHunabSZ8B9/GWaYl5ERESkCBR0H4LmT6njT//6OtZ96fzu4LuZej6Xey8npX/IBiaRJdojcB4oAB90cL7s1/tfaBERERHpl4LuQ9wdHziNt504nUR0T4R9Q+xWEngApF1swLG9BzvuNwBeWhPqiIiIiBSBcrpHkKa2FF/+3Ur+tnQZ/y/ya86PPknKJZhiO4gcYF/LoLOmI2rA9JPhrM/B7z4OV9wLtZOGo/giIiIiZa8kOd1mdo6ZrTSz1WZ2fR+Pm5ndGD7+vJmdGC6fYWZ/NrMVZrbczD5asM0NZrbJzJaGf6OmaTY/oc7j//kurvzi3Yz/fCP/XPcNWsOZLKFnKslQrqciOCKAwyBWCT9/mybXERERERkmRQu6zSwKfBc4F5gPXGZm83utdi4wN/y7Bvh+uDwH/Itz7mhgMfDhXtt+0zm3MPwb1b3+vn3NObzNvsn/eG9gp6vm0XCYwXZXgYcNKvDOp6AEM1w63LrHwMsEDxZOrqMAXERERGS/FHNynFOA1c65NQBmdgdwAfBiwToXAD9zQY7LE2ZWb2ZTnHNbgC0Azrk2M1sBTOu1rRC0ft/xLxdw48PHcNIT68nH2KdFlvGT+Fepsmy/2xYG2z3u914nP7nOC78MxvLWRDoiIiIiQ1LM9JJpwMaC+43hsiGtY2azgBOAJwsWfyRMR7nZzMYOW4lHqIm1Sb544bE8+emzuHzxTOor43w+/j89Zrcs5Bz4fXSw7KvDZY8RUrxc90Q67plbNIW8iIiIyCAVM+juq2tf72SHAdcxsxrgbuBa59zucPH3gcOBhQSt4V/v8+Bm15jZEjNb0tzcPMSij0z54Hvpv53NuP/3AOlYTfeTWZhq0k6SNqqGlPMNvSbX8X3SX5uP/9N/wDWthO+eqsBbREREpB/FDLobgRkF96cDmwe7jpnFCQLuXzjnugeQds5tc855zjkfuIkgjWUvzrkfOecWOecWNTQ0HHBlRpoJkw+j6tpnsEVXQ9V4ou/4KbuOuZwdrpZrsv/MJ7LvJ9fnNc++5VNSEuSIGBg+LrWLp751Md5XjoAvNMDWZfC378AXJu65/e8T4L9PgBfvhW8dB99epEBdRERERoWiDRloZjHgZeAsYBPwNPBO59zygnXOBz4CnAecCtzonDvFzAy4FdjpnLu2137zOd+Y2ceAU51zlw5UlnIZMnA45IcdvGbZOznCNhG14Xv9C99KPgTDGDrY5aoZG+nofswIHwBI1EDtFHj9Z+HR/9IQhSIiIjKiHfQhA51zOYKA+vfACuAu59xyM/ugmX0wXO0BYA2wmqDV+kPh8tOBK4DX9zE04FfM7AUzex54HfCxYtWhHOWHHZx33Z+wSHTIKSYD6R4BxYI3loXLxlpHcJt8PtGeg7pMO27HKtwv3x2MkPL1I+Fv34avzoV/b4Bnbu2ZM16YQ36g+eTDuS8RERGRAWhynNHqlvNg/f8FHSoZwqyVw6iv2TJd4Q0LWsV9i/Db+Nm85oL3MebX78J5WfwZp5HY+kwwkspJV8ExF+6ZzGf7yn1P7LP2Mbjt4mD7ma+CTU/v2ZdGZREREZH91F9Lt4Lu0e7Fe+GuK/t8aEhTyA+jPoNxBz5GBIdZd0wePJa/bRGIxPeMMW4GH/gLrHkEHv53eP+fYPKxwWPfWwzNq8DlgsmAcl3B8qrx8PE1PQ++9rGhBfQiIiIyapVkRkoZAR75LyAKBMGrHy52hLNTlkBfATcEs2Z2r1P4mMv/7+NyafKld76P/4PTcX/4NC6Xxv/hGfDv4+GGuiCVxYVDKuYDboDOHcHj339VEJgvvxd+/vZg/buu3HP7jzcMT2X7SnFZfq/SXkRGKqWtFc9oe25HW333xwh7jhR0j3ZX3AuL3g1V47F33EokHO3E6mcSiQTBONEk3dnY+znN/IEozBXvbyzx/PK9JvopSJ8x38P5OVwYpPcuf+Fyt205XtMK3C+vxHnp4PGNT0D+9nO3wf9+DG4YGwTpX5oJnx8Lnx8PX5sH35gfjM6SD6CX3wvfWrD3stsuDlrc735fcLtpJdz93uD/GxfCL94R3j5xz2gvvU8s+ZFhvnYUfOXwsAxH9T06zME+QZXqhFjKE/HB7newr3UOpS+loZTlUCr3YMuUT1vr/fnND6m69rE9IzcdyChOQ31u9ve5LGwI6H3+6r2vYr9G+ee28HzZvCqYqK1Uivl+PhTrW6iwDoUjlRXz/djXfg7l56gPSi+RvrVtC964L/4Gzv9G8OZe/mvaxh9HovFxci5od64mHYxUQmlSUYqhML0l//Hor/W9W35Aln6eg56LI+z5TWGIYpVB+ozzgr3Gkj1b6ntLVMNp/wh/+TpUTYDULvCyEImFaTgROPJsaFnXM33mzE/CQ5+FaKLnyDLbV8K9Hwbfg1waunYG+6ieAH4O0rvhvK8Fx8t0BL8c4KDuMGjdEJRpygmwbRnUTYc3fD7Y95mfhIc+E+y3cHQbgHgVvPv+YEbUfJpQ186+U37y5WtvAi+3d75+Pve/sH7Hvg3++s1gv/lUpPO+Cn/6D+jaAdUNwXOWaoG6GbDgkj3r9y5H97GzEKsA5wflGEy/g7WPBdtHK+Csz8L9/wodzUHa1MzTg3pk05Csgw8/ued4bVuD17LhaGheEexr0rHQvDJIl+raGTw+YzFsfa7vvgsDpVD191j++IXl7doZvNZvvAH+8Jngsfz758xPwq/fH1y4FpY1/36oGh+8L+OVwWvy6JeD90I8GTyPuUzPuufL8eiXYclPoHpisF7nzmC/ZlAzMXgtunaCReH8r8NfvhaU69i3Be/Tuhmw4OI9t99wQ8/3e/44L/wyqBMOJh4NTS8Ft49/J1z0/eCY31scBNl9fb7HHwE7Vvf9OZ1/YVD/JTfD2V+Ap27a+7P3wi+DX9iqJkBH055zgEWC2/FKiFcHn8FTroEnfxh8xhZcEjxHFoGJx8C25/d+3gs/u3/6YvB8JcZAamcfhQ0/nzNPh41PgZ+FqScGr+dg+8Xk31P9nWcKPxP55/8nb4SW9cHyWBJyqeB21Xh4x0/3fi+mWuD9fx74M937feF7e843he/v/P18mfPv57uvDuo/fm742jo4/Cxo27x3Pbr7EJ0GGx4PnvOZp8P2VdDVAqdeE7zu+fPKz98erOO8Pl6CWHDOjUSD8vZ3jo5XwXFvg8e+vud1TrX0PKdVTQg+d7jgnJ5//zwRvqeTdcGyqvB4eZ07gs9k4XvdosHnzs9BRS0c9WZ47vbgc7XwXcF587yvwoOf6NmfKn9eO/UD4edwes/z7ORjg8D+jzf0/LxC8JnPfxa6vzciMP7wnq/bQU4JVU63gu6iyX37VGzHKqJ4eG7vADwfxPYVwB6svPFS56f3l6cOez8fvZf1tT/oOdNUf1Xr69NdHtdGBRcukVhwkrdo319Qg9Yr2N+XeDVkw6Ewa6cGX7SDOkwk+KIAmPnq4GIk0wmd2wd/7Eg8+LI/IJHgzRRN7LlwK3wOJxwVfGEN5TkpK4UXx/t4b1Q1QOfomIRtcMLnq6IeMruD99WpHwguBBI1wcV/4Xr9bT9ihT+/Lv5QUGcIzlEjuk4lYtHgwifXOfRtEzXBxYWX7nlxfBAo6FbQXTyFreKxJOzeDDj8SIKc75HAw3fw4exHeX3kWf4h+jgRfOL4fQai5dJifijq/fz2DuL11IuISNmJJeEzBy89TR0ppXhqJwU/J358TfBz3qL3QtV4Im+7iUSYL9568d2MP+Vi/qvio9xz/t/ZFJ2OF779cgX51T5751v3lXst+6f3BU3hfQXcIiJSlnKpoP/VLefte90iUku3lEZfOeMv/ibIzZswD/74b7gXfgV+tkcw6Bx0UEE1aTzy467sncIxUIu5WtNFRERGoZmnw3seKPphlF6ioHvk6T2WtvOCDmGFY2mHwbu/7Ne8YjOZ3fkcERxr/MkcHtm6z3zp3jnX+8qpFhERkRHoIAXcoPQSGYmuuDfoCV81Hi76AZxwxZ6e6nlhakvk+nXM/cSjxD7fQuTzrRwxZSwWiQHghSN89wi2gayLdE+087fYKexwtfxn9SdopbI7+B4o1aVwHa+PYQiHcj1bjGvfUXA9LQLovS4i+1Ax5qAF3ANRS7eUp36GPOSw02Djk3untMw+o89tW8/6Misf+yULWv5ElhjL7EhOcC+SJcoT/nxOjKziI9l/4nH/GBpo4brY7bwl+gRxchiOSK/W88KPW36oxS4SeESoITWoNBnngguFSK8Weh+I9jpGX/sZbHrNQNvky5Bf1PtXgv5uD/X4hfsY7Pb7qv9gj1m4bKi/fPT1fPXe72C3Haqhplb1NcLOUJ/voTxfg9l39wUtPfsaDDQC0GBv7+v92V+ZCw2183Hhei78Z6DnOn846297IB2voyLb2u96Ax1/X2UuPL46WMvIZ3DVfT2/54t9RKWXKOiWg6OpLcXNDz7J7OXf5cLEU/x6yj8TXf8XLqpYQvtbbuLrqybzwAtbuO5N87jlr2t5uamdOck23p37JW+OPs7z/uEsjqwgG2as54PxnIsQDUd8Sbk4nSRY4h/FSZGX+Uz2vbwqspzzok/ykew/scuv4duJ/+YI20qKBC2uisnWAsBz/myOimwkgk8Unwh7vljzFwIAL/ozmBdpJIrDx4iawznIESVuHjtcLeekv8w/xn7N+dEn+Ur2Et4T+x1H2ib+O/dWJthuzos+yaez7+X1kWd5S/QJssR4wZ/JKZGX9srH7+t2zhkRHM/7szkq0kiOCC2ummm2kxxBTr8BXhgeRID/yL2L+7zTB30B1F9glnNGFNfv4zBwepJz4IXP549z5/H66LMcblvD0lj3cz9QGQqPkb+d32dhQJQjLCt7X5D1Lmdfx/MdPOEfzSmRl4jg2OVqGGftOCBNnCRZzMALX4/89u0kecafy+LICgxHljitroqptrPHcQrL9Fz4WmaJYfhUk+6zrjtcLSelg+HWGmjhDxXXUU9Hd3kt3H/KxekgyafDz0B/78Xzo4/v9Xl5S/T/SJKlwnI9PsN9PXf593v+feWFn5wqMj227et5ft6fzbxIIxnifDz7/h6fh2f9Izg18hJZYjzuz+ekyMt8JXsJ74v9liNsK10keNqfx6mRl8gQ5wPZj/G4f0z385IvT35fr4m80P3cBJ9XIx5+druI0+qqu88FhQF1Dgs+JzDgc5b/TPf1XN8Uvs+PCN/nkXB/+7pAzxIlYg5zDjPHcuYwz60nbt5efXr6u5gtvN3uKmh0EzjSNrHGpjOHRow9/YFwPT87hiMKpCzOTr+WqbYTzyyYXM0cqegYKnO7wcCqxgdjVFsUH8NzPtFwFuQOKqmha+gXKZXjoWsHXiSJ72eJhWc0L3wO9774yS8B5p0bjJf+5m/Cmkf3NDT977V7hmbsMZxqwbYFtwa+wCrY5rSPwOo/BvMAnPmJYC6Bwgatsz4XjPHdvBJO+3C47ksw9QTY9mJQDosEQ5VOOR7WPw74wdwC214MbuePN+FI2P5ycNxoAirHQnvBCCS9h4mtHAtdYZ3zr1O8Ct5550ENuEFBt4JuGZkG6nA6+wxe3NLKtXcs5eVt7cydWM2qpmDc6J6n1QPTQEt3YN07uM9/+R/ofvsL2A/0GAMd56bceZwZfW6fgVlhkNH7ouhxfz6LIiv7vfjZV9n7em7P73GMPUFZ74BroP31Vc6TIqt6BEuDLev+vP6D3aavoLG/uhbjffj7xMc53DYTM58uFydOjpg5Mi64JCp87vZ1nGJ9TobqUCnHgZYreG02EQsvGGBPcJ0hSpoK/iP7zqKdN/ZH7zIXXvB8a5Dl6++cMNRzzEDP92p/2oDHKOb5uD/JmJHKBY0jiViEVDYYIz8ZNVLevr/NLjphKr/5++bgAh8gfK9EIjB9bBW//OBpTKxNFqXsfVHQraBbZNCa2lLc+PBqHnhhC1+88Fj+tno7DyzbynfeeQKvOnxCj8eve9M8bnr0Fdbs6BzxU1rI6HKoBqgyMl+bkVjm0eLyxTP54oXHHrTjKehW0C1ySNpXgL+v9b544TE8vKKZ/31uE354OvN919dE3CIiMgqNq07w7GffeNCOp6BbQbeIHKCh/gJwy1/XsqqpnatfPYvHXt7Oy03tHD25hpXb2vEd1FfGaOkK8okTUcj0M4v9mGSU3akDmeJeREROnT2OOz9wWtGPo6BbQbeIjGID/VLwt1d2di9/+MVt/PaFLSRiEb78tgU97n/qvKO7U4mSMWNMMk5Te9CRsPACQkTkUHL5qYfxxYuOO2jHU9CtoFtEZFQZ7IXG31Zv57fPb2HRrLE8u6Glx7rXvelIbnpsDWu2dxKBvdKWIuFoKgBViSjXvmEu3354FW1pj0TUqKtM0NyePuC6RAlGrhGRoVN6yUGkoFtERKQ4htov47fPbeaoybU8uXZnnx2ve1/IfPbN87t/YamMR1gwrZ6n1gXbJmPG2KoEW3anScYjnDxrLE+8soPsPjp15IewzI92oT4g5asqEeXHVy3q8V4sNgXdCrpFRERESqL3xdnDL27jf58PhvmLRyPUJWM9Lp6eWrurO82t8EKuvirOtXcsZdW2di48YSr3P7+FjOc4dfZYdnZkWd3UzqfOP4r3v+bwktVVQbeCbhEREREpsv6C7khfKw/jQc8xs5VmttrMru/jcTOzG8PHnzezE/e1rZmNM7OHzGxV+P/YYtZBRERERORAFS3oNrMo8F3gXGA+cJmZze+12rnA3PDvGuD7g9j2euBh59xc4OHwvoiIiIjIIauYLd2nAKudc2uccxngDuCCXutcAPzMBZ4A6s1syj62vQC4Nbx9K3BhEesgIiIiInLAihl0TwM2FtxvDJcNZp2Btp3knNsCEP4/cRjLLCIiIiIy7IoZdFsfy3r32uxvncFsO/DBza4xsyVmtqS5uXkom4qIiIiIDKtYEffdCMwouD8d2DzIdRIDbLvNzKY457aEqShNfR3cOfcj4EcAZtZsZuv3tyIHYAKwvQTHLSXVeXRQnUcH1Xl0UJ1Hh9FY51KZ2dfCYgbdTwNzzWw2sAm4FHhnr3XuAz5iZncApwKtYTDdPMC29wFXAV8K/793XwVxzjUMQ32GzMyW9DVkTDlTnUcH1Xl0UJ1HB9V5dBiNdT7UFC3ods7lzOwjwO8JZrC92Tm33Mw+GD7+A+AB4DxgNdAJvGegbcNdfwm4y8yuBjYA7yhWHUREREREhkMxW7pxzj1AEFgXLvtBwW0HfHiw24bLdwBnDW9JRURERESKp6iT40iQUz7KqM6jg+o8OqjOo4PqPDqMxjofUkbFNPAiIiIiIqWklm4RERERkSJT0F0kZnaOma00s9VmVjZT1ZvZzWbWZGbLCpaNM7OHzGxV+P/Ygsc+GT4HK83sTaUp9f4zsxlm9mczW2Fmy83so+Hycq5z0syeMrPnwjp/PlxetnXOM7Oomf3dzH4b3i/rOpvZOjN7wcyWmtmScFm517nezH5lZi+Fn+vTyrnOZjYvfH3zf7vN7NpyrjOAmX0sPH8tM7Pbw/Naudf5o2F9l5vZteGysq7ziOOc098w/xGMuPIKMIdgzPHngPmlLtcw1e0M4ERgWcGyrwDXh7evB74c3p4f1r0CmB0+J9FS12GI9Z0CnBjergVeDutVznU2oCa8HQeeBBaXc50L6v7PwG3Ab8P7ZV1nYB0wodeycq/zrcD7wtsJoL7c61xQ9yiwlWAM4bKtM8EM1muByvD+XcC7y7zOxwLLgCqCQTL+CMwt5zqPxD+1dBfHKcBq59wa51wGuAO4oMRlGhbOuceAnb0WX0DwRUb4/4UFy+9wzqWdc2sJhoY85WCUc7g457Y4554Nb7cBKwhO6OVcZ+ecaw/vxsM/RxnXGcDMpgPnAz8uWFzWde5H2dbZzMYQNBz8BMA5l3HOtVDGde7lLOAV59x6yr/OMaDSzGIEgehmyrvORwNPOOc6nXM54FHgIsq7ziOOgu7imAZsLLjfGC4rV5Occ1sgCFKBieHysnoezGwWcAJBy29Z1zlMs1hKMOPrQ865sq8z8C3g44BfsKzc6+yAP5jZM2Z2TbisnOs8B2gGbgnTiH5sZtWUd50LXQrcHt4u2zo75zYBXyOYy2MLwcR7f6CM60zQyn2GmY03syqCOVBmUN51HnEUdBeH9bFsNA4TUzbPg5nVAHcD1zrndg+0ah/LRlydnXOec24hMB04xcyOHWD1EV9nM3sz0OSce2awm/SxbETVOXS6c+5E4Fzgw2Z2xgDrlkOdYwTpcd93zp0AdBD85N6fcqgzAGaWAN4C/HJfq/axbETVOcxbvoAgbWIqUG1mlw+0SR/LRlSdnXMrgC8DDwEPEqSO5AbYZMTXeSRS0F0cjQRXmHnTCX7aKlfbzGwKQPh/U7i8LJ4HM4sTBNy/cM79Olxc1nXOC396fwQ4h/Ku8+nAW8xsHUE62OvN7OeUd51xzm0O/28C7iH4ebmc69wINIa/3AD8iiAIL+c6550LPOuc2xbeL+c6vwFY65xrds5lgV8Dr6K864xz7ifOuROdc2cQpIGuoszrPNIo6C6Op4G5ZjY7bF24FLivxGUqpvuAq8LbVwH3Fiy/1MwqzGw2QaeOp0pQvv1mZkaQ/7nCOfeNgofKuc4NZlYf3q4k+AJ7iTKus3Puk8656c65WQSf1z855y6njOtsZtVmVpu/DZxN8BN12dbZObcV2Ghm88JFZwEvUsZ1LnAZe1JLoLzrvAFYbGZV4Tn8LIL+OOVcZ8xsYvj/YcBbCV7vsq7ziFPqnpzl+keQT/UyQY/gT5e6PMNYr9sJcuSyBFfKVwPjgYcJrqofBsYVrP/p8DlYCZxb6vLvR31fTfCT2/PA0vDvvDKv8wLg72GdlwGfC5eXbZ171f+17Bm9pGzrTJDf/Fz4tzx/nirnOod1WAgsCd/fvwHGjoI6VwE7gLqCZeVe588TNBYsA/6HYJSOcq/zXwguIp8DzhoNr/NI+9OMlCIiIiIiRab0EhERERGRIlPQLSIiIiJSZAq6RURERESKTEG3iIiIiEiRKegWERERESkyBd0iIjJoZvZaM/ttqcshIjLSKOgWERERESkyBd0iImXIzC43s6fMbKmZ/dDMombWbmZfN7NnzexhM2sI111oZk+Y2fNmdo+ZjQ2XH2FmfzSz58JtDg93X2NmvzKzl8zsF+Gsf5jZl8zsxXA/XytR1UVEDkkKukVEyoyZHQ1cApzunFsIeMC7gGrgWefcicCjwL+Fm/wM+IRzbgHwQsHyXwDfdc4dD7yKYDZagBOAa4H5BDNbnm5m44CLgGPC/XyxmHUUERlpFHSLiJSfs4CTgKfNbGl4fw7gA3eG6/wceLWZ1QH1zrlHw+W3AmeYWS0wzTl3D4BzLuWc6wzXeco51+ic84GlwCxgN5ACfmxmbwXy64qICAq6RUTKkQG3OucWhn/znHM39LGe28c++pMuuO0BMedcDjgFuBu4EHhwaEUWESlvCrpFRMrPw8DbzWwigJmNM7OZBOf8t4frvBP4q3OuFdhlZq8Jl18BPOqc2w00mtmF4T4qzKyqvwOaWQ1Q55x7gCD1ZOGw10pEZASLlboAIiIyvJxzL5rZZ4A/mFkEyAIfBjqAY8zsGaCVIO8b4CrgB2FQvQZ4T7j8CuCHZvbv4T7eMcBha4F7zSxJ0Er+sWGulojIiGbODfTrooiIlAsza3fO1ZS6HCIio5HSS0REREREikwt3SIiIiIiRaaWbhERERGRIlPQLSIiIiJSZAq6RURERESKTEG3iIiIiEiRKegWERERESkyBd0iIiIiIkX2/wHyt7ul4DCjnwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 864x1008 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history, n_epochs=n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "fc83567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b66ef19250>"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,n_epochs+1), (history['loss_on_train']), label='train')\n",
    "plt.plot(range(1,n_epochs+1), (history['loss_on_test']), label='test')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "6986b941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv_stack): Sequential(\n",
       "    (0): Linear(in_features=7, out_features=300, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (7): Tanh()\n",
       "    (8): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (conv_stack1): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=100, out_features=30, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       "  (conv_stack2): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=100, out_features=30, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       "  (conv_stack3): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=100, out_features=30, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "c7685f5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc for Lsat= 0.05240373882380398 \n",
      "acc for Psat= 0.09526704724388893 \n",
      "acc for optim= 0.14665070495220148\n"
     ]
    }
   ],
   "source": [
    "def relative_abs_error(true, pred):\n",
    "    num = (np.abs(true - pred))\n",
    "    den = (np.abs(true))\n",
    "    squared_error = num/den\n",
    "    rrmse_loss = np.sum(squared_error)\n",
    "    return rrmse_loss/len(true)\n",
    "\n",
    "\n",
    "def test(model, val_loader):\n",
    "    cumloss1 = 0\n",
    "    cumloss2 = 0\n",
    "    cumloss3 = 0\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    l3 = []\n",
    "    l4 = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x_train, y_train = batch # parse data\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device) # compute on gpu\n",
    "            y_pred = torch.cat(model(x_train),1) # get predictions\n",
    "            y_pred = scaler2.inverse_transform(y_pred.cpu().detach().numpy())\n",
    "            y_train = scaler2.inverse_transform(y_train.cpu().detach().numpy())\n",
    "            loss1 = relative_abs_error(np.exp(y_pred[:,0]), \n",
    "                                                    np.exp(y_train[:,0])) # compute loss\n",
    "            loss2 = relative_abs_error(np.exp(y_pred[:,1]), \n",
    "                                                    np.exp(y_train[:,1]))\n",
    "            loss3 = relative_abs_error((y_pred[:,2]), \n",
    "                                                    (y_train[:,2]))\n",
    "            cumloss1 += loss1\n",
    "            cumloss2 += loss2\n",
    "            cumloss3 += loss3\n",
    "            l1.append(loss1)\n",
    "            l2.append(loss2)\n",
    "            l3.append(loss3)\n",
    "            l4.append(x_train.cpu().detach().numpy())\n",
    "    return cumloss1 / len(val_loader), cumloss2 / len(val_loader), cumloss3 / len(val_loader), l1, l2, l3, l4\n",
    "\n",
    "\n",
    "l = test(model, test_loader)\n",
    "print('acc for Lsat=', l[0],'\\n' 'acc for Psat=', l[1], '\\n' 'acc for optim=', l[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "39faae6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Lsat error')"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(l[3])\n",
    "plt.title(\"Lsat error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "fe589d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Power error')"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(l[4])\n",
    "plt.title(\"Power error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "4214a0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.068814\n",
       "7    0.078765\n",
       "2    0.084265\n",
       "0    0.085898\n",
       "6    0.092084\n",
       "1    0.092343\n",
       "4    0.103825\n",
       "3    0.110305\n",
       "8    0.141104\n",
       "dtype: float64"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(l[4]).sort_values().tail(20)\n",
    "#scaler.inverse_transform(np.array([X_test[103].cpu().detach().numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "c55302e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'optim error')"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(l[5])\n",
    "plt.title(\"optim error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "6404a9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lsat= 13.647080039978027 Psat= 20210030.326433476 loptim= 5.104742674048302e-07\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import jv\n",
    "X = pd.DataFrame({'k':[2.13], 'lu':[2.8], 'I':[56],  'sgamma':[0.0002], \n",
    "                  'r':[0.00012], 'log0':[10], 'sqgamma': [np.log(300)]})\n",
    "\n",
    "P_0 = X['log0'][0]\n",
    "\n",
    "ksi = X['k']**2/(1+X['k']**2/2)/4\n",
    "f = jv(0,ksi)-jv(1,ksi)\n",
    "rho = 1/2/(np.exp(X['sqgamma']))*(X['I']/X['r']**2/np.pi/4/np.pi/17000*(X['lu']/100*X['k']*f)**2)**(1/3)\n",
    "\n",
    "X['log0'] = np.log(X['log0']/X['I']/np.exp(X['sqgamma'])/511000)\n",
    "\n",
    "X2 = scaler.transform(X)\n",
    "\n",
    "input1 = torch.Tensor(X2)\n",
    "\n",
    "a = (torch.cat(model(input1.to(device)),1)).cpu().detach().numpy()\n",
    "a= scaler2.inverse_transform(a)\n",
    "\n",
    "\n",
    "\n",
    "L_sat = np.exp(a[0][0])*X['lu'][0]\n",
    "P_sat = (np.exp(a[0][1])*np.exp(X['sqgamma'])*511000*X['I'])[0]\n",
    "\n",
    "print('Lsat=', L_sat, 'Psat=', P_sat, \n",
    "'loptim=',(((a[0][2])*rho+1)*X['lu']/100/2/np.exp(X['sqgamma'])/np.exp(X['sqgamma'])*(1+X['k']*X['k']/2))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "9992987d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1707"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "38dd6206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21475806.967502937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b66efae700>]"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def P(z,P_0,P_sat,L_sat):\n",
    "    def A(z,L):\n",
    "        return 1/9*(3+2*np.cosh(z/L)+4*np.cos(3**(0.5)/2*z/L)*np.cosh(z/(2*L)))\n",
    "    L = L_sat/1.1/np.log(9*P_sat/P_0)\n",
    "    if z<L_sat:\n",
    "        return P_0*(A(z,L)*np.exp(0.233*z/L_sat))/(1+P_0/P_sat*(A(z,L)-1))\n",
    "    else: \n",
    "        return P_0*(A(L_sat,L)*np.exp(0.233*L_sat/L_sat))/(1+P_0/P_sat*(A(L_sat,L)-1))\n",
    "\n",
    "print(P(L_sat,P_0,P_sat,L_sat))\n",
    "\n",
    " \n",
    "# exponential function y = 10^x\n",
    "data = [P(i,P_0,P_sat,L_sat) for i in np.arange(0, int(1.3*L_sat), X['lu'][0]/100)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "# convert y-axis to Logarithmic scale\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('z, m')\n",
    "plt.ylabel('P, W')\n",
    "plt.title('FEL power evolution')\n",
    "\n",
    " \n",
    "plt.plot(np.arange(0, int(1.3*L_sat), X['lu'][0]/100), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "4ba039b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [i.item() for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "15d3a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).to_csv('pandas.txt', header=None, index=None, sep=' ', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "57c36cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split, SubsetRandomSampler, ConcatDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "0a148458",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "set_random_seed(42)\n",
    "\n",
    "num_epochs=1000\n",
    "batch_size=64\n",
    "k=5\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "foldperf={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "a1a8b1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch:1/1000\n",
      "Loss on train= 0.17502261698246002\n",
      "Loss on test= 0.08933575451374054\n",
      "acc for Lsat= 0.4137352602349387 \n",
      "acc for Psat= 0.4734729594654507 \n",
      "acc for optim= 0.19333456622229683\n",
      "Epoch:2/1000\n",
      "Loss on train= 0.07279211282730103\n",
      "Loss on test= 0.06221079081296921\n",
      "acc for Lsat= 0.2874059756596883 \n",
      "acc for Psat= 0.4410259015030331 \n",
      "acc for optim= 0.18167139954037137\n",
      "Epoch:3/1000\n",
      "Loss on train= 0.05794120207428932\n",
      "Loss on test= 0.058518607169389725\n",
      "acc for Lsat= 0.2831901682747735 \n",
      "acc for Psat= 0.4221358630392287 \n",
      "acc for optim= 0.1753588627609942\n",
      "Epoch:4/1000\n",
      "Loss on train= 0.055183473974466324\n",
      "Loss on test= 0.05859746038913727\n",
      "acc for Lsat= 0.2860080450773239 \n",
      "acc for Psat= 0.3686269376013014 \n",
      "acc for optim= 0.17728466573688717\n",
      "Epoch:5/1000\n",
      "Loss on train= 0.04949769005179405\n",
      "Loss on test= 0.05156971886754036\n",
      "acc for Lsat= 0.2826350685622957 \n",
      "acc for Psat= 0.3664298011196984 \n",
      "acc for optim= 0.1751679881579346\n",
      "Epoch:6/1000\n",
      "Loss on train= 0.048310283571481705\n",
      "Loss on test= 0.056986331939697266\n",
      "acc for Lsat= 0.30551732546753363 \n",
      "acc for Psat= 0.35438963837093773 \n",
      "acc for optim= 0.18754102355904048\n",
      "Epoch:7/1000\n",
      "Loss on train= 0.047602854669094086\n",
      "Loss on test= 0.05344341695308685\n",
      "acc for Lsat= 0.28671755360232465 \n",
      "acc for Psat= 0.3462911996576522 \n",
      "acc for optim= 0.17693574958377412\n",
      "Epoch:8/1000\n",
      "Loss on train= 0.04564039409160614\n",
      "Loss on test= 0.0501730851829052\n",
      "acc for Lsat= 0.27297029859489874 \n",
      "acc for Psat= 0.3598481357097626 \n",
      "acc for optim= 0.18095883263481985\n",
      "Epoch:9/1000\n",
      "Loss on train= 0.04336322844028473\n",
      "Loss on test= 0.04882378876209259\n",
      "acc for Lsat= 0.281875307030148 \n",
      "acc for Psat= 0.33306399716271295 \n",
      "acc for optim= 0.17713643978867266\n",
      "Epoch:10/1000\n",
      "Loss on train= 0.0440068319439888\n",
      "Loss on test= 0.04911314696073532\n",
      "acc for Lsat= 0.27747604880068033 \n",
      "acc for Psat= 0.3437093244658576 \n",
      "acc for optim= 0.17886298530631603\n",
      "Epoch:11/1000\n",
      "Loss on train= 0.042534928768873215\n",
      "Loss on test= 0.048049215227365494\n",
      "acc for Lsat= 0.2680374393860499 \n",
      "acc for Psat= 0.35124322507116523 \n",
      "acc for optim= 0.17730744000938203\n",
      "Epoch:12/1000\n",
      "Loss on train= 0.04192211851477623\n",
      "Loss on test= 0.04612677916884422\n",
      "acc for Lsat= 0.2686593486203088 \n",
      "acc for Psat= 0.3430129143926832 \n",
      "acc for optim= 0.1817986402246687\n",
      "Epoch:13/1000\n",
      "Loss on train= 0.03977767750620842\n",
      "Loss on test= 0.04716282710433006\n",
      "acc for Lsat= 0.2760091013378567 \n",
      "acc for Psat= 0.3194344600041707 \n",
      "acc for optim= 0.17650989360279506\n",
      "Epoch:14/1000\n",
      "Loss on train= 0.03995177522301674\n",
      "Loss on test= 0.04604974016547203\n",
      "acc for Lsat= 0.27026229500770566 \n",
      "acc for Psat= 0.34286431074142454 \n",
      "acc for optim= 0.182247961892022\n",
      "Epoch:15/1000\n",
      "Loss on train= 0.03877798467874527\n",
      "Loss on test= 0.04393523931503296\n",
      "acc for Lsat= 0.3008009279767672 \n",
      "acc for Psat= 0.32313751578331 \n",
      "acc for optim= 0.19497541288534798\n",
      "Epoch:16/1000\n",
      "Loss on train= 0.039609771221876144\n",
      "Loss on test= 0.04491020739078522\n",
      "acc for Lsat= 0.26842496660020615 \n",
      "acc for Psat= 0.3873154415024651 \n",
      "acc for optim= 0.18147867172956464\n",
      "Epoch:17/1000\n",
      "Loss on train= 0.03907518833875656\n",
      "Loss on test= 0.04557429999113083\n",
      "acc for Lsat= 0.2688199950589074 \n",
      "acc for Psat= 0.34100634654362993 \n",
      "acc for optim= 0.17937051488293543\n",
      "Epoch:18/1000\n",
      "Loss on train= 0.03875884413719177\n",
      "Loss on test= 0.044105660170316696\n",
      "acc for Lsat= 0.2808862894773483 \n",
      "acc for Psat= 0.3135499901241726 \n",
      "acc for optim= 0.18100076880719929\n",
      "Epoch:19/1000\n",
      "Loss on train= 0.03726513683795929\n",
      "Loss on test= 0.04444335401058197\n",
      "acc for Lsat= 0.29696996741824677 \n",
      "acc for Psat= 0.31144114269150625 \n",
      "acc for optim= 0.1879831509457694\n",
      "Epoch:20/1000\n",
      "Loss on train= 0.037575509399175644\n",
      "Loss on test= 0.04033411666750908\n",
      "acc for Lsat= 0.27751548687616984 \n",
      "acc for Psat= 0.31818790833155314 \n",
      "acc for optim= 0.1765743450158172\n",
      "Epoch:21/1000\n",
      "Loss on train= 0.03696664422750473\n",
      "Loss on test= 0.04335858300328255\n",
      "acc for Lsat= 0.26789913939105137 \n",
      "acc for Psat= 0.30975115829043914 \n",
      "acc for optim= 0.18910796642303465\n",
      "Epoch:22/1000\n",
      "Loss on train= 0.03736778348684311\n",
      "Loss on test= 0.04253446310758591\n",
      "acc for Lsat= 0.2899667955107159 \n",
      "acc for Psat= 0.3123258100615607 \n",
      "acc for optim= 0.18045709696080947\n",
      "Epoch:23/1000\n",
      "Loss on train= 0.03631451353430748\n",
      "Loss on test= 0.04493242874741554\n",
      "acc for Lsat= 0.2688908020655314 \n",
      "acc for Psat= 0.3938558843400744 \n",
      "acc for optim= 0.18596407042609323\n",
      "Epoch:24/1000\n",
      "Loss on train= 0.03544699400663376\n",
      "Loss on test= 0.04216583073139191\n",
      "acc for Lsat= 0.2944086631139119 \n",
      "acc for Psat= 0.3077179438538022 \n",
      "acc for optim= 0.19444822536574471\n",
      "Epoch:25/1000\n",
      "Loss on train= 0.03784090653061867\n",
      "Loss on test= 0.041565902531147\n",
      "acc for Lsat= 0.2605534364779791 \n",
      "acc for Psat= 0.33033440179295004 \n",
      "acc for optim= 0.17715107599894206\n",
      "Epoch:26/1000\n",
      "Loss on train= 0.03427949175238609\n",
      "Loss on test= 0.04386752098798752\n",
      "acc for Lsat= 0.27232053279876706 \n",
      "acc for Psat= 0.31833104491233827 \n",
      "acc for optim= 0.18503591650062137\n",
      "Epoch:27/1000\n",
      "Loss on train= 0.034913577139377594\n",
      "Loss on test= 0.0424344427883625\n",
      "acc for Lsat= 0.25214414464102847 \n",
      "acc for Psat= 0.3502979603078631 \n",
      "acc for optim= 0.17883750696976983\n",
      "Epoch:28/1000\n",
      "Loss on train= 0.034110214561223984\n",
      "Loss on test= 0.03866775706410408\n",
      "acc for Lsat= 0.2687830507755279 \n",
      "acc for Psat= 0.31211597853236733 \n",
      "acc for optim= 0.18947062724166444\n",
      "Epoch:29/1000\n",
      "Loss on train= 0.0344848670065403\n",
      "Loss on test= 0.04137216508388519\n",
      "acc for Lsat= 0.25609357158343 \n",
      "acc for Psat= 0.31319880617989426 \n",
      "acc for optim= 0.178146270248625\n",
      "Epoch:30/1000\n",
      "Loss on train= 0.03468552604317665\n",
      "Loss on test= 0.03866690397262573\n",
      "acc for Lsat= 0.26324478288491565 \n",
      "acc for Psat= 0.2878804902235667 \n",
      "acc for optim= 0.1814586116207971\n",
      "Epoch:31/1000\n",
      "Loss on train= 0.03425319865345955\n",
      "Loss on test= 0.04011240601539612\n",
      "acc for Lsat= 0.24991755187511439 \n",
      "acc for Psat= 0.3299543725119697 \n",
      "acc for optim= 0.1780262018243472\n",
      "Epoch:32/1000\n",
      "Loss on train= 0.033287789672613144\n",
      "Loss on test= 0.04126150161027908\n",
      "acc for Lsat= 0.2859681771861183 \n",
      "acc for Psat= 0.2799896869394514 \n",
      "acc for optim= 0.17603711469305885\n",
      "Epoch:33/1000\n",
      "Loss on train= 0.033087100833654404\n",
      "Loss on test= 0.03620975837111473\n",
      "acc for Lsat= 0.24737638052966857 \n",
      "acc for Psat= 0.2733916673395369 \n",
      "acc for optim= 0.18207375738355852\n",
      "Epoch:34/1000\n",
      "Loss on train= 0.03402915224432945\n",
      "Loss on test= 0.038376715034246445\n",
      "acc for Lsat= 0.2441354042953915 \n",
      "acc for Psat= 0.2804324958059523 \n",
      "acc for optim= 0.17951378077268598\n",
      "Epoch:35/1000\n",
      "Loss on train= 0.03348076716065407\n",
      "Loss on test= 0.04289740324020386\n",
      "acc for Lsat= 0.2772639552752177 \n",
      "acc for Psat= 0.2946840484937032 \n",
      "acc for optim= 0.1771324121289783\n",
      "Epoch:36/1000\n",
      "Loss on train= 0.035475097596645355\n",
      "Loss on test= 0.04124967381358147\n",
      "acc for Lsat= 0.23884078330463834 \n",
      "acc for Psat= 0.3079798579216004 \n",
      "acc for optim= 0.20103970600499046\n",
      "Epoch:37/1000\n",
      "Loss on train= 0.03235167637467384\n",
      "Loss on test= 0.03943343460559845\n",
      "acc for Lsat= 0.2427123910850949 \n",
      "acc for Psat= 0.2886559347311656 \n",
      "acc for optim= 0.184906091954973\n",
      "Epoch:38/1000\n",
      "Loss on train= 0.03170941025018692\n",
      "Loss on test= 0.03622301667928696\n",
      "acc for Lsat= 0.23897024889787039 \n",
      "acc for Psat= 0.28079823321766323 \n",
      "acc for optim= 0.1780639206369718\n",
      "Epoch:39/1000\n",
      "Loss on train= 0.030635129660367966\n",
      "Loss on test= 0.037037063390016556\n",
      "acc for Lsat= 0.22587011555830638 \n",
      "acc for Psat= 0.3063331849045223 \n",
      "acc for optim= 0.1786999311712053\n",
      "Epoch:40/1000\n",
      "Loss on train= 0.030604064464569092\n",
      "Loss on test= 0.03466014191508293\n",
      "acc for Lsat= 0.23141356408596037 \n",
      "acc for Psat= 0.2602333214547899 \n",
      "acc for optim= 0.17762315869331358\n",
      "Epoch:41/1000\n",
      "Loss on train= 0.0302895475178957\n",
      "Loss on test= 0.038758955895900726\n",
      "acc for Lsat= 0.2333858321110407 \n",
      "acc for Psat= 0.2997835305002 \n",
      "acc for optim= 0.1915953271918827\n",
      "Epoch:42/1000\n",
      "Loss on train= 0.030704440549016\n",
      "Loss on test= 0.03853848576545715\n",
      "acc for Lsat= 0.22479603638251622 \n",
      "acc for Psat= 0.2673300243086285 \n",
      "acc for optim= 0.17770944039026895\n",
      "Epoch:43/1000\n",
      "Loss on train= 0.03060942329466343\n",
      "Loss on test= 0.03328848257660866\n",
      "acc for Lsat= 0.22602428164747027 \n",
      "acc for Psat= 0.25131753351953295 \n",
      "acc for optim= 0.18120886236429212\n",
      "Epoch:44/1000\n",
      "Loss on train= 0.029927540570497513\n",
      "Loss on test= 0.034544918686151505\n",
      "acc for Lsat= 0.22929731276300222 \n",
      "acc for Psat= 0.27141417132483586 \n",
      "acc for optim= 0.18103581335809496\n",
      "Epoch:45/1000\n",
      "Loss on train= 0.031141245737671852\n",
      "Loss on test= 0.03548518568277359\n",
      "acc for Lsat= 0.22171593507130938 \n",
      "acc for Psat= 0.27116149332788253 \n",
      "acc for optim= 0.19204989903502995\n",
      "Epoch:46/1000\n",
      "Loss on train= 0.029898744076490402\n",
      "Loss on test= 0.03474218770861626\n",
      "acc for Lsat= 0.23137360711892452 \n",
      "acc for Psat= 0.26853993932406106 \n",
      "acc for optim= 0.18152824971410964\n",
      "Epoch:47/1000\n",
      "Loss on train= 0.0299936905503273\n",
      "Loss on test= 0.03619764745235443\n",
      "acc for Lsat= 0.22747141619523367 \n",
      "acc for Psat= 0.24888107445504926 \n",
      "acc for optim= 0.18158697717719607\n",
      "Epoch:48/1000\n",
      "Loss on train= 0.02910236269235611\n",
      "Loss on test= 0.034746624529361725\n",
      "acc for Lsat= 0.21005714701281653 \n",
      "acc for Psat= 0.28393610318501794 \n",
      "acc for optim= 0.17742114464441935\n",
      "Epoch:49/1000\n",
      "Loss on train= 0.02835283800959587\n",
      "Loss on test= 0.03595760837197304\n",
      "acc for Lsat= 0.21307665871249304 \n",
      "acc for Psat= 0.3027056296666463 \n",
      "acc for optim= 0.1883136093616486\n",
      "Epoch:50/1000\n",
      "Loss on train= 0.029666095972061157\n",
      "Loss on test= 0.036830492317676544\n",
      "acc for Lsat= 0.23017408781581455 \n",
      "acc for Psat= 0.24597862727112238 \n",
      "acc for optim= 0.1762347612116072\n",
      "Epoch:51/1000\n",
      "Loss on train= 0.028429267928004265\n",
      "Loss on test= 0.034315094351768494\n",
      "acc for Lsat= 0.20595522473255795 \n",
      "acc for Psat= 0.22982549667358398 \n",
      "acc for optim= 0.18455322086811063\n",
      "Epoch:52/1000\n",
      "Loss on train= 0.027534203603863716\n",
      "Loss on test= 0.03628864139318466\n",
      "acc for Lsat= 0.19762942128711275 \n",
      "acc for Psat= 0.24507327410909863 \n",
      "acc for optim= 0.1781342527932591\n",
      "Epoch:53/1000\n",
      "Loss on train= 0.027841173112392426\n",
      "Loss on test= 0.03537847846746445\n",
      "acc for Lsat= 0.19290478626887006 \n",
      "acc for Psat= 0.2565281947453817 \n",
      "acc for optim= 0.18565234310097167\n",
      "Epoch:54/1000\n",
      "Loss on train= 0.02768723852932453\n",
      "Loss on test= 0.03197670355439186\n",
      "acc for Lsat= 0.19517381207810508 \n",
      "acc for Psat= 0.24358789722124735 \n",
      "acc for optim= 0.18641302585601807\n",
      "Epoch:55/1000\n",
      "Loss on train= 0.026204608380794525\n",
      "Loss on test= 0.030124429613351822\n",
      "acc for Lsat= 0.19327384134133657 \n",
      "acc for Psat= 0.21967945032649572 \n",
      "acc for optim= 0.18088320791721343\n",
      "Epoch:56/1000\n",
      "Loss on train= 0.02700110711157322\n",
      "Loss on test= 0.031039515510201454\n",
      "acc for Lsat= 0.19455351018243366 \n",
      "acc for Psat= 0.2177045206228892 \n",
      "acc for optim= 0.18270347317059835\n",
      "Epoch:57/1000\n",
      "Loss on train= 0.026456261053681374\n",
      "Loss on test= 0.029001308605074883\n",
      "acc for Lsat= 0.17290655937459734 \n",
      "acc for Psat= 0.2138905021879408 \n",
      "acc for optim= 0.18317980269591014\n",
      "Epoch:58/1000\n",
      "Loss on train= 0.02562849037349224\n",
      "Loss on test= 0.03003367781639099\n",
      "acc for Lsat= 0.1793395464619001 \n",
      "acc for Psat= 0.2095732072989146 \n",
      "acc for optim= 0.18063649998770817\n",
      "Epoch:59/1000\n",
      "Loss on train= 0.025984175503253937\n",
      "Loss on test= 0.03053651563823223\n",
      "acc for Lsat= 0.1754857548409038 \n",
      "acc for Psat= 0.20493275390730964 \n",
      "acc for optim= 0.18957080874178145\n",
      "Epoch:60/1000\n",
      "Loss on train= 0.024786055088043213\n",
      "Loss on test= 0.028746604919433594\n",
      "acc for Lsat= 0.1585492072833909 \n",
      "acc for Psat= 0.20981412728627521 \n",
      "acc for optim= 0.17962638139724732\n",
      "Epoch:61/1000\n",
      "Loss on train= 0.025687159970402718\n",
      "Loss on test= 0.03162996470928192\n",
      "acc for Lsat= 0.16780939118729696 \n",
      "acc for Psat= 0.21771219472090403 \n",
      "acc for optim= 0.1919983330700132\n",
      "Epoch:62/1000\n",
      "Loss on train= 0.024956611916422844\n",
      "Loss on test= 0.029479563236236572\n",
      "acc for Lsat= 0.14844139615694682 \n",
      "acc for Psat= 0.21005600492159524 \n",
      "acc for optim= 0.17905264629258047\n",
      "Epoch:63/1000\n",
      "Loss on train= 0.02591308392584324\n",
      "Loss on test= 0.03216947242617607\n",
      "acc for Lsat= 0.16802299850516852 \n",
      "acc for Psat= 0.23908044497172043 \n",
      "acc for optim= 0.17734797447919842\n",
      "Epoch:64/1000\n",
      "Loss on train= 0.0252540186047554\n",
      "Loss on test= 0.029903246089816093\n",
      "acc for Lsat= 0.15397326598564787 \n",
      "acc for Psat= 0.20686951213412816 \n",
      "acc for optim= 0.18428529302279156\n",
      "Epoch:65/1000\n",
      "Loss on train= 0.02445230633020401\n",
      "Loss on test= 0.028702519834041595\n",
      "acc for Lsat= 0.14723722255892221 \n",
      "acc for Psat= 0.19721739027235244 \n",
      "acc for optim= 0.1835679125454691\n",
      "Epoch:66/1000\n",
      "Loss on train= 0.024120677262544632\n",
      "Loss on test= 0.028483135625720024\n",
      "acc for Lsat= 0.1494701971610387 \n",
      "acc for Psat= 0.19365865720642939 \n",
      "acc for optim= 0.17990356021457252\n",
      "Epoch:67/1000\n",
      "Loss on train= 0.023957975208759308\n",
      "Loss on test= 0.030675776302814484\n",
      "acc for Lsat= 0.19244995299312803 \n",
      "acc for Psat= 0.1990376853280597 \n",
      "acc for optim= 0.18247523771391974\n",
      "Epoch:68/1000\n",
      "Loss on train= 0.02459564059972763\n",
      "Loss on test= 0.029626259580254555\n",
      "acc for Lsat= 0.13451082938247258 \n",
      "acc for Psat= 0.19315760201878016 \n",
      "acc for optim= 0.1778071151839362\n",
      "Epoch:69/1000\n",
      "Loss on train= 0.023051081225275993\n",
      "Loss on test= 0.028646405786275864\n",
      "acc for Lsat= 0.1251437160703871 \n",
      "acc for Psat= 0.1916977908876207 \n",
      "acc for optim= 0.1777963813808229\n",
      "Epoch:70/1000\n",
      "Loss on train= 0.023173337802290916\n",
      "Loss on test= 0.029310528188943863\n",
      "acc for Lsat= 0.15165291064315373 \n",
      "acc for Psat= 0.2233124819066789 \n",
      "acc for optim= 0.17732820428080032\n",
      "Epoch:71/1000\n",
      "Loss on train= 0.023208336904644966\n",
      "Loss on test= 0.027496809139847755\n",
      "acc for Lsat= 0.11735844065745675 \n",
      "acc for Psat= 0.17960883213414086 \n",
      "acc for optim= 0.18426635199122962\n",
      "Epoch:72/1000\n",
      "Loss on train= 0.023544736206531525\n",
      "Loss on test= 0.027416493743658066\n",
      "acc for Lsat= 0.12369575500488282 \n",
      "acc for Psat= 0.18733882473574737 \n",
      "acc for optim= 0.1885224782758289\n",
      "Epoch:73/1000\n",
      "Loss on train= 0.022048097103834152\n",
      "Loss on test= 0.027815626934170723\n",
      "acc for Lsat= 0.13653669324186113 \n",
      "acc for Psat= 0.18200956020090317 \n",
      "acc for optim= 0.17878566715452404\n",
      "Epoch:74/1000\n",
      "Loss on train= 0.022473983466625214\n",
      "Loss on test= 0.027159564197063446\n",
      "acc for Lsat= 0.12732089708248776 \n",
      "acc for Psat= 0.1915982279512617 \n",
      "acc for optim= 0.17905366818110147\n",
      "Epoch:75/1000\n",
      "Loss on train= 0.022286325693130493\n",
      "Loss on test= 0.027897311374545097\n",
      "acc for Lsat= 0.11858167813883888 \n",
      "acc for Psat= 0.18479989535278743 \n",
      "acc for optim= 0.18088372614648607\n",
      "Epoch:76/1000\n",
      "Loss on train= 0.021873541176319122\n",
      "Loss on test= 0.02802317962050438\n",
      "acc for Lsat= 0.1131964133845435 \n",
      "acc for Psat= 0.19574939608573916 \n",
      "acc for optim= 0.179384774963061\n",
      "Epoch:77/1000\n",
      "Loss on train= 0.022125329822301865\n",
      "Loss on test= 0.0290250051766634\n",
      "acc for Lsat= 0.14146903273132114 \n",
      "acc for Psat= 0.20419583419958753 \n",
      "acc for optim= 0.1784287727541394\n",
      "Epoch:78/1000\n",
      "Loss on train= 0.023096024990081787\n",
      "Loss on test= 0.028539104387164116\n",
      "acc for Lsat= 0.1261140623026424 \n",
      "acc for Psat= 0.17722639871968163 \n",
      "acc for optim= 0.17898687306377625\n",
      "Epoch:79/1000\n",
      "Loss on train= 0.02312564104795456\n",
      "Loss on test= 0.028053581714630127\n",
      "acc for Lsat= 0.11389164593484669 \n",
      "acc for Psat= 0.1738581094476912 \n",
      "acc for optim= 0.1819010939862993\n",
      "Epoch:80/1000\n",
      "Loss on train= 0.024542277678847313\n",
      "Loss on test= 0.02753000147640705\n",
      "acc for Lsat= 0.12959369785255856 \n",
      "acc for Psat= 0.19187195499738058 \n",
      "acc for optim= 0.17820925977494978\n",
      "Epoch:81/1000\n",
      "Loss on train= 0.021849961951375008\n",
      "Loss on test= 0.02630697377026081\n",
      "acc for Lsat= 0.1315422038237254 \n",
      "acc for Psat= 0.2021474089887407 \n",
      "acc for optim= 0.17835180825657315\n",
      "Epoch:82/1000\n",
      "Loss on train= 0.021320773288607597\n",
      "Loss on test= 0.026236897334456444\n",
      "acc for Lsat= 0.12272379481130175 \n",
      "acc for Psat= 0.19229123062557643 \n",
      "acc for optim= 0.18222195241186356\n",
      "Epoch:83/1000\n",
      "Loss on train= 0.021848609670996666\n",
      "Loss on test= 0.0244552344083786\n",
      "acc for Lsat= 0.11053304341104296 \n",
      "acc for Psat= 0.17647909720738728 \n",
      "acc for optim= 0.17753914362854423\n",
      "Epoch:84/1000\n",
      "Loss on train= 0.021136876195669174\n",
      "Loss on test= 0.02639617584645748\n",
      "acc for Lsat= 0.1098957199189398 \n",
      "acc for Psat= 0.20070338977707752 \n",
      "acc for optim= 0.17696456793281767\n",
      "Epoch:85/1000\n",
      "Loss on train= 0.021622423082590103\n",
      "Loss on test= 0.02749008871614933\n",
      "acc for Lsat= 0.14647089921765855 \n",
      "acc for Psat= 0.2084435535801782 \n",
      "acc for optim= 0.1797680050134659\n",
      "Epoch:86/1000\n",
      "Loss on train= 0.021249352023005486\n",
      "Loss on test= 0.025937974452972412\n",
      "acc for Lsat= 0.11528137144115237 \n",
      "acc for Psat= 0.19919374187787373 \n",
      "acc for optim= 0.18119312756591371\n",
      "Epoch:87/1000\n",
      "Loss on train= 0.021261729300022125\n",
      "Loss on test= 0.026310408487915993\n",
      "acc for Lsat= 0.13455444011423323 \n",
      "acc for Psat= 0.19393979410330456 \n",
      "acc for optim= 0.17765259378486212\n",
      "Epoch:88/1000\n",
      "Loss on train= 0.021452557295560837\n",
      "Loss on test= 0.02720782719552517\n",
      "acc for Lsat= 0.1020303600364261 \n",
      "acc for Psat= 0.16993497014045716 \n",
      "acc for optim= 0.18786188496483697\n",
      "Epoch:89/1000\n",
      "Loss on train= 0.02083679661154747\n",
      "Loss on test= 0.026769433170557022\n",
      "acc for Lsat= 0.11070949931939442 \n",
      "acc for Psat= 0.21617612342039744 \n",
      "acc for optim= 0.1822730580965678\n",
      "Epoch:90/1000\n",
      "Loss on train= 0.02123435027897358\n",
      "Loss on test= 0.026810530573129654\n",
      "acc for Lsat= 0.11148927294545702 \n",
      "acc for Psat= 0.21291903456052144 \n",
      "acc for optim= 0.17815523710515765\n",
      "Epoch:91/1000\n",
      "Loss on train= 0.020518813282251358\n",
      "Loss on test= 0.025503698736429214\n",
      "acc for Lsat= 0.1279386422700352 \n",
      "acc for Psat= 0.1754805866214964 \n",
      "acc for optim= 0.1781276133325365\n",
      "Epoch:92/1000\n",
      "Loss on train= 0.020964276045560837\n",
      "Loss on test= 0.028753098100423813\n",
      "acc for Lsat= 0.1117257669568062 \n",
      "acc for Psat= 0.18485557933648425 \n",
      "acc for optim= 0.17330738024579156\n",
      "Epoch:93/1000\n",
      "Loss on train= 0.021054789423942566\n",
      "Loss on test= 0.025805164128541946\n",
      "acc for Lsat= 0.10623189740710787 \n",
      "acc for Psat= 0.186065767871009 \n",
      "acc for optim= 0.1836904164817598\n",
      "Epoch:94/1000\n",
      "Loss on train= 0.02013205550611019\n",
      "Loss on test= 0.0251139048486948\n",
      "acc for Lsat= 0.10667692654662664 \n",
      "acc for Psat= 0.16954534980985855 \n",
      "acc for optim= 0.1772783921824561\n",
      "Epoch:95/1000\n",
      "Loss on train= 0.02006174623966217\n",
      "Loss on test= 0.025542788207530975\n",
      "acc for Lsat= 0.10500331355465782 \n",
      "acc for Psat= 0.1772762596607208 \n",
      "acc for optim= 0.1800642771853341\n",
      "Epoch:96/1000\n",
      "Loss on train= 0.02079828828573227\n",
      "Loss on test= 0.02496018074452877\n",
      "acc for Lsat= 0.0941375290354093 \n",
      "acc for Psat= 0.1638904915915595 \n",
      "acc for optim= 0.17972624202569326\n",
      "Epoch:97/1000\n",
      "Loss on train= 0.019772224128246307\n",
      "Loss on test= 0.026805201545357704\n",
      "acc for Lsat= 0.13407309436135822 \n",
      "acc for Psat= 0.17343631651666425 \n",
      "acc for optim= 0.17873802383740747\n",
      "Epoch:98/1000\n",
      "Loss on train= 0.01958807185292244\n",
      "Loss on test= 0.02573586255311966\n",
      "acc for Lsat= 0.10807426704300777 \n",
      "acc for Psat= 0.18800860544045764 \n",
      "acc for optim= 0.1890489273601108\n",
      "Epoch:99/1000\n",
      "Loss on train= 0.019893266260623932\n",
      "Loss on test= 0.02587781846523285\n",
      "acc for Lsat= 0.10377806160185071 \n",
      "acc for Psat= 0.16938819123639 \n",
      "acc for optim= 0.17673970692687563\n",
      "Epoch:100/1000\n",
      "Loss on train= 0.020063716918230057\n",
      "Loss on test= 0.023842239752411842\n",
      "acc for Lsat= 0.11438263091776103 \n",
      "acc for Psat= 0.18539205193519592 \n",
      "acc for optim= 0.17573232220278845\n",
      "Epoch:101/1000\n",
      "Loss on train= 0.020556362345814705\n",
      "Loss on test= 0.022696243599057198\n",
      "acc for Lsat= 0.10010537107785543 \n",
      "acc for Psat= 0.16284188396400875 \n",
      "acc for optim= 0.17753334078523844\n",
      "Epoch:102/1000\n",
      "Loss on train= 0.020324232056736946\n",
      "Loss on test= 0.025512652471661568\n",
      "acc for Lsat= 0.10553756439023547 \n",
      "acc for Psat= 0.17549026674694485 \n",
      "acc for optim= 0.18022499399052727\n",
      "Epoch:103/1000\n",
      "Loss on train= 0.020536033436655998\n",
      "Loss on test= 0.024087795987725258\n",
      "acc for Lsat= 0.1045353430840704 \n",
      "acc for Psat= 0.1652052534951104 \n",
      "acc for optim= 0.175934800836775\n",
      "Epoch:104/1000\n",
      "Loss on train= 0.020640330389142036\n",
      "Loss on test= 0.025132915005087852\n",
      "acc for Lsat= 0.12730677127838136 \n",
      "acc for Psat= 0.16843997604317135 \n",
      "acc for optim= 0.1758051057656606\n",
      "Epoch:105/1000\n",
      "Loss on train= 0.02037280984222889\n",
      "Loss on test= 0.02571946755051613\n",
      "acc for Lsat= 0.11811890701452891 \n",
      "acc for Psat= 0.19221340947681 \n",
      "acc for optim= 0.17679066658020015\n",
      "Epoch:106/1000\n",
      "Loss on train= 0.019470663741230965\n",
      "Loss on test= 0.02469063363969326\n",
      "acc for Lsat= 0.09556077884303199 \n",
      "acc for Psat= 0.1653815385368135 \n",
      "acc for optim= 0.17532329459985097\n",
      "Epoch:107/1000\n",
      "Loss on train= 0.01944822259247303\n",
      "Loss on test= 0.024941284209489822\n",
      "acc for Lsat= 0.10990954885880153 \n",
      "acc for Psat= 0.17100872761673397 \n",
      "acc for optim= 0.1774713314241833\n",
      "Epoch:108/1000\n",
      "Loss on train= 0.019352056086063385\n",
      "Loss on test= 0.025879546999931335\n",
      "acc for Lsat= 0.10098537206649782 \n",
      "acc for Psat= 0.18026324808597563 \n",
      "acc for optim= 0.17747236390908558\n",
      "Epoch:109/1000\n",
      "Loss on train= 0.019305841997265816\n",
      "Loss on test= 0.02349083684384823\n",
      "acc for Lsat= 0.09199074639214408 \n",
      "acc for Psat= 0.15340737932258183 \n",
      "acc for optim= 0.17527802255418565\n",
      "Epoch:110/1000\n",
      "Loss on train= 0.0186702199280262\n",
      "Loss on test= 0.024032585322856903\n",
      "acc for Lsat= 0.09142824080255296 \n",
      "acc for Psat= 0.1513020485639572 \n",
      "acc for optim= 0.1819262461529838\n",
      "Epoch:111/1000\n",
      "Loss on train= 0.019453346729278564\n",
      "Loss on test= 0.025348106399178505\n",
      "acc for Lsat= 0.11713900963465369 \n",
      "acc for Psat= 0.19869235422876147 \n",
      "acc for optim= 0.17831191635794108\n",
      "Epoch:112/1000\n",
      "Loss on train= 0.01955016702413559\n",
      "Loss on test= 0.024567149579524994\n",
      "acc for Lsat= 0.0968226671218872 \n",
      "acc for Psat= 0.17119734949535795 \n",
      "acc for optim= 0.17482455968856814\n",
      "Epoch:113/1000\n",
      "Loss on train= 0.01883496530354023\n",
      "Loss on test= 0.02351205237209797\n",
      "acc for Lsat= 0.10071155312988493 \n",
      "acc for Psat= 0.16338940329021878 \n",
      "acc for optim= 0.17151587141884694\n",
      "Epoch:114/1000\n",
      "Loss on train= 0.018260642886161804\n",
      "Loss on test= 0.024120233952999115\n",
      "acc for Lsat= 0.09157323191563288 \n",
      "acc for Psat= 0.1632926070027881 \n",
      "acc for optim= 0.1795454528596666\n",
      "Epoch:115/1000\n",
      "Loss on train= 0.01880471035838127\n",
      "Loss on test= 0.02287411503493786\n",
      "acc for Lsat= 0.1071282928188642 \n",
      "acc for Psat= 0.15164033406310612 \n",
      "acc for optim= 0.178129960430993\n",
      "Epoch:116/1000\n",
      "Loss on train= 0.018713360652327538\n",
      "Loss on test= 0.022482948377728462\n",
      "acc for Lsat= 0.09424061245388454 \n",
      "acc for Psat= 0.15682314104504055 \n",
      "acc for optim= 0.17385767069127822\n",
      "Epoch:117/1000\n",
      "Loss on train= 0.017915254458785057\n",
      "Loss on test= 0.02349727600812912\n",
      "acc for Lsat= 0.08514930804570514 \n",
      "acc for Psat= 0.16021773484018112 \n",
      "acc for optim= 0.17340126087268193\n",
      "Epoch:118/1000\n",
      "Loss on train= 0.01955508254468441\n",
      "Loss on test= 0.02211737260222435\n",
      "acc for Lsat= 0.09834601167175507 \n",
      "acc for Psat= 0.1568243188990487 \n",
      "acc for optim= 0.16972644593980574\n",
      "Epoch:119/1000\n",
      "Loss on train= 0.01819942705333233\n",
      "Loss on test= 0.023329677060246468\n",
      "acc for Lsat= 0.10262659887472789 \n",
      "acc for Psat= 0.15899875462055205 \n",
      "acc for optim= 0.17594087007972928\n",
      "Epoch:120/1000\n",
      "Loss on train= 0.018410688266158104\n",
      "Loss on test= 0.023432813584804535\n",
      "acc for Lsat= 0.09758503519826466 \n",
      "acc for Psat= 0.17829972505569458 \n",
      "acc for optim= 0.17726506955093804\n",
      "Epoch:121/1000\n",
      "Loss on train= 0.017951982095837593\n",
      "Loss on test= 0.02312741056084633\n",
      "acc for Lsat= 0.09615895251433054 \n",
      "acc for Psat= 0.15165171192751992 \n",
      "acc for optim= 0.17134158064921695\n",
      "Epoch:122/1000\n",
      "Loss on train= 0.018370551988482475\n",
      "Loss on test= 0.022995997220277786\n",
      "acc for Lsat= 0.11698917216724819 \n",
      "acc for Psat= 0.17594535218344798 \n",
      "acc for optim= 0.17178423355023067\n",
      "Epoch:123/1000\n",
      "Loss on train= 0.017973914742469788\n",
      "Loss on test= 0.024711154401302338\n",
      "acc for Lsat= 0.11048813727166917 \n",
      "acc for Psat= 0.1611770189470715 \n",
      "acc for optim= 0.1739051207900047\n",
      "Epoch:124/1000\n",
      "Loss on train= 0.018048889935016632\n",
      "Loss on test= 0.022195279598236084\n",
      "acc for Lsat= 0.11076307876242533 \n",
      "acc for Psat= 0.16259228620264266 \n",
      "acc for optim= 0.1728507993949784\n",
      "Epoch:125/1000\n",
      "Loss on train= 0.018553737550973892\n",
      "Loss on test= 0.023447450250387192\n",
      "acc for Lsat= 0.1057109167178472 \n",
      "acc for Psat= 0.15854663716422188 \n",
      "acc for optim= 0.1697066671318478\n",
      "Epoch:126/1000\n",
      "Loss on train= 0.017806297168135643\n",
      "Loss on test= 0.024126067757606506\n",
      "acc for Lsat= 0.10804861055480108 \n",
      "acc for Psat= 0.14854893783728282 \n",
      "acc for optim= 0.17379739615652298\n",
      "Epoch:127/1000\n",
      "Loss on train= 0.018507786095142365\n",
      "Loss on test= 0.022874269634485245\n",
      "acc for Lsat= 0.10617028408580356 \n",
      "acc for Psat= 0.17579744855562848 \n",
      "acc for optim= 0.17060900777578356\n",
      "Epoch:128/1000\n",
      "Loss on train= 0.019170211628079414\n",
      "Loss on test= 0.02230154164135456\n",
      "acc for Lsat= 0.08920665035645167 \n",
      "acc for Psat= 0.151687721742524 \n",
      "acc for optim= 0.1669034813013342\n",
      "Epoch:129/1000\n",
      "Loss on train= 0.017292730510234833\n",
      "Loss on test= 0.024644741788506508\n",
      "acc for Lsat= 0.09501751595073277 \n",
      "acc for Psat= 0.16267926593621573 \n",
      "acc for optim= 0.17894024054209393\n",
      "Epoch:130/1000\n",
      "Loss on train= 0.017730282619595528\n",
      "Loss on test= 0.023486999794840813\n",
      "acc for Lsat= 0.08882825275262196 \n",
      "acc for Psat= 0.1470810701449712 \n",
      "acc for optim= 0.17051791631513172\n",
      "Epoch:131/1000\n",
      "Loss on train= 0.017268672585487366\n",
      "Loss on test= 0.02279488369822502\n",
      "acc for Lsat= 0.08522991769843631 \n",
      "acc for Psat= 0.14697047306431663 \n",
      "acc for optim= 0.17931466566191778\n",
      "Epoch:132/1000\n",
      "Loss on train= 0.017948949709534645\n",
      "Loss on test= 0.02104104496538639\n",
      "acc for Lsat= 0.08870215763648351 \n",
      "acc for Psat= 0.1578422857655419 \n",
      "acc for optim= 0.1721805895368258\n",
      "Epoch:133/1000\n",
      "Loss on train= 0.017583167180418968\n",
      "Loss on test= 0.02309521846473217\n",
      "acc for Lsat= 0.11068212555514441 \n",
      "acc for Psat= 0.16938092013200123 \n",
      "acc for optim= 0.1760703672965368\n",
      "Epoch:134/1000\n",
      "Loss on train= 0.017163215205073357\n",
      "Loss on test= 0.02216041274368763\n",
      "acc for Lsat= 0.08688975870609283 \n",
      "acc for Psat= 0.15322429736455281 \n",
      "acc for optim= 0.17102479736010234\n",
      "Epoch:135/1000\n",
      "Loss on train= 0.01720580644905567\n",
      "Loss on test= 0.02121821790933609\n",
      "acc for Lsat= 0.08481514495280054 \n",
      "acc for Psat= 0.14209456476900315 \n",
      "acc for optim= 0.17006103959348462\n",
      "Epoch:136/1000\n",
      "Loss on train= 0.017547233030200005\n",
      "Loss on test= 0.022222040221095085\n",
      "acc for Lsat= 0.08527163300249313 \n",
      "acc for Psat= 0.14576178458001876 \n",
      "acc for optim= 0.1728458440966076\n",
      "Epoch:137/1000\n",
      "Loss on train= 0.017255514860153198\n",
      "Loss on test= 0.021690906956791878\n",
      "acc for Lsat= 0.09239012019501792 \n",
      "acc for Psat= 0.14368033011754353 \n",
      "acc for optim= 0.168925298915969\n",
      "Epoch:138/1000\n",
      "Loss on train= 0.01750737428665161\n",
      "Loss on test= 0.022939365357160568\n",
      "acc for Lsat= 0.09461279577679106 \n",
      "acc for Psat= 0.16718762185838487 \n",
      "acc for optim= 0.17625742786460452\n",
      "Epoch:139/1000\n",
      "Loss on train= 0.017155155539512634\n",
      "Loss on test= 0.02291720360517502\n",
      "acc for Lsat= 0.10783870087729558 \n",
      "acc for Psat= 0.1742744995488061 \n",
      "acc for optim= 0.17398789988623725\n",
      "Epoch:140/1000\n",
      "Loss on train= 0.017759336158633232\n",
      "Loss on test= 0.021364357322454453\n",
      "acc for Lsat= 0.09499348302682241 \n",
      "acc for Psat= 0.14245578580432466 \n",
      "acc for optim= 0.167495674557156\n",
      "Epoch:141/1000\n",
      "Loss on train= 0.017461592331528664\n",
      "Loss on test= 0.022432556375861168\n",
      "acc for Lsat= 0.10045815126763451 \n",
      "acc for Psat= 0.15811623367998334 \n",
      "acc for optim= 0.17052761216958362\n",
      "Epoch:142/1000\n",
      "Loss on train= 0.016765447333455086\n",
      "Loss on test= 0.02141094207763672\n",
      "acc for Lsat= 0.08600236177444459 \n",
      "acc for Psat= 0.16327184471819134 \n",
      "acc for optim= 0.171248730023702\n",
      "Epoch:143/1000\n",
      "Loss on train= 0.017242129892110825\n",
      "Loss on test= 0.0221803467720747\n",
      "acc for Lsat= 0.0795844480395317 \n",
      "acc for Psat= 0.14020786020490858 \n",
      "acc for optim= 0.1657862186431885\n",
      "Epoch:144/1000\n",
      "Loss on train= 0.017116326838731766\n",
      "Loss on test= 0.02300521545112133\n",
      "acc for Lsat= 0.10744455556074779 \n",
      "acc for Psat= 0.14794276522265537 \n",
      "acc for optim= 0.16729397939311133\n",
      "Epoch:145/1000\n",
      "Loss on train= 0.016801830381155014\n",
      "Loss on test= 0.021429667249321938\n",
      "acc for Lsat= 0.08401341206497617 \n",
      "acc for Psat= 0.150571514500512 \n",
      "acc for optim= 0.16676964859167734\n",
      "Epoch:146/1000\n",
      "Loss on train= 0.017302177846431732\n",
      "Loss on test= 0.020409123972058296\n",
      "acc for Lsat= 0.08393582105636596 \n",
      "acc for Psat= 0.1495407740275065 \n",
      "acc for optim= 0.16859940456019507\n",
      "Epoch:147/1000\n",
      "Loss on train= 0.01685512252151966\n",
      "Loss on test= 0.021881943568587303\n",
      "acc for Lsat= 0.09211572657028835 \n",
      "acc for Psat= 0.1439089407523473 \n",
      "acc for optim= 0.1710922959778044\n",
      "Epoch:148/1000\n",
      "Loss on train= 0.017032979056239128\n",
      "Loss on test= 0.021314360201358795\n",
      "acc for Lsat= 0.08419598639011382 \n",
      "acc for Psat= 0.14517952170636922 \n",
      "acc for optim= 0.16612644692262013\n",
      "Epoch:149/1000\n",
      "Loss on train= 0.016743794083595276\n",
      "Loss on test= 0.02100999839603901\n",
      "acc for Lsat= 0.0764824709130658 \n",
      "acc for Psat= 0.1375114785300361 \n",
      "acc for optim= 0.16597114354372025\n",
      "Epoch:150/1000\n",
      "Loss on train= 0.016678830608725548\n",
      "Loss on test= 0.0209715086966753\n",
      "acc for Lsat= 0.08210374712944032 \n",
      "acc for Psat= 0.1423534568813112 \n",
      "acc for optim= 0.16720324622260196\n",
      "Epoch:151/1000\n",
      "Loss on train= 0.01654762402176857\n",
      "Loss on test= 0.020555680617690086\n",
      "acc for Lsat= 0.09948672138982348 \n",
      "acc for Psat= 0.14407693412568834 \n",
      "acc for optim= 0.16266785777277418\n",
      "Epoch:152/1000\n",
      "Loss on train= 0.01663709059357643\n",
      "Loss on test= 0.0202623438090086\n",
      "acc for Lsat= 0.10600203540590075 \n",
      "acc for Psat= 0.17897802790006 \n",
      "acc for optim= 0.16780934962961408\n",
      "Epoch:153/1000\n",
      "Loss on train= 0.016795264557003975\n",
      "Loss on test= 0.020080890506505966\n",
      "acc for Lsat= 0.09128743559122085 \n",
      "acc for Psat= 0.14806432359748417 \n",
      "acc for optim= 0.16372874892420242\n",
      "Epoch:154/1000\n",
      "Loss on train= 0.016619501635432243\n",
      "Loss on test= 0.021034859120845795\n",
      "acc for Lsat= 0.09283790075116688 \n",
      "acc for Psat= 0.14272670845190683 \n",
      "acc for optim= 0.16546125246418847\n",
      "Epoch:155/1000\n",
      "Loss on train= 0.01673128642141819\n",
      "Loss on test= 0.019900180399417877\n",
      "acc for Lsat= 0.09428152491648992 \n",
      "acc for Psat= 0.15839820206165312 \n",
      "acc for optim= 0.16703349351882937\n",
      "Epoch:156/1000\n",
      "Loss on train= 0.017034245654940605\n",
      "Loss on test= 0.020380450412631035\n",
      "acc for Lsat= 0.0950623803668552 \n",
      "acc for Psat= 0.14876668287648095 \n",
      "acc for optim= 0.16857792205280728\n",
      "Epoch:157/1000\n",
      "Loss on train= 0.01664161868393421\n",
      "Loss on test= 0.021256597712635994\n",
      "acc for Lsat= 0.08258888887034523 \n",
      "acc for Psat= 0.14509196513228945 \n",
      "acc for optim= 0.16664560039838155\n",
      "Epoch:158/1000\n",
      "Loss on train= 0.016007743775844574\n",
      "Loss on test= 0.0207216776907444\n",
      "acc for Lsat= 0.09883331540558073 \n",
      "acc for Psat= 0.14572142362594603 \n",
      "acc for optim= 0.16904545923074088\n",
      "Epoch:159/1000\n",
      "Loss on train= 0.016402525827288628\n",
      "Loss on test= 0.01969829946756363\n",
      "acc for Lsat= 0.09551306151681477 \n",
      "acc for Psat= 0.15101070205370584 \n",
      "acc for optim= 0.16705814070171784\n",
      "Epoch:160/1000\n",
      "Loss on train= 0.01647789590060711\n",
      "Loss on test= 0.021576354280114174\n",
      "acc for Lsat= 0.10959579563803143 \n",
      "acc for Psat= 0.18333243264092342 \n",
      "acc for optim= 0.16837007883522245\n",
      "Epoch:161/1000\n",
      "Loss on train= 0.017024250701069832\n",
      "Loss on test= 0.020777054131031036\n",
      "acc for Lsat= 0.08183673636780846 \n",
      "acc for Psat= 0.1360710577832328 \n",
      "acc for optim= 0.17089356813165876\n",
      "Epoch:162/1000\n",
      "Loss on train= 0.01611587218940258\n",
      "Loss on test= 0.020470090210437775\n",
      "acc for Lsat= 0.08394627802901798 \n",
      "acc for Psat= 0.13427577118078865 \n",
      "acc for optim= 0.1645334011978573\n",
      "Epoch:163/1000\n",
      "Loss on train= 0.015999766066670418\n",
      "Loss on test= 0.02191833034157753\n",
      "acc for Lsat= 0.08186777035395304 \n",
      "acc for Psat= 0.14819435510370466 \n",
      "acc for optim= 0.17048457049661214\n",
      "Epoch:164/1000\n",
      "Loss on train= 0.016553789377212524\n",
      "Loss on test= 0.019525758922100067\n",
      "acc for Lsat= 0.08044335179858737 \n",
      "acc for Psat= 0.13154478636052874 \n",
      "acc for optim= 0.16784060729874506\n",
      "Epoch:165/1000\n",
      "Loss on train= 0.01615223102271557\n",
      "Loss on test= 0.020263798534870148\n",
      "acc for Lsat= 0.08822677284479143 \n",
      "acc for Psat= 0.15024672283066642 \n",
      "acc for optim= 0.16515105267365773\n",
      "Epoch:166/1000\n",
      "Loss on train= 0.01628027856349945\n",
      "Loss on test= 0.022788172587752342\n",
      "acc for Lsat= 0.11699308024512398 \n",
      "acc for Psat= 0.16511885954274072 \n",
      "acc for optim= 0.16758837749560673\n",
      "Epoch:167/1000\n",
      "Loss on train= 0.016275420784950256\n",
      "Loss on test= 0.021759992465376854\n",
      "acc for Lsat= 0.09323590629630621 \n",
      "acc for Psat= 0.16190639171335433 \n",
      "acc for optim= 0.17289535005887346\n",
      "Epoch:168/1000\n",
      "Loss on train= 0.016045603901147842\n",
      "Loss on test= 0.020576942712068558\n",
      "acc for Lsat= 0.09037047955724928 \n",
      "acc for Psat= 0.1467259897126092 \n",
      "acc for optim= 0.1709791004657745\n",
      "Epoch:169/1000\n",
      "Loss on train= 0.01596648059785366\n",
      "Loss on test= 0.021160557866096497\n",
      "acc for Lsat= 0.0825979318883684 \n",
      "acc for Psat= 0.13724480701817407 \n",
      "acc for optim= 0.1601283644636472\n",
      "Epoch:170/1000\n",
      "Loss on train= 0.01646299473941326\n",
      "Loss on test= 0.021425539627671242\n",
      "acc for Lsat= 0.09464108513461218 \n",
      "acc for Psat= 0.15638370282120173 \n",
      "acc for optim= 0.17302408864100774\n",
      "Epoch:171/1000\n",
      "Loss on train= 0.015905039384961128\n",
      "Loss on test= 0.02068765088915825\n",
      "acc for Lsat= 0.0913801521062851 \n",
      "acc for Psat= 0.1284141023953756 \n",
      "acc for optim= 0.16665980948342216\n",
      "Epoch:172/1000\n",
      "Loss on train= 0.01629842072725296\n",
      "Loss on test= 0.020588509738445282\n",
      "acc for Lsat= 0.09150485396385191 \n",
      "acc for Psat= 0.13608540362781948 \n",
      "acc for optim= 0.16322790251837838\n",
      "Epoch:173/1000\n",
      "Loss on train= 0.016102351248264313\n",
      "Loss on test= 0.01930505596101284\n",
      "acc for Lsat= 0.08407953066958321 \n",
      "acc for Psat= 0.14320649007956188 \n",
      "acc for optim= 0.16560997168223063\n",
      "Epoch:174/1000\n",
      "Loss on train= 0.015747465193271637\n",
      "Loss on test= 0.02016867697238922\n",
      "acc for Lsat= 0.0806834225853284 \n",
      "acc for Psat= 0.137583726644516 \n",
      "acc for optim= 0.16317495041423374\n",
      "Epoch:175/1000\n",
      "Loss on train= 0.015229729004204273\n",
      "Loss on test= 0.020970003679394722\n",
      "acc for Lsat= 0.08513633724715974 \n",
      "acc for Psat= 0.14626155661212073 \n",
      "acc for optim= 0.16664689812395306\n",
      "Epoch:176/1000\n",
      "Loss on train= 0.016109056770801544\n",
      "Loss on test= 0.019934751093387604\n",
      "acc for Lsat= 0.07857964113354683 \n",
      "acc for Psat= 0.13054896427525411 \n",
      "acc for optim= 0.16250471621751783\n",
      "Epoch:177/1000\n",
      "Loss on train= 0.016321968287229538\n",
      "Loss on test= 0.020942149683833122\n",
      "acc for Lsat= 0.09323502365085815 \n",
      "acc for Psat= 0.13881056540542178 \n",
      "acc for optim= 0.16123936772346498\n",
      "Epoch:178/1000\n",
      "Loss on train= 0.015651313588023186\n",
      "Loss on test= 0.02032565139234066\n",
      "acc for Lsat= 0.08382762670516969 \n",
      "acc for Psat= 0.12888002296288809 \n",
      "acc for optim= 0.16221093982458115\n",
      "Epoch:179/1000\n",
      "Loss on train= 0.016038699075579643\n",
      "Loss on test= 0.020543329417705536\n",
      "acc for Lsat= 0.08327549133035872 \n",
      "acc for Psat= 0.1504404173956977 \n",
      "acc for optim= 0.16530400895410113\n",
      "Epoch:180/1000\n",
      "Loss on train= 0.015709372237324715\n",
      "Loss on test= 0.02038196660578251\n",
      "acc for Lsat= 0.09297490053706699 \n",
      "acc for Psat= 0.13953713311089408 \n",
      "acc for optim= 0.16556460890505054\n",
      "Epoch:181/1000\n",
      "Loss on train= 0.015867501497268677\n",
      "Loss on test= 0.02124103158712387\n",
      "acc for Lsat= 0.08138616035381953 \n",
      "acc for Psat= 0.13043285343382094 \n",
      "acc for optim= 0.16223719285594096\n",
      "Epoch:182/1000\n",
      "Loss on train= 0.01546909287571907\n",
      "Loss on test= 0.020528243854641914\n",
      "acc for Lsat= 0.07564922935432858 \n",
      "acc for Psat= 0.12675547036859725 \n",
      "acc for optim= 0.16852956414222717\n",
      "Epoch:183/1000\n",
      "Loss on train= 0.015543523244559765\n",
      "Loss on test= 0.019000248983502388\n",
      "acc for Lsat= 0.08035030547115538 \n",
      "acc for Psat= 0.14328396419684092 \n",
      "acc for optim= 0.1646289762523439\n",
      "Epoch:184/1000\n",
      "Loss on train= 0.015774406492710114\n",
      "Loss on test= 0.021081307902932167\n",
      "acc for Lsat= 0.08416585723559061 \n",
      "acc for Psat= 0.155156538883845 \n",
      "acc for optim= 0.16386995613574978\n",
      "Epoch:185/1000\n",
      "Loss on train= 0.015975594520568848\n",
      "Loss on test= 0.02010626718401909\n",
      "acc for Lsat= 0.07499659607807795 \n",
      "acc for Psat= 0.12483477824264104 \n",
      "acc for optim= 0.16147181292374932\n",
      "Epoch:186/1000\n",
      "Loss on train= 0.01513194665312767\n",
      "Loss on test= 0.019967639818787575\n",
      "acc for Lsat= 0.0978446645869149 \n",
      "acc for Psat= 0.12962148090203604 \n",
      "acc for optim= 0.15919440264503162\n",
      "Epoch:187/1000\n",
      "Loss on train= 0.015049496665596962\n",
      "Loss on test= 0.02114945650100708\n",
      "acc for Lsat= 0.08205018978979851 \n",
      "acc for Psat= 0.13958097298940023 \n",
      "acc for optim= 0.1608436887462934\n",
      "Epoch:188/1000\n",
      "Loss on train= 0.015377643518149853\n",
      "Loss on test= 0.019253095611929893\n",
      "acc for Lsat= 0.09849512941307494 \n",
      "acc for Psat= 0.14654028316338857 \n",
      "acc for optim= 0.16841381672355862\n",
      "Epoch:189/1000\n",
      "Loss on train= 0.015383264981210232\n",
      "Loss on test= 0.0200986135751009\n",
      "acc for Lsat= 0.0792584467265341 \n",
      "acc for Psat= 0.13096624182330238 \n",
      "acc for optim= 0.16893337567647299\n",
      "Epoch:190/1000\n",
      "Loss on train= 0.015436122193932533\n",
      "Loss on test= 0.01889338158071041\n",
      "acc for Lsat= 0.07870784716473685 \n",
      "acc for Psat= 0.1295436296198103 \n",
      "acc for optim= 0.1662491861316893\n",
      "Epoch:191/1000\n",
      "Loss on train= 0.015568560920655727\n",
      "Loss on test= 0.020853791385889053\n",
      "acc for Lsat= 0.0813535460167461 \n",
      "acc for Psat= 0.15701460738976794 \n",
      "acc for optim= 0.1638208990295728\n",
      "Epoch:192/1000\n",
      "Loss on train= 0.015326503664255142\n",
      "Loss on test= 0.01920502632856369\n",
      "acc for Lsat= 0.09316040807300144 \n",
      "acc for Psat= 0.1263597561253442 \n",
      "acc for optim= 0.16321938882271447\n",
      "Epoch:193/1000\n",
      "Loss on train= 0.015084178186953068\n",
      "Loss on test= 0.020764164626598358\n",
      "acc for Lsat= 0.08394699262248147 \n",
      "acc for Psat= 0.15435209373633066 \n",
      "acc for optim= 0.1686408794588513\n",
      "Epoch:194/1000\n",
      "Loss on train= 0.015318593941628933\n",
      "Loss on test= 0.01952388510107994\n",
      "acc for Lsat= 0.07724638597832786 \n",
      "acc for Psat= 0.12446602119339836 \n",
      "acc for optim= 0.16575118501981095\n",
      "Epoch:195/1000\n",
      "Loss on train= 0.01471218466758728\n",
      "Loss on test= 0.020270708948373795\n",
      "acc for Lsat= 0.07654459410243564 \n",
      "acc for Psat= 0.12342201040850746 \n",
      "acc for optim= 0.15661631176869079\n",
      "Epoch:196/1000\n",
      "Loss on train= 0.015412788838148117\n",
      "Loss on test= 0.018931204453110695\n",
      "acc for Lsat= 0.0773395718799697 \n",
      "acc for Psat= 0.12508507404062483 \n",
      "acc for optim= 0.1607275946272744\n",
      "Epoch:197/1000\n",
      "Loss on train= 0.015042484737932682\n",
      "Loss on test= 0.020818553864955902\n",
      "acc for Lsat= 0.0885560949643453 \n",
      "acc for Psat= 0.18390074074268342 \n",
      "acc for optim= 0.1607527587148878\n",
      "Epoch:198/1000\n",
      "Loss on train= 0.015275037847459316\n",
      "Loss on test= 0.019421866163611412\n",
      "acc for Lsat= 0.07787543510397277 \n",
      "acc for Psat= 0.12830167843235862 \n",
      "acc for optim= 0.1585121692882644\n",
      "Epoch:199/1000\n",
      "Loss on train= 0.015259378589689732\n",
      "Loss on test= 0.018420446664094925\n",
      "acc for Lsat= 0.09094527463118234 \n",
      "acc for Psat= 0.130942835410436 \n",
      "acc for optim= 0.1652026537391874\n",
      "Epoch:200/1000\n",
      "Loss on train= 0.015291702002286911\n",
      "Loss on test= 0.020196713507175446\n",
      "acc for Lsat= 0.09245304034815893 \n",
      "acc for Psat= 0.12992825276321834 \n",
      "acc for optim= 0.16189366214805184\n",
      "Epoch:201/1000\n",
      "Loss on train= 0.015271610580384731\n",
      "Loss on test= 0.020562347024679184\n",
      "acc for Lsat= 0.07598468868268861 \n",
      "acc for Psat= 0.12957127723428938 \n",
      "acc for optim= 0.15885769261254204\n",
      "Epoch:202/1000\n",
      "Loss on train= 0.015445670112967491\n",
      "Loss on test= 0.018460560590028763\n",
      "acc for Lsat= 0.0794798990090688 \n",
      "acc for Psat= 0.1232984983258777 \n",
      "acc for optim= 0.16068659582071834\n",
      "Epoch:203/1000\n",
      "Loss on train= 0.014708281494677067\n",
      "Loss on test= 0.019353259354829788\n",
      "acc for Lsat= 0.07329745640357334 \n",
      "acc for Psat= 0.11877908143732283 \n",
      "acc for optim= 0.16665850347942776\n",
      "Epoch:204/1000\n",
      "Loss on train= 0.014894247055053711\n",
      "Loss on test= 0.018766172230243683\n",
      "acc for Lsat= 0.08147803213861254 \n",
      "acc for Psat= 0.13625581396950615 \n",
      "acc for optim= 0.1622212313943439\n",
      "Epoch:205/1000\n",
      "Loss on train= 0.01486849132925272\n",
      "Loss on test= 0.018599677830934525\n",
      "acc for Lsat= 0.08112664173046749 \n",
      "acc for Psat= 0.1351905819442537 \n",
      "acc for optim= 0.1635462376806471\n",
      "Epoch:206/1000\n",
      "Loss on train= 0.01546806376427412\n",
      "Loss on test= 0.01930229738354683\n",
      "acc for Lsat= 0.07800344510210885 \n",
      "acc for Psat= 0.12494830323590171 \n",
      "acc for optim= 0.1641671011845271\n",
      "Epoch:207/1000\n",
      "Loss on train= 0.01498208288103342\n",
      "Loss on test= 0.01974785514175892\n",
      "acc for Lsat= 0.09771412296427621 \n",
      "acc for Psat= 0.13768061995506287 \n",
      "acc for optim= 0.15859694845146602\n",
      "Epoch:208/1000\n",
      "Loss on train= 0.014701184816658497\n",
      "Loss on test= 0.019355028867721558\n",
      "acc for Lsat= 0.07338840266068775 \n",
      "acc for Psat= 0.13165768848525153 \n",
      "acc for optim= 0.1671126421954897\n",
      "Epoch:209/1000\n",
      "Loss on train= 0.014804008416831493\n",
      "Loss on test= 0.021382028236985207\n",
      "acc for Lsat= 0.07339683779411846 \n",
      "acc for Psat= 0.11996515029006533 \n",
      "acc for optim= 0.16001425898737376\n",
      "Epoch:210/1000\n",
      "Loss on train= 0.0147556122392416\n",
      "Loss on test= 0.01991797238588333\n",
      "acc for Lsat= 0.09161961484286522 \n",
      "acc for Psat= 0.12421469489733378 \n",
      "acc for optim= 0.158679234319263\n",
      "Epoch:211/1000\n",
      "Loss on train= 0.014720103703439236\n",
      "Loss on test= 0.021286793053150177\n",
      "acc for Lsat= 0.0941215788324674 \n",
      "acc for Psat= 0.14769385953744252 \n",
      "acc for optim= 0.15802693217992783\n",
      "Epoch:212/1000\n",
      "Loss on train= 0.015448048710823059\n",
      "Loss on test= 0.020605839788913727\n",
      "acc for Lsat= 0.09937584267722237 \n",
      "acc for Psat= 0.13345210717784034 \n",
      "acc for optim= 0.1639862702952491\n",
      "Epoch:213/1000\n",
      "Loss on train= 0.014507927931845188\n",
      "Loss on test= 0.020572366192936897\n",
      "acc for Lsat= 0.07240985880295436 \n",
      "acc for Psat= 0.12102697094281517 \n",
      "acc for optim= 0.16765489876270298\n",
      "Epoch:214/1000\n",
      "Loss on train= 0.01501118578016758\n",
      "Loss on test= 0.018917126581072807\n",
      "acc for Lsat= 0.07805359760920207 \n",
      "acc for Psat= 0.12664201789432103 \n",
      "acc for optim= 0.16343101693524256\n",
      "Epoch:215/1000\n",
      "Loss on train= 0.01470162346959114\n",
      "Loss on test= 0.02075584977865219\n",
      "acc for Lsat= 0.10404254264301725 \n",
      "acc for Psat= 0.14757251275910271 \n",
      "acc for optim= 0.16120369409521415\n",
      "Epoch:216/1000\n",
      "Loss on train= 0.014521997421979904\n",
      "Loss on test= 0.020521922037005424\n",
      "acc for Lsat= 0.07676607709791926 \n",
      "acc for Psat= 0.12142306996716395 \n",
      "acc for optim= 0.15932455791367428\n",
      "Epoch:217/1000\n",
      "Loss on train= 0.014408040791749954\n",
      "Loss on test= 0.019092420116066933\n",
      "acc for Lsat= 0.09155787030855815 \n",
      "acc for Psat= 0.14118939207659828 \n",
      "acc for optim= 0.16646570977237488\n",
      "Epoch:218/1000\n",
      "Loss on train= 0.014785575680434704\n",
      "Loss on test= 0.01997409574687481\n",
      "acc for Lsat= 0.08219756897952821 \n",
      "acc for Psat= 0.12571599781513215 \n",
      "acc for optim= 0.16301027950313354\n",
      "Epoch:219/1000\n",
      "Loss on train= 0.01519826427102089\n",
      "Loss on test= 0.01887444779276848\n",
      "acc for Lsat= 0.09105725569857491 \n",
      "acc for Psat= 0.14420968360371061 \n",
      "acc for optim= 0.16451098902357947\n",
      "Epoch:220/1000\n",
      "Loss on train= 0.01429049763828516\n",
      "Loss on test= 0.02064104750752449\n",
      "acc for Lsat= 0.0788350083761745 \n",
      "acc for Psat= 0.12649405499299368 \n",
      "acc for optim= 0.16273159782091778\n",
      "Epoch:221/1000\n",
      "Loss on train= 0.014341148547828197\n",
      "Loss on test= 0.019859589636325836\n",
      "acc for Lsat= 0.08272190408574209 \n",
      "acc for Psat= 0.12736153304576878 \n",
      "acc for optim= 0.16465137170420757\n",
      "Epoch:222/1000\n",
      "Loss on train= 0.014277397654950619\n",
      "Loss on test= 0.018901579082012177\n",
      "acc for Lsat= 0.08434228665298886 \n",
      "acc for Psat= 0.1382184929317898 \n",
      "acc for optim= 0.16176709036032358\n",
      "Epoch:223/1000\n",
      "Loss on train= 0.014600553549826145\n",
      "Loss on test= 0.018753333017230034\n",
      "acc for Lsat= 0.08532423277695975 \n",
      "acc for Psat= 0.1323599567015966 \n",
      "acc for optim= 0.15982491572697952\n",
      "Epoch:224/1000\n",
      "Loss on train= 0.014570292085409164\n",
      "Loss on test= 0.019330499693751335\n",
      "acc for Lsat= 0.07563953308595553 \n",
      "acc for Psat= 0.12899812228149837 \n",
      "acc for optim= 0.15911236223247313\n",
      "Epoch:225/1000\n",
      "Loss on train= 0.013977151364088058\n",
      "Loss on test= 0.018886785954236984\n",
      "acc for Lsat= 0.08173911654286914 \n",
      "acc for Psat= 0.1321165445778105 \n",
      "acc for optim= 0.16228007574876147\n",
      "Epoch:226/1000\n",
      "Loss on train= 0.014144305139780045\n",
      "Loss on test= 0.01957450620830059\n",
      "acc for Lsat= 0.08079692423343658 \n",
      "acc for Psat= 0.13343305653995938 \n",
      "acc for optim= 0.16661722544166777\n",
      "Epoch:227/1000\n",
      "Loss on train= 0.014170882292091846\n",
      "Loss on test= 0.020687587559223175\n",
      "acc for Lsat= 0.07756584700610902 \n",
      "acc for Psat= 0.13180756337112853 \n",
      "acc for optim= 0.16441406607627865\n",
      "Epoch:228/1000\n",
      "Loss on train= 0.014547538943588734\n",
      "Loss on test= 0.02105405367910862\n",
      "acc for Lsat= 0.08756019357177945 \n",
      "acc for Psat= 0.1291450709104538 \n",
      "acc for optim= 0.16203450908263523\n",
      "Epoch:229/1000\n",
      "Loss on train= 0.014691335149109364\n",
      "Loss on test= 0.020619316026568413\n",
      "acc for Lsat= 0.07146658988462555 \n",
      "acc for Psat= 0.12848853237099117 \n",
      "acc for optim= 0.15974074337217536\n",
      "Epoch:230/1000\n",
      "Loss on train= 0.015363702550530434\n",
      "Loss on test= 0.02056097239255905\n",
      "acc for Lsat= 0.08892966310183206 \n",
      "acc for Psat= 0.13115246627065869 \n",
      "acc for optim= 0.156232787668705\n",
      "Epoch:231/1000\n",
      "Loss on train= 0.014237070456147194\n",
      "Loss on test= 0.018566027283668518\n",
      "acc for Lsat= 0.07570411513249078 \n",
      "acc for Psat= 0.12814029985004 \n",
      "acc for optim= 0.15979531274901493\n",
      "Epoch:232/1000\n",
      "Loss on train= 0.013847953639924526\n",
      "Loss on test= 0.020073605701327324\n",
      "acc for Lsat= 0.07228665467765595 \n",
      "acc for Psat= 0.11822653512159981 \n",
      "acc for optim= 0.15616826398505104\n",
      "Epoch:233/1000\n",
      "Loss on train= 0.01420736312866211\n",
      "Loss on test= 0.01984822005033493\n",
      "acc for Lsat= 0.0902458096543948 \n",
      "acc for Psat= 0.13430769907103646 \n",
      "acc for optim= 0.1571415069202582\n",
      "Epoch:234/1000\n",
      "Loss on train= 0.014715476892888546\n",
      "Loss on test= 0.01867656782269478\n",
      "acc for Lsat= 0.07341600077019798 \n",
      "acc for Psat= 0.12456356452571021 \n",
      "acc for optim= 0.15909742050700715\n",
      "Epoch:235/1000\n",
      "Loss on train= 0.013998795300722122\n",
      "Loss on test= 0.019646400585770607\n",
      "acc for Lsat= 0.0809101778599951 \n",
      "acc for Psat= 0.11916842063268025 \n",
      "acc for optim= 0.15476938080456526\n",
      "Epoch:236/1000\n",
      "Loss on train= 0.014053083024919033\n",
      "Loss on test= 0.019413679838180542\n",
      "acc for Lsat= 0.08290439877245162 \n",
      "acc for Psat= 0.12575863732231987 \n",
      "acc for optim= 0.1625484463241365\n",
      "Epoch:237/1000\n",
      "Loss on train= 0.013929695822298527\n",
      "Loss on test= 0.019147507846355438\n",
      "acc for Lsat= 0.08141536364952723 \n",
      "acc for Psat= 0.11580124994119008 \n",
      "acc for optim= 0.1589473894900746\n",
      "Epoch:238/1000\n",
      "Loss on train= 0.014311272650957108\n",
      "Loss on test= 0.017810218036174774\n",
      "acc for Lsat= 0.07928020705779391 \n",
      "acc for Psat= 0.116790399617619 \n",
      "acc for optim= 0.16182000670168137\n",
      "Epoch:239/1000\n",
      "Loss on train= 0.014234012924134731\n",
      "Loss on test= 0.019053641706705093\n",
      "acc for Lsat= 0.08341233366065555 \n",
      "acc for Psat= 0.13957645694414775 \n",
      "acc for optim= 0.15904430631134248\n",
      "Epoch:240/1000\n",
      "Loss on train= 0.014042064547538757\n",
      "Loss on test= 0.019261064007878304\n",
      "acc for Lsat= 0.08020825253592598 \n",
      "acc for Psat= 0.12033457391791874 \n",
      "acc for optim= 0.15643059934178988\n",
      "Epoch:241/1000\n",
      "Loss on train= 0.01402325090020895\n",
      "Loss on test= 0.019288411363959312\n",
      "acc for Lsat= 0.07461925976806216 \n",
      "acc for Psat= 0.1185520718495051 \n",
      "acc for optim= 0.157119438631667\n",
      "Epoch:242/1000\n",
      "Loss on train= 0.014029246754944324\n",
      "Loss on test= 0.019588865339756012\n",
      "acc for Lsat= 0.07980889429648715 \n",
      "acc for Psat= 0.13325138323836855 \n",
      "acc for optim= 0.16119910329580303\n",
      "Epoch:243/1000\n",
      "Loss on train= 0.01436246931552887\n",
      "Loss on test= 0.01900291070342064\n",
      "acc for Lsat= 0.07602605405781003 \n",
      "acc for Psat= 0.11960047913922203 \n",
      "acc for optim= 0.15739472342862024\n",
      "Epoch:244/1000\n",
      "Loss on train= 0.014020731672644615\n",
      "Loss on test= 0.019858701154589653\n",
      "acc for Lsat= 0.08140806059042613 \n",
      "acc for Psat= 0.13340692387686837 \n",
      "acc for optim= 0.16246404465701847\n",
      "Epoch:245/1000\n",
      "Loss on train= 0.014019331894814968\n",
      "Loss on test= 0.018789537250995636\n",
      "acc for Lsat= 0.07234088844723173 \n",
      "acc for Psat= 0.12142599688635933 \n",
      "acc for optim= 0.15719679726494687\n",
      "Epoch:246/1000\n",
      "Loss on train= 0.013829526491463184\n",
      "Loss on test= 0.02166346274316311\n",
      "acc for Lsat= 0.11857684519555835 \n",
      "acc for Psat= 0.15341673857635924 \n",
      "acc for optim= 0.16073157621754539\n",
      "Epoch:247/1000\n",
      "Loss on train= 0.014684042893350124\n",
      "Loss on test= 0.019144967198371887\n",
      "acc for Lsat= 0.07975389940871133 \n",
      "acc for Psat= 0.11393080982897016 \n",
      "acc for optim= 0.15811322149303225\n",
      "Epoch:248/1000\n",
      "Loss on train= 0.013703160919249058\n",
      "Loss on test= 0.01785411313176155\n",
      "acc for Lsat= 0.07175789972146353 \n",
      "acc for Psat= 0.12145152886708578 \n",
      "acc for optim= 0.15542616297801337\n",
      "Epoch:249/1000\n",
      "Loss on train= 0.013563784770667553\n",
      "Loss on test= 0.018502404913306236\n",
      "acc for Lsat= 0.08160282307200961 \n",
      "acc for Psat= 0.12775827944278717 \n",
      "acc for optim= 0.15836361514197456\n",
      "Epoch:250/1000\n",
      "Loss on train= 0.014024543575942516\n",
      "Loss on test= 0.0199133213609457\n",
      "acc for Lsat= 0.0700446335805787 \n",
      "acc for Psat= 0.11566359268294442 \n",
      "acc for optim= 0.16470446685949963\n",
      "Epoch:251/1000\n",
      "Loss on train= 0.013844836503267288\n",
      "Loss on test= 0.019561827182769775\n",
      "acc for Lsat= 0.07843704654110802 \n",
      "acc for Psat= 0.13502597643269432 \n",
      "acc for optim= 0.15791566487815648\n",
      "Epoch:252/1000\n",
      "Loss on train= 0.013848748058080673\n",
      "Loss on test= 0.018524272367358208\n",
      "acc for Lsat= 0.08263902134365507 \n",
      "acc for Psat= 0.1195437722735935 \n",
      "acc for optim= 0.15916957242621316\n",
      "Epoch:253/1000\n",
      "Loss on train= 0.01369803212583065\n",
      "Loss on test= 0.019640885293483734\n",
      "acc for Lsat= 0.07518936643997827 \n",
      "acc for Psat= 0.13189713458220162 \n",
      "acc for optim= 0.16586929361025493\n",
      "Epoch:254/1000\n",
      "Loss on train= 0.01357353013008833\n",
      "Loss on test= 0.018659744411706924\n",
      "acc for Lsat= 0.10878790517648063 \n",
      "acc for Psat= 0.13855388263861337 \n",
      "acc for optim= 0.16198300073544183\n",
      "Epoch:255/1000\n",
      "Loss on train= 0.013566664420068264\n",
      "Loss on test= 0.01839963160455227\n",
      "acc for Lsat= 0.08609480808178584 \n",
      "acc for Psat= 0.13918721940782333 \n",
      "acc for optim= 0.15842835489246582\n",
      "Epoch:256/1000\n",
      "Loss on train= 0.013785924762487411\n",
      "Loss on test= 0.01925511658191681\n",
      "acc for Lsat= 0.08677676129672263 \n",
      "acc for Psat= 0.12119659814569683 \n",
      "acc for optim= 0.15566518124606876\n",
      "Epoch:257/1000\n",
      "Loss on train= 0.013727276585996151\n",
      "Loss on test= 0.018640752881765366\n",
      "acc for Lsat= 0.07372897681262759 \n",
      "acc for Psat= 0.12416059772173564 \n",
      "acc for optim= 0.15761916985114416\n",
      "Epoch:258/1000\n",
      "Loss on train= 0.013941878452897072\n",
      "Loss on test= 0.01776544190943241\n",
      "acc for Lsat= 0.08309845758808983 \n",
      "acc for Psat= 0.11601621210575103 \n",
      "acc for optim= 0.1584813710716036\n",
      "Epoch:259/1000\n",
      "Loss on train= 0.013722673058509827\n",
      "Loss on test= 0.01901576668024063\n",
      "acc for Lsat= 0.07242671118842231 \n",
      "acc for Psat= 0.12324369748433428 \n",
      "acc for optim= 0.15444536126322211\n",
      "Epoch:260/1000\n",
      "Loss on train= 0.013625461608171463\n",
      "Loss on test= 0.01880962960422039\n",
      "acc for Lsat= 0.06740950329436197 \n",
      "acc for Psat= 0.12431397868527307 \n",
      "acc for optim= 0.15585204942358863\n",
      "Epoch:261/1000\n",
      "Loss on train= 0.013269390910863876\n",
      "Loss on test= 0.01903313212096691\n",
      "acc for Lsat= 0.08379092415173847 \n",
      "acc for Psat= 0.11386289927694533 \n",
      "acc for optim= 0.1580622567070855\n",
      "Epoch:262/1000\n",
      "Loss on train= 0.01360290963202715\n",
      "Loss on test= 0.01957554742693901\n",
      "acc for Lsat= 0.08861411545011734 \n",
      "acc for Psat= 0.13732971615261502 \n",
      "acc for optim= 0.15816607947150862\n",
      "Epoch:263/1000\n",
      "Loss on train= 0.013426225632429123\n",
      "Loss on test= 0.018390854820609093\n",
      "acc for Lsat= 0.06860024109482767 \n",
      "acc for Psat= 0.12036462624867757 \n",
      "acc for optim= 0.15906491470005774\n",
      "Epoch:264/1000\n",
      "Loss on train= 0.013611262664198875\n",
      "Loss on test= 0.018235061317682266\n",
      "acc for Lsat= 0.06840765037470392 \n",
      "acc for Psat= 0.11681782835059698 \n",
      "acc for optim= 0.15514627347389856\n",
      "Epoch:265/1000\n",
      "Loss on train= 0.013524473644793034\n",
      "Loss on test= 0.019156385213136673\n",
      "acc for Lsat= 0.07449113726615907 \n",
      "acc for Psat= 0.11949320236841839 \n",
      "acc for optim= 0.16271444343858293\n",
      "Epoch:266/1000\n",
      "Loss on train= 0.01364584919065237\n",
      "Loss on test= 0.018878869712352753\n",
      "acc for Lsat= 0.06916768617100186 \n",
      "acc for Psat= 0.11008022493786283 \n",
      "acc for optim= 0.15698018438286251\n",
      "Epoch:267/1000\n",
      "Loss on train= 0.013236841186881065\n",
      "Loss on test= 0.019170837476849556\n",
      "acc for Lsat= 0.06954738373557727 \n",
      "acc for Psat= 0.11444306009345584 \n",
      "acc for optim= 0.16095015274153815\n",
      "Epoch:268/1000\n",
      "Loss on train= 0.013396107591688633\n",
      "Loss on test= 0.018850041553378105\n",
      "acc for Lsat= 0.07601940449741153 \n",
      "acc for Psat= 0.1116166611512502 \n",
      "acc for optim= 0.15673327032062745\n",
      "Epoch:269/1000\n",
      "Loss on train= 0.013242247514426708\n",
      "Loss on test= 0.019121523946523666\n",
      "acc for Lsat= 0.06718695983290673 \n",
      "acc for Psat= 0.11372685829798382 \n",
      "acc for optim= 0.16131528036461934\n",
      "Epoch:270/1000\n",
      "Loss on train= 0.013137051835656166\n",
      "Loss on test= 0.01838921755552292\n",
      "acc for Lsat= 0.07126588084631497 \n",
      "acc for Psat= 0.1138024154636595 \n",
      "acc for optim= 0.15674319350057175\n",
      "Epoch:271/1000\n",
      "Loss on train= 0.01330622099339962\n",
      "Loss on test= 0.019479045644402504\n",
      "acc for Lsat= 0.06759265652961201 \n",
      "acc for Psat= 0.11532115207778082 \n",
      "acc for optim= 0.15439986470672817\n",
      "Epoch:272/1000\n",
      "Loss on train= 0.013214852660894394\n",
      "Loss on test= 0.019565114751458168\n",
      "acc for Lsat= 0.07506844219234254 \n",
      "acc for Psat= 0.13030084901385836 \n",
      "acc for optim= 0.158195418284999\n",
      "Epoch:273/1000\n",
      "Loss on train= 0.013207190670073032\n",
      "Loss on test= 0.01906360313296318\n",
      "acc for Lsat= 0.07408745346797839 \n",
      "acc for Psat= 0.11124845213360257 \n",
      "acc for optim= 0.15353325737847223\n",
      "Epoch:274/1000\n",
      "Loss on train= 0.013781378045678139\n",
      "Loss on test= 0.020625444129109383\n",
      "acc for Lsat= 0.09740119460556242 \n",
      "acc for Psat= 0.16577506495846644 \n",
      "acc for optim= 0.15853910677962835\n",
      "Epoch:275/1000\n",
      "Loss on train= 0.013564989902079105\n",
      "Loss on test= 0.019611945375800133\n",
      "acc for Lsat= 0.08484378854433697 \n",
      "acc for Psat= 0.12410603827900354 \n",
      "acc for optim= 0.15822738740179274\n",
      "Epoch:276/1000\n",
      "Loss on train= 0.013664170168340206\n",
      "Loss on test= 0.019009822979569435\n",
      "acc for Lsat= 0.07729116049077774 \n",
      "acc for Psat= 0.11194098624918196 \n",
      "acc for optim= 0.15658234225379097\n",
      "Epoch:277/1000\n",
      "Loss on train= 0.013436543755233288\n",
      "Loss on test= 0.018559051677584648\n",
      "acc for Lsat= 0.08275411973396936 \n",
      "acc for Psat= 0.15053926706314089 \n",
      "acc for optim= 0.15582630526688365\n",
      "Epoch:278/1000\n",
      "Loss on train= 0.012963518500328064\n",
      "Loss on test= 0.019445568323135376\n",
      "acc for Lsat= 0.08003970682621003 \n",
      "acc for Psat= 0.12005345159106787 \n",
      "acc for optim= 0.1579409334394667\n",
      "Epoch:279/1000\n",
      "Loss on train= 0.013079822063446045\n",
      "Loss on test= 0.020306741818785667\n",
      "acc for Lsat= 0.09022438244687186 \n",
      "acc for Psat= 0.12312117947472467 \n",
      "acc for optim= 0.15421844141350852\n",
      "Epoch:280/1000\n",
      "Loss on train= 0.013417947106063366\n",
      "Loss on test= 0.019232355058193207\n",
      "acc for Lsat= 0.08727977292405235 \n",
      "acc for Psat= 0.12270386318365735 \n",
      "acc for optim= 0.15871938765048987\n",
      "Epoch:281/1000\n",
      "Loss on train= 0.013355404138565063\n",
      "Loss on test= 0.01867288537323475\n",
      "acc for Lsat= 0.07772545201910866 \n",
      "acc for Psat= 0.12689625554614598 \n",
      "acc for optim= 0.1568693584865994\n",
      "Epoch:282/1000\n",
      "Loss on train= 0.012663149274885654\n",
      "Loss on test= 0.020871594548225403\n",
      "acc for Lsat= 0.08218921803765827 \n",
      "acc for Psat= 0.11452669633759394 \n",
      "acc for optim= 0.15507487456003824\n",
      "Epoch:283/1000\n",
      "Loss on train= 0.013474666513502598\n",
      "Loss on test= 0.018877891823649406\n",
      "acc for Lsat= 0.07245374487506019 \n",
      "acc for Psat= 0.1117419785923428 \n",
      "acc for optim= 0.15573767067657573\n",
      "Epoch:284/1000\n",
      "Loss on train= 0.013053013943135738\n",
      "Loss on test= 0.019126014783978462\n",
      "acc for Lsat= 0.07374414487017526 \n",
      "acc for Psat= 0.12020825776788922 \n",
      "acc for optim= 0.15628943625423647\n",
      "Epoch:285/1000\n",
      "Loss on train= 0.013439467176795006\n",
      "Loss on test= 0.01819036155939102\n",
      "acc for Lsat= 0.07214316195911832 \n",
      "acc for Psat= 0.1120439502927992 \n",
      "acc for optim= 0.15867509726021023\n",
      "Epoch:286/1000\n",
      "Loss on train= 0.013396615162491798\n",
      "Loss on test= 0.018578171730041504\n",
      "acc for Lsat= 0.09361105859279634 \n",
      "acc for Psat= 0.1272226790587107 \n",
      "acc for optim= 0.1551705739564366\n",
      "Epoch:287/1000\n",
      "Loss on train= 0.013062063604593277\n",
      "Loss on test= 0.018058650195598602\n",
      "acc for Lsat= 0.06718040671613483 \n",
      "acc for Psat= 0.11101608077685038 \n",
      "acc for optim= 0.1629843751589457\n",
      "Epoch:288/1000\n",
      "Loss on train= 0.013148214668035507\n",
      "Loss on test= 0.01956532709300518\n",
      "acc for Lsat= 0.07448734823200438 \n",
      "acc for Psat= 0.11425784958733451 \n",
      "acc for optim= 0.15558480454815757\n",
      "Epoch:289/1000\n",
      "Loss on train= 0.012620527297258377\n",
      "Loss on test= 0.019114188849925995\n",
      "acc for Lsat= 0.06984285927481121 \n",
      "acc for Psat= 0.11085709432760876 \n",
      "acc for optim= 0.15766770756906934\n",
      "Epoch:290/1000\n",
      "Loss on train= 0.01286978367716074\n",
      "Loss on test= 0.019285012036561966\n",
      "acc for Lsat= 0.0784785302148925 \n",
      "acc for Psat= 0.11484992471006181 \n",
      "acc for optim= 0.15999575936132007\n",
      "Epoch:291/1000\n",
      "Loss on train= 0.012935679405927658\n",
      "Loss on test= 0.019518403336405754\n",
      "acc for Lsat= 0.09835025502575767 \n",
      "acc for Psat= 0.13846450083785586 \n",
      "acc for optim= 0.15537190520101124\n",
      "Epoch:292/1000\n",
      "Loss on train= 0.012893617153167725\n",
      "Loss on test= 0.018738334998488426\n",
      "acc for Lsat= 0.06783521009816064 \n",
      "acc for Psat= 0.10382074515024822 \n",
      "acc for optim= 0.15858786950508755\n",
      "Epoch:293/1000\n",
      "Loss on train= 0.013140708208084106\n",
      "Loss on test= 0.01831614226102829\n",
      "acc for Lsat= 0.06902095956934823 \n",
      "acc for Psat= 0.10959822667969597 \n",
      "acc for optim= 0.15638971775770186\n",
      "Epoch:294/1000\n",
      "Loss on train= 0.013332858681678772\n",
      "Loss on test= 0.019713347777724266\n",
      "acc for Lsat= 0.0878069430589676 \n",
      "acc for Psat= 0.11770810286204024 \n",
      "acc for optim= 0.1584355705314212\n",
      "Epoch:295/1000\n",
      "Loss on train= 0.013286993838846684\n",
      "Loss on test= 0.018966326490044594\n",
      "acc for Lsat= 0.09518020484182571 \n",
      "acc for Psat= 0.11886356108718449 \n",
      "acc for optim= 0.15924303233623505\n",
      "Epoch:296/1000\n",
      "Loss on train= 0.01291845366358757\n",
      "Loss on test= 0.01765156351029873\n",
      "acc for Lsat= 0.07312991321086884 \n",
      "acc for Psat= 0.1084069828192393 \n",
      "acc for optim= 0.15501798490683238\n",
      "Epoch:297/1000\n",
      "Loss on train= 0.012889356352388859\n",
      "Loss on test= 0.01829214207828045\n",
      "acc for Lsat= 0.07704774306880104 \n",
      "acc for Psat= 0.11265906658437516 \n",
      "acc for optim= 0.15830697715282438\n",
      "Epoch:298/1000\n",
      "Loss on train= 0.0133393919095397\n",
      "Loss on test= 0.018525538966059685\n",
      "acc for Lsat= 0.07525105542606779 \n",
      "acc for Psat= 0.12100137902630702 \n",
      "acc for optim= 0.15854738834831447\n",
      "Epoch:299/1000\n",
      "Loss on train= 0.012689954601228237\n",
      "Loss on test= 0.01815289631485939\n",
      "acc for Lsat= 0.06587415470017327 \n",
      "acc for Psat= 0.11519338654147256 \n",
      "acc for optim= 0.15232835958401364\n",
      "Epoch:300/1000\n",
      "Loss on train= 0.013119930401444435\n",
      "Loss on test= 0.01915072277188301\n",
      "acc for Lsat= 0.07283442322578695 \n",
      "acc for Psat= 0.10990410248438516 \n",
      "acc for optim= 0.15331645632783575\n",
      "Epoch:301/1000\n",
      "Loss on train= 0.01283997856080532\n",
      "Loss on test= 0.018406109884381294\n",
      "acc for Lsat= 0.07630779511398739 \n",
      "acc for Psat= 0.10877459281020692 \n",
      "acc for optim= 0.15345424471629993\n",
      "Epoch:302/1000\n",
      "Loss on train= 0.01235266961157322\n",
      "Loss on test= 0.018133901059627533\n",
      "acc for Lsat= 0.07022944821251764 \n",
      "acc for Psat= 0.11646226214038001 \n",
      "acc for optim= 0.15410069045093325\n",
      "Epoch:303/1000\n",
      "Loss on train= 0.01300883013755083\n",
      "Loss on test= 0.01928429678082466\n",
      "acc for Lsat= 0.08844417896535661 \n",
      "acc for Psat= 0.1375816798872418 \n",
      "acc for optim= 0.15716821948687235\n",
      "Epoch:304/1000\n",
      "Loss on train= 0.0135212242603302\n",
      "Loss on test= 0.018830783665180206\n",
      "acc for Lsat= 0.07873337467511494 \n",
      "acc for Psat= 0.10894479089313082 \n",
      "acc for optim= 0.1577169017659293\n",
      "Epoch:305/1000\n",
      "Loss on train= 0.012973488308489323\n",
      "Loss on test= 0.018087612465023994\n",
      "acc for Lsat= 0.07177575627962748 \n",
      "acc for Psat= 0.10878268149163987 \n",
      "acc for optim= 0.15866012821594877\n",
      "Epoch:306/1000\n",
      "Loss on train= 0.012655090540647507\n",
      "Loss on test= 0.018620528280735016\n",
      "acc for Lsat= 0.06523323704799017 \n",
      "acc for Psat= 0.11190741227732766 \n",
      "acc for optim= 0.15592232536938455\n",
      "Epoch:307/1000\n",
      "Loss on train= 0.012871268205344677\n",
      "Loss on test= 0.02071462571620941\n",
      "acc for Lsat= 0.06739961852629979 \n",
      "acc for Psat= 0.11255776551034716 \n",
      "acc for optim= 0.15767400662104292\n",
      "Epoch:308/1000\n",
      "Loss on train= 0.012659123167395592\n",
      "Loss on test= 0.018306856974959373\n",
      "acc for Lsat= 0.0696648749212424 \n",
      "acc for Psat= 0.10917304820484583 \n",
      "acc for optim= 0.15337145609988104\n",
      "Epoch:309/1000\n",
      "Loss on train= 0.0124059384688735\n",
      "Loss on test= 0.019580552354454994\n",
      "acc for Lsat= 0.06418768109546769 \n",
      "acc for Psat= 0.10536971489588419 \n",
      "acc for optim= 0.1575248168574439\n",
      "Epoch:310/1000\n",
      "Loss on train= 0.012705733999609947\n",
      "Loss on test= 0.018658043816685677\n",
      "acc for Lsat= 0.06523782726791169 \n",
      "acc for Psat= 0.10599258773856693 \n",
      "acc for optim= 0.15925911565621698\n",
      "Epoch:311/1000\n",
      "Loss on train= 0.012767720967531204\n",
      "Loss on test= 0.018468378111720085\n",
      "acc for Lsat= 0.07602664828300476 \n",
      "acc for Psat= 0.10971727238761056 \n",
      "acc for optim= 0.1543999012145731\n",
      "Epoch:312/1000\n",
      "Loss on train= 0.013098731637001038\n",
      "Loss on test= 0.020106574520468712\n",
      "acc for Lsat= 0.06868332185679012 \n",
      "acc for Psat= 0.1182995743221707 \n",
      "acc for optim= 0.15594249210423897\n",
      "Epoch:313/1000\n",
      "Loss on train= 0.012570693157613277\n",
      "Loss on test= 0.01842740923166275\n",
      "acc for Lsat= 0.06202121112081739 \n",
      "acc for Psat= 0.11050901843441857 \n",
      "acc for optim= 0.15497968768080078\n",
      "Epoch:314/1000\n",
      "Loss on train= 0.012296194210648537\n",
      "Loss on test= 0.018235772848129272\n",
      "acc for Lsat= 0.07055866171916327 \n",
      "acc for Psat= 0.10981497201654647 \n",
      "acc for optim= 0.1577662220431699\n",
      "Epoch:315/1000\n",
      "Loss on train= 0.0124222282320261\n",
      "Loss on test= 0.018652338534593582\n",
      "acc for Lsat= 0.07612238137258423 \n",
      "acc for Psat= 0.10861383742756314 \n",
      "acc for optim= 0.1646554172039032\n",
      "Epoch:316/1000\n",
      "Loss on train= 0.012756621465086937\n",
      "Loss on test= 0.018629463389515877\n",
      "acc for Lsat= 0.0818205444349183 \n",
      "acc for Psat= 0.11642451021406387 \n",
      "acc for optim= 0.15456834369235567\n",
      "Epoch:317/1000\n",
      "Loss on train= 0.012318175286054611\n",
      "Loss on test= 0.019437093287706375\n",
      "acc for Lsat= 0.07380651591552628 \n",
      "acc for Psat= 0.10976859165562525 \n",
      "acc for optim= 0.1576885627375709\n",
      "Epoch:318/1000\n",
      "Loss on train= 0.012454036623239517\n",
      "Loss on test= 0.01926286891102791\n",
      "acc for Lsat= 0.07149185819758308 \n",
      "acc for Psat= 0.10312970942921111 \n",
      "acc for optim= 0.15476916142635874\n",
      "Epoch:319/1000\n",
      "Loss on train= 0.012823312543332577\n",
      "Loss on test= 0.01833532750606537\n",
      "acc for Lsat= 0.06696101675430934 \n",
      "acc for Psat= 0.10534387959374321 \n",
      "acc for optim= 0.16338960147566264\n",
      "Epoch:320/1000\n",
      "Loss on train= 0.01220434345304966\n",
      "Loss on test= 0.018653400242328644\n",
      "acc for Lsat= 0.0631695852511459 \n",
      "acc for Psat= 0.10927940474616155 \n",
      "acc for optim= 0.15108423282702763\n",
      "Epoch:321/1000\n",
      "Loss on train= 0.012777522206306458\n",
      "Loss on test= 0.0180740337818861\n",
      "acc for Lsat= 0.07145366445183755 \n",
      "acc for Psat= 0.11446273956033919 \n",
      "acc for optim= 0.15480415655506982\n",
      "Epoch:322/1000\n",
      "Loss on train= 0.01267681922763586\n",
      "Loss on test= 0.019689785316586494\n",
      "acc for Lsat= 0.08465583961870936 \n",
      "acc for Psat= 0.11214948130978478 \n",
      "acc for optim= 0.1515004298753209\n",
      "Epoch:323/1000\n",
      "Loss on train= 0.013052854686975479\n",
      "Loss on test= 0.018928468227386475\n",
      "acc for Lsat= 0.08475342508819368 \n",
      "acc for Psat= 0.11902535193496279 \n",
      "acc for optim= 0.15158317362268767\n",
      "Epoch:324/1000\n",
      "Loss on train= 0.012891268357634544\n",
      "Loss on test= 0.02015780471265316\n",
      "acc for Lsat= 0.08282860931422976 \n",
      "acc for Psat= 0.11325619949234857 \n",
      "acc for optim= 0.1512840556601683\n",
      "Epoch:325/1000\n",
      "Loss on train= 0.01267077587544918\n",
      "Loss on test= 0.017683491110801697\n",
      "acc for Lsat= 0.06581075721316867 \n",
      "acc for Psat= 0.10288229485352836 \n",
      "acc for optim= 0.15958686388201185\n",
      "Epoch:326/1000\n",
      "Loss on train= 0.012826766818761826\n",
      "Loss on test= 0.01806662231683731\n",
      "acc for Lsat= 0.06926349790559874 \n",
      "acc for Psat= 0.10258601340982648 \n",
      "acc for optim= 0.1565153173274464\n",
      "Epoch:327/1000\n",
      "Loss on train= 0.012383764609694481\n",
      "Loss on test= 0.01910576969385147\n",
      "acc for Lsat= 0.08055829066369269 \n",
      "acc for Psat= 0.11683119469218786 \n",
      "acc for optim= 0.15379348289635444\n",
      "Epoch:328/1000\n",
      "Loss on train= 0.012366512790322304\n",
      "Loss on test= 0.018315911293029785\n",
      "acc for Lsat= 0.06769874592622122 \n",
      "acc for Psat= 0.10755409565236833 \n",
      "acc for optim= 0.15485695881976022\n",
      "Epoch:329/1000\n",
      "Loss on train= 0.012412413954734802\n",
      "Loss on test= 0.017524974420666695\n",
      "acc for Lsat= 0.07339575439691542 \n",
      "acc for Psat= 0.10644324719905854 \n",
      "acc for optim= 0.1535310249361727\n",
      "Epoch:330/1000\n",
      "Loss on train= 0.012321140617132187\n",
      "Loss on test= 0.019452670589089394\n",
      "acc for Lsat= 0.07180663132005266 \n",
      "acc for Psat= 0.10171848833560942 \n",
      "acc for optim= 0.15598517043723\n",
      "Epoch:331/1000\n",
      "Loss on train= 0.01232392992824316\n",
      "Loss on test= 0.019259342923760414\n",
      "acc for Lsat= 0.06552475318312645 \n",
      "acc for Psat= 0.10983968675136566 \n",
      "acc for optim= 0.15390665481487908\n",
      "Epoch:332/1000\n",
      "Loss on train= 0.011776433326303959\n",
      "Loss on test= 0.018717993050813675\n",
      "acc for Lsat= 0.0695275613003307 \n",
      "acc for Psat= 0.1073007004128562 \n",
      "acc for optim= 0.1579932227730751\n",
      "Epoch:333/1000\n",
      "Loss on train= 0.012189581990242004\n",
      "Loss on test= 0.018118588253855705\n",
      "acc for Lsat= 0.06756017415059937 \n",
      "acc for Psat= 0.11309469507800207 \n",
      "acc for optim= 0.15724279632170995\n",
      "Epoch:334/1000\n",
      "Loss on train= 0.012512006796896458\n",
      "Loss on test= 0.017231039702892303\n",
      "acc for Lsat= 0.06371150877740647 \n",
      "acc for Psat= 0.10345037910673355 \n",
      "acc for optim= 0.15561226912670664\n",
      "Epoch:335/1000\n",
      "Loss on train= 0.011971092782914639\n",
      "Loss on test= 0.018728850409388542\n",
      "acc for Lsat= 0.0632605398694674 \n",
      "acc for Psat= 0.10767821504010094 \n",
      "acc for optim= 0.15553682504428756\n",
      "Epoch:336/1000\n",
      "Loss on train= 0.012129872106015682\n",
      "Loss on test= 0.018274424597620964\n",
      "acc for Lsat= 0.08574507600731318 \n",
      "acc for Psat= 0.11209622058603497 \n",
      "acc for optim= 0.15370376557111742\n",
      "Epoch:337/1000\n",
      "Loss on train= 0.012438169680535793\n",
      "Loss on test= 0.020489640533924103\n",
      "acc for Lsat= 0.08013843016491996 \n",
      "acc for Psat= 0.12517990105681948 \n",
      "acc for optim= 0.15482780461510026\n",
      "Epoch:338/1000\n",
      "Loss on train= 0.011960850097239017\n",
      "Loss on test= 0.019707798957824707\n",
      "acc for Lsat= 0.06959602683782579 \n",
      "acc for Psat= 0.10733443564838835 \n",
      "acc for optim= 0.15542310145166183\n",
      "Epoch:339/1000\n",
      "Loss on train= 0.012218335643410683\n",
      "Loss on test= 0.01918938383460045\n",
      "acc for Lsat= 0.07101886942982674 \n",
      "acc for Psat= 0.10179476539293925 \n",
      "acc for optim= 0.15309069106976192\n",
      "Epoch:340/1000\n",
      "Loss on train= 0.012150422669947147\n",
      "Loss on test= 0.019946586340665817\n",
      "acc for Lsat= 0.0778542677561442 \n",
      "acc for Psat= 0.11520824763509961 \n",
      "acc for optim= 0.1547395533985562\n",
      "Epoch:341/1000\n",
      "Loss on train= 0.01248100958764553\n",
      "Loss on test= 0.018047302961349487\n",
      "acc for Lsat= 0.06732959234052235 \n",
      "acc for Psat= 0.11087424821323819 \n",
      "acc for optim= 0.15327924572759205\n",
      "Epoch:342/1000\n",
      "Loss on train= 0.012157481163740158\n",
      "Loss on test= 0.018750153481960297\n",
      "acc for Lsat= 0.07683623101976184 \n",
      "acc for Psat= 0.10791312621699439 \n",
      "acc for optim= 0.1608425839079751\n",
      "Epoch:343/1000\n",
      "Loss on train= 0.011905426159501076\n",
      "Loss on test= 0.01935539022088051\n",
      "acc for Lsat= 0.07954059814413389 \n",
      "acc for Psat= 0.1132314400540458 \n",
      "acc for optim= 0.15536106195714738\n",
      "Epoch:344/1000\n",
      "Loss on train= 0.012278222478926182\n",
      "Loss on test= 0.018337655812501907\n",
      "acc for Lsat= 0.07182411915726132 \n",
      "acc for Psat= 0.10527524087164136 \n",
      "acc for optim= 0.1587012175056669\n",
      "Epoch:345/1000\n",
      "Loss on train= 0.012135650962591171\n",
      "Loss on test= 0.01882724091410637\n",
      "acc for Lsat= 0.08080027947823207 \n",
      "acc for Psat= 0.11160777939690486 \n",
      "acc for optim= 0.16232765548759037\n",
      "Epoch:346/1000\n",
      "Loss on train= 0.012281343340873718\n",
      "Loss on test= 0.018628263846039772\n",
      "acc for Lsat= 0.08050242728657192 \n",
      "acc for Psat= 0.11786618034044902 \n",
      "acc for optim= 0.15776286779178517\n",
      "Epoch:347/1000\n",
      "Loss on train= 0.01246984675526619\n",
      "Loss on test= 0.019051700830459595\n",
      "acc for Lsat= 0.08515625016556845 \n",
      "acc for Psat= 0.11186790135171679 \n",
      "acc for optim= 0.15595500469207763\n",
      "Epoch:348/1000\n",
      "Loss on train= 0.012006819248199463\n",
      "Loss on test= 0.017687492072582245\n",
      "acc for Lsat= 0.06448362883594301 \n",
      "acc for Psat= 0.10768317116631404 \n",
      "acc for optim= 0.152747612612115\n",
      "Epoch:349/1000\n",
      "Loss on train= 0.012148743495345116\n",
      "Loss on test= 0.018601659685373306\n",
      "acc for Lsat= 0.06943911959727604 \n",
      "acc for Psat= 0.11061487462785508 \n",
      "acc for optim= 0.15398838784959584\n",
      "Epoch:350/1000\n",
      "Loss on train= 0.012053676880896091\n",
      "Loss on test= 0.019087109714746475\n",
      "acc for Lsat= 0.06408351411422093 \n",
      "acc for Psat= 0.10563486052883997 \n",
      "acc for optim= 0.15157117131683562\n",
      "Epoch:351/1000\n",
      "Loss on train= 0.011782143265008926\n",
      "Loss on test= 0.01873980276286602\n",
      "acc for Lsat= 0.08126941356394027 \n",
      "acc for Psat= 0.11163211166858675 \n",
      "acc for optim= 0.157496062748962\n",
      "Epoch:352/1000\n",
      "Loss on train= 0.012082058005034924\n",
      "Loss on test= 0.018654529005289078\n",
      "acc for Lsat= 0.07955277247561349 \n",
      "acc for Psat= 0.10605393350124358 \n",
      "acc for optim= 0.152453832493888\n",
      "Epoch:353/1000\n",
      "Loss on train= 0.012047884054481983\n",
      "Loss on test= 0.017542511224746704\n",
      "acc for Lsat= 0.0731972922053602 \n",
      "acc for Psat= 0.1081749243868722 \n",
      "acc for optim= 0.15161286178562375\n",
      "Epoch:354/1000\n",
      "Loss on train= 0.011803570203483105\n",
      "Loss on test= 0.018321268260478973\n",
      "acc for Lsat= 0.06901264058219063 \n",
      "acc for Psat= 0.10881475541326736 \n",
      "acc for optim= 0.15479962892002533\n",
      "Epoch:355/1000\n",
      "Loss on train= 0.011702647432684898\n",
      "Loss on test= 0.019931510090827942\n",
      "acc for Lsat= 0.09201386736498939 \n",
      "acc for Psat= 0.14726473887761432 \n",
      "acc for optim= 0.1636032919088999\n",
      "Epoch:356/1000\n",
      "Loss on train= 0.012089073657989502\n",
      "Loss on test= 0.018204303458333015\n",
      "acc for Lsat= 0.07563899175988303 \n",
      "acc for Psat= 0.10885169439845616 \n",
      "acc for optim= 0.16074531359805\n",
      "Epoch:357/1000\n",
      "Loss on train= 0.011553936637938023\n",
      "Loss on test= 0.018803492188453674\n",
      "acc for Lsat= 0.06674871088729964 \n",
      "acc for Psat= 0.10132586227522958 \n",
      "acc for optim= 0.15561236374908025\n",
      "Epoch:358/1000\n",
      "Loss on train= 0.01162877306342125\n",
      "Loss on test= 0.01815914176404476\n",
      "acc for Lsat= 0.0717352586487929 \n",
      "acc for Psat= 0.11604243417580923 \n",
      "acc for optim= 0.15061732952793438\n",
      "Epoch:359/1000\n",
      "Loss on train= 0.012114327400922775\n",
      "Loss on test= 0.019278669729828835\n",
      "acc for Lsat= 0.07133368336492113 \n",
      "acc for Psat= 0.12950492865509455 \n",
      "acc for optim= 0.15553188853793673\n",
      "Epoch:360/1000\n",
      "Loss on train= 0.012305420823395252\n",
      "Loss on test= 0.018053891137242317\n",
      "acc for Lsat= 0.07236767278777229 \n",
      "acc for Psat= 0.11176578270064458 \n",
      "acc for optim= 0.1583629260460536\n",
      "Epoch:361/1000\n",
      "Loss on train= 0.012091586366295815\n",
      "Loss on test= 0.018710708245635033\n",
      "acc for Lsat= 0.0839377135038376 \n",
      "acc for Psat= 0.11548256907198165 \n",
      "acc for optim= 0.1536626415948073\n",
      "Epoch:362/1000\n",
      "Loss on train= 0.01253539603203535\n",
      "Loss on test= 0.018612587824463844\n",
      "acc for Lsat= 0.06807304422060648 \n",
      "acc for Psat= 0.10556650128629472 \n",
      "acc for optim= 0.15336482243405444\n",
      "Epoch:363/1000\n",
      "Loss on train= 0.011960954405367374\n",
      "Loss on test= 0.01856091618537903\n",
      "acc for Lsat= 0.06622457669840918 \n",
      "acc for Psat= 0.10817059146033392 \n",
      "acc for optim= 0.15132191131512326\n",
      "Epoch:364/1000\n",
      "Loss on train= 0.011781095527112484\n",
      "Loss on test= 0.018107635900378227\n",
      "acc for Lsat= 0.0678383150862323 \n",
      "acc for Psat= 0.11193659239345127 \n",
      "acc for optim= 0.14858266943030884\n",
      "Epoch:365/1000\n",
      "Loss on train= 0.012412909418344498\n",
      "Loss on test= 0.018137626349925995\n",
      "acc for Lsat= 0.0690285835829046 \n",
      "acc for Psat= 0.10342165595955319 \n",
      "acc for optim= 0.1547409331632985\n",
      "Epoch:366/1000\n",
      "Loss on train= 0.012209444306790829\n",
      "Loss on test= 0.01915288344025612\n",
      "acc for Lsat= 0.06332253283924527 \n",
      "acc for Psat= 0.09880470666620468 \n",
      "acc for optim= 0.1547049943771627\n",
      "Epoch:367/1000\n",
      "Loss on train= 0.011714672669768333\n",
      "Loss on test= 0.019525151699781418\n",
      "acc for Lsat= 0.07444997140102917 \n",
      "acc for Psat= 0.11446337434980608 \n",
      "acc for optim= 0.16024139225482936\n",
      "Epoch:368/1000\n",
      "Loss on train= 0.012055251747369766\n",
      "Loss on test= 0.017787991091609\n",
      "acc for Lsat= 0.06373575073149468 \n",
      "acc for Psat= 0.10039817558394538 \n",
      "acc for optim= 0.15991168965895972\n",
      "Epoch:369/1000\n",
      "Loss on train= 0.011988370679318905\n",
      "Loss on test= 0.01888311468064785\n",
      "acc for Lsat= 0.06983264593614472 \n",
      "acc for Psat= 0.10653924412197537 \n",
      "acc for optim= 0.159292029009925\n",
      "Epoch:370/1000\n",
      "Loss on train= 0.011841585859656334\n",
      "Loss on test= 0.018122995272278786\n",
      "acc for Lsat= 0.06870514154434203 \n",
      "acc for Psat= 0.10372565521134271 \n",
      "acc for optim= 0.1530555038816399\n",
      "Epoch:371/1000\n",
      "Loss on train= 0.011435785330832005\n",
      "Loss on test= 0.018999598920345306\n",
      "acc for Lsat= 0.07252372718519634 \n",
      "acc for Psat= 0.1092253824075063 \n",
      "acc for optim= 0.1578466248181131\n",
      "Epoch:372/1000\n",
      "Loss on train= 0.011631360277533531\n",
      "Loss on test= 0.01872461475431919\n",
      "acc for Lsat= 0.060312146113978476 \n",
      "acc for Psat= 0.10046962896982829 \n",
      "acc for optim= 0.15481026710735427\n",
      "Epoch:373/1000\n",
      "Loss on train= 0.011468708515167236\n",
      "Loss on test= 0.018583057448267937\n",
      "acc for Lsat= 0.06589557205637295 \n",
      "acc for Psat= 0.10187209977044001 \n",
      "acc for optim= 0.15444296316968073\n",
      "Epoch:374/1000\n",
      "Loss on train= 0.01161989290267229\n",
      "Loss on test= 0.019371267408132553\n",
      "acc for Lsat= 0.07767877644962734 \n",
      "acc for Psat= 0.11780956188837689 \n",
      "acc for optim= 0.15143169917994076\n",
      "Epoch:375/1000\n",
      "Loss on train= 0.011253147386014462\n",
      "Loss on test= 0.01873578503727913\n",
      "acc for Lsat= 0.06841173809435634 \n",
      "acc for Psat= 0.10003617803255718 \n",
      "acc for optim= 0.15920146306355798\n",
      "Epoch:376/1000\n",
      "Loss on train= 0.011565105058252811\n",
      "Loss on test= 0.019289158284664154\n",
      "acc for Lsat= 0.06872869713438882 \n",
      "acc for Psat= 0.10767381091912588 \n",
      "acc for optim= 0.15459773192803067\n",
      "Epoch:377/1000\n",
      "Loss on train= 0.01139933429658413\n",
      "Loss on test= 0.01804165542125702\n",
      "acc for Lsat= 0.07233280109034644 \n",
      "acc for Psat= 0.10870615376366509 \n",
      "acc for optim= 0.15494366553094652\n",
      "Epoch:378/1000\n",
      "Loss on train= 0.011262462474405766\n",
      "Loss on test= 0.018517442047595978\n",
      "acc for Lsat= 0.06500531691643927 \n",
      "acc for Psat= 0.10447395576371087 \n",
      "acc for optim= 0.1535760857992702\n",
      "Epoch:379/1000\n",
      "Loss on train= 0.01213010773062706\n",
      "Loss on test= 0.01893104799091816\n",
      "acc for Lsat= 0.0711520665221744 \n",
      "acc for Psat= 0.0983655552069346 \n",
      "acc for optim= 0.1537223305139277\n",
      "Epoch:380/1000\n",
      "Loss on train= 0.01154429093003273\n",
      "Loss on test= 0.017831899225711823\n",
      "acc for Lsat= 0.0721823742820157 \n",
      "acc for Psat= 0.10351189805401696 \n",
      "acc for optim= 0.15446458293331997\n",
      "Epoch:381/1000\n",
      "Loss on train= 0.0114593505859375\n",
      "Loss on test= 0.018499718979001045\n",
      "acc for Lsat= 0.0800243036614524 \n",
      "acc for Psat= 0.11245994799666935 \n",
      "acc for optim= 0.155343623045418\n",
      "Epoch:382/1000\n",
      "Loss on train= 0.011424987576901913\n",
      "Loss on test= 0.018791217356920242\n",
      "acc for Lsat= 0.072206829322709 \n",
      "acc for Psat= 0.10649530622694227 \n",
      "acc for optim= 0.1537595341602961\n",
      "Epoch:383/1000\n",
      "Loss on train= 0.011734317988157272\n",
      "Loss on test= 0.018631495535373688\n",
      "acc for Lsat= 0.06522414634625118 \n",
      "acc for Psat= 0.10496652722358701 \n",
      "acc for optim= 0.15354382453693283\n",
      "Epoch:384/1000\n",
      "Loss on train= 0.011863910593092442\n",
      "Loss on test= 0.01791856251657009\n",
      "acc for Lsat= 0.07454883274104859 \n",
      "acc for Psat= 0.11474933359358046 \n",
      "acc for optim= 0.15287969542874227\n",
      "Epoch:385/1000\n",
      "Loss on train= 0.011259757913649082\n",
      "Loss on test= 0.0192398801445961\n",
      "acc for Lsat= 0.08274754898415672 \n",
      "acc for Psat= 0.1198452310429679 \n",
      "acc for optim= 0.15943097306622397\n",
      "Epoch:386/1000\n",
      "Loss on train= 0.011736128479242325\n",
      "Loss on test= 0.018294498324394226\n",
      "acc for Lsat= 0.0701411344938808 \n",
      "acc for Psat= 0.10704748928546906 \n",
      "acc for optim= 0.1539822722474734\n",
      "Epoch:387/1000\n",
      "Loss on train= 0.011357050389051437\n",
      "Loss on test= 0.019896356388926506\n",
      "acc for Lsat= 0.06999083128240374 \n",
      "acc for Psat= 0.10493016309208339 \n",
      "acc for optim= 0.15797105795807306\n",
      "Epoch:388/1000\n",
      "Loss on train= 0.011332831345498562\n",
      "Loss on test= 0.018268978223204613\n",
      "acc for Lsat= 0.07340222630235883 \n",
      "acc for Psat= 0.10943428542878894 \n",
      "acc for optim= 0.15969532330830893\n",
      "Epoch:389/1000\n",
      "Loss on train= 0.011871314607560635\n",
      "Loss on test= 0.019429853186011314\n",
      "acc for Lsat= 0.0869132227367825 \n",
      "acc for Psat= 0.13561573856406742 \n",
      "acc for optim= 0.15375455187426673\n",
      "Epoch:390/1000\n",
      "Loss on train= 0.011751250363886356\n",
      "Loss on test= 0.01898542046546936\n",
      "acc for Lsat= 0.07846923155917063 \n",
      "acc for Psat= 0.12271916965643566 \n",
      "acc for optim= 0.15183511260482996\n",
      "Epoch:391/1000\n",
      "Loss on train= 0.011350158601999283\n",
      "Loss on test= 0.018269814550876617\n",
      "acc for Lsat= 0.08399948014153374 \n",
      "acc for Psat= 0.12837847570578256 \n",
      "acc for optim= 0.1601743921637535\n",
      "Epoch:392/1000\n",
      "Loss on train= 0.011923250742256641\n",
      "Loss on test= 0.01879652589559555\n",
      "acc for Lsat= 0.08341341697507434 \n",
      "acc for Psat= 0.1069163633717431 \n",
      "acc for optim= 0.15783379707071515\n",
      "Epoch:393/1000\n",
      "Loss on train= 0.011567450128495693\n",
      "Loss on test= 0.01854681596159935\n",
      "acc for Lsat= 0.061779995924896663 \n",
      "acc for Psat= 0.11934416260984208 \n",
      "acc for optim= 0.15451577934953903\n",
      "Epoch:394/1000\n",
      "Loss on train= 0.01147636491805315\n",
      "Loss on test= 0.017879880964756012\n",
      "acc for Lsat= 0.06846855299340354 \n",
      "acc for Psat= 0.10823914508024851 \n",
      "acc for optim= 0.15437935317556067\n",
      "Epoch:395/1000\n",
      "Loss on train= 0.011441604234278202\n",
      "Loss on test= 0.01889798231422901\n",
      "acc for Lsat= 0.06999014773302609 \n",
      "acc for Psat= 0.1196315258741379 \n",
      "acc for optim= 0.15397063311603337\n",
      "Epoch:396/1000\n",
      "Loss on train= 0.011208844371140003\n",
      "Loss on test= 0.01817450486123562\n",
      "acc for Lsat= 0.06029757671058178 \n",
      "acc for Psat= 0.09882603453265297 \n",
      "acc for optim= 0.15346964846054711\n",
      "Epoch:397/1000\n",
      "Loss on train= 0.011103407479822636\n",
      "Loss on test= 0.018000109121203423\n",
      "acc for Lsat= 0.06551577796538671 \n",
      "acc for Psat= 0.10138620138168336 \n",
      "acc for optim= 0.15682981519235506\n",
      "Epoch:398/1000\n",
      "Loss on train= 0.011179860681295395\n",
      "Loss on test= 0.018978694453835487\n",
      "acc for Lsat= 0.07522084961334866 \n",
      "acc for Psat= 0.11040626333819496 \n",
      "acc for optim= 0.1569241027037303\n",
      "Epoch:399/1000\n",
      "Loss on train= 0.01149033010005951\n",
      "Loss on test= 0.019142702221870422\n",
      "acc for Lsat= 0.07079429932766491 \n",
      "acc for Psat= 0.10906360083156161 \n",
      "acc for optim= 0.15409738322099054\n",
      "Epoch:400/1000\n",
      "Loss on train= 0.011516138911247253\n",
      "Loss on test= 0.01858282834291458\n",
      "acc for Lsat= 0.07551050302055146 \n",
      "acc for Psat= 0.11622701154814824 \n",
      "acc for optim= 0.15255015757348803\n",
      "Epoch:401/1000\n",
      "Loss on train= 0.01174863800406456\n",
      "Loss on test= 0.01811044290661812\n",
      "acc for Lsat= 0.06373788267374038 \n",
      "acc for Psat= 0.11114929682678645 \n",
      "acc for optim= 0.16045775777763788\n",
      "Epoch:402/1000\n",
      "Loss on train= 0.011531060561537743\n",
      "Loss on test= 0.01855551265180111\n",
      "acc for Lsat= 0.07209236754311456 \n",
      "acc for Psat= 0.10496128565735287 \n",
      "acc for optim= 0.15477616555160945\n",
      "Epoch:403/1000\n",
      "Loss on train= 0.011634448543190956\n",
      "Loss on test= 0.018051382154226303\n",
      "acc for Lsat= 0.07121187465058433 \n",
      "acc for Psat= 0.10765950414869521 \n",
      "acc for optim= 0.1556864127516747\n",
      "Epoch:404/1000\n",
      "Loss on train= 0.011195354163646698\n",
      "Loss on test= 0.018164368346333504\n",
      "acc for Lsat= 0.06263394438558155 \n",
      "acc for Psat= 0.10483935144212511 \n",
      "acc for optim= 0.1537657979461882\n",
      "Epoch:405/1000\n",
      "Loss on train= 0.011061141267418861\n",
      "Loss on test= 0.019379518926143646\n",
      "acc for Lsat= 0.07074126270082261 \n",
      "acc for Psat= 0.1119379778703054 \n",
      "acc for optim= 0.15629658682478798\n",
      "Epoch:406/1000\n",
      "Loss on train= 0.011170579120516777\n",
      "Loss on test= 0.01723085157573223\n",
      "acc for Lsat= 0.0707814794447687 \n",
      "acc for Psat= 0.10166918800936804 \n",
      "acc for optim= 0.15517900933821996\n",
      "Epoch:407/1000\n",
      "Loss on train= 0.010863319039344788\n",
      "Loss on test= 0.019012074917554855\n",
      "acc for Lsat= 0.07293938100337981 \n",
      "acc for Psat= 0.1202470209863451 \n",
      "acc for optim= 0.1522305727005005\n",
      "Epoch:408/1000\n",
      "Loss on train= 0.011086737737059593\n",
      "Loss on test= 0.018672065809369087\n",
      "acc for Lsat= 0.06113511365320948 \n",
      "acc for Psat= 0.10020382603009541 \n",
      "acc for optim= 0.15270078877607982\n",
      "Epoch:409/1000\n",
      "Loss on train= 0.011580049991607666\n",
      "Loss on test= 0.019094401970505714\n",
      "acc for Lsat= 0.08718919058640798 \n",
      "acc for Psat= 0.11490339471234215 \n",
      "acc for optim= 0.15104826556311712\n",
      "Epoch:410/1000\n",
      "Loss on train= 0.011228671297430992\n",
      "Loss on test= 0.018711838871240616\n",
      "acc for Lsat= 0.06176376193761826 \n",
      "acc for Psat= 0.10542038049962785 \n",
      "acc for optim= 0.15736780944797726\n",
      "Epoch:411/1000\n",
      "Loss on train= 0.01098008081316948\n",
      "Loss on test= 0.01776084117591381\n",
      "acc for Lsat= 0.0686449503733052 \n",
      "acc for Psat= 0.10607332222991521 \n",
      "acc for optim= 0.15160276103350856\n",
      "Epoch:412/1000\n",
      "Loss on train= 0.01135392114520073\n",
      "Loss on test= 0.018572881817817688\n",
      "acc for Lsat= 0.0754912734031677 \n",
      "acc for Psat= 0.10464581615395015 \n",
      "acc for optim= 0.15302820520268545\n",
      "Epoch:413/1000\n",
      "Loss on train= 0.011254631914198399\n",
      "Loss on test= 0.017888391390442848\n",
      "acc for Lsat= 0.06813179378708203 \n",
      "acc for Psat= 0.10456362929609087 \n",
      "acc for optim= 0.1530890839795271\n",
      "Epoch:414/1000\n",
      "Loss on train= 0.011299088597297668\n",
      "Loss on test= 0.018467001616954803\n",
      "acc for Lsat= 0.07784814420673582 \n",
      "acc for Psat= 0.10265092816617756 \n",
      "acc for optim= 0.15201129722926351\n",
      "Epoch:415/1000\n",
      "Loss on train= 0.011022827588021755\n",
      "Loss on test= 0.019092639908194542\n",
      "acc for Lsat= 0.06278968610697322 \n",
      "acc for Psat= 0.10398106111420527 \n",
      "acc for optim= 0.15654959628979365\n",
      "Epoch:416/1000\n",
      "Loss on train= 0.011256822384893894\n",
      "Loss on test= 0.018486546352505684\n",
      "acc for Lsat= 0.07173702592651049 \n",
      "acc for Psat= 0.10437736643685233 \n",
      "acc for optim= 0.15394370870457752\n",
      "Epoch:417/1000\n",
      "Loss on train= 0.011370470747351646\n",
      "Loss on test= 0.019650761038064957\n",
      "acc for Lsat= 0.06588753486673038 \n",
      "acc for Psat= 0.10221199658181933 \n",
      "acc for optim= 0.15428489446640012\n",
      "Epoch:418/1000\n",
      "Loss on train= 0.010994591750204563\n",
      "Loss on test= 0.017706528306007385\n",
      "acc for Lsat= 0.06258851347698105 \n",
      "acc for Psat= 0.10649632546636795 \n",
      "acc for optim= 0.15468312733703188\n",
      "Epoch:419/1000\n",
      "Loss on train= 0.012230958789587021\n",
      "Loss on test= 0.01824832335114479\n",
      "acc for Lsat= 0.078659302327368 \n",
      "acc for Psat= 0.1062411132786009 \n",
      "acc for optim= 0.151192440589269\n",
      "Epoch:420/1000\n",
      "Loss on train= 0.01150443870574236\n",
      "Loss on test= 0.01836058683693409\n",
      "acc for Lsat= 0.06261686268779967 \n",
      "acc for Psat= 0.10230176945527397 \n",
      "acc for optim= 0.1540186756187015\n",
      "Epoch:421/1000\n",
      "Loss on train= 0.011059782467782497\n",
      "Loss on test= 0.01737065427005291\n",
      "acc for Lsat= 0.0642953500151634 \n",
      "acc for Psat= 0.10288616120815278 \n",
      "acc for optim= 0.15230619129207398\n",
      "Epoch:422/1000\n",
      "Loss on train= 0.011196883395314217\n",
      "Loss on test= 0.01890871487557888\n",
      "acc for Lsat= 0.0707203519013193 \n",
      "acc for Psat= 0.10470173789395226 \n",
      "acc for optim= 0.1513747400707669\n",
      "Epoch:423/1000\n",
      "Loss on train= 0.011226198635995388\n",
      "Loss on test= 0.018136121332645416\n",
      "acc for Lsat= 0.07224371375309097 \n",
      "acc for Psat= 0.10030173858006797 \n",
      "acc for optim= 0.15240043939815628\n",
      "Epoch:424/1000\n",
      "Loss on train= 0.011192455887794495\n",
      "Loss on test= 0.01851256750524044\n",
      "acc for Lsat= 0.06868953853845595 \n",
      "acc for Psat= 0.10260552797052595 \n",
      "acc for optim= 0.15863412585523395\n",
      "Epoch:425/1000\n",
      "Loss on train= 0.011110180988907814\n",
      "Loss on test= 0.018372032791376114\n",
      "acc for Lsat= 0.06740330482522645 \n",
      "acc for Psat= 0.10040444201893277 \n",
      "acc for optim= 0.15991005798180893\n",
      "Epoch:426/1000\n",
      "Loss on train= 0.01097178179770708\n",
      "Loss on test= 0.019052211195230484\n",
      "acc for Lsat= 0.08368594878249698 \n",
      "acc for Psat= 0.10067381858825682 \n",
      "acc for optim= 0.15690926959117255\n",
      "Epoch:427/1000\n",
      "Loss on train= 0.010973118245601654\n",
      "Loss on test= 0.017246177420020103\n",
      "acc for Lsat= 0.06430861817465888 \n",
      "acc for Psat= 0.10168954332669576 \n",
      "acc for optim= 0.15582021673520405\n",
      "Epoch:428/1000\n",
      "Loss on train= 0.010818484239280224\n",
      "Loss on test= 0.01917383447289467\n",
      "acc for Lsat= 0.06499683881799381 \n",
      "acc for Psat= 0.10338467591338688 \n",
      "acc for optim= 0.15049241673615243\n",
      "Epoch:429/1000\n",
      "Loss on train= 0.011341269128024578\n",
      "Loss on test= 0.01945943385362625\n",
      "acc for Lsat= 0.075475579003493 \n",
      "acc for Psat= 0.11414313515027362 \n",
      "acc for optim= 0.15205423956116043\n",
      "Epoch:430/1000\n",
      "Loss on train= 0.011482403613626957\n",
      "Loss on test= 0.018987752497196198\n",
      "acc for Lsat= 0.0799069184396002 \n",
      "acc for Psat= 0.10655566553274792 \n",
      "acc for optim= 0.15341646605067782\n",
      "Epoch:431/1000\n",
      "Loss on train= 0.010941495187580585\n",
      "Loss on test= 0.018652047961950302\n",
      "acc for Lsat= 0.06953166822592417 \n",
      "acc for Psat= 0.10416025122006735 \n",
      "acc for optim= 0.15117730655603936\n",
      "Epoch:432/1000\n",
      "Loss on train= 0.011057835072278976\n",
      "Loss on test= 0.019273780286312103\n",
      "acc for Lsat= 0.06959802011648815 \n",
      "acc for Psat= 0.10590028001202477 \n",
      "acc for optim= 0.15914556632439297\n",
      "Epoch:433/1000\n",
      "Loss on train= 0.011171563528478146\n",
      "Loss on test= 0.017502227798104286\n",
      "acc for Lsat= 0.07111223025454415 \n",
      "acc for Psat= 0.10169321133030788 \n",
      "acc for optim= 0.15317831867270998\n",
      "Epoch:434/1000\n",
      "Loss on train= 0.011043956503272057\n",
      "Loss on test= 0.017633818089962006\n",
      "acc for Lsat= 0.06845844818486108 \n",
      "acc for Psat= 0.11244702604081895 \n",
      "acc for optim= 0.15432841165198222\n",
      "Epoch:435/1000\n",
      "Loss on train= 0.010964491404592991\n",
      "Loss on test= 0.018725620582699776\n",
      "acc for Lsat= 0.06578677627775405 \n",
      "acc for Psat= 0.10392796794573467 \n",
      "acc for optim= 0.15526062299807866\n",
      "Epoch:436/1000\n",
      "Loss on train= 0.010966205038130283\n",
      "Loss on test= 0.017805563285946846\n",
      "acc for Lsat= 0.06876759446329542 \n",
      "acc for Psat= 0.10409590701262156 \n",
      "acc for optim= 0.1531057937277688\n",
      "Epoch:437/1000\n",
      "Loss on train= 0.010904494673013687\n",
      "Loss on test= 0.01836295798420906\n",
      "acc for Lsat= 0.061717164516448965 \n",
      "acc for Psat= 0.10201273494296605 \n",
      "acc for optim= 0.15303893809517224\n",
      "Epoch:438/1000\n",
      "Loss on train= 0.01095836516469717\n",
      "Loss on test= 0.01872720755636692\n",
      "acc for Lsat= 0.05965432119038369 \n",
      "acc for Psat= 0.105568109287156 \n",
      "acc for optim= 0.15454101694954767\n",
      "Epoch:439/1000\n",
      "Loss on train= 0.010721837170422077\n",
      "Loss on test= 0.019368715584278107\n",
      "acc for Lsat= 0.07507899138662551 \n",
      "acc for Psat= 0.119981101486418 \n",
      "acc for optim= 0.1579352551036411\n",
      "Epoch:440/1000\n",
      "Loss on train= 0.01073523797094822\n",
      "Loss on test= 0.018497340381145477\n",
      "acc for Lsat= 0.06427591020862261 \n",
      "acc for Psat= 0.10010868509610493 \n",
      "acc for optim= 0.1562308594584465\n",
      "Epoch:441/1000\n",
      "Loss on train= 0.01099170371890068\n",
      "Loss on test= 0.0181405209004879\n",
      "acc for Lsat= 0.07040088027715684 \n",
      "acc for Psat= 0.10009769300619761 \n",
      "acc for optim= 0.15219661800397766\n",
      "Epoch:442/1000\n",
      "Loss on train= 0.010617600753903389\n",
      "Loss on test= 0.018685467541217804\n",
      "acc for Lsat= 0.07031054695447286 \n",
      "acc for Psat= 0.10259129338794284 \n",
      "acc for optim= 0.1522436524430911\n",
      "Epoch:443/1000\n",
      "Loss on train= 0.010646612383425236\n",
      "Loss on test= 0.01739497296512127\n",
      "acc for Lsat= 0.07401993721723557 \n",
      "acc for Psat= 0.10616121821933322 \n",
      "acc for optim= 0.15178706414169738\n",
      "Epoch:444/1000\n",
      "Loss on train= 0.010658600367605686\n",
      "Loss on test= 0.01779463142156601\n",
      "acc for Lsat= 0.06296128920382923 \n",
      "acc for Psat= 0.10117304689354366 \n",
      "acc for optim= 0.15032836265034144\n",
      "Epoch:445/1000\n",
      "Loss on train= 0.011293982155621052\n",
      "Loss on test= 0.017221521586179733\n",
      "acc for Lsat= 0.06581374845570988 \n",
      "acc for Psat= 0.1029303037457996 \n",
      "acc for optim= 0.15092918963895902\n",
      "Epoch:446/1000\n",
      "Loss on train= 0.01067154761403799\n",
      "Loss on test= 0.01803646981716156\n",
      "acc for Lsat= 0.06257597183187803 \n",
      "acc for Psat= 0.10360965530077618 \n",
      "acc for optim= 0.1531363655295637\n",
      "Epoch:447/1000\n",
      "Loss on train= 0.011015895754098892\n",
      "Loss on test= 0.017971672117710114\n",
      "acc for Lsat= 0.06904483445816571 \n",
      "acc for Psat= 0.10629753470420837 \n",
      "acc for optim= 0.15451418914728693\n",
      "Epoch:448/1000\n",
      "Loss on train= 0.010677927173674107\n",
      "Loss on test= 0.017943406477570534\n",
      "acc for Lsat= 0.06310933464103276 \n",
      "acc for Psat= 0.10496741599506802 \n",
      "acc for optim= 0.15271764637695417\n",
      "Epoch:449/1000\n",
      "Loss on train= 0.010934277437627316\n",
      "Loss on test= 0.017334097996354103\n",
      "acc for Lsat= 0.07018703818321229 \n",
      "acc for Psat= 0.103384025560485 \n",
      "acc for optim= 0.1559166167345312\n",
      "Epoch:450/1000\n",
      "Loss on train= 0.010407393798232079\n",
      "Loss on test= 0.019666539505124092\n",
      "acc for Lsat= 0.07343938963280783 \n",
      "acc for Psat= 0.10902999705738492 \n",
      "acc for optim= 0.15200885948207643\n",
      "Epoch:451/1000\n",
      "Loss on train= 0.010551423765718937\n",
      "Loss on test= 0.018398797139525414\n",
      "acc for Lsat= 0.06806583909524812 \n",
      "acc for Psat= 0.109922817018297 \n",
      "acc for optim= 0.15021656023131477\n",
      "Epoch:452/1000\n",
      "Loss on train= 0.010655930265784264\n",
      "Loss on test= 0.019370174035429955\n",
      "acc for Lsat= 0.08689079085985821 \n",
      "acc for Psat= 0.10948924786514708 \n",
      "acc for optim= 0.1531853251159191\n",
      "Epoch:453/1000\n",
      "Loss on train= 0.010480289347469807\n",
      "Loss on test= 0.0192515030503273\n",
      "acc for Lsat= 0.06793913659122255 \n",
      "acc for Psat= 0.10023445785045623 \n",
      "acc for optim= 0.1555133004983266\n",
      "Epoch:454/1000\n",
      "Loss on train= 0.010674660094082355\n",
      "Loss on test= 0.017542416229844093\n",
      "acc for Lsat= 0.06603614538908004 \n",
      "acc for Psat= 0.10250735448466408 \n",
      "acc for optim= 0.1518566891551018\n",
      "Epoch:455/1000\n",
      "Loss on train= 0.010685465298593044\n",
      "Loss on test= 0.018358387053012848\n",
      "acc for Lsat= 0.06017605165640513 \n",
      "acc for Psat= 0.09902045958571964 \n",
      "acc for optim= 0.15175792649388317\n",
      "Epoch:456/1000\n",
      "Loss on train= 0.010591457597911358\n",
      "Loss on test= 0.01809616945683956\n",
      "acc for Lsat= 0.06836144005258879 \n",
      "acc for Psat= 0.10861353609297011 \n",
      "acc for optim= 0.15131181743409897\n",
      "Epoch:457/1000\n",
      "Loss on train= 0.010592443868517876\n",
      "Loss on test= 0.017530810087919235\n",
      "acc for Lsat= 0.07139717770947351 \n",
      "acc for Psat= 0.10544859535164303 \n",
      "acc for optim= 0.15292840335104205\n",
      "Epoch:458/1000\n",
      "Loss on train= 0.010757120326161385\n",
      "Loss on test= 0.01883106492459774\n",
      "acc for Lsat= 0.06839050004879633 \n",
      "acc for Psat= 0.1039667033486896 \n",
      "acc for optim= 0.15193109114964803\n",
      "Epoch:459/1000\n",
      "Loss on train= 0.01063010934740305\n",
      "Loss on test= 0.01853068359196186\n",
      "acc for Lsat= 0.07881687548425463 \n",
      "acc for Psat= 0.10058349304729039 \n",
      "acc for optim= 0.15164057082600063\n",
      "Epoch:460/1000\n",
      "Loss on train= 0.010613514110445976\n",
      "Loss on test= 0.01797831431031227\n",
      "acc for Lsat= 0.06953640075193511 \n",
      "acc for Psat= 0.10848768883281285 \n",
      "acc for optim= 0.15323645174503328\n",
      "Epoch:461/1000\n",
      "Loss on train= 0.010520177893340588\n",
      "Loss on test= 0.019142644479870796\n",
      "acc for Lsat= 0.06317272732655208 \n",
      "acc for Psat= 0.10379852652549744 \n",
      "acc for optim= 0.15184558745887544\n",
      "Epoch:462/1000\n",
      "Loss on train= 0.010845133103430271\n",
      "Loss on test= 0.018295157700777054\n",
      "acc for Lsat= 0.06712197247478696 \n",
      "acc for Psat= 0.10554836955335406 \n",
      "acc for optim= 0.15515079051256178\n",
      "Epoch:463/1000\n",
      "Loss on train= 0.010589038021862507\n",
      "Loss on test= 0.018405886366963387\n",
      "acc for Lsat= 0.0761144071817398 \n",
      "acc for Psat= 0.10994588136672973 \n",
      "acc for optim= 0.1534597744544347\n",
      "Epoch:464/1000\n",
      "Loss on train= 0.010524366982281208\n",
      "Loss on test= 0.018793174996972084\n",
      "acc for Lsat= 0.07828357194860776 \n",
      "acc for Psat= 0.1014808734258016 \n",
      "acc for optim= 0.1513120430211226\n",
      "Epoch:465/1000\n",
      "Loss on train= 0.010461575351655483\n",
      "Loss on test= 0.017821025103330612\n",
      "acc for Lsat= 0.06804161502255333 \n",
      "acc for Psat= 0.09774211578898959 \n",
      "acc for optim= 0.15384604119592246\n",
      "Epoch:466/1000\n",
      "Loss on train= 0.01063571311533451\n",
      "Loss on test= 0.018874499946832657\n",
      "acc for Lsat= 0.0669495435224639 \n",
      "acc for Psat= 0.1037199705839157 \n",
      "acc for optim= 0.15253856637411645\n",
      "Epoch:467/1000\n",
      "Loss on train= 0.01093963161110878\n",
      "Loss on test= 0.01871846616268158\n",
      "acc for Lsat= 0.0691297953327497 \n",
      "acc for Psat= 0.1055411527554194 \n",
      "acc for optim= 0.15711620698372522\n",
      "Epoch:468/1000\n",
      "Loss on train= 0.010529030114412308\n",
      "Loss on test= 0.01867307722568512\n",
      "acc for Lsat= 0.06988283064630296 \n",
      "acc for Psat= 0.10515330301390753 \n",
      "acc for optim= 0.15255697874559296\n",
      "Epoch:469/1000\n",
      "Loss on train= 0.010501309297978878\n",
      "Loss on test= 0.018855445086956024\n",
      "acc for Lsat= 0.05996700169311629 \n",
      "acc for Psat= 0.1014871918492847 \n",
      "acc for optim= 0.15635770277844535\n",
      "Epoch:470/1000\n",
      "Loss on train= 0.010473726317286491\n",
      "Loss on test= 0.017677081748843193\n",
      "acc for Lsat= 0.06441615861323144 \n",
      "acc for Psat= 0.09874783754348755 \n",
      "acc for optim= 0.15203681737184524\n",
      "Epoch:471/1000\n",
      "Loss on train= 0.010383803397417068\n",
      "Loss on test= 0.019050369039177895\n",
      "acc for Lsat= 0.07531151870886485 \n",
      "acc for Psat= 0.10743516782919565 \n",
      "acc for optim= 0.15171560438142884\n",
      "Epoch:472/1000\n",
      "Loss on train= 0.010690943337976933\n",
      "Loss on test= 0.018955906853079796\n",
      "acc for Lsat= 0.06784172985288833 \n",
      "acc for Psat= 0.10971741312079959 \n",
      "acc for optim= 0.15608539597855672\n",
      "Epoch:473/1000\n",
      "Loss on train= 0.01072489283978939\n",
      "Loss on test= 0.01943996176123619\n",
      "acc for Lsat= 0.07468390464782716 \n",
      "acc for Psat= 0.10136497616767882 \n",
      "acc for optim= 0.1552772852281729\n",
      "Epoch:474/1000\n",
      "Loss on train= 0.010435100644826889\n",
      "Loss on test= 0.018072254955768585\n",
      "acc for Lsat= 0.06954604726698664 \n",
      "acc for Psat= 0.11616519225968253 \n",
      "acc for optim= 0.15486161882678667\n",
      "Epoch:475/1000\n",
      "Loss on train= 0.010605359449982643\n",
      "Loss on test= 0.019371066242456436\n",
      "acc for Lsat= 0.063571155236827 \n",
      "acc for Psat= 0.1018960217634837 \n",
      "acc for optim= 0.15328559743033515\n",
      "Epoch:476/1000\n",
      "Loss on train= 0.010424826294183731\n",
      "Loss on test= 0.01845800131559372\n",
      "acc for Lsat= 0.07020962768130831 \n",
      "acc for Psat= 0.11660023894574906 \n",
      "acc for optim= 0.15351181742217807\n",
      "Epoch:477/1000\n",
      "Loss on train= 0.0107816681265831\n",
      "Loss on test= 0.01986849121749401\n",
      "acc for Lsat= 0.05993130703767142 \n",
      "acc for Psat= 0.09941680994298724 \n",
      "acc for optim= 0.15762215753396353\n",
      "Epoch:478/1000\n",
      "Loss on train= 0.010300304740667343\n",
      "Loss on test= 0.0186565313488245\n",
      "acc for Lsat= 0.0689241495397356 \n",
      "acc for Psat= 0.10076117018858594 \n",
      "acc for optim= 0.15302933744258349\n",
      "Epoch:479/1000\n",
      "Loss on train= 0.010228804312646389\n",
      "Loss on test= 0.01868162304162979\n",
      "acc for Lsat= 0.06691041853692797 \n",
      "acc for Psat= 0.10617474118868511 \n",
      "acc for optim= 0.15049691920479139\n",
      "Epoch:480/1000\n",
      "Loss on train= 0.010532081127166748\n",
      "Loss on test= 0.019103609025478363\n",
      "acc for Lsat= 0.0692853248781628 \n",
      "acc for Psat= 0.10206275052494473 \n",
      "acc for optim= 0.15508630358510547\n",
      "Epoch:481/1000\n",
      "Loss on train= 0.010426574386656284\n",
      "Loss on test= 0.01825648732483387\n",
      "acc for Lsat= 0.07665285434987809 \n",
      "acc for Psat= 0.1078751434882482 \n",
      "acc for optim= 0.15391826894548205\n",
      "Epoch:482/1000\n",
      "Loss on train= 0.010272637009620667\n",
      "Loss on test= 0.0186997652053833\n",
      "acc for Lsat= 0.0719076567225986 \n",
      "acc for Psat= 0.10784642663266923 \n",
      "acc for optim= 0.15916578736570147\n",
      "Epoch:483/1000\n",
      "Loss on train= 0.010711874812841415\n",
      "Loss on test= 0.01904550939798355\n",
      "acc for Lsat= 0.0772275244196256 \n",
      "acc for Psat= 0.11103048125902812 \n",
      "acc for optim= 0.15908766322665743\n",
      "Epoch:484/1000\n",
      "Loss on train= 0.010630127973854542\n",
      "Loss on test= 0.01833598129451275\n",
      "acc for Lsat= 0.06323820534679624 \n",
      "acc for Psat= 0.10832088225417667 \n",
      "acc for optim= 0.15445663332939147\n",
      "Epoch:485/1000\n",
      "Loss on train= 0.010557714849710464\n",
      "Loss on test= 0.018290439620614052\n",
      "acc for Lsat= 0.06029579614599545 \n",
      "acc for Psat= 0.09703903562492794 \n",
      "acc for optim= 0.159123886624972\n",
      "Epoch:486/1000\n",
      "Loss on train= 0.01043915469199419\n",
      "Loss on test= 0.017608754336833954\n",
      "acc for Lsat= 0.07523074423273406 \n",
      "acc for Psat= 0.1032507853375541 \n",
      "acc for optim= 0.15196654755208228\n",
      "Epoch:487/1000\n",
      "Loss on train= 0.010382630862295628\n",
      "Loss on test= 0.01881117932498455\n",
      "acc for Lsat= 0.06664057804478539 \n",
      "acc for Psat= 0.0955842736694548 \n",
      "acc for optim= 0.15801576574643456\n",
      "Epoch:488/1000\n",
      "Loss on train= 0.010228480212390423\n",
      "Loss on test= 0.01811795122921467\n",
      "acc for Lsat= 0.06062019608087009 \n",
      "acc for Psat= 0.09934719734721714 \n",
      "acc for optim= 0.15268304811583625\n",
      "Epoch:489/1000\n",
      "Loss on train= 0.010358796454966068\n",
      "Loss on test= 0.01898922771215439\n",
      "acc for Lsat= 0.07341213921705883 \n",
      "acc for Psat= 0.10149142907725439 \n",
      "acc for optim= 0.15407903550399676\n",
      "Epoch:490/1000\n",
      "Loss on train= 0.010537266731262207\n",
      "Loss on test= 0.01930287852883339\n",
      "acc for Lsat= 0.06587328165769576 \n",
      "acc for Psat= 0.10976440310478211 \n",
      "acc for optim= 0.15389926706751186\n",
      "Epoch:491/1000\n",
      "Loss on train= 0.009974448941648006\n",
      "Loss on test= 0.017683768644928932\n",
      "acc for Lsat= 0.06036376274294324 \n",
      "acc for Psat= 0.1008412520090739 \n",
      "acc for optim= 0.15200071160991988\n",
      "Epoch:492/1000\n",
      "Loss on train= 0.010060761123895645\n",
      "Loss on test= 0.01892135478556156\n",
      "acc for Lsat= 0.06356840866307417 \n",
      "acc for Psat= 0.09609050684505037 \n",
      "acc for optim= 0.15378228632940186\n",
      "Epoch:493/1000\n",
      "Loss on train= 0.010226069949567318\n",
      "Loss on test= 0.017080416902899742\n",
      "acc for Lsat= 0.05891984477639199 \n",
      "acc for Psat= 0.0979388286670049 \n",
      "acc for optim= 0.1525534391403198\n",
      "Epoch:494/1000\n",
      "Loss on train= 0.010449081659317017\n",
      "Loss on test= 0.01819821260869503\n",
      "acc for Lsat= 0.060841068873802834 \n",
      "acc for Psat= 0.09727126691076489 \n",
      "acc for optim= 0.1533623447848691\n",
      "Epoch:495/1000\n",
      "Loss on train= 0.01018479187041521\n",
      "Loss on test= 0.018935024738311768\n",
      "acc for Lsat= 0.061917774213684926 \n",
      "acc for Psat= 0.10154253509309556 \n",
      "acc for optim= 0.15268843033247526\n",
      "Epoch:496/1000\n",
      "Loss on train= 0.01056866254657507\n",
      "Loss on test= 0.018499666824936867\n",
      "acc for Lsat= 0.06443093592921895 \n",
      "acc for Psat= 0.10404556029372743 \n",
      "acc for optim= 0.15421755115191138\n",
      "Epoch:497/1000\n",
      "Loss on train= 0.010262488387525082\n",
      "Loss on test= 0.017471853643655777\n",
      "acc for Lsat= 0.06304867284165488 \n",
      "acc for Psat= 0.09791805777284836 \n",
      "acc for optim= 0.1566517763667636\n",
      "Epoch:498/1000\n",
      "Loss on train= 0.010030779987573624\n",
      "Loss on test= 0.01904008537530899\n",
      "acc for Lsat= 0.07023604661226272 \n",
      "acc for Psat= 0.10119522048367395 \n",
      "acc for optim= 0.15455384196506608\n",
      "Epoch:499/1000\n",
      "Loss on train= 0.010206558741629124\n",
      "Loss on test= 0.01846124790608883\n",
      "acc for Lsat= 0.06373231543434991 \n",
      "acc for Psat= 0.10046461323897043 \n",
      "acc for optim= 0.15574628412723537\n",
      "Epoch:500/1000\n",
      "Loss on train= 0.010310452431440353\n",
      "Loss on test= 0.018267549574375153\n",
      "acc for Lsat= 0.0756610040863355 \n",
      "acc for Psat= 0.11098282833894092 \n",
      "acc for optim= 0.1582051760620541\n",
      "Epoch:501/1000\n",
      "Loss on train= 0.010614021681249142\n",
      "Loss on test= 0.017887363210320473\n",
      "acc for Lsat= 0.0714972998532984 \n",
      "acc for Psat= 0.10816393494606016 \n",
      "acc for optim= 0.1537296776142385\n",
      "Epoch:502/1000\n",
      "Loss on train= 0.010019221343100071\n",
      "Loss on test= 0.018425695598125458\n",
      "acc for Lsat= 0.06788669882549178 \n",
      "acc for Psat= 0.10141107638676962 \n",
      "acc for optim= 0.1518422580427594\n",
      "Epoch:503/1000\n",
      "Loss on train= 0.010035369545221329\n",
      "Loss on test= 0.01814896985888481\n",
      "acc for Lsat= 0.06845342641075453 \n",
      "acc for Psat= 0.10192538797855377 \n",
      "acc for optim= 0.15243872412376935\n",
      "Epoch:504/1000\n",
      "Loss on train= 0.010292205028235912\n",
      "Loss on test= 0.016723085194826126\n",
      "acc for Lsat= 0.06583073122633828 \n",
      "acc for Psat= 0.10210289491547479 \n",
      "acc for optim= 0.16053413781854842\n",
      "Epoch:505/1000\n",
      "Loss on train= 0.009891453199088573\n",
      "Loss on test= 0.017265668138861656\n",
      "acc for Lsat= 0.06254949784941144 \n",
      "acc for Psat= 0.09659126798311869 \n",
      "acc for optim= 0.1545334647099177\n",
      "Epoch:506/1000\n",
      "Loss on train= 0.010181182995438576\n",
      "Loss on test= 0.018924850970506668\n",
      "acc for Lsat= 0.06757190674543381 \n",
      "acc for Psat= 0.09973756240473854 \n",
      "acc for optim= 0.15169546488258573\n",
      "Epoch:507/1000\n",
      "Loss on train= 0.010396481491625309\n",
      "Loss on test= 0.01852952316403389\n",
      "acc for Lsat= 0.06714831698271963 \n",
      "acc for Psat= 0.11113838023609585 \n",
      "acc for optim= 0.15630035764641229\n",
      "Epoch:508/1000\n",
      "Loss on train= 0.010011713951826096\n",
      "Loss on test= 0.01810530759394169\n",
      "acc for Lsat= 0.06690543591976165 \n",
      "acc for Psat= 0.10185537570052677 \n",
      "acc for optim= 0.15402343057923845\n",
      "Epoch:509/1000\n",
      "Loss on train= 0.010269271209836006\n",
      "Loss on test= 0.018436182290315628\n",
      "acc for Lsat= 0.08403171598911285 \n",
      "acc for Psat= 0.10863766305976444 \n",
      "acc for optim= 0.15466925327976544\n",
      "Epoch:510/1000\n",
      "Loss on train= 0.010226547718048096\n",
      "Loss on test= 0.019107850268483162\n",
      "acc for Lsat= 0.06581657835178906 \n",
      "acc for Psat= 0.09661351177427506 \n",
      "acc for optim= 0.15347345057460998\n",
      "Epoch:511/1000\n",
      "Loss on train= 0.010212639346718788\n",
      "Loss on test= 0.019387943670153618\n",
      "acc for Lsat= 0.08228661368290585 \n",
      "acc for Psat= 0.10597791969776153 \n",
      "acc for optim= 0.15680638915962644\n",
      "Epoch:512/1000\n",
      "Loss on train= 0.0098864221945405\n",
      "Loss on test= 0.019249312579631805\n",
      "acc for Lsat= 0.07682599574327469 \n",
      "acc for Psat= 0.10591009689701927 \n",
      "acc for optim= 0.15599495967229207\n",
      "Epoch:513/1000\n",
      "Loss on train= 0.010177447460591793\n",
      "Loss on test= 0.018901463598012924\n",
      "acc for Lsat= 0.06703303100334274 \n",
      "acc for Psat= 0.10464805960655213 \n",
      "acc for optim= 0.15144683859414523\n",
      "Epoch:514/1000\n",
      "Loss on train= 0.009875896386802197\n",
      "Loss on test= 0.01787429302930832\n",
      "acc for Lsat= 0.06483058912886513 \n",
      "acc for Psat= 0.10204250084029304 \n",
      "acc for optim= 0.15325559261772367\n",
      "Epoch:515/1000\n",
      "Loss on train= 0.010074579156935215\n",
      "Loss on test= 0.01928945630788803\n",
      "acc for Lsat= 0.061353140489922624 \n",
      "acc for Psat= 0.10222317112816705 \n",
      "acc for optim= 0.15195075621207552\n",
      "Epoch:516/1000\n",
      "Loss on train= 0.010481086559593678\n",
      "Loss on test= 0.01765282452106476\n",
      "acc for Lsat= 0.06216316264536645 \n",
      "acc for Psat= 0.11047826011975606 \n",
      "acc for optim= 0.15335074795616996\n",
      "Epoch:517/1000\n",
      "Loss on train= 0.009821604937314987\n",
      "Loss on test= 0.019446883350610733\n",
      "acc for Lsat= 0.0711838651034567 \n",
      "acc for Psat= 0.10251604417959848 \n",
      "acc for optim= 0.15417749202913708\n",
      "Epoch:518/1000\n",
      "Loss on train= 0.010275022126734257\n",
      "Loss on test= 0.01810871995985508\n",
      "acc for Lsat= 0.06353759600056544 \n",
      "acc for Psat= 0.09787066446410285 \n",
      "acc for optim= 0.153589167445898\n",
      "Epoch:519/1000\n",
      "Loss on train= 0.009959978982806206\n",
      "Loss on test= 0.01812553033232689\n",
      "acc for Lsat= 0.06716503004233042 \n",
      "acc for Psat= 0.0987683915429645 \n",
      "acc for optim= 0.1554384644660685\n",
      "Epoch:520/1000\n",
      "Loss on train= 0.009831566363573074\n",
      "Loss on test= 0.018782353028655052\n",
      "acc for Lsat= 0.06450471778710683 \n",
      "acc for Psat= 0.09849596089786955 \n",
      "acc for optim= 0.15341970125834148\n",
      "Epoch:521/1000\n",
      "Loss on train= 0.00975966826081276\n",
      "Loss on test= 0.018841758370399475\n",
      "acc for Lsat= 0.0622413565715154 \n",
      "acc for Psat= 0.1009081612030665 \n",
      "acc for optim= 0.15398025264342627\n",
      "Epoch:522/1000\n",
      "Loss on train= 0.0103640491142869\n",
      "Loss on test= 0.018041525036096573\n",
      "acc for Lsat= 0.07385901196135414 \n",
      "acc for Psat= 0.11316107014815012 \n",
      "acc for optim= 0.15706626474857333\n",
      "Epoch:523/1000\n",
      "Loss on train= 0.009946368634700775\n",
      "Loss on test= 0.019213132560253143\n",
      "acc for Lsat= 0.07224177685048846 \n",
      "acc for Psat= 0.11049211389488645 \n",
      "acc for optim= 0.15855274349451062\n",
      "Epoch:524/1000\n",
      "Loss on train= 0.009759451262652874\n",
      "Loss on test= 0.018366336822509766\n",
      "acc for Lsat= 0.06479966714978219 \n",
      "acc for Psat= 0.09988175862365299 \n",
      "acc for optim= 0.15619302582409644\n",
      "Epoch:525/1000\n",
      "Loss on train= 0.010554390959441662\n",
      "Loss on test= 0.018314935266971588\n",
      "acc for Lsat= 0.06638220217492846 \n",
      "acc for Psat= 0.10246712730990516 \n",
      "acc for optim= 0.15491246895657648\n",
      "Epoch:526/1000\n",
      "Loss on train= 0.010298024863004684\n",
      "Loss on test= 0.01972307451069355\n",
      "acc for Lsat= 0.0737063373128573 \n",
      "acc for Psat= 0.10096092720826466 \n",
      "acc for optim= 0.1549125958647993\n",
      "Epoch:527/1000\n",
      "Loss on train= 0.010143783874809742\n",
      "Loss on test= 0.01864743046462536\n",
      "acc for Lsat= 0.06234961822628974 \n",
      "acc for Psat= 0.10877386364671919 \n",
      "acc for optim= 0.15324402476350468\n",
      "Epoch:528/1000\n",
      "Loss on train= 0.009890003129839897\n",
      "Loss on test= 0.017571210861206055\n",
      "acc for Lsat= 0.0707499787211418 \n",
      "acc for Psat= 0.10879878136846755 \n",
      "acc for optim= 0.15321282827191884\n",
      "Epoch:529/1000\n",
      "Loss on train= 0.009829511865973473\n",
      "Loss on test= 0.018918389454483986\n",
      "acc for Lsat= 0.06486744996574191 \n",
      "acc for Psat= 0.10190803408622742 \n",
      "acc for optim= 0.15403331369161605\n",
      "Epoch:530/1000\n",
      "Loss on train= 0.009635386057198048\n",
      "Loss on test= 0.01826283149421215\n",
      "acc for Lsat= 0.06878056526184083 \n",
      "acc for Psat= 0.11468062036567266 \n",
      "acc for optim= 0.15225867488318023\n",
      "Epoch:531/1000\n",
      "Loss on train= 0.01030720118433237\n",
      "Loss on test= 0.01812315359711647\n",
      "acc for Lsat= 0.062475230958726674 \n",
      "acc for Psat= 0.10568561090363397 \n",
      "acc for optim= 0.15385774903827246\n",
      "Epoch:532/1000\n",
      "Loss on train= 0.009812482632696629\n",
      "Loss on test= 0.01980721205472946\n",
      "acc for Lsat= 0.067414438062244 \n",
      "acc for Psat= 0.10554136600759295 \n",
      "acc for optim= 0.15980265140533448\n",
      "Epoch:533/1000\n",
      "Loss on train= 0.009696035645902157\n",
      "Loss on test= 0.018711451441049576\n",
      "acc for Lsat= 0.060788235978947745 \n",
      "acc for Psat= 0.10069910486539203 \n",
      "acc for optim= 0.15229910744561093\n",
      "Epoch:534/1000\n",
      "Loss on train= 0.009887486696243286\n",
      "Loss on test= 0.019553018733859062\n",
      "acc for Lsat= 0.0720012593600485 \n",
      "acc for Psat= 0.10097222725550335 \n",
      "acc for optim= 0.1551917536391152\n",
      "Epoch:535/1000\n",
      "Loss on train= 0.010100800544023514\n",
      "Loss on test= 0.018320173025131226\n",
      "acc for Lsat= 0.07492842442459531 \n",
      "acc for Psat= 0.10952451361550225 \n",
      "acc for optim= 0.15607082578870987\n",
      "Epoch:536/1000\n",
      "Loss on train= 0.009800485335290432\n",
      "Loss on test= 0.01973431371152401\n",
      "acc for Lsat= 0.06891438381539451 \n",
      "acc for Psat= 0.10033868766493267 \n",
      "acc for optim= 0.15770300378402072\n",
      "Epoch:537/1000\n",
      "Loss on train= 0.00965893641114235\n",
      "Loss on test= 0.01828519068658352\n",
      "acc for Lsat= 0.06230669087833829 \n",
      "acc for Psat= 0.09872603151533338 \n",
      "acc for optim= 0.156604761381944\n",
      "Epoch:538/1000\n",
      "Loss on train= 0.009923414327204227\n",
      "Loss on test= 0.01855769008398056\n",
      "acc for Lsat= 0.07672751214769152 \n",
      "acc for Psat= 0.10011627243624792 \n",
      "acc for optim= 0.15591143667697907\n",
      "Epoch:539/1000\n",
      "Loss on train= 0.010030318051576614\n",
      "Loss on test= 0.018608223646879196\n",
      "acc for Lsat= 0.06776402435368961 \n",
      "acc for Psat= 0.09728093412187365 \n",
      "acc for optim= 0.15786275102032554\n",
      "Epoch:540/1000\n",
      "Loss on train= 0.009839565493166447\n",
      "Loss on test= 0.018081458285450935\n",
      "acc for Lsat= 0.06787987997134527 \n",
      "acc for Psat= 0.1002797951300939 \n",
      "acc for optim= 0.1596782604853312\n",
      "Epoch:541/1000\n",
      "Loss on train= 0.010203377343714237\n",
      "Loss on test= 0.018575308844447136\n",
      "acc for Lsat= 0.06556484889652994 \n",
      "acc for Psat= 0.09885594679249658 \n",
      "acc for optim= 0.154751448912753\n",
      "Epoch:542/1000\n",
      "Loss on train= 0.009833517484366894\n",
      "Loss on test= 0.01867150515317917\n",
      "acc for Lsat= 0.05797738879919052 \n",
      "acc for Psat= 0.09686527947584789 \n",
      "acc for optim= 0.15511714319388073\n",
      "Epoch:543/1000\n",
      "Loss on train= 0.010166110470890999\n",
      "Loss on test= 0.01900162175297737\n",
      "acc for Lsat= 0.0746057477262285 \n",
      "acc for Psat= 0.10367201732264625 \n",
      "acc for optim= 0.1553579587075445\n",
      "Epoch:544/1000\n",
      "Loss on train= 0.009538795799016953\n",
      "Loss on test= 0.01812228001654148\n",
      "acc for Lsat= 0.06264523983829551 \n",
      "acc for Psat= 0.09752853843900894 \n",
      "acc for optim= 0.1589365699225002\n",
      "Epoch:545/1000\n",
      "Loss on train= 0.009830128401517868\n",
      "Loss on test= 0.017696071416139603\n",
      "acc for Lsat= 0.07087063433395492 \n",
      "acc for Psat= 0.12029563784599305 \n",
      "acc for optim= 0.1541915848023362\n",
      "Epoch:546/1000\n",
      "Loss on train= 0.010080032050609589\n",
      "Loss on test= 0.018299011513590813\n",
      "acc for Lsat= 0.06589521624975733 \n",
      "acc for Psat= 0.10984420312775506 \n",
      "acc for optim= 0.1557411244346036\n",
      "Epoch:547/1000\n",
      "Loss on train= 0.010181542485952377\n",
      "Loss on test= 0.01751602254807949\n",
      "acc for Lsat= 0.061513429631789546 \n",
      "acc for Psat= 0.09816319776905906 \n",
      "acc for optim= 0.15712780157725018\n",
      "Epoch:548/1000\n",
      "Loss on train= 0.009670734405517578\n",
      "Loss on test= 0.019165772944688797\n",
      "acc for Lsat= 0.07045106035139827 \n",
      "acc for Psat= 0.10292174385653602 \n",
      "acc for optim= 0.15524156093597413\n",
      "Epoch:549/1000\n",
      "Loss on train= 0.010048982687294483\n",
      "Loss on test= 0.018381796777248383\n",
      "acc for Lsat= 0.05861063285006417 \n",
      "acc for Psat= 0.09706651154491637 \n",
      "acc for optim= 0.15752411153581408\n",
      "Epoch:550/1000\n",
      "Loss on train= 0.009532889351248741\n",
      "Loss on test= 0.018487218767404556\n",
      "acc for Lsat= 0.0635370775229401 \n",
      "acc for Psat= 0.10345725615819296 \n",
      "acc for optim= 0.1548654869198799\n",
      "Epoch:551/1000\n",
      "Loss on train= 0.009953832253813744\n",
      "Loss on test= 0.018546991050243378\n",
      "acc for Lsat= 0.06746106545130412 \n",
      "acc for Psat= 0.10193802581893074 \n",
      "acc for optim= 0.15243514569269287\n",
      "Epoch:552/1000\n",
      "Loss on train= 0.009998139925301075\n",
      "Loss on test= 0.01831304468214512\n",
      "acc for Lsat= 0.06587737012240621 \n",
      "acc for Psat= 0.10114578704039254 \n",
      "acc for optim= 0.15544494862357772\n",
      "Epoch:553/1000\n",
      "Loss on train= 0.01028190553188324\n",
      "Loss on test= 0.019693562760949135\n",
      "acc for Lsat= 0.07114546572168669 \n",
      "acc for Psat= 0.10647019876374136 \n",
      "acc for optim= 0.15342449852161935\n",
      "Epoch:554/1000\n",
      "Loss on train= 0.00990863237529993\n",
      "Loss on test= 0.017870403826236725\n",
      "acc for Lsat= 0.06535451081064013 \n",
      "acc for Psat= 0.10069851014349195 \n",
      "acc for optim= 0.15266249767608112\n",
      "Epoch:555/1000\n",
      "Loss on train= 0.009847143664956093\n",
      "Loss on test= 0.017656587064266205\n",
      "acc for Lsat= 0.06496130600571631 \n",
      "acc for Psat= 0.1006794282131725 \n",
      "acc for optim= 0.15705196476644942\n",
      "Epoch:556/1000\n",
      "Loss on train= 0.009787210263311863\n",
      "Loss on test= 0.017233513295650482\n",
      "acc for Lsat= 0.06202388430635134 \n",
      "acc for Psat= 0.10358274148570165 \n",
      "acc for optim= 0.15584192574024203\n",
      "Epoch:557/1000\n",
      "Loss on train= 0.010366507805883884\n",
      "Loss on test= 0.01769198291003704\n",
      "acc for Lsat= 0.0651577069527573 \n",
      "acc for Psat= 0.0972339646683799 \n",
      "acc for optim= 0.15329954483442837\n",
      "Epoch:558/1000\n",
      "Loss on train= 0.010019323788583279\n",
      "Loss on test= 0.01825648359954357\n",
      "acc for Lsat= 0.07148459553718568 \n",
      "acc for Psat= 0.09709515935844842 \n",
      "acc for optim= 0.15307334926393293\n",
      "Epoch:559/1000\n",
      "Loss on train= 0.009617304429411888\n",
      "Loss on test= 0.01782989501953125\n",
      "acc for Lsat= 0.06158365507920584 \n",
      "acc for Psat= 0.09699899355570475 \n",
      "acc for optim= 0.1506006655593713\n",
      "Epoch:560/1000\n",
      "Loss on train= 0.009799173101782799\n",
      "Loss on test= 0.01938401535153389\n",
      "acc for Lsat= 0.0724260656370057 \n",
      "acc for Psat= 0.10301919711960686 \n",
      "acc for optim= 0.15333654615614148\n",
      "Epoch:561/1000\n",
      "Loss on train= 0.00960575882345438\n",
      "Loss on test= 0.019213777035474777\n",
      "acc for Lsat= 0.0629650647441546 \n",
      "acc for Psat= 0.09679376913441552 \n",
      "acc for optim= 0.1532695314950413\n",
      "Epoch:562/1000\n",
      "Loss on train= 0.009831404313445091\n",
      "Loss on test= 0.01783626899123192\n",
      "acc for Lsat= 0.0649876669049263 \n",
      "acc for Psat= 0.09891342851850723 \n",
      "acc for optim= 0.15319733280274603\n",
      "Epoch:563/1000\n",
      "Loss on train= 0.009674723260104656\n",
      "Loss on test= 0.019390879198908806\n",
      "acc for Lsat= 0.06593779176473619 \n",
      "acc for Psat= 0.10323239299986098 \n",
      "acc for optim= 0.15497315029303235\n",
      "Epoch:564/1000\n",
      "Loss on train= 0.009733162820339203\n",
      "Loss on test= 0.016887439414858818\n",
      "acc for Lsat= 0.06458837820423974 \n",
      "acc for Psat= 0.10271584623389773 \n",
      "acc for optim= 0.1517548776335186\n",
      "Epoch:565/1000\n",
      "Loss on train= 0.009885416366159916\n",
      "Loss on test= 0.018228212371468544\n",
      "acc for Lsat= 0.06487251760231125 \n",
      "acc for Psat= 0.10594466262393527 \n",
      "acc for optim= 0.15269596527020132\n",
      "Epoch:566/1000\n",
      "Loss on train= 0.009530014358460903\n",
      "Loss on test= 0.01949198730289936\n",
      "acc for Lsat= 0.06648459732532502 \n",
      "acc for Psat= 0.10279002255863615 \n",
      "acc for optim= 0.15539808736907113\n",
      "Epoch:567/1000\n",
      "Loss on train= 0.009687889367341995\n",
      "Loss on test= 0.01738416776061058\n",
      "acc for Lsat= 0.06083515476849344 \n",
      "acc for Psat= 0.09514400826560125 \n",
      "acc for optim= 0.1575756724509928\n",
      "Epoch:568/1000\n",
      "Loss on train= 0.009674962610006332\n",
      "Loss on test= 0.01863980107009411\n",
      "acc for Lsat= 0.07023462669716943 \n",
      "acc for Psat= 0.10647357536686791 \n",
      "acc for optim= 0.15307695584164727\n",
      "Epoch:569/1000\n",
      "Loss on train= 0.009904456324875355\n",
      "Loss on test= 0.018147317692637444\n",
      "acc for Lsat= 0.07784356607331169 \n",
      "acc for Psat= 0.0990152209997177 \n",
      "acc for optim= 0.1543606489068932\n",
      "Epoch:570/1000\n",
      "Loss on train= 0.009899129159748554\n",
      "Loss on test= 0.019771559163928032\n",
      "acc for Lsat= 0.06504162831438914 \n",
      "acc for Psat= 0.10304621954758962 \n",
      "acc for optim= 0.15499152955081724\n",
      "Epoch:571/1000\n",
      "Loss on train= 0.00957893580198288\n",
      "Loss on test= 0.01900009624660015\n",
      "acc for Lsat= 0.06354470402002335 \n",
      "acc for Psat= 0.10045763254165649 \n",
      "acc for optim= 0.15466839224100112\n",
      "Epoch:572/1000\n",
      "Loss on train= 0.009811708703637123\n",
      "Loss on test= 0.017280150204896927\n",
      "acc for Lsat= 0.06757716801431443 \n",
      "acc for Psat= 0.09949129720528921 \n",
      "acc for optim= 0.15377320117420618\n",
      "Epoch:573/1000\n",
      "Loss on train= 0.0091746486723423\n",
      "Loss on test= 0.0179924163967371\n",
      "acc for Lsat= 0.06925315823819903 \n",
      "acc for Psat= 0.10636325114303165 \n",
      "acc for optim= 0.15364051080412333\n",
      "Epoch:574/1000\n",
      "Loss on train= 0.009862883016467094\n",
      "Loss on test= 0.018062256276607513\n",
      "acc for Lsat= 0.07305022974809011 \n",
      "acc for Psat= 0.10027255382802752 \n",
      "acc for optim= 0.15721915480163362\n",
      "Epoch:575/1000\n",
      "Loss on train= 0.009665282443165779\n",
      "Loss on test= 0.017753340303897858\n",
      "acc for Lsat= 0.06455190777778624 \n",
      "acc for Psat= 0.09891903632216982 \n",
      "acc for optim= 0.16032126545906067\n",
      "Epoch:576/1000\n",
      "Loss on train= 0.009587197564542294\n",
      "Loss on test= 0.017978183925151825\n",
      "acc for Lsat= 0.06590226342280706 \n",
      "acc for Psat= 0.09926606151792738 \n",
      "acc for optim= 0.15476457186871106\n",
      "Epoch:577/1000\n",
      "Loss on train= 0.009599116630852222\n",
      "Loss on test= 0.018577463924884796\n",
      "acc for Lsat= 0.06561993617150519 \n",
      "acc for Psat= 0.10203071965111626 \n",
      "acc for optim= 0.15735571467214163\n",
      "Epoch:578/1000\n",
      "Loss on train= 0.009533638134598732\n",
      "Loss on test= 0.01767561584711075\n",
      "acc for Lsat= 0.06678823315434984 \n",
      "acc for Psat= 0.09991001586119334 \n",
      "acc for optim= 0.1545365466011895\n",
      "Epoch:579/1000\n",
      "Loss on train= 0.009583322331309319\n",
      "Loss on test= 0.019155940040946007\n",
      "acc for Lsat= 0.06848802467187246 \n",
      "acc for Psat= 0.1019394920931922 \n",
      "acc for optim= 0.15529950410127638\n",
      "Epoch:580/1000\n",
      "Loss on train= 0.009262923151254654\n",
      "Loss on test= 0.019022904336452484\n",
      "acc for Lsat= 0.06630153614613746 \n",
      "acc for Psat= 0.10441096789307064 \n",
      "acc for optim= 0.15588469000326263\n",
      "Epoch:581/1000\n",
      "Loss on train= 0.00932402815669775\n",
      "Loss on test= 0.018863392993807793\n",
      "acc for Lsat= 0.06255568721228175 \n",
      "acc for Psat= 0.10134237143728467 \n",
      "acc for optim= 0.15363874253299503\n",
      "Epoch:582/1000\n",
      "Loss on train= 0.009383157826960087\n",
      "Loss on test= 0.018765350803732872\n",
      "acc for Lsat= 0.06128208032912678 \n",
      "acc for Psat= 0.09768458174334632 \n",
      "acc for optim= 0.15732603031727999\n",
      "Epoch:583/1000\n",
      "Loss on train= 0.009210973046720028\n",
      "Loss on test= 0.018505312502384186\n",
      "acc for Lsat= 0.06783091028531392 \n",
      "acc for Psat= 0.10469965073797438 \n",
      "acc for optim= 0.15532680617438424\n",
      "Epoch:584/1000\n",
      "Loss on train= 0.00965828076004982\n",
      "Loss on test= 0.017534546554088593\n",
      "acc for Lsat= 0.07022414886289173 \n",
      "acc for Psat= 0.1003103901942571 \n",
      "acc for optim= 0.15510664441519315\n",
      "Epoch:585/1000\n",
      "Loss on train= 0.009345008991658688\n",
      "Loss on test= 0.018120862543582916\n",
      "acc for Lsat= 0.06059170713027319 \n",
      "acc for Psat= 0.09838883678118387 \n",
      "acc for optim= 0.1572940991984473\n",
      "Epoch:586/1000\n",
      "Loss on train= 0.009606628678739071\n",
      "Loss on test= 0.018473763018846512\n",
      "acc for Lsat= 0.06528119751148755 \n",
      "acc for Psat= 0.10303116374545628 \n",
      "acc for optim= 0.1536393422219488\n",
      "Epoch:587/1000\n",
      "Loss on train= 0.009367025457322598\n",
      "Loss on test= 0.017352653667330742\n",
      "acc for Lsat= 0.06342770639393064 \n",
      "acc for Psat= 0.09863049255477058 \n",
      "acc for optim= 0.15619533782203993\n",
      "Epoch:588/1000\n",
      "Loss on train= 0.00945186149328947\n",
      "Loss on test= 0.01803250052034855\n",
      "acc for Lsat= 0.06920767376820246 \n",
      "acc for Psat= 0.10071300466855367 \n",
      "acc for optim= 0.15738631420665317\n",
      "Epoch:589/1000\n",
      "Loss on train= 0.00955137051641941\n",
      "Loss on test= 0.018803531304001808\n",
      "acc for Lsat= 0.06783035082949533 \n",
      "acc for Psat= 0.11602094504568312 \n",
      "acc for optim= 0.1607527062296867\n",
      "Epoch:590/1000\n",
      "Loss on train= 0.01017279177904129\n",
      "Loss on test= 0.01892666332423687\n",
      "acc for Lsat= 0.07015894154707591 \n",
      "acc for Psat= 0.0998119721810023 \n",
      "acc for optim= 0.15368494391441345\n",
      "Epoch:591/1000\n",
      "Loss on train= 0.009481343440711498\n",
      "Loss on test= 0.018392134457826614\n",
      "acc for Lsat= 0.061808705081542334 \n",
      "acc for Psat= 0.09845404492484199 \n",
      "acc for optim= 0.15455031991004942\n",
      "Epoch:592/1000\n",
      "Loss on train= 0.00944732315838337\n",
      "Loss on test= 0.019349511712789536\n",
      "acc for Lsat= 0.0589947169025739 \n",
      "acc for Psat= 0.09886479212178124 \n",
      "acc for optim= 0.15190428669253983\n",
      "Epoch:593/1000\n",
      "Loss on train= 0.009704696014523506\n",
      "Loss on test= 0.018896404653787613\n",
      "acc for Lsat= 0.06246610863341225 \n",
      "acc for Psat= 0.10050318009323544 \n",
      "acc for optim= 0.15681453562445113\n",
      "Epoch:594/1000\n",
      "Loss on train= 0.009661958552896976\n",
      "Loss on test= 0.019776005297899246\n",
      "acc for Lsat= 0.0667759467330244 \n",
      "acc for Psat= 0.10271026094754537 \n",
      "acc for optim= 0.15494952094223766\n",
      "Epoch:595/1000\n",
      "Loss on train= 0.009740323759615421\n",
      "Loss on test= 0.017542773857712746\n",
      "acc for Lsat= 0.06427824811802971 \n",
      "acc for Psat= 0.09881889389620888 \n",
      "acc for optim= 0.15615802928805353\n",
      "Epoch:596/1000\n",
      "Loss on train= 0.009798173792660236\n",
      "Loss on test= 0.018271418288350105\n",
      "acc for Lsat= 0.06501525847448242 \n",
      "acc for Psat= 0.10646158324347602 \n",
      "acc for optim= 0.15459328409698278\n",
      "Epoch:597/1000\n",
      "Loss on train= 0.009375905618071556\n",
      "Loss on test= 0.019480250775814056\n",
      "acc for Lsat= 0.06909179795119498 \n",
      "acc for Psat= 0.09900525377856362 \n",
      "acc for optim= 0.15556409657001496\n",
      "Epoch:598/1000\n",
      "Loss on train= 0.009149267338216305\n",
      "Loss on test= 0.018523328006267548\n",
      "acc for Lsat= 0.0665388188428349 \n",
      "acc for Psat= 0.11685500939687093 \n",
      "acc for optim= 0.15534974543584715\n",
      "Epoch:599/1000\n",
      "Loss on train= 0.009493553079664707\n",
      "Loss on test= 0.01811406761407852\n",
      "acc for Lsat= 0.061307886491219196 \n",
      "acc for Psat= 0.0966463112168842 \n",
      "acc for optim= 0.15440277506907782\n",
      "Epoch:600/1000\n",
      "Loss on train= 0.009495940059423447\n",
      "Loss on test= 0.019145624712109566\n",
      "acc for Lsat= 0.0783061502708329 \n",
      "acc for Psat= 0.10714411106374529 \n",
      "acc for optim= 0.1551267556846142\n",
      "Epoch:601/1000\n",
      "Loss on train= 0.009431470185518265\n",
      "Loss on test= 0.018450304865837097\n",
      "acc for Lsat= 0.06243411352237067 \n",
      "acc for Psat= 0.09826455513636272 \n",
      "acc for optim= 0.15682440540856787\n",
      "Epoch:602/1000\n",
      "Loss on train= 0.009278961457312107\n",
      "Loss on test= 0.018651898950338364\n",
      "acc for Lsat= 0.0645546696252293 \n",
      "acc for Psat= 0.10278562373585172 \n",
      "acc for optim= 0.1566782740255197\n",
      "Epoch:603/1000\n",
      "Loss on train= 0.009530377574265003\n",
      "Loss on test= 0.017814425751566887\n",
      "acc for Lsat= 0.06373167865806156 \n",
      "acc for Psat= 0.10060106416543325 \n",
      "acc for optim= 0.1557713795867231\n",
      "Epoch:604/1000\n",
      "Loss on train= 0.0095357745885849\n",
      "Loss on test= 0.018258603289723396\n",
      "acc for Lsat= 0.06513433555761974 \n",
      "acc for Psat= 0.09974439475271438 \n",
      "acc for optim= 0.15684107244014736\n",
      "Epoch:605/1000\n",
      "Loss on train= 0.009291432797908783\n",
      "Loss on test= 0.017733633518218994\n",
      "acc for Lsat= 0.0640451026873456 \n",
      "acc for Psat= 0.10058861904674105 \n",
      "acc for optim= 0.15929793053203156\n",
      "Epoch:606/1000\n",
      "Loss on train= 0.009241507388651371\n",
      "Loss on test= 0.020035885274410248\n",
      "acc for Lsat= 0.06560964973436462 \n",
      "acc for Psat= 0.09966168105602263 \n",
      "acc for optim= 0.15687019584907427\n",
      "Epoch:607/1000\n",
      "Loss on train= 0.008831233717501163\n",
      "Loss on test= 0.017893530428409576\n",
      "acc for Lsat= 0.060566476898060904 \n",
      "acc for Psat= 0.09747145871321361 \n",
      "acc for optim= 0.15525873088174394\n",
      "Epoch:608/1000\n",
      "Loss on train= 0.009030137211084366\n",
      "Loss on test= 0.018708107993006706\n",
      "acc for Lsat= 0.06092169202036329 \n",
      "acc for Psat= 0.09569163471460342 \n",
      "acc for optim= 0.15965579748153683\n",
      "Epoch:609/1000\n",
      "Loss on train= 0.009801638312637806\n",
      "Loss on test= 0.017968518659472466\n",
      "acc for Lsat= 0.06372294061713749 \n",
      "acc for Psat= 0.10128494368659127 \n",
      "acc for optim= 0.15753490096992914\n",
      "Epoch:610/1000\n",
      "Loss on train= 0.009272403083741665\n",
      "Loss on test= 0.017745241522789\n",
      "acc for Lsat= 0.06226310183604559 \n",
      "acc for Psat= 0.09761666804552079 \n",
      "acc for optim= 0.15512607544660567\n",
      "Epoch:611/1000\n",
      "Loss on train= 0.009481670334935188\n",
      "Loss on test= 0.01860770396888256\n",
      "acc for Lsat= 0.06071001489957174 \n",
      "acc for Psat= 0.0950547112358941 \n",
      "acc for optim= 0.15428553546468415\n",
      "Epoch:612/1000\n",
      "Loss on train= 0.009065402671694756\n",
      "Loss on test= 0.017621494829654694\n",
      "acc for Lsat= 0.07004105713632372 \n",
      "acc for Psat= 0.09713273644447329 \n",
      "acc for optim= 0.15581557436121835\n",
      "Epoch:613/1000\n",
      "Loss on train= 0.00930096860975027\n",
      "Loss on test= 0.01841653138399124\n",
      "acc for Lsat= 0.06164288810557791 \n",
      "acc for Psat= 0.09552506970034705 \n",
      "acc for optim= 0.15478992097907598\n",
      "Epoch:614/1000\n",
      "Loss on train= 0.009229025803506374\n",
      "Loss on test= 0.018617281690239906\n",
      "acc for Lsat= 0.07033618440230688 \n",
      "acc for Psat= 0.1010610881778929 \n",
      "acc for optim= 0.15432623699307443\n",
      "Epoch:615/1000\n",
      "Loss on train= 0.008962798863649368\n",
      "Loss on test= 0.017505906522274017\n",
      "acc for Lsat= 0.06991819656557506 \n",
      "acc for Psat= 0.09686836765872107 \n",
      "acc for optim= 0.15412545725703242\n",
      "Epoch:616/1000\n",
      "Loss on train= 0.00929612573236227\n",
      "Loss on test= 0.017227189615368843\n",
      "acc for Lsat= 0.06618044094906911 \n",
      "acc for Psat= 0.10106883611943988 \n",
      "acc for optim= 0.1587934626473321\n",
      "Epoch:617/1000\n",
      "Loss on train= 0.009357824921607971\n",
      "Loss on test= 0.017773447558283806\n",
      "acc for Lsat= 0.06014081525305906 \n",
      "acc for Psat= 0.09369894795947606 \n",
      "acc for optim= 0.15847313867674936\n",
      "Epoch:618/1000\n",
      "Loss on train= 0.009467354975640774\n",
      "Loss on test= 0.018204160034656525\n",
      "acc for Lsat= 0.0704118340379662 \n",
      "acc for Psat= 0.10391411648856269 \n",
      "acc for optim= 0.15417599570420057\n",
      "Epoch:619/1000\n",
      "Loss on train= 0.009328828193247318\n",
      "Loss on test= 0.017857305705547333\n",
      "acc for Lsat= 0.0669470391339726 \n",
      "acc for Psat= 0.09925149397717582 \n",
      "acc for optim= 0.15752192081676591\n",
      "Epoch:620/1000\n",
      "Loss on train= 0.009760336019098759\n",
      "Loss on test= 0.018146084621548653\n",
      "acc for Lsat= 0.06523238859242864 \n",
      "acc for Psat= 0.10038206742869482 \n",
      "acc for optim= 0.155169202056196\n",
      "Epoch:621/1000\n",
      "Loss on train= 0.009196852333843708\n",
      "Loss on test= 0.01726396195590496\n",
      "acc for Lsat= 0.06516969303290049 \n",
      "acc for Psat= 0.09504515826702119 \n",
      "acc for optim= 0.15409417748451226\n",
      "Epoch:622/1000\n",
      "Loss on train= 0.009469798766076565\n",
      "Loss on test= 0.01810021698474884\n",
      "acc for Lsat= 0.06587785830100377 \n",
      "acc for Psat= 0.0965467115243276 \n",
      "acc for optim= 0.15853407316737705\n",
      "Epoch:623/1000\n",
      "Loss on train= 0.009251917712390423\n",
      "Loss on test= 0.017993422225117683\n",
      "acc for Lsat= 0.06380960221091908 \n",
      "acc for Psat= 0.10209495325883229 \n",
      "acc for optim= 0.1569634070826901\n",
      "Epoch:624/1000\n",
      "Loss on train= 0.00919037964195013\n",
      "Loss on test= 0.01911715604364872\n",
      "acc for Lsat= 0.06544618440998926 \n",
      "acc for Psat= 0.1074954777956009 \n",
      "acc for optim= 0.15711055464214752\n",
      "Epoch:625/1000\n",
      "Loss on train= 0.009228975512087345\n",
      "Loss on test= 0.0177767276763916\n",
      "acc for Lsat= 0.06938896129528681 \n",
      "acc for Psat= 0.11147108243571388 \n",
      "acc for optim= 0.1534441038966179\n",
      "Epoch:626/1000\n",
      "Loss on train= 0.009061372838914394\n",
      "Loss on test= 0.01881621778011322\n",
      "acc for Lsat= 0.06672119961844551 \n",
      "acc for Psat= 0.10250636057721244 \n",
      "acc for optim= 0.15490424036979678\n",
      "Epoch:627/1000\n",
      "Loss on train= 0.009466676972806454\n",
      "Loss on test= 0.01991712488234043\n",
      "acc for Lsat= 0.06906850553221174 \n",
      "acc for Psat= 0.10344740483495925 \n",
      "acc for optim= 0.15220082799593607\n",
      "Epoch:628/1000\n",
      "Loss on train= 0.009330897592008114\n",
      "Loss on test= 0.018059637397527695\n",
      "acc for Lsat= 0.06108325686719683 \n",
      "acc for Psat= 0.09776568677690292 \n",
      "acc for optim= 0.15764299076464441\n",
      "Epoch:629/1000\n",
      "Loss on train= 0.009260592982172966\n",
      "Loss on test= 0.017588071525096893\n",
      "acc for Lsat= 0.06885615752802955 \n",
      "acc for Psat= 0.09899653775824442 \n",
      "acc for optim= 0.15594915598630904\n",
      "Epoch:630/1000\n",
      "Loss on train= 0.009279214777052402\n",
      "Loss on test= 0.0179865974932909\n",
      "acc for Lsat= 0.06619196203019884 \n",
      "acc for Psat= 0.09786955465873082 \n",
      "acc for optim= 0.15492249288492735\n",
      "Epoch:631/1000\n",
      "Loss on train= 0.009042778052389622\n",
      "Loss on test= 0.01895298808813095\n",
      "acc for Lsat= 0.0648339572880003 \n",
      "acc for Psat= 0.10361455248461829 \n",
      "acc for optim= 0.15831508570247227\n",
      "Epoch:632/1000\n",
      "Loss on train= 0.009159530512988567\n",
      "Loss on test= 0.018137825652956963\n",
      "acc for Lsat= 0.06849843892786239 \n",
      "acc for Psat= 0.09658205840322706 \n",
      "acc for optim= 0.15783853406707443\n",
      "Epoch:633/1000\n",
      "Loss on train= 0.009135059081017971\n",
      "Loss on test= 0.017904132604599\n",
      "acc for Lsat= 0.06712957132193777 \n",
      "acc for Psat= 0.11098785003026326 \n",
      "acc for optim= 0.1522666122350428\n",
      "Epoch:634/1000\n",
      "Loss on train= 0.009070429019629955\n",
      "Loss on test= 0.019261019304394722\n",
      "acc for Lsat= 0.06461873973409335 \n",
      "acc for Psat= 0.10116684701707629 \n",
      "acc for optim= 0.15389512090219393\n",
      "Epoch:635/1000\n",
      "Loss on train= 0.008782940916717052\n",
      "Loss on test= 0.01818060129880905\n",
      "acc for Lsat= 0.059551749957932366 \n",
      "acc for Psat= 0.09503983524110582 \n",
      "acc for optim= 0.152853841914071\n",
      "Epoch:636/1000\n",
      "Loss on train= 0.009262354113161564\n",
      "Loss on test= 0.020590024068951607\n",
      "acc for Lsat= 0.0752095177769661 \n",
      "acc for Psat= 0.10689231488439772 \n",
      "acc for optim= 0.15733895748853685\n",
      "Epoch:637/1000\n",
      "Loss on train= 0.009200678206980228\n",
      "Loss on test= 0.018629100173711777\n",
      "acc for Lsat= 0.060410496261384755 \n",
      "acc for Psat= 0.0974569779303339 \n",
      "acc for optim= 0.15697270722852813\n",
      "Epoch:638/1000\n",
      "Loss on train= 0.00881294533610344\n",
      "Loss on test= 0.01784469746053219\n",
      "acc for Lsat= 0.06952810436487197 \n",
      "acc for Psat= 0.10102866523795657 \n",
      "acc for optim= 0.15238675499955814\n",
      "Epoch:639/1000\n",
      "Loss on train= 0.00906345248222351\n",
      "Loss on test= 0.018731562420725822\n",
      "acc for Lsat= 0.06498103895121149 \n",
      "acc for Psat= 0.10000585582521228 \n",
      "acc for optim= 0.15450750142335892\n",
      "Epoch:640/1000\n",
      "Loss on train= 0.009110099636018276\n",
      "Loss on test= 0.018864469602704048\n",
      "acc for Lsat= 0.06823598841826121 \n",
      "acc for Psat= 0.1031915588511361 \n",
      "acc for optim= 0.1564985556734933\n",
      "Epoch:641/1000\n",
      "Loss on train= 0.009106838144361973\n",
      "Loss on test= 0.018673600628972054\n",
      "acc for Lsat= 0.06019013325373332 \n",
      "acc for Psat= 0.09630254771974354 \n",
      "acc for optim= 0.1549497692949242\n",
      "Epoch:642/1000\n",
      "Loss on train= 0.009265237487852573\n",
      "Loss on test= 0.01847585290670395\n",
      "acc for Lsat= 0.062030125492148924 \n",
      "acc for Psat= 0.09324099388387469 \n",
      "acc for optim= 0.15365855097770695\n",
      "Epoch:643/1000\n",
      "Loss on train= 0.00899488478899002\n",
      "Loss on test= 0.01773652620613575\n",
      "acc for Lsat= 0.06352308235234685 \n",
      "acc for Psat= 0.104389289021492 \n",
      "acc for optim= 0.15701251046525103\n",
      "Epoch:644/1000\n",
      "Loss on train= 0.009123207069933414\n",
      "Loss on test= 0.017931295558810234\n",
      "acc for Lsat= 0.06245967853400442 \n",
      "acc for Psat= 0.0958679887983534 \n",
      "acc for optim= 0.15732337335745494\n",
      "Epoch:645/1000\n",
      "Loss on train= 0.008899208158254623\n",
      "Loss on test= 0.018556319177150726\n",
      "acc for Lsat= 0.06475957168473138 \n",
      "acc for Psat= 0.10222592022683885 \n",
      "acc for optim= 0.15866476827197604\n",
      "Epoch:646/1000\n",
      "Loss on train= 0.009109928272664547\n",
      "Loss on test= 0.02007385343313217\n",
      "acc for Lsat= 0.06524088606238364 \n",
      "acc for Psat= 0.10687755213843453 \n",
      "acc for optim= 0.15544844741622604\n",
      "Epoch:647/1000\n",
      "Loss on train= 0.009089482948184013\n",
      "Loss on test= 0.018724972382187843\n",
      "acc for Lsat= 0.05987904237376318 \n",
      "acc for Psat= 0.09827136761612362 \n",
      "acc for optim= 0.15819753987921606\n",
      "Epoch:648/1000\n",
      "Loss on train= 0.009377746842801571\n",
      "Loss on test= 0.018229153007268906\n",
      "acc for Lsat= 0.06333597815699046 \n",
      "acc for Psat= 0.09764973951710593 \n",
      "acc for optim= 0.15526505220267509\n",
      "Epoch:649/1000\n",
      "Loss on train= 0.009223987348377705\n",
      "Loss on test= 0.017316527664661407\n",
      "acc for Lsat= 0.07444795717795691 \n",
      "acc for Psat= 0.10505880183643763 \n",
      "acc for optim= 0.15627033958832426\n",
      "Epoch:650/1000\n",
      "Loss on train= 0.009125906974077225\n",
      "Loss on test= 0.019130386412143707\n",
      "acc for Lsat= 0.0600637972354889 \n",
      "acc for Psat= 0.09697367151578268 \n",
      "acc for optim= 0.1543622753686375\n",
      "Epoch:651/1000\n",
      "Loss on train= 0.00917417649179697\n",
      "Loss on test= 0.019057517871260643\n",
      "acc for Lsat= 0.06888140100571843 \n",
      "acc for Psat= 0.09547976719008551 \n",
      "acc for optim= 0.15712574687269\n",
      "Epoch:652/1000\n",
      "Loss on train= 0.009148313663899899\n",
      "Loss on test= 0.01784568466246128\n",
      "acc for Lsat= 0.07061404469940397 \n",
      "acc for Psat= 0.10474137034681108 \n",
      "acc for optim= 0.15657427807648971\n",
      "Epoch:653/1000\n",
      "Loss on train= 0.009423937648534775\n",
      "Loss on test= 0.018998883664608\n",
      "acc for Lsat= 0.06258316818210813 \n",
      "acc for Psat= 0.10895601146750979 \n",
      "acc for optim= 0.15487827741437488\n",
      "Epoch:654/1000\n",
      "Loss on train= 0.009136146865785122\n",
      "Loss on test= 0.018366243690252304\n",
      "acc for Lsat= 0.06509401533338759 \n",
      "acc for Psat= 0.09778116295735044 \n",
      "acc for optim= 0.1583360064360831\n",
      "Epoch:655/1000\n",
      "Loss on train= 0.008926082402467728\n",
      "Loss on test= 0.01936732605099678\n",
      "acc for Lsat= 0.08702378504806095 \n",
      "acc for Psat= 0.10835414760642582 \n",
      "acc for optim= 0.1542538060910172\n",
      "Epoch:656/1000\n",
      "Loss on train= 0.008578501641750336\n",
      "Loss on test= 0.018335118889808655\n",
      "acc for Lsat= 0.06324868409170044 \n",
      "acc for Psat= 0.09689954651726616 \n",
      "acc for optim= 0.15420506563451558\n",
      "Epoch:657/1000\n",
      "Loss on train= 0.009263324551284313\n",
      "Loss on test= 0.020213229581713676\n",
      "acc for Lsat= 0.06821798914008671 \n",
      "acc for Psat= 0.10491062998771668 \n",
      "acc for optim= 0.15497381645772193\n",
      "Epoch:658/1000\n",
      "Loss on train= 0.009288168512284756\n",
      "Loss on test= 0.017964916303753853\n",
      "acc for Lsat= 0.06355596002605227 \n",
      "acc for Psat= 0.10379694054524104 \n",
      "acc for optim= 0.1576898160080115\n",
      "Epoch:659/1000\n",
      "Loss on train= 0.008910595439374447\n",
      "Loss on test= 0.019505206495523453\n",
      "acc for Lsat= 0.06216042339801789 \n",
      "acc for Psat= 0.10036559700965883 \n",
      "acc for optim= 0.15417162328958509\n",
      "Epoch:660/1000\n",
      "Loss on train= 0.008913285098969936\n",
      "Loss on test= 0.019090969115495682\n",
      "acc for Lsat= 0.05912954699661996 \n",
      "acc for Psat= 0.09709509876039292 \n",
      "acc for optim= 0.15389466964536241\n",
      "Epoch:661/1000\n",
      "Loss on train= 0.008926273323595524\n",
      "Loss on test= 0.018772980198264122\n",
      "acc for Lsat= 0.058401164578066926 \n",
      "acc for Psat= 0.09852218925952912 \n",
      "acc for optim= 0.1563128661778238\n",
      "Epoch:662/1000\n",
      "Loss on train= 0.008904548361897469\n",
      "Loss on test= 0.018815452232956886\n",
      "acc for Lsat= 0.06369730672902531 \n",
      "acc for Psat= 0.10163969000180564 \n",
      "acc for optim= 0.1556580624646611\n",
      "Epoch:663/1000\n",
      "Loss on train= 0.009234054014086723\n",
      "Loss on test= 0.017578957602381706\n",
      "acc for Lsat= 0.060980431073241755 \n",
      "acc for Psat= 0.10008566743797725 \n",
      "acc for optim= 0.1568085130718019\n",
      "Epoch:664/1000\n",
      "Loss on train= 0.008912283927202225\n",
      "Loss on test= 0.017755158245563507\n",
      "acc for Lsat= 0.061238644520441685 \n",
      "acc for Psat= 0.0986563715669844 \n",
      "acc for optim= 0.15468920850091508\n",
      "Epoch:665/1000\n",
      "Loss on train= 0.008896122686564922\n",
      "Loss on test= 0.019206581637263298\n",
      "acc for Lsat= 0.05763201134072409 \n",
      "acc for Psat= 0.09869809746742249 \n",
      "acc for optim= 0.15445365831255914\n",
      "Epoch:666/1000\n",
      "Loss on train= 0.00914252083748579\n",
      "Loss on test= 0.018329743295907974\n",
      "acc for Lsat= 0.06341071691777972 \n",
      "acc for Psat= 0.09566422932677798 \n",
      "acc for optim= 0.15385068472888735\n",
      "Epoch:667/1000\n",
      "Loss on train= 0.008979739621281624\n",
      "Loss on test= 0.019018886610865593\n",
      "acc for Lsat= 0.06376966395311885 \n",
      "acc for Psat= 0.09476231286923092 \n",
      "acc for optim= 0.1550459580288993\n",
      "Epoch:668/1000\n",
      "Loss on train= 0.008921455591917038\n",
      "Loss on test= 0.017423711717128754\n",
      "acc for Lsat= 0.07768204195631875 \n",
      "acc for Psat= 0.10554405219025081 \n",
      "acc for optim= 0.15733505123191407\n",
      "Epoch:669/1000\n",
      "Loss on train= 0.008990221656858921\n",
      "Loss on test= 0.01785547472536564\n",
      "acc for Lsat= 0.055700969282123786 \n",
      "acc for Psat= 0.10162440935770671 \n",
      "acc for optim= 0.15364927765395908\n",
      "Epoch:670/1000\n",
      "Loss on train= 0.009142803028225899\n",
      "Loss on test= 0.01959465816617012\n",
      "acc for Lsat= 0.07186066524849999 \n",
      "acc for Psat= 0.10929933355914223 \n",
      "acc for optim= 0.15513109092911084\n",
      "Epoch:671/1000\n",
      "Loss on train= 0.008876742795109749\n",
      "Loss on test= 0.01841316558420658\n",
      "acc for Lsat= 0.05969193031390509 \n",
      "acc for Psat= 0.09481163455380336 \n",
      "acc for optim= 0.1557525175313155\n",
      "Epoch:672/1000\n",
      "Loss on train= 0.0086923623457551\n",
      "Loss on test= 0.02021249569952488\n",
      "acc for Lsat= 0.061649546523888904 \n",
      "acc for Psat= 0.09822330872217813 \n",
      "acc for optim= 0.157229565249549\n",
      "Epoch:673/1000\n",
      "Loss on train= 0.009126582182943821\n",
      "Loss on test= 0.018488312140107155\n",
      "acc for Lsat= 0.07072386840979257 \n",
      "acc for Psat= 0.10894383721881443 \n",
      "acc for optim= 0.1568415068089962\n",
      "Epoch:674/1000\n",
      "Loss on train= 0.008759669028222561\n",
      "Loss on test= 0.018998615443706512\n",
      "acc for Lsat= 0.06417783763673571 \n",
      "acc for Psat= 0.0992920974890391 \n",
      "acc for optim= 0.15290679269366797\n",
      "Epoch:675/1000\n",
      "Loss on train= 0.008947578258812428\n",
      "Loss on test= 0.01797187328338623\n",
      "acc for Lsat= 0.0764393428961436 \n",
      "acc for Psat= 0.1088795976506339 \n",
      "acc for optim= 0.15421983102957407\n",
      "Epoch:676/1000\n",
      "Loss on train= 0.009379157796502113\n",
      "Loss on test= 0.01796320639550686\n",
      "acc for Lsat= 0.05959211447172695 \n",
      "acc for Psat= 0.0968865399559339 \n",
      "acc for optim= 0.15360128465625975\n",
      "Epoch:677/1000\n",
      "Loss on train= 0.009023330174386501\n",
      "Loss on test= 0.01775708794593811\n",
      "acc for Lsat= 0.06394671565956539 \n",
      "acc for Psat= 0.09764820900228288 \n",
      "acc for optim= 0.15368524483508533\n",
      "Epoch:678/1000\n",
      "Loss on train= 0.009003687649965286\n",
      "Loss on test= 0.018899986520409584\n",
      "acc for Lsat= 0.06014589783218173 \n",
      "acc for Psat= 0.09887168291542266 \n",
      "acc for optim= 0.15705518540408878\n",
      "Epoch:679/1000\n",
      "Loss on train= 0.00887753814458847\n",
      "Loss on test= 0.01950284093618393\n",
      "acc for Lsat= 0.06247863132092689 \n",
      "acc for Psat= 0.09877315296067131 \n",
      "acc for optim= 0.16197622732983694\n",
      "Epoch:680/1000\n",
      "Loss on train= 0.00894283875823021\n",
      "Loss on test= 0.018531352281570435\n",
      "acc for Lsat= 0.06300025184949239 \n",
      "acc for Psat= 0.0967886808845732 \n",
      "acc for optim= 0.1548085636562771\n",
      "Epoch:681/1000\n",
      "Loss on train= 0.008700844831764698\n",
      "Loss on test= 0.018168549984693527\n",
      "acc for Lsat= 0.06292903472979863 \n",
      "acc for Psat= 0.09325290256076388 \n",
      "acc for optim= 0.15508499327633116\n",
      "Epoch:682/1000\n",
      "Loss on train= 0.009034905582666397\n",
      "Loss on test= 0.01833195984363556\n",
      "acc for Lsat= 0.06467597881952922 \n",
      "acc for Psat= 0.09757690297232732 \n",
      "acc for optim= 0.1551378627618154\n",
      "Epoch:683/1000\n",
      "Loss on train= 0.008936209604144096\n",
      "Loss on test= 0.018129747360944748\n",
      "acc for Lsat= 0.06906593268116316 \n",
      "acc for Psat= 0.10178843206829495 \n",
      "acc for optim= 0.15591193818383745\n",
      "Epoch:684/1000\n",
      "Loss on train= 0.008790292777121067\n",
      "Loss on test= 0.017655257135629654\n",
      "acc for Lsat= 0.06956571688254674 \n",
      "acc for Psat= 0.10102136582136155 \n",
      "acc for optim= 0.1547550626926952\n",
      "Epoch:685/1000\n",
      "Loss on train= 0.008532268926501274\n",
      "Loss on test= 0.0184580460190773\n",
      "acc for Lsat= 0.06281658311684926 \n",
      "acc for Psat= 0.09470200555192099 \n",
      "acc for optim= 0.15396766497029196\n",
      "Epoch:686/1000\n",
      "Loss on train= 0.008778732270002365\n",
      "Loss on test= 0.01853938028216362\n",
      "acc for Lsat= 0.06753442486127219 \n",
      "acc for Psat= 0.10586861504448786 \n",
      "acc for optim= 0.15404906297723453\n",
      "Epoch:687/1000\n",
      "Loss on train= 0.009234318509697914\n",
      "Loss on test= 0.018325813114643097\n",
      "acc for Lsat= 0.06178923439648417 \n",
      "acc for Psat= 0.09611719846725465 \n",
      "acc for optim= 0.15158775912390815\n",
      "Epoch:688/1000\n",
      "Loss on train= 0.009003420360386372\n",
      "Loss on test= 0.017286188900470734\n",
      "acc for Lsat= 0.06205658043424289 \n",
      "acc for Psat= 0.09826308720641666 \n",
      "acc for optim= 0.1547325336270862\n",
      "Epoch:689/1000\n",
      "Loss on train= 0.008728418499231339\n",
      "Loss on test= 0.019619174301624298\n",
      "acc for Lsat= 0.06984178970257442 \n",
      "acc for Psat= 0.0976015802886751 \n",
      "acc for optim= 0.15560618051224287\n",
      "Epoch:690/1000\n",
      "Loss on train= 0.008551396429538727\n",
      "Loss on test= 0.019036926329135895\n",
      "acc for Lsat= 0.06637217717038262 \n",
      "acc for Psat= 0.094844418267409 \n",
      "acc for optim= 0.15502453678184086\n",
      "Epoch:691/1000\n",
      "Loss on train= 0.008773320354521275\n",
      "Loss on test= 0.018456650897860527\n",
      "acc for Lsat= 0.0706890528400739 \n",
      "acc for Psat= 0.10133798254860771 \n",
      "acc for optim= 0.15512498617172243\n",
      "Epoch:692/1000\n",
      "Loss on train= 0.008673704229295254\n",
      "Loss on test= 0.018699148669838905\n",
      "acc for Lsat= 0.0650922555062506 \n",
      "acc for Psat= 0.10030676573514938 \n",
      "acc for optim= 0.15430096727278494\n",
      "Epoch:693/1000\n",
      "Loss on train= 0.008931963704526424\n",
      "Loss on test= 0.018067942932248116\n",
      "acc for Lsat= 0.06150615496767891 \n",
      "acc for Psat= 0.0921159886651569 \n",
      "acc for optim= 0.1544034842815664\n",
      "Epoch:694/1000\n",
      "Loss on train= 0.00884607806801796\n",
      "Loss on test= 0.01894114352762699\n",
      "acc for Lsat= 0.062765438357989 \n",
      "acc for Psat= 0.10954342948065864 \n",
      "acc for optim= 0.159984394411246\n",
      "Epoch:695/1000\n",
      "Loss on train= 0.008717251941561699\n",
      "Loss on test= 0.018803901970386505\n",
      "acc for Lsat= 0.0647888019680977 \n",
      "acc for Psat= 0.1014838520023558 \n",
      "acc for optim= 0.15519788828161027\n",
      "Epoch:696/1000\n",
      "Loss on train= 0.00860496237874031\n",
      "Loss on test= 0.01867181621491909\n",
      "acc for Lsat= 0.07041444496976006 \n",
      "acc for Psat= 0.10779648986127642 \n",
      "acc for optim= 0.1536212803588973\n",
      "Epoch:697/1000\n",
      "Loss on train= 0.008739985525608063\n",
      "Loss on test= 0.019364861771464348\n",
      "acc for Lsat= 0.059127005438009896 \n",
      "acc for Psat= 0.09366317854987252 \n",
      "acc for optim= 0.15457367267873554\n",
      "Epoch:698/1000\n",
      "Loss on train= 0.008669315837323666\n",
      "Loss on test= 0.01704454980790615\n",
      "acc for Lsat= 0.07016826735602484 \n",
      "acc for Psat= 0.09464940544631745 \n",
      "acc for optim= 0.1562286119494173\n",
      "Epoch:699/1000\n",
      "Loss on train= 0.008716922253370285\n",
      "Loss on test= 0.019209034740924835\n",
      "acc for Lsat= 0.06483739208843974 \n",
      "acc for Psat= 0.09452255335119035 \n",
      "acc for optim= 0.15335554571615323\n",
      "Epoch:700/1000\n",
      "Loss on train= 0.009118262678384781\n",
      "Loss on test= 0.01827472820878029\n",
      "acc for Lsat= 0.07333510319391887 \n",
      "acc for Psat= 0.0967055771085951 \n",
      "acc for optim= 0.15508228184448347\n",
      "Epoch:701/1000\n",
      "Loss on train= 0.009025060571730137\n",
      "Loss on test= 0.018459420651197433\n",
      "acc for Lsat= 0.056484350479311406 \n",
      "acc for Psat= 0.09785665190882153 \n",
      "acc for optim= 0.1560773953795433\n",
      "Epoch:702/1000\n",
      "Loss on train= 0.009115674532949924\n",
      "Loss on test= 0.018348297104239464\n",
      "acc for Lsat= 0.05996983796358108 \n",
      "acc for Psat= 0.09688305192523533 \n",
      "acc for optim= 0.15477226550380385\n",
      "Epoch:703/1000\n",
      "Loss on train= 0.008440584875643253\n",
      "Loss on test= 0.019000966101884842\n",
      "acc for Lsat= 0.05898002634445825 \n",
      "acc for Psat= 0.09712932705879213 \n",
      "acc for optim= 0.15444713350799344\n",
      "Epoch:704/1000\n",
      "Loss on train= 0.008958500809967518\n",
      "Loss on test= 0.018135862424969673\n",
      "acc for Lsat= 0.06288285727302234 \n",
      "acc for Psat= 0.09997473806142806 \n",
      "acc for optim= 0.15914796383844482\n",
      "Epoch:705/1000\n",
      "Loss on train= 0.00947574432939291\n",
      "Loss on test= 0.01919320411980152\n",
      "acc for Lsat= 0.06756489525238672 \n",
      "acc for Psat= 0.10609063473012713 \n",
      "acc for optim= 0.1524714082479477\n",
      "Epoch:706/1000\n",
      "Loss on train= 0.00883717741817236\n",
      "Loss on test= 0.019102923572063446\n",
      "acc for Lsat= 0.062266905605793 \n",
      "acc for Psat= 0.09960055715507932 \n",
      "acc for optim= 0.15887708432144587\n",
      "Epoch:707/1000\n",
      "Loss on train= 0.008717111311852932\n",
      "Loss on test= 0.01859571970999241\n",
      "acc for Lsat= 0.061462896482812036 \n",
      "acc for Psat= 0.0987244490120146 \n",
      "acc for optim= 0.15645606799258127\n",
      "Epoch:708/1000\n",
      "Loss on train= 0.008780105970799923\n",
      "Loss on test= 0.01964704878628254\n",
      "acc for Lsat= 0.06092846343914668 \n",
      "acc for Psat= 0.10051955382029215 \n",
      "acc for optim= 0.15575680500931213\n",
      "Epoch:709/1000\n",
      "Loss on train= 0.008976459503173828\n",
      "Loss on test= 0.01774449832737446\n",
      "acc for Lsat= 0.062171699437830186 \n",
      "acc for Psat= 0.09758634501033359 \n",
      "acc for optim= 0.15709381848573684\n",
      "Epoch:710/1000\n",
      "Loss on train= 0.008805832825601101\n",
      "Loss on test= 0.01814553327858448\n",
      "acc for Lsat= 0.06156867055429352 \n",
      "acc for Psat= 0.09420641627576615 \n",
      "acc for optim= 0.15807025697496202\n",
      "Epoch:711/1000\n",
      "Loss on train= 0.008625037968158722\n",
      "Loss on test= 0.019617877900600433\n",
      "acc for Lsat= 0.06364597669906086 \n",
      "acc for Psat= 0.10136354896757338 \n",
      "acc for optim= 0.1549888042940034\n",
      "Epoch:712/1000\n",
      "Loss on train= 0.009051242843270302\n",
      "Loss on test= 0.018100392073392868\n",
      "acc for Lsat= 0.06316660212145911 \n",
      "acc for Psat= 0.09847293496131897 \n",
      "acc for optim= 0.15288674806555114\n",
      "Epoch:713/1000\n",
      "Loss on train= 0.008609532378613949\n",
      "Loss on test= 0.019144203513860703\n",
      "acc for Lsat= 0.0647493440243933 \n",
      "acc for Psat= 0.10525762637456258 \n",
      "acc for optim= 0.15824851791063943\n",
      "Epoch:714/1000\n",
      "Loss on train= 0.008797108195722103\n",
      "Loss on test= 0.019056767225265503\n",
      "acc for Lsat= 0.056887462735176086 \n",
      "acc for Psat= 0.094269151157803 \n",
      "acc for optim= 0.1572423812415865\n",
      "Epoch:715/1000\n",
      "Loss on train= 0.008497622795403004\n",
      "Loss on test= 0.019459588453173637\n",
      "acc for Lsat= 0.06708594684799513 \n",
      "acc for Psat= 0.10282067987653941 \n",
      "acc for optim= 0.15529104429814553\n",
      "Epoch:716/1000\n",
      "Loss on train= 0.008649038150906563\n",
      "Loss on test= 0.019644422456622124\n",
      "acc for Lsat= 0.0640832448999087 \n",
      "acc for Psat= 0.09518015417787765 \n",
      "acc for optim= 0.1570053994655609\n",
      "Epoch:717/1000\n",
      "Loss on train= 0.008573230355978012\n",
      "Loss on test= 0.01883525215089321\n",
      "acc for Lsat= 0.06494374738799201 \n",
      "acc for Psat= 0.10061396923330096 \n",
      "acc for optim= 0.1575660192304187\n",
      "Epoch:718/1000\n",
      "Loss on train= 0.008924524299800396\n",
      "Loss on test= 0.017660290002822876\n",
      "acc for Lsat= 0.0648611872560448 \n",
      "acc for Psat= 0.09557605783144635 \n",
      "acc for optim= 0.15835175075464777\n",
      "Epoch:719/1000\n",
      "Loss on train= 0.008740426041185856\n",
      "Loss on test= 0.018603643402457237\n",
      "acc for Lsat= 0.060514840318097014 \n",
      "acc for Psat= 0.09948349429501428 \n",
      "acc for optim= 0.15540320922931036\n",
      "Epoch:720/1000\n",
      "Loss on train= 0.00892842561006546\n",
      "Loss on test= 0.016730595380067825\n",
      "acc for Lsat= 0.0662818408674664 \n",
      "acc for Psat= 0.09979086981879341 \n",
      "acc for optim= 0.15266627338197497\n",
      "Epoch:721/1000\n",
      "Loss on train= 0.008648134768009186\n",
      "Loss on test= 0.018577106297016144\n",
      "acc for Lsat= 0.06575733290778266 \n",
      "acc for Psat= 0.0953843782345454 \n",
      "acc for optim= 0.15448494255542755\n",
      "Epoch:722/1000\n",
      "Loss on train= 0.008581343106925488\n",
      "Loss on test= 0.01785123534500599\n",
      "acc for Lsat= 0.06933464639716679 \n",
      "acc for Psat= 0.102064558201366 \n",
      "acc for optim= 0.1556013781163428\n",
      "Epoch:723/1000\n",
      "Loss on train= 0.00843601580709219\n",
      "Loss on test= 0.019721098244190216\n",
      "acc for Lsat= 0.05895297047164706 \n",
      "acc for Psat= 0.09835407237211863 \n",
      "acc for optim= 0.15238566042648424\n",
      "Epoch:724/1000\n",
      "Loss on train= 0.008716961368918419\n",
      "Loss on test= 0.01898864470422268\n",
      "acc for Lsat= 0.05906308144330979 \n",
      "acc for Psat= 0.09355363531245126 \n",
      "acc for optim= 0.1512840851313538\n",
      "Epoch:725/1000\n",
      "Loss on train= 0.008790566585958004\n",
      "Loss on test= 0.019485877826809883\n",
      "acc for Lsat= 0.06766165958510503 \n",
      "acc for Psat= 0.10513572825325859 \n",
      "acc for optim= 0.15373400383525426\n",
      "Epoch:726/1000\n",
      "Loss on train= 0.008271240629255772\n",
      "Loss on test= 0.017822137102484703\n",
      "acc for Lsat= 0.06608694601390099 \n",
      "acc for Psat= 0.09626365188095305 \n",
      "acc for optim= 0.15324373932348356\n",
      "Epoch:727/1000\n",
      "Loss on train= 0.008944696746766567\n",
      "Loss on test= 0.01989070326089859\n",
      "acc for Lsat= 0.08152108622921836 \n",
      "acc for Psat= 0.10875930885473888 \n",
      "acc for optim= 0.154003763364421\n",
      "Epoch:728/1000\n",
      "Loss on train= 0.008647087030112743\n",
      "Loss on test= 0.018008911982178688\n",
      "acc for Lsat= 0.0608681283891201 \n",
      "acc for Psat= 0.09654702146848043 \n",
      "acc for optim= 0.15149459060695436\n",
      "Epoch:729/1000\n",
      "Loss on train= 0.008391499519348145\n",
      "Loss on test= 0.019239548593759537\n",
      "acc for Lsat= 0.06977990716695785 \n",
      "acc for Psat= 0.10308563676145344 \n",
      "acc for optim= 0.1539027486410406\n",
      "Epoch:730/1000\n",
      "Loss on train= 0.008172335103154182\n",
      "Loss on test= 0.018279949203133583\n",
      "acc for Lsat= 0.06194695631663005 \n",
      "acc for Psat= 0.09342666235235003 \n",
      "acc for optim= 0.15273977824383309\n",
      "Epoch:731/1000\n",
      "Loss on train= 0.00863875076174736\n",
      "Loss on test= 0.018257243558764458\n",
      "acc for Lsat= 0.059853679107295145 \n",
      "acc for Psat= 0.09487320168150795 \n",
      "acc for optim= 0.15427787113520833\n",
      "Epoch:732/1000\n",
      "Loss on train= 0.008435244672000408\n",
      "Loss on test= 0.01995558850467205\n",
      "acc for Lsat= 0.0642675220966339 \n",
      "acc for Psat= 0.09864634176095326 \n",
      "acc for optim= 0.15399748575356267\n",
      "Epoch:733/1000\n",
      "Loss on train= 0.008427067659795284\n",
      "Loss on test= 0.018645334988832474\n",
      "acc for Lsat= 0.07525854557752609 \n",
      "acc for Psat= 0.10716236895985072 \n",
      "acc for optim= 0.15399234982000456\n",
      "Epoch:734/1000\n",
      "Loss on train= 0.008535516448318958\n",
      "Loss on test= 0.01787230186164379\n",
      "acc for Lsat= 0.06699519703785578 \n",
      "acc for Psat= 0.10109338363011675 \n",
      "acc for optim= 0.1531472855144077\n",
      "Epoch:735/1000\n",
      "Loss on train= 0.00874247495085001\n",
      "Loss on test= 0.018137240782380104\n",
      "acc for Lsat= 0.060293419824706186 \n",
      "acc for Psat= 0.09760403931140899 \n",
      "acc for optim= 0.15104566456543075\n",
      "Epoch:736/1000\n",
      "Loss on train= 0.008269324898719788\n",
      "Loss on test= 0.018684901297092438\n",
      "acc for Lsat= 0.06316433333688314 \n",
      "acc for Psat= 0.09917971789836884 \n",
      "acc for optim= 0.15883859246969223\n",
      "Epoch:737/1000\n",
      "Loss on train= 0.008511790074408054\n",
      "Loss on test= 0.01887989416718483\n",
      "acc for Lsat= 0.06052802925308546 \n",
      "acc for Psat= 0.09429098235236273 \n",
      "acc for optim= 0.1577717430889606\n",
      "Epoch:738/1000\n",
      "Loss on train= 0.008458045311272144\n",
      "Loss on test= 0.018760494887828827\n",
      "acc for Lsat= 0.060895339068439275 \n",
      "acc for Psat= 0.0974966992934545 \n",
      "acc for optim= 0.15215645217233234\n",
      "Epoch:739/1000\n",
      "Loss on train= 0.008516276255249977\n",
      "Loss on test= 0.018295474350452423\n",
      "acc for Lsat= 0.05951971693171395 \n",
      "acc for Psat= 0.09379072520467971 \n",
      "acc for optim= 0.15495163781775367\n",
      "Epoch:740/1000\n",
      "Loss on train= 0.008400619961321354\n",
      "Loss on test= 0.017971940338611603\n",
      "acc for Lsat= 0.06568195323149362 \n",
      "acc for Psat= 0.09714536302619511 \n",
      "acc for optim= 0.15803693557778992\n",
      "Epoch:741/1000\n",
      "Loss on train= 0.008568988181650639\n",
      "Loss on test= 0.017849136143922806\n",
      "acc for Lsat= 0.06308173727658059 \n",
      "acc for Psat= 0.09585981070995331 \n",
      "acc for optim= 0.15313469701343113\n",
      "Epoch:742/1000\n",
      "Loss on train= 0.008371522650122643\n",
      "Loss on test= 0.01849927380681038\n",
      "acc for Lsat= 0.0617788036664327 \n",
      "acc for Psat= 0.09877450068791707 \n",
      "acc for optim= 0.15576522656612923\n",
      "Epoch:743/1000\n",
      "Loss on train= 0.008856657892465591\n",
      "Loss on test= 0.019283344969153404\n",
      "acc for Lsat= 0.06043995262848007 \n",
      "acc for Psat= 0.09973619795507854 \n",
      "acc for optim= 0.15443186552988156\n",
      "Epoch:744/1000\n",
      "Loss on train= 0.008753211237490177\n",
      "Loss on test= 0.01795981638133526\n",
      "acc for Lsat= 0.06396805461910035 \n",
      "acc for Psat= 0.10242415302329594 \n",
      "acc for optim= 0.15346972197294234\n",
      "Epoch:745/1000\n",
      "Loss on train= 0.008782416582107544\n",
      "Loss on test= 0.01846674084663391\n",
      "acc for Lsat= 0.060457529541518965 \n",
      "acc for Psat= 0.09545859628253513 \n",
      "acc for optim= 0.15588920215765636\n",
      "Epoch:746/1000\n",
      "Loss on train= 0.00847959890961647\n",
      "Loss on test= 0.018847709521651268\n",
      "acc for Lsat= 0.07220986700720258 \n",
      "acc for Psat= 0.11160391536023881 \n",
      "acc for optim= 0.15719207003712654\n",
      "Epoch:747/1000\n",
      "Loss on train= 0.008592624217271805\n",
      "Loss on test= 0.01770036853849888\n",
      "acc for Lsat= 0.058265348441070994 \n",
      "acc for Psat= 0.0916907121737798 \n",
      "acc for optim= 0.15283259583844078\n",
      "Epoch:748/1000\n",
      "Loss on train= 0.008489036001265049\n",
      "Loss on test= 0.01746959052979946\n",
      "acc for Lsat= 0.060147074196073756 \n",
      "acc for Psat= 0.09402696655856241 \n",
      "acc for optim= 0.15640220592419307\n",
      "Epoch:749/1000\n",
      "Loss on train= 0.008352166041731834\n",
      "Loss on test= 0.019334297627210617\n",
      "acc for Lsat= 0.059653420829110675 \n",
      "acc for Psat= 0.0946767704354392 \n",
      "acc for optim= 0.15445833578705784\n",
      "Epoch:750/1000\n",
      "Loss on train= 0.00873999297618866\n",
      "Loss on test= 0.017650838941335678\n",
      "acc for Lsat= 0.05985290002491739 \n",
      "acc for Psat= 0.09489035473929511 \n",
      "acc for optim= 0.15332237316502464\n",
      "Epoch:751/1000\n",
      "Loss on train= 0.008583114482462406\n",
      "Loss on test= 0.018246211111545563\n",
      "acc for Lsat= 0.06176863842540318 \n",
      "acc for Psat= 0.09701632360617321 \n",
      "acc for optim= 0.15746346231963904\n",
      "Epoch:752/1000\n",
      "Loss on train= 0.0086358105763793\n",
      "Loss on test= 0.01942293904721737\n",
      "acc for Lsat= 0.06272785796059503 \n",
      "acc for Psat= 0.09945057928562163 \n",
      "acc for optim= 0.15610648931728469\n",
      "Epoch:753/1000\n",
      "Loss on train= 0.008407763205468655\n",
      "Loss on test= 0.018656713888049126\n",
      "acc for Lsat= 0.06725931813319524 \n",
      "acc for Psat= 0.09589894629187055 \n",
      "acc for optim= 0.15273715257644654\n",
      "Epoch:754/1000\n",
      "Loss on train= 0.008569004014134407\n",
      "Loss on test= 0.018575681373476982\n",
      "acc for Lsat= 0.05890250098374155 \n",
      "acc for Psat= 0.09590652320120069 \n",
      "acc for optim= 0.1549031285776032\n",
      "Epoch:755/1000\n",
      "Loss on train= 0.008501104079186916\n",
      "Loss on test= 0.018207363784313202\n",
      "acc for Lsat= 0.07232644673850801 \n",
      "acc for Psat= 0.1057742135392295 \n",
      "acc for optim= 0.1561844809187783\n",
      "Epoch:756/1000\n",
      "Loss on train= 0.008598064072430134\n",
      "Loss on test= 0.0187864750623703\n",
      "acc for Lsat= 0.07025096085336474 \n",
      "acc for Psat= 0.09552328752146827 \n",
      "acc for optim= 0.15386530516876118\n",
      "Epoch:757/1000\n",
      "Loss on train= 0.008800853975117207\n",
      "Loss on test= 0.018985729664564133\n",
      "acc for Lsat= 0.062265895803769414 \n",
      "acc for Psat= 0.10187052388985952 \n",
      "acc for optim= 0.15742947508891422\n",
      "Epoch:758/1000\n",
      "Loss on train= 0.008481026627123356\n",
      "Loss on test= 0.01736580766737461\n",
      "acc for Lsat= 0.057306660380628385 \n",
      "acc for Psat= 0.09379169742266338 \n",
      "acc for optim= 0.1542087493671311\n",
      "Epoch:759/1000\n",
      "Loss on train= 0.008291485719382763\n",
      "Loss on test= 0.01792173832654953\n",
      "acc for Lsat= 0.061381578279866095 \n",
      "acc for Psat= 0.09341663403643502 \n",
      "acc for optim= 0.1537901291416751\n",
      "Epoch:760/1000\n",
      "Loss on train= 0.008437858894467354\n",
      "Loss on test= 0.018427656963467598\n",
      "acc for Lsat= 0.06638967858420478 \n",
      "acc for Psat= 0.09974157942665947 \n",
      "acc for optim= 0.15381469395425582\n",
      "Epoch:761/1000\n",
      "Loss on train= 0.008899952284991741\n",
      "Loss on test= 0.017483573406934738\n",
      "acc for Lsat= 0.058327499694294405 \n",
      "acc for Psat= 0.09776564041773478 \n",
      "acc for optim= 0.15443448291884526\n",
      "Epoch:762/1000\n",
      "Loss on train= 0.00850082840770483\n",
      "Loss on test= 0.01841391995549202\n",
      "acc for Lsat= 0.05759612032108838 \n",
      "acc for Psat= 0.09375078429778416 \n",
      "acc for optim= 0.15326995336347157\n",
      "Epoch:763/1000\n",
      "Loss on train= 0.00831097736954689\n",
      "Loss on test= 0.017881175503134727\n",
      "acc for Lsat= 0.06204948706759347 \n",
      "acc for Psat= 0.10077740848064423 \n",
      "acc for optim= 0.15535511581434142\n",
      "Epoch:764/1000\n",
      "Loss on train= 0.008734239265322685\n",
      "Loss on test= 0.0179597195237875\n",
      "acc for Lsat= 0.06310303707917532 \n",
      "acc for Psat= 0.10044877264234754 \n",
      "acc for optim= 0.15205926547447837\n",
      "Epoch:765/1000\n",
      "Loss on train= 0.008331160992383957\n",
      "Loss on test= 0.01898239552974701\n",
      "acc for Lsat= 0.06539517169197401 \n",
      "acc for Psat= 0.0979293362961875 \n",
      "acc for optim= 0.15382689858476323\n",
      "Epoch:766/1000\n",
      "Loss on train= 0.008345547132194042\n",
      "Loss on test= 0.018003957346081734\n",
      "acc for Lsat= 0.06865857823027505 \n",
      "acc for Psat= 0.10196097360716926 \n",
      "acc for optim= 0.1545389715996053\n",
      "Epoch:767/1000\n",
      "Loss on train= 0.008687395602464676\n",
      "Loss on test= 0.019138328731060028\n",
      "acc for Lsat= 0.06169972800546222 \n",
      "acc for Psat= 0.09821092204915151 \n",
      "acc for optim= 0.15391508092482883\n",
      "Epoch:768/1000\n",
      "Loss on train= 0.008489269763231277\n",
      "Loss on test= 0.01754993572831154\n",
      "acc for Lsat= 0.06674630459811953 \n",
      "acc for Psat= 0.0956064177883996 \n",
      "acc for optim= 0.15719676290949183\n",
      "Epoch:769/1000\n",
      "Loss on train= 0.008481351658701897\n",
      "Loss on test= 0.01811857894062996\n",
      "acc for Lsat= 0.0633347342411677 \n",
      "acc for Psat= 0.10414737529224821 \n",
      "acc for optim= 0.1559463649160332\n",
      "Epoch:770/1000\n",
      "Loss on train= 0.00807666964828968\n",
      "Loss on test= 0.018223125487565994\n",
      "acc for Lsat= 0.06155029634634653 \n",
      "acc for Psat= 0.10298630164729222 \n",
      "acc for optim= 0.15529052880075245\n",
      "Epoch:771/1000\n",
      "Loss on train= 0.008714192546904087\n",
      "Loss on test= 0.018280528485774994\n",
      "acc for Lsat= 0.05652176696393225 \n",
      "acc for Psat= 0.09390489690833623 \n",
      "acc for optim= 0.15505449647704758\n",
      "Epoch:772/1000\n",
      "Loss on train= 0.008654722943902016\n",
      "Loss on test= 0.018088648095726967\n",
      "acc for Lsat= 0.060353027780850735 \n",
      "acc for Psat= 0.09263549082809025 \n",
      "acc for optim= 0.1549182782570521\n",
      "Epoch:773/1000\n",
      "Loss on train= 0.008492397144436836\n",
      "Loss on test= 0.01851891539990902\n",
      "acc for Lsat= 0.05969266676240497 \n",
      "acc for Psat= 0.0925063921345605 \n",
      "acc for optim= 0.15561986656652554\n",
      "Epoch:774/1000\n",
      "Loss on train= 0.008131718263030052\n",
      "Loss on test= 0.018631894141435623\n",
      "acc for Lsat= 0.061045318841934204 \n",
      "acc for Psat= 0.09684030993117228 \n",
      "acc for optim= 0.1549033140142759\n",
      "Epoch:775/1000\n",
      "Loss on train= 0.008335772901773453\n",
      "Loss on test= 0.018054751679301262\n",
      "acc for Lsat= 0.0586103081703186 \n",
      "acc for Psat= 0.09384463810258442 \n",
      "acc for optim= 0.15580822419789103\n",
      "Epoch:776/1000\n",
      "Loss on train= 0.008118829689919949\n",
      "Loss on test= 0.017097391188144684\n",
      "acc for Lsat= 0.06841710110505422 \n",
      "acc for Psat= 0.09862648977173699 \n",
      "acc for optim= 0.15109550605217614\n",
      "Epoch:777/1000\n",
      "Loss on train= 0.008321384899318218\n",
      "Loss on test= 0.017867054790258408\n",
      "acc for Lsat= 0.05999858106176058 \n",
      "acc for Psat= 0.09464208649264443 \n",
      "acc for optim= 0.15454458933737542\n",
      "Epoch:778/1000\n",
      "Loss on train= 0.008567238226532936\n",
      "Loss on test= 0.01821817457675934\n",
      "acc for Lsat= 0.0629864134722286 \n",
      "acc for Psat= 0.0980655852291319 \n",
      "acc for optim= 0.15285749360918996\n",
      "Epoch:779/1000\n",
      "Loss on train= 0.008575678803026676\n",
      "Loss on test= 0.017702342942357063\n",
      "acc for Lsat= 0.06272566806938913 \n",
      "acc for Psat= 0.09849204321702322 \n",
      "acc for optim= 0.15157998038662804\n",
      "Epoch:780/1000\n",
      "Loss on train= 0.008532577194273472\n",
      "Loss on test= 0.018086688593029976\n",
      "acc for Lsat= 0.058914107415411206 \n",
      "acc for Psat= 0.09089545309543608 \n",
      "acc for optim= 0.15222069356176587\n",
      "Epoch:781/1000\n",
      "Loss on train= 0.008356010541319847\n",
      "Loss on test= 0.018172791227698326\n",
      "acc for Lsat= 0.05822983450359768 \n",
      "acc for Psat= 0.09254497769806119 \n",
      "acc for optim= 0.15461770602398447\n",
      "Epoch:782/1000\n",
      "Loss on train= 0.00831194780766964\n",
      "Loss on test= 0.019069718196988106\n",
      "acc for Lsat= 0.06856629699468612 \n",
      "acc for Psat= 0.10180909732977551 \n",
      "acc for optim= 0.15615868344902992\n",
      "Epoch:783/1000\n",
      "Loss on train= 0.008398071862757206\n",
      "Loss on test= 0.01835952140390873\n",
      "acc for Lsat= 0.06200921287139257 \n",
      "acc for Psat= 0.09421347677707674 \n",
      "acc for optim= 0.15451980390482478\n",
      "Epoch:784/1000\n",
      "Loss on train= 0.008015412837266922\n",
      "Loss on test= 0.017802992835640907\n",
      "acc for Lsat= 0.06561170799864663 \n",
      "acc for Psat= 0.09988544947571226 \n",
      "acc for optim= 0.15459128841757772\n",
      "Epoch:785/1000\n",
      "Loss on train= 0.008268372155725956\n",
      "Loss on test= 0.018454423174262047\n",
      "acc for Lsat= 0.0685595817036099 \n",
      "acc for Psat= 0.10129601723617976 \n",
      "acc for optim= 0.15199718773365023\n",
      "Epoch:786/1000\n",
      "Loss on train= 0.0084099555388093\n",
      "Loss on test= 0.018405655398964882\n",
      "acc for Lsat= 0.06347686052322388 \n",
      "acc for Psat= 0.09901541570822397 \n",
      "acc for optim= 0.1550178988940186\n",
      "Epoch:787/1000\n",
      "Loss on train= 0.008352085947990417\n",
      "Loss on test= 0.01856963336467743\n",
      "acc for Lsat= 0.05852139534221755 \n",
      "acc for Psat= 0.09688992069827186 \n",
      "acc for optim= 0.15163825675845144\n",
      "Epoch:788/1000\n",
      "Loss on train= 0.00820950698107481\n",
      "Loss on test= 0.019416697323322296\n",
      "acc for Lsat= 0.06122517064213752 \n",
      "acc for Psat= 0.09482575837108825 \n",
      "acc for optim= 0.15376071077254083\n",
      "Epoch:789/1000\n",
      "Loss on train= 0.008529294282197952\n",
      "Loss on test= 0.018063636496663094\n",
      "acc for Lsat= 0.06497575284706221 \n",
      "acc for Psat= 0.09531917075316113 \n",
      "acc for optim= 0.1532550383773115\n",
      "Epoch:790/1000\n",
      "Loss on train= 0.008493461646139622\n",
      "Loss on test= 0.01783176138997078\n",
      "acc for Lsat= 0.07400823367966546 \n",
      "acc for Psat= 0.10422415633996326 \n",
      "acc for optim= 0.15193970484866035\n",
      "Epoch:791/1000\n",
      "Loss on train= 0.008797972463071346\n",
      "Loss on test= 0.018440788611769676\n",
      "acc for Lsat= 0.057029204318920765 \n",
      "acc for Psat= 0.09419481257597606 \n",
      "acc for optim= 0.15205911190973387\n",
      "Epoch:792/1000\n",
      "Loss on train= 0.008632540702819824\n",
      "Loss on test= 0.018252070993185043\n",
      "acc for Lsat= 0.06423585108584827 \n",
      "acc for Psat= 0.09361785766151216 \n",
      "acc for optim= 0.1546445303493076\n",
      "Epoch:793/1000\n",
      "Loss on train= 0.008079858496785164\n",
      "Loss on test= 0.01846463605761528\n",
      "acc for Lsat= 0.07393403616216448 \n",
      "acc for Psat= 0.10035053888956706 \n",
      "acc for optim= 0.1582455046474934\n",
      "Epoch:794/1000\n",
      "Loss on train= 0.007960569113492966\n",
      "Loss on test= 0.017961159348487854\n",
      "acc for Lsat= 0.061323789755503344 \n",
      "acc for Psat= 0.09289500001404022 \n",
      "acc for optim= 0.152169500456916\n",
      "Epoch:795/1000\n",
      "Loss on train= 0.008715597912669182\n",
      "Loss on test= 0.01852230541408062\n",
      "acc for Lsat= 0.06861509250269997 \n",
      "acc for Psat= 0.10769680043061575 \n",
      "acc for optim= 0.15680491659376353\n",
      "Epoch:796/1000\n",
      "Loss on train= 0.008244472555816174\n",
      "Loss on test= 0.017957672476768494\n",
      "acc for Lsat= 0.06535158513320818 \n",
      "acc for Psat= 0.10110570589701336 \n",
      "acc for optim= 0.15404873755243087\n",
      "Epoch:797/1000\n",
      "Loss on train= 0.007840247824788094\n",
      "Loss on test= 0.018551655113697052\n",
      "acc for Lsat= 0.06418347739511066 \n",
      "acc for Psat= 0.10182528826925491 \n",
      "acc for optim= 0.15615217900938458\n",
      "Epoch:798/1000\n",
      "Loss on train= 0.008143334649503231\n",
      "Loss on test= 0.01854335516691208\n",
      "acc for Lsat= 0.059572218358516685 \n",
      "acc for Psat= 0.0960365096728007 \n",
      "acc for optim= 0.15365742155247264\n",
      "Epoch:799/1000\n",
      "Loss on train= 0.00838206522166729\n",
      "Loss on test= 0.017930136993527412\n",
      "acc for Lsat= 0.0723266997271114 \n",
      "acc for Psat= 0.10751382774776881 \n",
      "acc for optim= 0.15746901747253206\n",
      "Epoch:800/1000\n",
      "Loss on train= 0.008196369744837284\n",
      "Loss on test= 0.019029565155506134\n",
      "acc for Lsat= 0.06755813575453228 \n",
      "acc for Psat= 0.10170185367266335 \n",
      "acc for optim= 0.15209240963061654\n",
      "Epoch:801/1000\n",
      "Loss on train= 0.008327538147568703\n",
      "Loss on test= 0.01836276799440384\n",
      "acc for Lsat= 0.05934972291191418 \n",
      "acc for Psat= 0.09387820114692053 \n",
      "acc for optim= 0.15404373092783824\n",
      "Epoch:802/1000\n",
      "Loss on train= 0.00848583783954382\n",
      "Loss on test= 0.019683338701725006\n",
      "acc for Lsat= 0.06052133532034026 \n",
      "acc for Psat= 0.0984200777279006 \n",
      "acc for optim= 0.1544516002966298\n",
      "Epoch:803/1000\n",
      "Loss on train= 0.008166918531060219\n",
      "Loss on test= 0.01896660216152668\n",
      "acc for Lsat= 0.059085609681076484 \n",
      "acc for Psat= 0.0918788926468955 \n",
      "acc for optim= 0.15295950406127504\n",
      "Epoch:804/1000\n",
      "Loss on train= 0.008364983834326267\n",
      "Loss on test= 0.0183312576264143\n",
      "acc for Lsat= 0.06527231021059884 \n",
      "acc for Psat= 0.09846717019875845 \n",
      "acc for optim= 0.153472839627001\n",
      "Epoch:805/1000\n",
      "Loss on train= 0.008624589070677757\n",
      "Loss on test= 0.01985766366124153\n",
      "acc for Lsat= 0.061852576749192346 \n",
      "acc for Psat= 0.09560486276944478 \n",
      "acc for optim= 0.1515804034140375\n",
      "Epoch:806/1000\n",
      "Loss on train= 0.008339687250554562\n",
      "Loss on test= 0.017843900248408318\n",
      "acc for Lsat= 0.06396945243080457 \n",
      "acc for Psat= 0.09882899887031978 \n",
      "acc for optim= 0.1570391825503773\n",
      "Epoch:807/1000\n",
      "Loss on train= 0.008152077905833721\n",
      "Loss on test= 0.01811336912214756\n",
      "acc for Lsat= 0.06633249670267105 \n",
      "acc for Psat= 0.10590202146106298 \n",
      "acc for optim= 0.15400217142370012\n",
      "Epoch:808/1000\n",
      "Loss on train= 0.008097094483673573\n",
      "Loss on test= 0.01737849973142147\n",
      "acc for Lsat= 0.06437808523575465 \n",
      "acc for Psat= 0.09575151701768239 \n",
      "acc for optim= 0.15377584952447151\n",
      "Epoch:809/1000\n",
      "Loss on train= 0.008432203903794289\n",
      "Loss on test= 0.018872050568461418\n",
      "acc for Lsat= 0.07042662700017294 \n",
      "acc for Psat= 0.10297423468695747 \n",
      "acc for optim= 0.15581836766666834\n",
      "Epoch:810/1000\n",
      "Loss on train= 0.008214931935071945\n",
      "Loss on test= 0.01798289269208908\n",
      "acc for Lsat= 0.060355324712064534 \n",
      "acc for Psat= 0.09980894757641687 \n",
      "acc for optim= 0.15526501147283447\n",
      "Epoch:811/1000\n",
      "Loss on train= 0.008122231811285019\n",
      "Loss on test= 0.01881415955722332\n",
      "acc for Lsat= 0.059268159502082404 \n",
      "acc for Psat= 0.09418908539745541 \n",
      "acc for optim= 0.15411493480205538\n",
      "Epoch:812/1000\n",
      "Loss on train= 0.007951918989419937\n",
      "Loss on test= 0.018684184178709984\n",
      "acc for Lsat= 0.07069490800301234 \n",
      "acc for Psat= 0.09895484646161397 \n",
      "acc for optim= 0.15538070814477076\n",
      "Epoch:813/1000\n",
      "Loss on train= 0.008232776075601578\n",
      "Loss on test= 0.01953454129397869\n",
      "acc for Lsat= 0.06665397269858254 \n",
      "acc for Psat= 0.10120097696781157 \n",
      "acc for optim= 0.15381820193595355\n",
      "Epoch:814/1000\n",
      "Loss on train= 0.008187243714928627\n",
      "Loss on test= 0.018764710053801537\n",
      "acc for Lsat= 0.05843767838345632 \n",
      "acc for Psat= 0.09369421948989233 \n",
      "acc for optim= 0.1531177726884683\n",
      "Epoch:815/1000\n",
      "Loss on train= 0.00831945613026619\n",
      "Loss on test= 0.017776787281036377\n",
      "acc for Lsat= 0.06397897957099809 \n",
      "acc for Psat= 0.10028204189406502 \n",
      "acc for optim= 0.15598098064462343\n",
      "Epoch:816/1000\n",
      "Loss on train= 0.008175455033779144\n",
      "Loss on test= 0.018894493579864502\n",
      "acc for Lsat= 0.05880598740445242 \n",
      "acc for Psat= 0.09195339812172784 \n",
      "acc for optim= 0.157071331805653\n",
      "Epoch:817/1000\n",
      "Loss on train= 0.008150420151650906\n",
      "Loss on test= 0.018355483189225197\n",
      "acc for Lsat= 0.0572378305097421 \n",
      "acc for Psat= 0.09605561594168346 \n",
      "acc for optim= 0.15562763139605523\n",
      "Epoch:818/1000\n",
      "Loss on train= 0.008069782517850399\n",
      "Loss on test= 0.02049851417541504\n",
      "acc for Lsat= 0.0662530639105373 \n",
      "acc for Psat= 0.0980795618560579 \n",
      "acc for optim= 0.15391388676232765\n",
      "Epoch:819/1000\n",
      "Loss on train= 0.008393176831305027\n",
      "Loss on test= 0.018683427944779396\n",
      "acc for Lsat= 0.07029255429903666 \n",
      "acc for Psat= 0.1048479997449451 \n",
      "acc for optim= 0.15359898524151905\n",
      "Epoch:820/1000\n",
      "Loss on train= 0.00803027767688036\n",
      "Loss on test= 0.018247593194246292\n",
      "acc for Lsat= 0.06837727394368913 \n",
      "acc for Psat= 0.10278005202611289 \n",
      "acc for optim= 0.15576558344893982\n",
      "Epoch:821/1000\n",
      "Loss on train= 0.008438329212367535\n",
      "Loss on test= 0.018145281821489334\n",
      "acc for Lsat= 0.06403398381339179 \n",
      "acc for Psat= 0.09932316707240212 \n",
      "acc for optim= 0.15273776484860313\n",
      "Epoch:822/1000\n",
      "Loss on train= 0.008258501999080181\n",
      "Loss on test= 0.018487581983208656\n",
      "acc for Lsat= 0.059994211792945853 \n",
      "acc for Psat= 0.09685064173407026 \n",
      "acc for optim= 0.1565777412719197\n",
      "Epoch:823/1000\n",
      "Loss on train= 0.008211576379835606\n",
      "Loss on test= 0.018804317340254784\n",
      "acc for Lsat= 0.060732393960158025 \n",
      "acc for Psat= 0.09494889196422365 \n",
      "acc for optim= 0.15563688360982472\n",
      "Epoch:824/1000\n",
      "Loss on train= 0.008330656215548515\n",
      "Loss on test= 0.018588921055197716\n",
      "acc for Lsat= 0.061617486758364565 \n",
      "acc for Psat= 0.09701068848371505 \n",
      "acc for optim= 0.15920861346854104\n",
      "Epoch:825/1000\n",
      "Loss on train= 0.008330381475389004\n",
      "Loss on test= 0.019191153347492218\n",
      "acc for Lsat= 0.0583144563767645 \n",
      "acc for Psat= 0.09243941273954182 \n",
      "acc for optim= 0.15447403018673259\n",
      "Epoch:826/1000\n",
      "Loss on train= 0.008328905329108238\n",
      "Loss on test= 0.018619313836097717\n",
      "acc for Lsat= 0.0581161176164945 \n",
      "acc for Psat= 0.09293708172109393 \n",
      "acc for optim= 0.15691456215249167\n",
      "Epoch:827/1000\n",
      "Loss on train= 0.008016137406229973\n",
      "Loss on test= 0.01835663430392742\n",
      "acc for Lsat= 0.058991036729680174 \n",
      "acc for Psat= 0.09860538078678979 \n",
      "acc for optim= 0.15447974354028704\n",
      "Epoch:828/1000\n",
      "Loss on train= 0.008331337943673134\n",
      "Loss on test= 0.019126856699585915\n",
      "acc for Lsat= 0.06177765296565162 \n",
      "acc for Psat= 0.09669890668657091 \n",
      "acc for optim= 0.15603432837459777\n",
      "Epoch:829/1000\n",
      "Loss on train= 0.008008302189409733\n",
      "Loss on test= 0.017805177718400955\n",
      "acc for Lsat= 0.06334224119782447 \n",
      "acc for Psat= 0.0927005229724778 \n",
      "acc for optim= 0.15357613820168706\n",
      "Epoch:830/1000\n",
      "Loss on train= 0.008285610005259514\n",
      "Loss on test= 0.017246505245566368\n",
      "acc for Lsat= 0.0729161767496003 \n",
      "acc for Psat= 0.09614023483461805 \n",
      "acc for optim= 0.15642171593175994\n",
      "Epoch:831/1000\n",
      "Loss on train= 0.008386298082768917\n",
      "Loss on test= 0.01770619861781597\n",
      "acc for Lsat= 0.06582315928406185 \n",
      "acc for Psat= 0.09543768001927269 \n",
      "acc for optim= 0.15547906334201497\n",
      "Epoch:832/1000\n",
      "Loss on train= 0.008243418298661709\n",
      "Loss on test= 0.019498197361826897\n",
      "acc for Lsat= 0.06084700574477515 \n",
      "acc for Psat= 0.09867974983321295 \n",
      "acc for optim= 0.15822308849957253\n",
      "Epoch:833/1000\n",
      "Loss on train= 0.008008723147213459\n",
      "Loss on test= 0.018559804186224937\n",
      "acc for Lsat= 0.057690566447046066 \n",
      "acc for Psat= 0.09533741904629603 \n",
      "acc for optim= 0.1550896286136574\n",
      "Epoch:834/1000\n",
      "Loss on train= 0.00837002880871296\n",
      "Loss on test= 0.018255257979035378\n",
      "acc for Lsat= 0.060035128063625764 \n",
      "acc for Psat= 0.09642342560821109 \n",
      "acc for optim= 0.1558040259612931\n",
      "Epoch:835/1000\n",
      "Loss on train= 0.008157655596733093\n",
      "Loss on test= 0.01699819788336754\n",
      "acc for Lsat= 0.06509120364983875 \n",
      "acc for Psat= 0.09414598676893446 \n",
      "acc for optim= 0.15579718997081118\n",
      "Epoch:836/1000\n",
      "Loss on train= 0.007882397621870041\n",
      "Loss on test= 0.017502816393971443\n",
      "acc for Lsat= 0.06656692491637337 \n",
      "acc for Psat= 0.09217225395970875 \n",
      "acc for optim= 0.15404770084553296\n",
      "Epoch:837/1000\n",
      "Loss on train= 0.008137242868542671\n",
      "Loss on test= 0.0185304693877697\n",
      "acc for Lsat= 0.07022629049089221 \n",
      "acc for Psat= 0.10447086956765915 \n",
      "acc for optim= 0.15386388492253095\n",
      "Epoch:838/1000\n",
      "Loss on train= 0.008705985732376575\n",
      "Loss on test= 0.018694676458835602\n",
      "acc for Lsat= 0.06622157759136624 \n",
      "acc for Psat= 0.09644952399863137 \n",
      "acc for optim= 0.15386262941691614\n",
      "Epoch:839/1000\n",
      "Loss on train= 0.008258960209786892\n",
      "Loss on test= 0.01852560229599476\n",
      "acc for Lsat= 0.0598623878426022 \n",
      "acc for Psat= 0.09660094479719798 \n",
      "acc for optim= 0.15627168416976928\n",
      "Epoch:840/1000\n",
      "Loss on train= 0.007927361875772476\n",
      "Loss on test= 0.018687671050429344\n",
      "acc for Lsat= 0.06161974171797435 \n",
      "acc for Psat= 0.09651441673437754 \n",
      "acc for optim= 0.15192983489897516\n",
      "Epoch:841/1000\n",
      "Loss on train= 0.008339522406458855\n",
      "Loss on test= 0.018069827929139137\n",
      "acc for Lsat= 0.06354291852977541 \n",
      "acc for Psat= 0.10109623206986322 \n",
      "acc for optim= 0.1571016450723012\n",
      "Epoch:842/1000\n",
      "Loss on train= 0.008177122101187706\n",
      "Loss on test= 0.018091611564159393\n",
      "acc for Lsat= 0.061582852154970155 \n",
      "acc for Psat= 0.09215978069437875 \n",
      "acc for optim= 0.1555204803744952\n",
      "Epoch:843/1000\n",
      "Loss on train= 0.008211513981223106\n",
      "Loss on test= 0.017384709790349007\n",
      "acc for Lsat= 0.0573135784930653 \n",
      "acc for Psat= 0.09588892476426229 \n",
      "acc for optim= 0.15638138519393074\n",
      "Epoch:844/1000\n",
      "Loss on train= 0.00812629982829094\n",
      "Loss on test= 0.017931077629327774\n",
      "acc for Lsat= 0.06662371373838849 \n",
      "acc for Psat= 0.09869797594017453 \n",
      "acc for optim= 0.15585667433010209\n",
      "Epoch:845/1000\n",
      "Loss on train= 0.008240006864070892\n",
      "Loss on test= 0.019032394513487816\n",
      "acc for Lsat= 0.06315072062942716 \n",
      "acc for Psat= 0.10187773174709744 \n",
      "acc for optim= 0.15727251403861575\n",
      "Epoch:846/1000\n",
      "Loss on train= 0.008405155502259731\n",
      "Loss on test= 0.019385600462555885\n",
      "acc for Lsat= 0.06285546289549934 \n",
      "acc for Psat= 0.09512875427802406 \n",
      "acc for optim= 0.15304886582824923\n",
      "Epoch:847/1000\n",
      "Loss on train= 0.008292768150568008\n",
      "Loss on test= 0.01836242713034153\n",
      "acc for Lsat= 0.057965563899940925 \n",
      "acc for Psat= 0.09700621167818706 \n",
      "acc for optim= 0.15398883629176352\n",
      "Epoch:848/1000\n",
      "Loss on train= 0.008461925201117992\n",
      "Loss on test= 0.01848999410867691\n",
      "acc for Lsat= 0.06011896348661848 \n",
      "acc for Psat= 0.09920168220996858 \n",
      "acc for optim= 0.15726465028193265\n",
      "Epoch:849/1000\n",
      "Loss on train= 0.008088447153568268\n",
      "Loss on test= 0.01906690187752247\n",
      "acc for Lsat= 0.056496536897288424 \n",
      "acc for Psat= 0.09409808284706539 \n",
      "acc for optim= 0.1564828126794762\n",
      "Epoch:850/1000\n",
      "Loss on train= 0.007814696989953518\n",
      "Loss on test= 0.019217489287257195\n",
      "acc for Lsat= 0.0563402714414729 \n",
      "acc for Psat= 0.09403218494521247 \n",
      "acc for optim= 0.15860812018314996\n",
      "Epoch:851/1000\n",
      "Loss on train= 0.008079112507402897\n",
      "Loss on test= 0.017890319228172302\n",
      "acc for Lsat= 0.07015308820539051 \n",
      "acc for Psat= 0.10399684839778477 \n",
      "acc for optim= 0.15340636703703137\n",
      "Epoch:852/1000\n",
      "Loss on train= 0.007936723530292511\n",
      "Loss on test= 0.01846310868859291\n",
      "acc for Lsat= 0.0634930850731002 \n",
      "acc for Psat= 0.09596425692240397 \n",
      "acc for optim= 0.15502999110354315\n",
      "Epoch:853/1000\n",
      "Loss on train= 0.00842541828751564\n",
      "Loss on test= 0.018526026979088783\n",
      "acc for Lsat= 0.06242082781261867 \n",
      "acc for Psat= 0.09575077427758111 \n",
      "acc for optim= 0.15398611616757182\n",
      "Epoch:854/1000\n",
      "Loss on train= 0.008070811629295349\n",
      "Loss on test= 0.0180853009223938\n",
      "acc for Lsat= 0.05878159122334586 \n",
      "acc for Psat= 0.0958846926689148 \n",
      "acc for optim= 0.154935235530138\n",
      "Epoch:855/1000\n",
      "Loss on train= 0.008001619018614292\n",
      "Loss on test= 0.018125884234905243\n",
      "acc for Lsat= 0.05989370263285106 \n",
      "acc for Psat= 0.10000577684905794 \n",
      "acc for optim= 0.15369565263390542\n",
      "Epoch:856/1000\n",
      "Loss on train= 0.008184697479009628\n",
      "Loss on test= 0.018520966172218323\n",
      "acc for Lsat= 0.060991161233849005 \n",
      "acc for Psat= 0.09509897232055664 \n",
      "acc for optim= 0.1551762567626106\n",
      "Epoch:857/1000\n",
      "Loss on train= 0.007850795052945614\n",
      "Loss on test= 0.018368400633335114\n",
      "acc for Lsat= 0.05872949245903228 \n",
      "acc for Psat= 0.09262626998954349 \n",
      "acc for optim= 0.15461018648412492\n",
      "Epoch:858/1000\n",
      "Loss on train= 0.008204704150557518\n",
      "Loss on test= 0.018388669937849045\n",
      "acc for Lsat= 0.059557558430565725 \n",
      "acc for Psat= 0.09582678311400943 \n",
      "acc for optim= 0.1540260405176216\n",
      "Epoch:859/1000\n",
      "Loss on train= 0.008162938989698887\n",
      "Loss on test= 0.01907206140458584\n",
      "acc for Lsat= 0.05944700340429941 \n",
      "acc for Psat= 0.09472321122884748 \n",
      "acc for optim= 0.15774415069156225\n",
      "Epoch:860/1000\n",
      "Loss on train= 0.007878802716732025\n",
      "Loss on test= 0.017921458929777145\n",
      "acc for Lsat= 0.06133305802941323 \n",
      "acc for Psat= 0.09319736311833064 \n",
      "acc for optim= 0.1544470409552256\n",
      "Epoch:861/1000\n",
      "Loss on train= 0.00831921212375164\n",
      "Loss on test= 0.018077081069350243\n",
      "acc for Lsat= 0.05903507355186674 \n",
      "acc for Psat= 0.09468863407770794 \n",
      "acc for optim= 0.15265370599097677\n",
      "Epoch:862/1000\n",
      "Loss on train= 0.007964012213051319\n",
      "Loss on test= 0.01880975253880024\n",
      "acc for Lsat= 0.0572579697602325 \n",
      "acc for Psat= 0.09723397278123432 \n",
      "acc for optim= 0.15627032981978525\n",
      "Epoch:863/1000\n",
      "Loss on train= 0.00785096362233162\n",
      "Loss on test= 0.017979275435209274\n",
      "acc for Lsat= 0.0639599735538165 \n",
      "acc for Psat= 0.10572377310858834 \n",
      "acc for optim= 0.153711464173264\n",
      "Epoch:864/1000\n",
      "Loss on train= 0.008241602219641209\n",
      "Loss on test= 0.018599100410938263\n",
      "acc for Lsat= 0.06316813644435672 \n",
      "acc for Psat= 0.0945989716384146 \n",
      "acc for optim= 0.1538381343914403\n",
      "Epoch:865/1000\n",
      "Loss on train= 0.0078027998097240925\n",
      "Loss on test= 0.018505897372961044\n",
      "acc for Lsat= 0.06295013245609073 \n",
      "acc for Psat= 0.09578526781664955 \n",
      "acc for optim= 0.15414282009005545\n",
      "Epoch:866/1000\n",
      "Loss on train= 0.008011795580387115\n",
      "Loss on test= 0.02023967355489731\n",
      "acc for Lsat= 0.06429270108540852 \n",
      "acc for Psat= 0.09255776653687159 \n",
      "acc for optim= 0.15338800839251945\n",
      "Epoch:867/1000\n",
      "Loss on train= 0.007954501546919346\n",
      "Loss on test= 0.01856359839439392\n",
      "acc for Lsat= 0.0621828291979101 \n",
      "acc for Psat= 0.09641412496566772 \n",
      "acc for optim= 0.15330546034706963\n",
      "Epoch:868/1000\n",
      "Loss on train= 0.008172797970473766\n",
      "Loss on test= 0.018321681767702103\n",
      "acc for Lsat= 0.06349076719747648 \n",
      "acc for Psat= 0.09838814238707225 \n",
      "acc for optim= 0.1579418653415309\n",
      "Epoch:869/1000\n",
      "Loss on train= 0.00820509810000658\n",
      "Loss on test= 0.016328364610671997\n",
      "acc for Lsat= 0.06393890778223674 \n",
      "acc for Psat= 0.09913103646702237 \n",
      "acc for optim= 0.15232390686869623\n",
      "Epoch:870/1000\n",
      "Loss on train= 0.008016510866582394\n",
      "Loss on test= 0.01790710538625717\n",
      "acc for Lsat= 0.0585601470536656 \n",
      "acc for Psat= 0.09492231441868675 \n",
      "acc for optim= 0.15236084668172734\n",
      "Epoch:871/1000\n",
      "Loss on train= 0.008067970164120197\n",
      "Loss on test= 0.017661377787590027\n",
      "acc for Lsat= 0.05953138810065058 \n",
      "acc for Psat= 0.09890916115707822 \n",
      "acc for optim= 0.1535493894583649\n",
      "Epoch:872/1000\n",
      "Loss on train= 0.007874519564211369\n",
      "Loss on test= 0.0199953131377697\n",
      "acc for Lsat= 0.07031831658548779 \n",
      "acc for Psat= 0.09327851649787693 \n",
      "acc for optim= 0.15253103863861822\n",
      "Epoch:873/1000\n",
      "Loss on train= 0.007596838753670454\n",
      "Loss on test= 0.01757880486547947\n",
      "acc for Lsat= 0.06144311949610711 \n",
      "acc for Psat= 0.09829437053865857 \n",
      "acc for optim= 0.15350531596276495\n",
      "Epoch:874/1000\n",
      "Loss on train= 0.007762049324810505\n",
      "Loss on test= 0.017927788197994232\n",
      "acc for Lsat= 0.05592970790134536 \n",
      "acc for Psat= 0.0929468712872929 \n",
      "acc for optim= 0.15161728618873493\n",
      "Epoch:875/1000\n",
      "Loss on train= 0.008011424914002419\n",
      "Loss on test= 0.018410034477710724\n",
      "acc for Lsat= 0.061838648716608685 \n",
      "acc for Psat= 0.0930363568994734 \n",
      "acc for optim= 0.15259433844023282\n",
      "Epoch:876/1000\n",
      "Loss on train= 0.008115749806165695\n",
      "Loss on test= 0.01816585659980774\n",
      "acc for Lsat= 0.059868547320365915 \n",
      "acc for Psat= 0.09797869937287436 \n",
      "acc for optim= 0.1532020981113116\n",
      "Epoch:877/1000\n",
      "Loss on train= 0.00767811993137002\n",
      "Loss on test= 0.017976010218262672\n",
      "acc for Lsat= 0.06090229095684157 \n",
      "acc for Psat= 0.09383112539847689 \n",
      "acc for optim= 0.1530261093543635\n",
      "Epoch:878/1000\n",
      "Loss on train= 0.008039133623242378\n",
      "Loss on test= 0.01844778284430504\n",
      "acc for Lsat= 0.06407559075289303 \n",
      "acc for Psat= 0.09251418958107631 \n",
      "acc for optim= 0.1552699782782131\n",
      "Epoch:879/1000\n",
      "Loss on train= 0.008009832352399826\n",
      "Loss on test= 0.01805048994719982\n",
      "acc for Lsat= 0.06505568789111243 \n",
      "acc for Psat= 0.09818365739451514 \n",
      "acc for optim= 0.1536105800833967\n",
      "Epoch:880/1000\n",
      "Loss on train= 0.008152542635798454\n",
      "Loss on test= 0.018761370331048965\n",
      "acc for Lsat= 0.06389336287975313 \n",
      "acc for Psat= 0.09638150847620436 \n",
      "acc for optim= 0.1548615746200085\n",
      "Epoch:881/1000\n",
      "Loss on train= 0.007836002856492996\n",
      "Loss on test= 0.018029646947979927\n",
      "acc for Lsat= 0.0578839995794826 \n",
      "acc for Psat= 0.09469862116707696 \n",
      "acc for optim= 0.15327799758977362\n",
      "Epoch:882/1000\n",
      "Loss on train= 0.008149406872689724\n",
      "Loss on test= 0.017764240503311157\n",
      "acc for Lsat= 0.06782727357414034 \n",
      "acc for Psat= 0.09755856063630847 \n",
      "acc for optim= 0.1535733034213384\n",
      "Epoch:883/1000\n",
      "Loss on train= 0.008093629963696003\n",
      "Loss on test= 0.018347928300499916\n",
      "acc for Lsat= 0.06506819509797628 \n",
      "acc for Psat= 0.09769116209612952 \n",
      "acc for optim= 0.1540499441325664\n",
      "Epoch:884/1000\n",
      "Loss on train= 0.007779346313327551\n",
      "Loss on test= 0.018180914223194122\n",
      "acc for Lsat= 0.058969659854968395 \n",
      "acc for Psat= 0.09363872508207956 \n",
      "acc for optim= 0.1529714489976565\n",
      "Epoch:885/1000\n",
      "Loss on train= 0.00797838531434536\n",
      "Loss on test= 0.01827777363359928\n",
      "acc for Lsat= 0.061521416902542106 \n",
      "acc for Psat= 0.09562565551863776 \n",
      "acc for optim= 0.1557282535566224\n",
      "Epoch:886/1000\n",
      "Loss on train= 0.0080448929220438\n",
      "Loss on test= 0.01803242787718773\n",
      "acc for Lsat= 0.06078509564201037 \n",
      "acc for Psat= 0.09345428231689665 \n",
      "acc for optim= 0.154188745137718\n",
      "Epoch:887/1000\n",
      "Loss on train= 0.007806137204170227\n",
      "Loss on test= 0.01845426857471466\n",
      "acc for Lsat= 0.06220882228679128 \n",
      "acc for Psat= 0.09906584686703153 \n",
      "acc for optim= 0.1567574054002762\n",
      "Epoch:888/1000\n",
      "Loss on train= 0.008090410381555557\n",
      "Loss on test= 0.018074143677949905\n",
      "acc for Lsat= 0.06066091317269537 \n",
      "acc for Psat= 0.08912943303585054 \n",
      "acc for optim= 0.15407194884286987\n",
      "Epoch:889/1000\n",
      "Loss on train= 0.007767321541905403\n",
      "Loss on test= 0.018853068351745605\n",
      "acc for Lsat= 0.058895316057735014 \n",
      "acc for Psat= 0.09257343643241459 \n",
      "acc for optim= 0.15601085515485869\n",
      "Epoch:890/1000\n",
      "Loss on train= 0.007995737716555595\n",
      "Loss on test= 0.017543986439704895\n",
      "acc for Lsat= 0.06337647429770893 \n",
      "acc for Psat= 0.10511412206623288 \n",
      "acc for optim= 0.15317267245716515\n",
      "Epoch:891/1000\n",
      "Loss on train= 0.007506511639803648\n",
      "Loss on test= 0.018327634781599045\n",
      "acc for Lsat= 0.05741394244962269 \n",
      "acc for Psat= 0.09161588615841336 \n",
      "acc for optim= 0.15339171861608822\n",
      "Epoch:892/1000\n",
      "Loss on train= 0.0077550532296299934\n",
      "Loss on test= 0.017587916925549507\n",
      "acc for Lsat= 0.06082781164182557 \n",
      "acc for Psat= 0.10161103506882985 \n",
      "acc for optim= 0.1528408516612318\n",
      "Epoch:893/1000\n",
      "Loss on train= 0.007789318449795246\n",
      "Loss on test= 0.019822092726826668\n",
      "acc for Lsat= 0.05962697764237722 \n",
      "acc for Psat= 0.0933139153652721 \n",
      "acc for optim= 0.1522571660578251\n",
      "Epoch:894/1000\n",
      "Loss on train= 0.00826845970004797\n",
      "Loss on test= 0.0193085428327322\n",
      "acc for Lsat= 0.06801050346758632 \n",
      "acc for Psat= 0.09610505849123002 \n",
      "acc for optim= 0.1569637283682823\n",
      "Epoch:895/1000\n",
      "Loss on train= 0.008239188231527805\n",
      "Loss on test= 0.01727442443370819\n",
      "acc for Lsat= 0.059384888327784005 \n",
      "acc for Psat= 0.09206437667210898 \n",
      "acc for optim= 0.1515256190465556\n",
      "Epoch:896/1000\n",
      "Loss on train= 0.008244062773883343\n",
      "Loss on test= 0.01814091205596924\n",
      "acc for Lsat= 0.06269609282414118 \n",
      "acc for Psat= 0.09226743347114989 \n",
      "acc for optim= 0.15585177681512302\n",
      "Epoch:897/1000\n",
      "Loss on train= 0.007984996773302555\n",
      "Loss on test= 0.018756305798888206\n",
      "acc for Lsat= 0.06429054422510994 \n",
      "acc for Psat= 0.09768161012066735 \n",
      "acc for optim= 0.1535052441060543\n",
      "Epoch:898/1000\n",
      "Loss on train= 0.008071972988545895\n",
      "Loss on test= 0.018006958067417145\n",
      "acc for Lsat= 0.06360744304127162 \n",
      "acc for Psat= 0.10095362961292266 \n",
      "acc for optim= 0.15485694342189366\n",
      "Epoch:899/1000\n",
      "Loss on train= 0.008026834577322006\n",
      "Loss on test= 0.018328573554754257\n",
      "acc for Lsat= 0.06397285858790078 \n",
      "acc for Psat= 0.09588549087444943 \n",
      "acc for optim= 0.15469968575570317\n",
      "Epoch:900/1000\n",
      "Loss on train= 0.007993943057954311\n",
      "Loss on test= 0.01804470084607601\n",
      "acc for Lsat= 0.06261361555920707 \n",
      "acc for Psat= 0.10248669369353187 \n",
      "acc for optim= 0.15325636598798967\n",
      "Epoch:901/1000\n",
      "Loss on train= 0.00771682383492589\n",
      "Loss on test= 0.016978003084659576\n",
      "acc for Lsat= 0.06646197934945425 \n",
      "acc for Psat= 0.10204858216974472 \n",
      "acc for optim= 0.15336869425243801\n",
      "Epoch:902/1000\n",
      "Loss on train= 0.007862669415771961\n",
      "Loss on test= 0.017797064036130905\n",
      "acc for Lsat= 0.05942165636354023 \n",
      "acc for Psat= 0.09401550094286601 \n",
      "acc for optim= 0.15409054648545056\n",
      "Epoch:903/1000\n",
      "Loss on train= 0.007731716148555279\n",
      "Loss on test= 0.017884589731693268\n",
      "acc for Lsat= 0.06254590219921535 \n",
      "acc for Psat= 0.09391011016236411 \n",
      "acc for optim= 0.15372587483790187\n",
      "Epoch:904/1000\n",
      "Loss on train= 0.008016246370971203\n",
      "Loss on test= 0.018039729446172714\n",
      "acc for Lsat= 0.0617662472029527 \n",
      "acc for Psat= 0.09178321212530136 \n",
      "acc for optim= 0.1570086197720634\n",
      "Epoch:905/1000\n",
      "Loss on train= 0.008082609623670578\n",
      "Loss on test= 0.018433192744851112\n",
      "acc for Lsat= 0.06279919875992669 \n",
      "acc for Psat= 0.09155338091982737 \n",
      "acc for optim= 0.1547028073834048\n",
      "Epoch:906/1000\n",
      "Loss on train= 0.007923300378024578\n",
      "Loss on test= 0.020336832851171494\n",
      "acc for Lsat= 0.06348073648081885 \n",
      "acc for Psat= 0.0967575771941079 \n",
      "acc for optim= 0.15252560186717246\n",
      "Epoch:907/1000\n",
      "Loss on train= 0.007964495569467545\n",
      "Loss on test= 0.018997784703969955\n",
      "acc for Lsat= 0.061542128192053895 \n",
      "acc for Psat= 0.09578348580333922 \n",
      "acc for optim= 0.1545989273322953\n",
      "Epoch:908/1000\n",
      "Loss on train= 0.007706054951995611\n",
      "Loss on test= 0.018979888409376144\n",
      "acc for Lsat= 0.05810283496975899 \n",
      "acc for Psat= 0.0934883624315262 \n",
      "acc for optim= 0.1546365533437994\n",
      "Epoch:909/1000\n",
      "Loss on train= 0.007989303208887577\n",
      "Loss on test= 0.017750486731529236\n",
      "acc for Lsat= 0.058051014029317435 \n",
      "acc for Psat= 0.09149386402633458 \n",
      "acc for optim= 0.15257875472307203\n",
      "Epoch:910/1000\n",
      "Loss on train= 0.008415499702095985\n",
      "Loss on test= 0.01933712512254715\n",
      "acc for Lsat= 0.06243767605887518 \n",
      "acc for Psat= 0.09822078545888263 \n",
      "acc for optim= 0.1539939545094967\n",
      "Epoch:911/1000\n",
      "Loss on train= 0.00792043749243021\n",
      "Loss on test= 0.018758736550807953\n",
      "acc for Lsat= 0.05976557665401034 \n",
      "acc for Psat= 0.10402711298730638 \n",
      "acc for optim= 0.15629236524303755\n",
      "Epoch:912/1000\n",
      "Loss on train= 0.007788253016769886\n",
      "Loss on test= 0.018134554848074913\n",
      "acc for Lsat= 0.07023567540778054 \n",
      "acc for Psat= 0.10685888495710161 \n",
      "acc for optim= 0.15368822962045667\n",
      "Epoch:913/1000\n",
      "Loss on train= 0.007714563515037298\n",
      "Loss on test= 0.018722284585237503\n",
      "acc for Lsat= 0.060857187873787355 \n",
      "acc for Psat= 0.09455037514368693 \n",
      "acc for optim= 0.1549751603768931\n",
      "Epoch:914/1000\n",
      "Loss on train= 0.007923944853246212\n",
      "Loss on test= 0.01860930770635605\n",
      "acc for Lsat= 0.05957348131471211 \n",
      "acc for Psat= 0.09599239081144333 \n",
      "acc for optim= 0.1547037339044942\n",
      "Epoch:915/1000\n",
      "Loss on train= 0.008060459047555923\n",
      "Loss on test= 0.017363715916872025\n",
      "acc for Lsat= 0.05797250908282067 \n",
      "acc for Psat= 0.0959256511595514 \n",
      "acc for optim= 0.15307355779740545\n",
      "Epoch:916/1000\n",
      "Loss on train= 0.007985795848071575\n",
      "Loss on test= 0.017943216487765312\n",
      "acc for Lsat= 0.05846798196434974 \n",
      "acc for Psat= 0.0915597011645635 \n",
      "acc for optim= 0.15387612125939792\n",
      "Epoch:917/1000\n",
      "Loss on train= 0.007697150576859713\n",
      "Loss on test= 0.01774582825601101\n",
      "acc for Lsat= 0.06323552090260716 \n",
      "acc for Psat= 0.0944648179743025 \n",
      "acc for optim= 0.15306269675493242\n",
      "Epoch:918/1000\n",
      "Loss on train= 0.008013098500669003\n",
      "Loss on test= 0.01902456022799015\n",
      "acc for Lsat= 0.06083543002605439 \n",
      "acc for Psat= 0.09918269250128003 \n",
      "acc for optim= 0.152677955561214\n",
      "Epoch:919/1000\n",
      "Loss on train= 0.007884816266596317\n",
      "Loss on test= 0.019152814522385597\n",
      "acc for Lsat= 0.06180424508121279 \n",
      "acc for Psat= 0.09272019482321209 \n",
      "acc for optim= 0.15246615078714157\n",
      "Epoch:920/1000\n",
      "Loss on train= 0.008124720305204391\n",
      "Loss on test= 0.018142661079764366\n",
      "acc for Lsat= 0.06651798105902142 \n",
      "acc for Psat= 0.0954425377978219 \n",
      "acc for optim= 0.15415023888150853\n",
      "Epoch:921/1000\n",
      "Loss on train= 0.008039756678044796\n",
      "Loss on test= 0.01858615316450596\n",
      "acc for Lsat= 0.05837697179781068 \n",
      "acc for Psat= 0.09496558937761519 \n",
      "acc for optim= 0.1537107228404946\n",
      "Epoch:922/1000\n",
      "Loss on train= 0.007930677384138107\n",
      "Loss on test= 0.018104705959558487\n",
      "acc for Lsat= 0.05915161818265914 \n",
      "acc for Psat= 0.09354710661702685 \n",
      "acc for optim= 0.15229131471779614\n",
      "Epoch:923/1000\n",
      "Loss on train= 0.007966462522745132\n",
      "Loss on test= 0.018075602129101753\n",
      "acc for Lsat= 0.05730930277042919 \n",
      "acc for Psat= 0.09285922331942453 \n",
      "acc for optim= 0.15082543103231327\n",
      "Epoch:924/1000\n",
      "Loss on train= 0.007668183650821447\n",
      "Loss on test= 0.019235830754041672\n",
      "acc for Lsat= 0.06171051967475148 \n",
      "acc for Psat= 0.09591430789894528 \n",
      "acc for optim= 0.1529971612824334\n",
      "Epoch:925/1000\n",
      "Loss on train= 0.007565335836261511\n",
      "Loss on test= 0.01786954328417778\n",
      "acc for Lsat= 0.062226916187339366 \n",
      "acc for Psat= 0.09614445302221511 \n",
      "acc for optim= 0.15333310986558593\n",
      "Epoch:926/1000\n",
      "Loss on train= 0.007741676177829504\n",
      "Loss on test= 0.018357858061790466\n",
      "acc for Lsat= 0.05736880501111349 \n",
      "acc for Psat= 0.09119382351636889 \n",
      "acc for optim= 0.15529253830512363\n",
      "Epoch:927/1000\n",
      "Loss on train= 0.007437583524733782\n",
      "Loss on test= 0.0199731532484293\n",
      "acc for Lsat= 0.05915856328275468 \n",
      "acc for Psat= 0.09398521929979325 \n",
      "acc for optim= 0.15405558943748474\n",
      "Epoch:928/1000\n",
      "Loss on train= 0.0075996736995875835\n",
      "Loss on test= 0.01930954121053219\n",
      "acc for Lsat= 0.059315499746137194 \n",
      "acc for Psat= 0.09321410920884875 \n",
      "acc for optim= 0.15258557870984077\n",
      "Epoch:929/1000\n",
      "Loss on train= 0.007872043177485466\n",
      "Loss on test= 0.01765737496316433\n",
      "acc for Lsat= 0.061579050951533855 \n",
      "acc for Psat= 0.10138150023089515 \n",
      "acc for optim= 0.1526192893584569\n",
      "Epoch:930/1000\n",
      "Loss on train= 0.008148928172886372\n",
      "Loss on test= 0.018532393500208855\n",
      "acc for Lsat= 0.064009885986646 \n",
      "acc for Psat= 0.09564449522230362 \n",
      "acc for optim= 0.15076806288626463\n",
      "Epoch:931/1000\n",
      "Loss on train= 0.007901106029748917\n",
      "Loss on test= 0.017367353662848473\n",
      "acc for Lsat= 0.05986211639311578 \n",
      "acc for Psat= 0.0935768781436814 \n",
      "acc for optim= 0.15609733247094684\n",
      "Epoch:932/1000\n",
      "Loss on train= 0.007615493144840002\n",
      "Loss on test= 0.01891465112566948\n",
      "acc for Lsat= 0.06313697977198496 \n",
      "acc for Psat= 0.09659072822994658 \n",
      "acc for optim= 0.14988392723931204\n",
      "Epoch:933/1000\n",
      "Loss on train= 0.00788901373744011\n",
      "Loss on test= 0.017416831105947495\n",
      "acc for Lsat= 0.05878651837507884 \n",
      "acc for Psat= 0.09457753830485875 \n",
      "acc for optim= 0.15500747751858499\n",
      "Epoch:934/1000\n",
      "Loss on train= 0.0078040058724582195\n",
      "Loss on test= 0.019113346934318542\n",
      "acc for Lsat= 0.059190567996766835 \n",
      "acc for Psat= 0.09397366940975188 \n",
      "acc for optim= 0.1544078796274132\n",
      "Epoch:935/1000\n",
      "Loss on train= 0.007742021698504686\n",
      "Loss on test= 0.018611565232276917\n",
      "acc for Lsat= 0.059103761199447824 \n",
      "acc for Psat= 0.10187526709503598 \n",
      "acc for optim= 0.15270992691318197\n",
      "Epoch:936/1000\n",
      "Loss on train= 0.007869470864534378\n",
      "Loss on test= 0.018404170870780945\n",
      "acc for Lsat= 0.06365406711896261 \n",
      "acc for Psat= 0.09965627491474154 \n",
      "acc for optim= 0.15125934680302935\n",
      "Epoch:937/1000\n",
      "Loss on train= 0.008026773110032082\n",
      "Loss on test= 0.01879062131047249\n",
      "acc for Lsat= 0.06027451679110526 \n",
      "acc for Psat= 0.09266949362225003 \n",
      "acc for optim= 0.1520385541849666\n",
      "Epoch:938/1000\n",
      "Loss on train= 0.00738143315538764\n",
      "Loss on test= 0.018840104341506958\n",
      "acc for Lsat= 0.060928962462478205 \n",
      "acc for Psat= 0.09317187617222468 \n",
      "acc for optim= 0.15244782856769032\n",
      "Epoch:939/1000\n",
      "Loss on train= 0.00790516845881939\n",
      "Loss on test= 0.019576098769903183\n",
      "acc for Lsat= 0.059957206994295124 \n",
      "acc for Psat= 0.09394509461190965 \n",
      "acc for optim= 0.15248205297523074\n",
      "Epoch:940/1000\n",
      "Loss on train= 0.0075710793025791645\n",
      "Loss on test= 0.018218187615275383\n",
      "acc for Lsat= 0.06558635416958067 \n",
      "acc for Psat= 0.09943347556723489 \n",
      "acc for optim= 0.15043212713466753\n",
      "Epoch:941/1000\n",
      "Loss on train= 0.007848355919122696\n",
      "Loss on test= 0.018845709040760994\n",
      "acc for Lsat= 0.06173992206652959 \n",
      "acc for Psat= 0.0972480124897427 \n",
      "acc for optim= 0.15394283897346922\n",
      "Epoch:942/1000\n",
      "Loss on train= 0.007748723495751619\n",
      "Loss on test= 0.017614468932151794\n",
      "acc for Lsat= 0.059582180943754004 \n",
      "acc for Psat= 0.09710259868039024 \n",
      "acc for optim= 0.15325927494300737\n",
      "Epoch:943/1000\n",
      "Loss on train= 0.007855587638914585\n",
      "Loss on test= 0.01817409321665764\n",
      "acc for Lsat= 0.06107681261168588 \n",
      "acc for Psat= 0.09295527223083709 \n",
      "acc for optim= 0.15105561663707098\n",
      "Epoch:944/1000\n",
      "Loss on train= 0.007980016060173512\n",
      "Loss on test= 0.019062962383031845\n",
      "acc for Lsat= 0.06046880533297857 \n",
      "acc for Psat= 0.0913185116317537 \n",
      "acc for optim= 0.15193308393160498\n",
      "Epoch:945/1000\n",
      "Loss on train= 0.007966467179358006\n",
      "Loss on test= 0.018587911501526833\n",
      "acc for Lsat= 0.06344433575868606 \n",
      "acc for Psat= 0.09384405712286631 \n",
      "acc for optim= 0.152833105954859\n",
      "Epoch:946/1000\n",
      "Loss on train= 0.007777280639857054\n",
      "Loss on test= 0.019184863194823265\n",
      "acc for Lsat= 0.06523669991228315 \n",
      "acc for Psat= 0.09610308408737184 \n",
      "acc for optim= 0.15303110116057927\n",
      "Epoch:947/1000\n",
      "Loss on train= 0.007518371567130089\n",
      "Loss on test= 0.018062161281704903\n",
      "acc for Lsat= 0.05979812244574228 \n",
      "acc for Psat= 0.09940649701489342 \n",
      "acc for optim= 0.15427702483203679\n",
      "Epoch:948/1000\n",
      "Loss on train= 0.007775614969432354\n",
      "Loss on test= 0.0184725783765316\n",
      "acc for Lsat= 0.06569081064727572 \n",
      "acc for Psat= 0.09349172893497681 \n",
      "acc for optim= 0.15529703365431896\n",
      "Epoch:949/1000\n",
      "Loss on train= 0.007953205145895481\n",
      "Loss on test= 0.019252844154834747\n",
      "acc for Lsat= 0.06782675716612073 \n",
      "acc for Psat= 0.10356441703107623 \n",
      "acc for optim= 0.15365465639366047\n",
      "Epoch:950/1000\n",
      "Loss on train= 0.007480427622795105\n",
      "Loss on test= 0.01733468845486641\n",
      "acc for Lsat= 0.06221576705574989 \n",
      "acc for Psat= 0.09594630168543922 \n",
      "acc for optim= 0.15266793179843163\n",
      "Epoch:951/1000\n",
      "Loss on train= 0.007573497015982866\n",
      "Loss on test= 0.018150566145777702\n",
      "acc for Lsat= 0.06794489605559244 \n",
      "acc for Psat= 0.09843013617727492 \n",
      "acc for optim= 0.15418816399243143\n",
      "Epoch:952/1000\n",
      "Loss on train= 0.008031947538256645\n",
      "Loss on test= 0.019332585856318474\n",
      "acc for Lsat= 0.05771039318707253 \n",
      "acc for Psat= 0.09670712384912704 \n",
      "acc for optim= 0.15364527752002077\n",
      "Epoch:953/1000\n",
      "Loss on train= 0.00787399336695671\n",
      "Loss on test= 0.018801789730787277\n",
      "acc for Lsat= 0.061028169923358494 \n",
      "acc for Psat= 0.09511105683114794 \n",
      "acc for optim= 0.15183436440096965\n",
      "Epoch:954/1000\n",
      "Loss on train= 0.008222006261348724\n",
      "Loss on test= 0.018348852172493935\n",
      "acc for Lsat= 0.06222908364401923 \n",
      "acc for Psat= 0.09251063764095306 \n",
      "acc for optim= 0.15339246177011068\n",
      "Epoch:955/1000\n",
      "Loss on train= 0.00808802805840969\n",
      "Loss on test= 0.018183626234531403\n",
      "acc for Lsat= 0.059575843645466704 \n",
      "acc for Psat= 0.09320558889044656 \n",
      "acc for optim= 0.1542784847319126\n",
      "Epoch:956/1000\n",
      "Loss on train= 0.0077248853631317616\n",
      "Loss on test= 0.018418902531266212\n",
      "acc for Lsat= 0.06070517814821668 \n",
      "acc for Psat= 0.09309029347366755 \n",
      "acc for optim= 0.1536391206085682\n",
      "Epoch:957/1000\n",
      "Loss on train= 0.007439709734171629\n",
      "Loss on test= 0.01902122050523758\n",
      "acc for Lsat= 0.05975847269097964 \n",
      "acc for Psat= 0.09772869414753384 \n",
      "acc for optim= 0.15239200649989973\n",
      "Epoch:958/1000\n",
      "Loss on train= 0.007784002926200628\n",
      "Loss on test= 0.01824064739048481\n",
      "acc for Lsat= 0.06347122506962882 \n",
      "acc for Psat= 0.10183140966627333 \n",
      "acc for optim= 0.15471249280704394\n",
      "Epoch:959/1000\n",
      "Loss on train= 0.007641639094799757\n",
      "Loss on test= 0.019117897376418114\n",
      "acc for Lsat= 0.06646862443950441 \n",
      "acc for Psat= 0.10333497888512083 \n",
      "acc for optim= 0.1536655358142323\n",
      "Epoch:960/1000\n",
      "Loss on train= 0.00791849847882986\n",
      "Loss on test= 0.019771074876189232\n",
      "acc for Lsat= 0.05712189243899451 \n",
      "acc for Psat= 0.09356865882873536 \n",
      "acc for optim= 0.15615405241648356\n",
      "Epoch:961/1000\n",
      "Loss on train= 0.007949196733534336\n",
      "Loss on test= 0.017978547140955925\n",
      "acc for Lsat= 0.06674732491374016 \n",
      "acc for Psat= 0.10259901318285201 \n",
      "acc for optim= 0.15429135287801427\n",
      "Epoch:962/1000\n",
      "Loss on train= 0.007880649529397488\n",
      "Loss on test= 0.016767673194408417\n",
      "acc for Lsat= 0.05663370274835163 \n",
      "acc for Psat= 0.09115075750483408 \n",
      "acc for optim= 0.15449624500340886\n",
      "Epoch:963/1000\n",
      "Loss on train= 0.008074392564594746\n",
      "Loss on test= 0.018766816705465317\n",
      "acc for Lsat= 0.058949819455544165 \n",
      "acc for Psat= 0.09369472149345609 \n",
      "acc for optim= 0.1517601552108923\n",
      "Epoch:964/1000\n",
      "Loss on train= 0.007883381098508835\n",
      "Loss on test= 0.018528630957007408\n",
      "acc for Lsat= 0.05911787243352995 \n",
      "acc for Psat= 0.0914414515097936 \n",
      "acc for optim= 0.152794102496571\n",
      "Epoch:965/1000\n",
      "Loss on train= 0.007381318137049675\n",
      "Loss on test= 0.019175659865140915\n",
      "acc for Lsat= 0.06210011591513952 \n",
      "acc for Psat= 0.09259443630774815 \n",
      "acc for optim= 0.1550408120784494\n",
      "Epoch:966/1000\n",
      "Loss on train= 0.008051188662648201\n",
      "Loss on test= 0.018231941387057304\n",
      "acc for Lsat= 0.05711411552296745 \n",
      "acc for Psat= 0.0939463292558988 \n",
      "acc for optim= 0.15140982965628308\n",
      "Epoch:967/1000\n",
      "Loss on train= 0.0076923370361328125\n",
      "Loss on test= 0.018347086384892464\n",
      "acc for Lsat= 0.0601695779297087 \n",
      "acc for Psat= 0.09408740682734383 \n",
      "acc for optim= 0.15330490536159946\n",
      "Epoch:968/1000\n",
      "Loss on train= 0.007807495072484016\n",
      "Loss on test= 0.01858336664736271\n",
      "acc for Lsat= 0.060837022711833305 \n",
      "acc for Psat= 0.09456964317295286 \n",
      "acc for optim= 0.1538546486033333\n",
      "Epoch:969/1000\n",
      "Loss on train= 0.007602788042277098\n",
      "Loss on test= 0.017050212249159813\n",
      "acc for Lsat= 0.07148014787170621 \n",
      "acc for Psat= 0.09593405971924462 \n",
      "acc for optim= 0.15352945948640503\n",
      "Epoch:970/1000\n",
      "Loss on train= 0.007407449185848236\n",
      "Loss on test= 0.017693059518933296\n",
      "acc for Lsat= 0.05776556357741355 \n",
      "acc for Psat= 0.09422617024845546 \n",
      "acc for optim= 0.15198614738053748\n",
      "Epoch:971/1000\n",
      "Loss on train= 0.00806147139519453\n",
      "Loss on test= 0.017824891954660416\n",
      "acc for Lsat= 0.0625417403048939 \n",
      "acc for Psat= 0.09342456691794926 \n",
      "acc for optim= 0.1531358920865589\n",
      "Epoch:972/1000\n",
      "Loss on train= 0.0078113083727657795\n",
      "Loss on test= 0.018028564751148224\n",
      "acc for Lsat= 0.0617513364387883 \n",
      "acc for Psat= 0.09660084678067102 \n",
      "acc for optim= 0.15237912494275307\n",
      "Epoch:973/1000\n",
      "Loss on train= 0.007821720093488693\n",
      "Loss on test= 0.0178548451513052\n",
      "acc for Lsat= 0.060758647074302036 \n",
      "acc for Psat= 0.09575061202049256 \n",
      "acc for optim= 0.1542277787294653\n",
      "Epoch:974/1000\n",
      "Loss on train= 0.007606396451592445\n",
      "Loss on test= 0.017776064574718475\n",
      "acc for Lsat= 0.05867297227183979 \n",
      "acc for Psat= 0.09300776802831227 \n",
      "acc for optim= 0.15266444252596964\n",
      "Epoch:975/1000\n",
      "Loss on train= 0.0077187116257846355\n",
      "Loss on test= 0.019499216228723526\n",
      "acc for Lsat= 0.06681382771995333 \n",
      "acc for Psat= 0.10191520717408922 \n",
      "acc for optim= 0.1547509964141581\n",
      "Epoch:976/1000\n",
      "Loss on train= 0.007539201062172651\n",
      "Loss on test= 0.018196117132902145\n",
      "acc for Lsat= 0.06365637307365736 \n",
      "acc for Psat= 0.09775508907106188 \n",
      "acc for optim= 0.15179040225015747\n",
      "Epoch:977/1000\n",
      "Loss on train= 0.007746837567538023\n",
      "Loss on test= 0.01676807925105095\n",
      "acc for Lsat= 0.057621920357147846 \n",
      "acc for Psat= 0.09242894517050851 \n",
      "acc for optim= 0.15114337379733722\n",
      "Epoch:978/1000\n",
      "Loss on train= 0.00792691484093666\n",
      "Loss on test= 0.018230445683002472\n",
      "acc for Lsat= 0.06408770183722177 \n",
      "acc for Psat= 0.09728488922119141 \n",
      "acc for optim= 0.15181201663282182\n",
      "Epoch:979/1000\n",
      "Loss on train= 0.007690588943660259\n",
      "Loss on test= 0.017681119963526726\n",
      "acc for Lsat= 0.06087700931562318 \n",
      "acc for Psat= 0.0952848505642679 \n",
      "acc for optim= 0.15139845079845854\n",
      "Epoch:980/1000\n",
      "Loss on train= 0.0077779293060302734\n",
      "Loss on test= 0.019423790276050568\n",
      "acc for Lsat= 0.06013106140825483 \n",
      "acc for Psat= 0.09214084148406983 \n",
      "acc for optim= 0.1511013063291709\n",
      "Epoch:981/1000\n",
      "Loss on train= 0.007832515984773636\n",
      "Loss on test= 0.017532378435134888\n",
      "acc for Lsat= 0.06383766987257533 \n",
      "acc for Psat= 0.10219616128338709 \n",
      "acc for optim= 0.15582855310704974\n",
      "Epoch:982/1000\n",
      "Loss on train= 0.0077451313845813274\n",
      "Loss on test= 0.017419852316379547\n",
      "acc for Lsat= 0.05675798580050468 \n",
      "acc for Psat= 0.09369908505015902 \n",
      "acc for optim= 0.1522560559213162\n",
      "Epoch:983/1000\n",
      "Loss on train= 0.007755361497402191\n",
      "Loss on test= 0.018856020644307137\n",
      "acc for Lsat= 0.05816275121437179 \n",
      "acc for Psat= 0.09327347063355976 \n",
      "acc for optim= 0.15450805185569658\n",
      "Epoch:984/1000\n",
      "Loss on train= 0.007488250266760588\n",
      "Loss on test= 0.01774912327528\n",
      "acc for Lsat= 0.05686761405732896 \n",
      "acc for Psat= 0.09052176276842755 \n",
      "acc for optim= 0.15221364026268322\n",
      "Epoch:985/1000\n",
      "Loss on train= 0.007748420815914869\n",
      "Loss on test= 0.018821826204657555\n",
      "acc for Lsat= 0.05642203895582094 \n",
      "acc for Psat= 0.09012057582537333 \n",
      "acc for optim= 0.153067868285709\n",
      "Epoch:986/1000\n",
      "Loss on train= 0.007679663598537445\n",
      "Loss on test= 0.017001567408442497\n",
      "acc for Lsat= 0.057260452873177 \n",
      "acc for Psat= 0.08954292353656555 \n",
      "acc for optim= 0.15141259415282143\n",
      "Epoch:987/1000\n",
      "Loss on train= 0.007463802117854357\n",
      "Loss on test= 0.01757493056356907\n",
      "acc for Lsat= 0.058457850664854046 \n",
      "acc for Psat= 0.0881073491440879 \n",
      "acc for optim= 0.15268171371685135\n",
      "Epoch:988/1000\n",
      "Loss on train= 0.007703155279159546\n",
      "Loss on test= 0.02021397091448307\n",
      "acc for Lsat= 0.0590608490837945 \n",
      "acc for Psat= 0.09127628919151093 \n",
      "acc for optim= 0.15461659398343827\n",
      "Epoch:989/1000\n",
      "Loss on train= 0.007680720183998346\n",
      "Loss on test= 0.019884340465068817\n",
      "acc for Lsat= 0.06137310730086433 \n",
      "acc for Psat= 0.09677012960116067 \n",
      "acc for optim= 0.1539815333982309\n",
      "Epoch:990/1000\n",
      "Loss on train= 0.00753041310235858\n",
      "Loss on test= 0.018512586131691933\n",
      "acc for Lsat= 0.06901628623406092 \n",
      "acc for Psat= 0.09305230908923678 \n",
      "acc for optim= 0.15282196982039345\n",
      "Epoch:991/1000\n",
      "Loss on train= 0.0076586962677538395\n",
      "Loss on test= 0.01727018505334854\n",
      "acc for Lsat= 0.0625270124938753 \n",
      "acc for Psat= 0.09582159337070252 \n",
      "acc for optim= 0.1524701197942098\n",
      "Epoch:992/1000\n",
      "Loss on train= 0.0074785933829844\n",
      "Loss on test= 0.018789634108543396\n",
      "acc for Lsat= 0.06029409534401364 \n",
      "acc for Psat= 0.09329936322238712 \n",
      "acc for optim= 0.1549191147916847\n",
      "Epoch:993/1000\n",
      "Loss on train= 0.007421006448566914\n",
      "Loss on test= 0.01817966252565384\n",
      "acc for Lsat= 0.06005904889769024 \n",
      "acc for Psat= 0.09377636992269092 \n",
      "acc for optim= 0.15458679157826638\n",
      "Epoch:994/1000\n",
      "Loss on train= 0.007834861986339092\n",
      "Loss on test= 0.01846083253622055\n",
      "acc for Lsat= 0.06121340418855349 \n",
      "acc for Psat= 0.093300841583146 \n",
      "acc for optim= 0.15241049147314498\n",
      "Epoch:995/1000\n",
      "Loss on train= 0.007567522116005421\n",
      "Loss on test= 0.018312830477952957\n",
      "acc for Lsat= 0.06012732121679519 \n",
      "acc for Psat= 0.09174113406075371 \n",
      "acc for optim= 0.15312424997488658\n",
      "Epoch:996/1000\n",
      "Loss on train= 0.007824108935892582\n",
      "Loss on test= 0.018515324220061302\n",
      "acc for Lsat= 0.05739475977089668 \n",
      "acc for Psat= 0.09065314514769449 \n",
      "acc for optim= 0.15486945766541693\n",
      "Epoch:997/1000\n",
      "Loss on train= 0.007673414424061775\n",
      "Loss on test= 0.017178993672132492\n",
      "acc for Lsat= 0.06163935015598933 \n",
      "acc for Psat= 0.0914968224035369 \n",
      "acc for optim= 0.15231742800937764\n",
      "Epoch:998/1000\n",
      "Loss on train= 0.008135339245200157\n",
      "Loss on test= 0.018160060048103333\n",
      "acc for Lsat= 0.06585011300113466 \n",
      "acc for Psat= 0.10563000473711227 \n",
      "acc for optim= 0.1533977452251646\n",
      "Epoch:999/1000\n",
      "Loss on train= 0.007540635298937559\n",
      "Loss on test= 0.017302105203270912\n",
      "acc for Lsat= 0.06654164161947039 \n",
      "acc for Psat= 0.09819401188029182 \n",
      "acc for optim= 0.1554193466901779\n",
      "Epoch:1000/1000\n",
      "Loss on train= 0.007632249966263771\n",
      "Loss on test= 0.01883067563176155\n",
      "acc for Lsat= 0.06264433422022396 \n",
      "acc for Psat= 0.09691910843054455 \n",
      "acc for optim= 0.15435176508294213\n",
      "Fold 2\n",
      "Epoch:1/1000\n",
      "Loss on train= 0.1991071105003357\n",
      "Loss on test= 0.1009916365146637\n",
      "acc for Lsat= 0.4308173583613502 \n",
      "acc for Psat= 0.5474297040038638 \n",
      "acc for optim= 0.17670894724627337\n",
      "Epoch:2/1000\n",
      "Loss on train= 0.08531062304973602\n",
      "Loss on test= 0.07033983618021011\n",
      "acc for Lsat= 0.2837568311227693 \n",
      "acc for Psat= 0.5277474092112648 \n",
      "acc for optim= 0.18544966909620497\n",
      "Epoch:3/1000\n",
      "Loss on train= 0.06940539926290512\n",
      "Loss on test= 0.06518688052892685\n",
      "acc for Lsat= 0.2643920693132612 \n",
      "acc for Psat= 0.5303454717000327 \n",
      "acc for optim= 0.17700236820512347\n",
      "Epoch:4/1000\n",
      "Loss on train= 0.060297880321741104\n",
      "Loss on test= 0.0606602281332016\n",
      "acc for Lsat= 0.25978761017322544 \n",
      "acc for Psat= 0.42921937108039854 \n",
      "acc for optim= 0.15628623734745714\n",
      "Epoch:5/1000\n",
      "Loss on train= 0.05819222703576088\n",
      "Loss on test= 0.061212871223688126\n",
      "acc for Lsat= 0.2876034769746993 \n",
      "acc for Psat= 0.4677884260813394 \n",
      "acc for optim= 0.16346746848689186\n",
      "Epoch:6/1000\n",
      "Loss on train= 0.055921971797943115\n",
      "Loss on test= 0.06054983660578728\n",
      "acc for Lsat= 0.24506455742650562 \n",
      "acc for Psat= 0.5411192986700271 \n",
      "acc for optim= 0.1621935373379125\n",
      "Epoch:7/1000\n",
      "Loss on train= 0.0560452900826931\n",
      "Loss on test= 0.053504593670368195\n",
      "acc for Lsat= 0.2604811641905043 \n",
      "acc for Psat= 0.36545632448461324 \n",
      "acc for optim= 0.15600358131859038\n",
      "Epoch:8/1000\n",
      "Loss on train= 0.052964888513088226\n",
      "Loss on test= 0.05171044170856476\n",
      "acc for Lsat= 0.24834738729728592 \n",
      "acc for Psat= 0.40653750499089564 \n",
      "acc for optim= 0.15773771941247913\n",
      "Epoch:9/1000\n",
      "Loss on train= 0.0523822195827961\n",
      "Loss on test= 0.0507596917450428\n",
      "acc for Lsat= 0.2508674500303136 \n",
      "acc for Psat= 0.3405084835158454 \n",
      "acc for optim= 0.15409494597050877\n",
      "Epoch:10/1000\n",
      "Loss on train= 0.05110107734799385\n",
      "Loss on test= 0.05216982215642929\n",
      "acc for Lsat= 0.2559603608316846 \n",
      "acc for Psat= 0.45608461499214176 \n",
      "acc for optim= 0.15908131007519033\n",
      "Epoch:11/1000\n",
      "Loss on train= 0.05154251679778099\n",
      "Loss on test= 0.04524831473827362\n",
      "acc for Lsat= 0.26988403710226216 \n",
      "acc for Psat= 0.3300890849696265 \n",
      "acc for optim= 0.15828285598092606\n",
      "Epoch:12/1000\n",
      "Loss on train= 0.047396112233400345\n",
      "Loss on test= 0.04462913051247597\n",
      "acc for Lsat= 0.2388811879687839 \n",
      "acc for Psat= 0.3345548407899009 \n",
      "acc for optim= 0.15983824100759297\n",
      "Epoch:13/1000\n",
      "Loss on train= 0.04710165038704872\n",
      "Loss on test= 0.04559389129281044\n",
      "acc for Lsat= 0.25051311552524563 \n",
      "acc for Psat= 0.3814614176750183 \n",
      "acc for optim= 0.1549498306380378\n",
      "Epoch:14/1000\n",
      "Loss on train= 0.04807703197002411\n",
      "Loss on test= 0.04127267003059387\n",
      "acc for Lsat= 0.24139768729607264 \n",
      "acc for Psat= 0.3209598448541429 \n",
      "acc for optim= 0.15850991110006968\n",
      "Epoch:15/1000\n",
      "Loss on train= 0.04674464464187622\n",
      "Loss on test= 0.046881888061761856\n",
      "acc for Lsat= 0.24014591243531969 \n",
      "acc for Psat= 0.3661511533790164 \n",
      "acc for optim= 0.1842678871419695\n",
      "Epoch:16/1000\n",
      "Loss on train= 0.04543658345937729\n",
      "Loss on test= 0.04379059374332428\n",
      "acc for Lsat= 0.24941856993569267 \n",
      "acc for Psat= 0.3022396398915185 \n",
      "acc for optim= 0.1587215348250336\n",
      "Epoch:17/1000\n",
      "Loss on train= 0.04475517198443413\n",
      "Loss on test= 0.038405030965805054\n",
      "acc for Lsat= 0.2453481977184613 \n",
      "acc for Psat= 0.31470183630784354 \n",
      "acc for optim= 0.1558495998382568\n",
      "Epoch:18/1000\n",
      "Loss on train= 0.04865371063351631\n",
      "Loss on test= 0.04263196885585785\n",
      "acc for Lsat= 0.24158022718297115 \n",
      "acc for Psat= 0.3463581353425979 \n",
      "acc for optim= 0.15512371491640808\n",
      "Epoch:19/1000\n",
      "Loss on train= 0.04504359886050224\n",
      "Loss on test= 0.03971242159605026\n",
      "acc for Lsat= 0.2340460873726341 \n",
      "acc for Psat= 0.3059480624066459 \n",
      "acc for optim= 0.15560521487560539\n",
      "Epoch:20/1000\n",
      "Loss on train= 0.04383579269051552\n",
      "Loss on test= 0.038515809923410416\n",
      "acc for Lsat= 0.23683434393670824 \n",
      "acc for Psat= 0.29698741121424566 \n",
      "acc for optim= 0.15490826304174127\n",
      "Epoch:21/1000\n",
      "Loss on train= 0.044452372938394547\n",
      "Loss on test= 0.03900967538356781\n",
      "acc for Lsat= 0.25090432717568345 \n",
      "acc for Psat= 0.29047943486107725 \n",
      "acc for optim= 0.1533692276519206\n",
      "Epoch:22/1000\n",
      "Loss on train= 0.042500127106904984\n",
      "Loss on test= 0.038773614913225174\n",
      "acc for Lsat= 0.23112478661868308 \n",
      "acc for Psat= 0.33216281665696035 \n",
      "acc for optim= 0.15442485265019865\n",
      "Epoch:23/1000\n",
      "Loss on train= 0.0438501201570034\n",
      "Loss on test= 0.03975958377122879\n",
      "acc for Lsat= 0.25707556092076833 \n",
      "acc for Psat= 0.2860883815420998 \n",
      "acc for optim= 0.15501205057112707\n",
      "Epoch:24/1000\n",
      "Loss on train= 0.04274918511509895\n",
      "Loss on test= 0.037523139268159866\n",
      "acc for Lsat= 0.23058876262770756 \n",
      "acc for Psat= 0.2916756040520138 \n",
      "acc for optim= 0.1597677742648456\n",
      "Epoch:25/1000\n",
      "Loss on train= 0.041339341551065445\n",
      "Loss on test= 0.04079810157418251\n",
      "acc for Lsat= 0.24937227028939457 \n",
      "acc for Psat= 0.2878265480200449 \n",
      "acc for optim= 0.1618479591276911\n",
      "Epoch:26/1000\n",
      "Loss on train= 0.04291178286075592\n",
      "Loss on test= 0.03918199613690376\n",
      "acc for Lsat= 0.23294957975546518 \n",
      "acc for Psat= 0.2836723301145765 \n",
      "acc for optim= 0.15632105962269835\n",
      "Epoch:27/1000\n",
      "Loss on train= 0.040832988917827606\n",
      "Loss on test= 0.03914657607674599\n",
      "acc for Lsat= 0.23575798355870775 \n",
      "acc for Psat= 0.2955871625079049 \n",
      "acc for optim= 0.154069917028149\n",
      "Epoch:28/1000\n",
      "Loss on train= 0.03965248167514801\n",
      "Loss on test= 0.03883158415555954\n",
      "acc for Lsat= 0.2390790542794598 \n",
      "acc for Psat= 0.276996644337972 \n",
      "acc for optim= 0.1518516431459122\n",
      "Epoch:29/1000\n",
      "Loss on train= 0.03895091637969017\n",
      "Loss on test= 0.035100921988487244\n",
      "acc for Lsat= 0.22590459610025088 \n",
      "acc for Psat= 0.2823598368300332 \n",
      "acc for optim= 0.16312514212396406\n",
      "Epoch:30/1000\n",
      "Loss on train= 0.0405275821685791\n",
      "Loss on test= 0.03860178589820862\n",
      "acc for Lsat= 0.22150615337822172 \n",
      "acc for Psat= 0.287187221315172 \n",
      "acc for optim= 0.15370844105879466\n",
      "Epoch:31/1000\n",
      "Loss on train= 0.03989936783909798\n",
      "Loss on test= 0.03911081328988075\n",
      "acc for Lsat= 0.22945760579572783 \n",
      "acc for Psat= 0.3387566106186972 \n",
      "acc for optim= 0.1544703475717041\n",
      "Epoch:32/1000\n",
      "Loss on train= 0.039420828223228455\n",
      "Loss on test= 0.03863079100847244\n",
      "acc for Lsat= 0.22771421132816214 \n",
      "acc for Psat= 0.2712062771121661 \n",
      "acc for optim= 0.1580245734916793\n",
      "Epoch:33/1000\n",
      "Loss on train= 0.039180442690849304\n",
      "Loss on test= 0.03529878333210945\n",
      "acc for Lsat= 0.2294868757327398 \n",
      "acc for Psat= 0.2711368958155314 \n",
      "acc for optim= 0.15648241233494545\n",
      "Epoch:34/1000\n",
      "Loss on train= 0.038466669619083405\n",
      "Loss on test= 0.035290539264678955\n",
      "acc for Lsat= 0.22325553943713505 \n",
      "acc for Psat= 0.2827075752947065 \n",
      "acc for optim= 0.15720121240657237\n",
      "Epoch:35/1000\n",
      "Loss on train= 0.039106354117393494\n",
      "Loss on test= 0.03408489376306534\n",
      "acc for Lsat= 0.24910617727372383 \n",
      "acc for Psat= 0.26324003173245325 \n",
      "acc for optim= 0.15581746651894515\n",
      "Epoch:36/1000\n",
      "Loss on train= 0.0388597808778286\n",
      "Loss on test= 0.03770025074481964\n",
      "acc for Lsat= 0.23444188684225084 \n",
      "acc for Psat= 0.2832516239749061 \n",
      "acc for optim= 0.15913660277922947\n",
      "Epoch:37/1000\n",
      "Loss on train= 0.03934448957443237\n",
      "Loss on test= 0.038057420402765274\n",
      "acc for Lsat= 0.2235781999925772 \n",
      "acc for Psat= 0.32606034460994926 \n",
      "acc for optim= 0.1632668005095588\n",
      "Epoch:38/1000\n",
      "Loss on train= 0.038980308920145035\n",
      "Loss on test= 0.037217553704977036\n",
      "acc for Lsat= 0.22006858612100277 \n",
      "acc for Psat= 0.2798365276720789 \n",
      "acc for optim= 0.1632461691896121\n",
      "Epoch:39/1000\n",
      "Loss on train= 0.03810664638876915\n",
      "Loss on test= 0.035514090210199356\n",
      "acc for Lsat= 0.2201943653739161 \n",
      "acc for Psat= 0.28439427746666796 \n",
      "acc for optim= 0.15501649158282413\n",
      "Epoch:40/1000\n",
      "Loss on train= 0.03564075008034706\n",
      "Loss on test= 0.03501829877495766\n",
      "acc for Lsat= 0.22913897815677858 \n",
      "acc for Psat= 0.29603288206789224 \n",
      "acc for optim= 0.15465865737448134\n",
      "Epoch:41/1000\n",
      "Loss on train= 0.03603678569197655\n",
      "Loss on test= 0.03700998052954674\n",
      "acc for Lsat= 0.23450886532664297 \n",
      "acc for Psat= 0.2589447375800875 \n",
      "acc for optim= 0.1534154068678618\n",
      "Epoch:42/1000\n",
      "Loss on train= 0.03666483983397484\n",
      "Loss on test= 0.03465912118554115\n",
      "acc for Lsat= 0.2201762913001908 \n",
      "acc for Psat= 0.31644985642698076 \n",
      "acc for optim= 0.15749893184337355\n",
      "Epoch:43/1000\n",
      "Loss on train= 0.03667493164539337\n",
      "Loss on test= 0.03499817103147507\n",
      "acc for Lsat= 0.22400396664937341 \n",
      "acc for Psat= 0.24440716438823273 \n",
      "acc for optim= 0.14889478055346342\n",
      "Epoch:44/1000\n",
      "Loss on train= 0.035797689110040665\n",
      "Loss on test= 0.033950623124837875\n",
      "acc for Lsat= 0.22265508506033158 \n",
      "acc for Psat= 0.24253536926375496 \n",
      "acc for optim= 0.15070147702677383\n",
      "Epoch:45/1000\n",
      "Loss on train= 0.035458941012620926\n",
      "Loss on test= 0.03684835135936737\n",
      "acc for Lsat= 0.22171082579427293 \n",
      "acc for Psat= 0.28782845420969855 \n",
      "acc for optim= 0.1574232181741132\n",
      "Epoch:46/1000\n",
      "Loss on train= 0.034959495067596436\n",
      "Loss on test= 0.03306245058774948\n",
      "acc for Lsat= 0.2192485344078806 \n",
      "acc for Psat= 0.26100679967138496 \n",
      "acc for optim= 0.15602494053956537\n",
      "Epoch:47/1000\n",
      "Loss on train= 0.03425149619579315\n",
      "Loss on test= 0.03436797857284546\n",
      "acc for Lsat= 0.21156501621007917 \n",
      "acc for Psat= 0.23609994600216544 \n",
      "acc for optim= 0.15049186082970764\n",
      "Epoch:48/1000\n",
      "Loss on train= 0.03470350801944733\n",
      "Loss on test= 0.03323344141244888\n",
      "acc for Lsat= 0.22271371119552186 \n",
      "acc for Psat= 0.2633066864477263 \n",
      "acc for optim= 0.15555147735608946\n",
      "Epoch:49/1000\n",
      "Loss on train= 0.034161221235990524\n",
      "Loss on test= 0.030541053041815758\n",
      "acc for Lsat= 0.21502135818203294 \n",
      "acc for Psat= 0.26839408377806345 \n",
      "acc for optim= 0.15351706054061653\n",
      "Epoch:50/1000\n",
      "Loss on train= 0.035738542675971985\n",
      "Loss on test= 0.032832249999046326\n",
      "acc for Lsat= 0.22099158192674323 \n",
      "acc for Psat= 0.31070300075742935 \n",
      "acc for optim= 0.15378263509935802\n",
      "Epoch:51/1000\n",
      "Loss on train= 0.03475237265229225\n",
      "Loss on test= 0.03203367814421654\n",
      "acc for Lsat= 0.2145324784848425 \n",
      "acc for Psat= 0.2583175251881282 \n",
      "acc for optim= 0.15611872739262056\n",
      "Epoch:52/1000\n",
      "Loss on train= 0.03494149446487427\n",
      "Loss on test= 0.03181799128651619\n",
      "acc for Lsat= 0.22423014276557496 \n",
      "acc for Psat= 0.2267238525880708 \n",
      "acc for optim= 0.15199633723952705\n",
      "Epoch:53/1000\n",
      "Loss on train= 0.03425029665231705\n",
      "Loss on test= 0.03243749961256981\n",
      "acc for Lsat= 0.21921512070629331 \n",
      "acc for Psat= 0.24991260965665182 \n",
      "acc for optim= 0.1540046808620294\n",
      "Epoch:54/1000\n",
      "Loss on train= 0.03310501202940941\n",
      "Loss on test= 0.03133219853043556\n",
      "acc for Lsat= 0.2143475480377674 \n",
      "acc for Psat= 0.2622636621197065 \n",
      "acc for optim= 0.14982677275935807\n",
      "Epoch:55/1000\n",
      "Loss on train= 0.03366459533572197\n",
      "Loss on test= 0.033679038286209106\n",
      "acc for Lsat= 0.21902490531404814 \n",
      "acc for Psat= 0.24123144348462422 \n",
      "acc for optim= 0.15948566289411648\n",
      "Epoch:56/1000\n",
      "Loss on train= 0.0344848670065403\n",
      "Loss on test= 0.02889346145093441\n",
      "acc for Lsat= 0.21348848889271418 \n",
      "acc for Psat= 0.2321187655131022 \n",
      "acc for optim= 0.15265275941540798\n",
      "Epoch:57/1000\n",
      "Loss on train= 0.03302760049700737\n",
      "Loss on test= 0.033577725291252136\n",
      "acc for Lsat= 0.22158700128396347 \n",
      "acc for Psat= 0.3164791855547163 \n",
      "acc for optim= 0.15037821821040576\n",
      "Epoch:58/1000\n",
      "Loss on train= 0.03419765457510948\n",
      "Loss on test= 0.03127467632293701\n",
      "acc for Lsat= 0.2104425461755858 \n",
      "acc for Psat= 0.24639111326800453 \n",
      "acc for optim= 0.15521634568770726\n",
      "Epoch:59/1000\n",
      "Loss on train= 0.03261124715209007\n",
      "Loss on test= 0.03134334087371826\n",
      "acc for Lsat= 0.22342092353436677 \n",
      "acc for Psat= 0.2444274625844426 \n",
      "acc for optim= 0.15515472739934924\n",
      "Epoch:60/1000\n",
      "Loss on train= 0.03391166776418686\n",
      "Loss on test= 0.030261388048529625\n",
      "acc for Lsat= 0.21675990604692033 \n",
      "acc for Psat= 0.2743575563033422 \n",
      "acc for optim= 0.15494877141382957\n",
      "Epoch:61/1000\n",
      "Loss on train= 0.034485090523958206\n",
      "Loss on test= 0.029507657513022423\n",
      "acc for Lsat= 0.2280767714811696 \n",
      "acc for Psat= 0.2304005010260476 \n",
      "acc for optim= 0.15688781225019033\n",
      "Epoch:62/1000\n",
      "Loss on train= 0.03381038457155228\n",
      "Loss on test= 0.033611852675676346\n",
      "acc for Lsat= 0.24605296444561742 \n",
      "acc for Psat= 0.23843813803460867 \n",
      "acc for optim= 0.1484303771828612\n",
      "Epoch:63/1000\n",
      "Loss on train= 0.035336729139089584\n",
      "Loss on test= 0.03163227066397667\n",
      "acc for Lsat= 0.2110906530585554 \n",
      "acc for Psat= 0.22231370972262487 \n",
      "acc for optim= 0.15452000399430593\n",
      "Epoch:64/1000\n",
      "Loss on train= 0.03223802149295807\n",
      "Loss on test= 0.029896758496761322\n",
      "acc for Lsat= 0.2301998319725196 \n",
      "acc for Psat= 0.2300272418393029 \n",
      "acc for optim= 0.14836048817055092\n",
      "Epoch:65/1000\n",
      "Loss on train= 0.0310958344489336\n",
      "Loss on test= 0.030743589624762535\n",
      "acc for Lsat= 0.22452648977438608 \n",
      "acc for Psat= 0.22853915823830498 \n",
      "acc for optim= 0.14981350017090636\n",
      "Epoch:66/1000\n",
      "Loss on train= 0.032805077731609344\n",
      "Loss on test= 0.02985972911119461\n",
      "acc for Lsat= 0.20829498254590564 \n",
      "acc for Psat= 0.23182394405206042 \n",
      "acc for optim= 0.15735003012749885\n",
      "Epoch:67/1000\n",
      "Loss on train= 0.031959425657987595\n",
      "Loss on test= 0.030029190704226494\n",
      "acc for Lsat= 0.2261805247929361 \n",
      "acc for Psat= 0.21989092446035807 \n",
      "acc for optim= 0.1478856329909629\n",
      "Epoch:68/1000\n",
      "Loss on train= 0.03156346455216408\n",
      "Loss on test= 0.03023773431777954\n",
      "acc for Lsat= 0.22277844556503826 \n",
      "acc for Psat= 0.2142173697551092 \n",
      "acc for optim= 0.15592842251062392\n",
      "Epoch:69/1000\n",
      "Loss on train= 0.03222149237990379\n",
      "Loss on test= 0.031427301466464996\n",
      "acc for Lsat= 0.20424120856655964 \n",
      "acc for Psat= 0.23060912009742524 \n",
      "acc for optim= 0.16248903506331971\n",
      "Epoch:70/1000\n",
      "Loss on train= 0.03134240210056305\n",
      "Loss on test= 0.02990514412522316\n",
      "acc for Lsat= 0.20924676573938797 \n",
      "acc for Psat= 0.22230038113064238 \n",
      "acc for optim= 0.16491685898767577\n",
      "Epoch:71/1000\n",
      "Loss on train= 0.030849941074848175\n",
      "Loss on test= 0.028717033565044403\n",
      "acc for Lsat= 0.22609666412075363 \n",
      "acc for Psat= 0.2151542264554236 \n",
      "acc for optim= 0.15579446630759375\n",
      "Epoch:72/1000\n",
      "Loss on train= 0.0318240225315094\n",
      "Loss on test= 0.028461763635277748\n",
      "acc for Lsat= 0.2264974757201142 \n",
      "acc for Psat= 0.22104940778679316 \n",
      "acc for optim= 0.1509209617972374\n",
      "Epoch:73/1000\n",
      "Loss on train= 0.03079524077475071\n",
      "Loss on test= 0.027724869549274445\n",
      "acc for Lsat= 0.20140309118562272 \n",
      "acc for Psat= 0.2348611017068227 \n",
      "acc for optim= 0.14945614708380572\n",
      "Epoch:74/1000\n",
      "Loss on train= 0.03144781291484833\n",
      "Loss on test= 0.02893706224858761\n",
      "acc for Lsat= 0.20329914093017576 \n",
      "acc for Psat= 0.22555017305745018 \n",
      "acc for optim= 0.15387580419580144\n",
      "Epoch:75/1000\n",
      "Loss on train= 0.03160913288593292\n",
      "Loss on test= 0.03264894336462021\n",
      "acc for Lsat= 0.20400607734918597 \n",
      "acc for Psat= 0.28158004250791335 \n",
      "acc for optim= 0.15780702382326126\n",
      "Epoch:76/1000\n",
      "Loss on train= 0.03254351764917374\n",
      "Loss on test= 0.028011364862322807\n",
      "acc for Lsat= 0.19858788665797975 \n",
      "acc for Psat= 0.23267001724905434 \n",
      "acc for optim= 0.15156787729097734\n",
      "Epoch:77/1000\n",
      "Loss on train= 0.03143290802836418\n",
      "Loss on test= 0.02986104227602482\n",
      "acc for Lsat= 0.19485348810752234 \n",
      "acc for Psat= 0.23778092397583853 \n",
      "acc for optim= 0.14992863734563192\n",
      "Epoch:78/1000\n",
      "Loss on train= 0.030098265036940575\n",
      "Loss on test= 0.030001023784279823\n",
      "acc for Lsat= 0.20004541741477114 \n",
      "acc for Psat= 0.230626685751809 \n",
      "acc for optim= 0.15098110224223799\n",
      "Epoch:79/1000\n",
      "Loss on train= 0.030117370188236237\n",
      "Loss on test= 0.029001928865909576\n",
      "acc for Lsat= 0.19557681522435613 \n",
      "acc for Psat= 0.24150451487965055 \n",
      "acc for optim= 0.16148971998029282\n",
      "Epoch:80/1000\n",
      "Loss on train= 0.029971646144986153\n",
      "Loss on test= 0.027804037556052208\n",
      "acc for Lsat= 0.19598553826411563 \n",
      "acc for Psat= 0.23120330539014602 \n",
      "acc for optim= 0.14919777603613008\n",
      "Epoch:81/1000\n",
      "Loss on train= 0.02945883199572563\n",
      "Loss on test= 0.030344359576702118\n",
      "acc for Lsat= 0.1840619943208165 \n",
      "acc for Psat= 0.21472453325986865 \n",
      "acc for optim= 0.14932422248853577\n",
      "Epoch:82/1000\n",
      "Loss on train= 0.03006957657635212\n",
      "Loss on test= 0.02887403964996338\n",
      "acc for Lsat= 0.19287743717432027 \n",
      "acc for Psat= 0.21829647372166316 \n",
      "acc for optim= 0.15336939576599334\n",
      "Epoch:83/1000\n",
      "Loss on train= 0.02947300113737583\n",
      "Loss on test= 0.02838316187262535\n",
      "acc for Lsat= 0.1747384015470743 \n",
      "acc for Psat= 0.22501234047942698 \n",
      "acc for optim= 0.1516838762702214\n",
      "Epoch:84/1000\n",
      "Loss on train= 0.028673481196165085\n",
      "Loss on test= 0.029352469369769096\n",
      "acc for Lsat= 0.17832756555742688 \n",
      "acc for Psat= 0.2548149224784639 \n",
      "acc for optim= 0.15897911575933293\n",
      "Epoch:85/1000\n",
      "Loss on train= 0.02908151037991047\n",
      "Loss on test= 0.02741588093340397\n",
      "acc for Lsat= 0.17781858974032932 \n",
      "acc for Psat= 0.2168719642692142 \n",
      "acc for optim= 0.15203599226143624\n",
      "Epoch:86/1000\n",
      "Loss on train= 0.030538910999894142\n",
      "Loss on test= 0.02587740309536457\n",
      "acc for Lsat= 0.17464000181191497 \n",
      "acc for Psat= 0.1956773390372594 \n",
      "acc for optim= 0.15236029947797458\n",
      "Epoch:87/1000\n",
      "Loss on train= 0.028424840420484543\n",
      "Loss on test= 0.024669863283634186\n",
      "acc for Lsat= 0.172237029671669 \n",
      "acc for Psat= 0.19596387313471897 \n",
      "acc for optim= 0.14908071069253814\n",
      "Epoch:88/1000\n",
      "Loss on train= 0.027591049671173096\n",
      "Loss on test= 0.025506984442472458\n",
      "acc for Lsat= 0.16659679251412554 \n",
      "acc for Psat= 0.19445900056097243 \n",
      "acc for optim= 0.15429896782669758\n",
      "Epoch:89/1000\n",
      "Loss on train= 0.028068862855434418\n",
      "Loss on test= 0.026882655918598175\n",
      "acc for Lsat= 0.154667408267657 \n",
      "acc for Psat= 0.19500849743684132 \n",
      "acc for optim= 0.14610443912032578\n",
      "Epoch:90/1000\n",
      "Loss on train= 0.028558755293488503\n",
      "Loss on test= 0.026135273277759552\n",
      "acc for Lsat= 0.18307821477452915 \n",
      "acc for Psat= 0.18827556206120385 \n",
      "acc for optim= 0.14895423038138284\n",
      "Epoch:91/1000\n",
      "Loss on train= 0.02862769179046154\n",
      "Loss on test= 0.02638871595263481\n",
      "acc for Lsat= 0.15888389001290004 \n",
      "acc for Psat= 0.22258195214801368 \n",
      "acc for optim= 0.15349169000983237\n",
      "Epoch:92/1000\n",
      "Loss on train= 0.028035152703523636\n",
      "Loss on test= 0.02354692481458187\n",
      "acc for Lsat= 0.15646910139669976 \n",
      "acc for Psat= 0.16500643458631303 \n",
      "acc for optim= 0.14720168471750286\n",
      "Epoch:93/1000\n",
      "Loss on train= 0.026738334447145462\n",
      "Loss on test= 0.024644702672958374\n",
      "acc for Lsat= 0.1557289883494377 \n",
      "acc for Psat= 0.197342112660408 \n",
      "acc for optim= 0.15371853800283533\n",
      "Epoch:94/1000\n",
      "Loss on train= 0.02619124762713909\n",
      "Loss on test= 0.026043811812996864\n",
      "acc for Lsat= 0.15434829261567856 \n",
      "acc for Psat= 0.1693749470843209 \n",
      "acc for optim= 0.15668608910507623\n",
      "Epoch:95/1000\n",
      "Loss on train= 0.02738788165152073\n",
      "Loss on test= 0.02506648376584053\n",
      "acc for Lsat= 0.142124747691883 \n",
      "acc for Psat= 0.171475356651677 \n",
      "acc for optim= 0.15476573954025905\n",
      "Epoch:96/1000\n",
      "Loss on train= 0.025539571419358253\n",
      "Loss on test= 0.02430163510143757\n",
      "acc for Lsat= 0.13942736577656534 \n",
      "acc for Psat= 0.17101703683535258 \n",
      "acc for optim= 0.15155619730552036\n",
      "Epoch:97/1000\n",
      "Loss on train= 0.026684479787945747\n",
      "Loss on test= 0.023757126182317734\n",
      "acc for Lsat= 0.12845363020896913 \n",
      "acc for Psat= 0.16456873565912247 \n",
      "acc for optim= 0.15105547457933427\n",
      "Epoch:98/1000\n",
      "Loss on train= 0.025916965678334236\n",
      "Loss on test= 0.026776324957609177\n",
      "acc for Lsat= 0.1542307778365082 \n",
      "acc for Psat= 0.18806922319862576 \n",
      "acc for optim= 0.14799541967610522\n",
      "Epoch:99/1000\n",
      "Loss on train= 0.02603423409163952\n",
      "Loss on test= 0.025045109912753105\n",
      "acc for Lsat= 0.14926548070377776 \n",
      "acc for Psat= 0.1982104433907403 \n",
      "acc for optim= 0.14887482788827683\n",
      "Epoch:100/1000\n",
      "Loss on train= 0.026865966618061066\n",
      "Loss on test= 0.024187009781599045\n",
      "acc for Lsat= 0.13271761727001932 \n",
      "acc for Psat= 0.1688787606027391 \n",
      "acc for optim= 0.15030767023563385\n",
      "Epoch:101/1000\n",
      "Loss on train= 0.02543485164642334\n",
      "Loss on test= 0.02345973812043667\n",
      "acc for Lsat= 0.12283485920892821 \n",
      "acc for Psat= 0.1642854803138309 \n",
      "acc for optim= 0.14403745821780625\n",
      "Epoch:102/1000\n",
      "Loss on train= 0.02561223693192005\n",
      "Loss on test= 0.024258838966488838\n",
      "acc for Lsat= 0.13139366399910712 \n",
      "acc for Psat= 0.16393116960922877 \n",
      "acc for optim= 0.14365292824804782\n",
      "Epoch:103/1000\n",
      "Loss on train= 0.024291016161441803\n",
      "Loss on test= 0.024555740877985954\n",
      "acc for Lsat= 0.12445428181025717 \n",
      "acc for Psat= 0.16980527473820584 \n",
      "acc for optim= 0.14777219328615399\n",
      "Epoch:104/1000\n",
      "Loss on train= 0.025193287059664726\n",
      "Loss on test= 0.02262156642973423\n",
      "acc for Lsat= 0.12214552662852737 \n",
      "acc for Psat= 0.15662364297442966 \n",
      "acc for optim= 0.14644469159344836\n",
      "Epoch:105/1000\n",
      "Loss on train= 0.025119490921497345\n",
      "Loss on test= 0.024039052426815033\n",
      "acc for Lsat= 0.1178253933787346 \n",
      "acc for Psat= 0.15952925814522637 \n",
      "acc for optim= 0.14691359251737598\n",
      "Epoch:106/1000\n",
      "Loss on train= 0.024431223049759865\n",
      "Loss on test= 0.02377108670771122\n",
      "acc for Lsat= 0.10949344254202313 \n",
      "acc for Psat= 0.15270351568857826 \n",
      "acc for optim= 0.14437394520888727\n",
      "Epoch:107/1000\n",
      "Loss on train= 0.023722469806671143\n",
      "Loss on test= 0.023404452949762344\n",
      "acc for Lsat= 0.11187183372676375 \n",
      "acc for Psat= 0.15360960364341733 \n",
      "acc for optim= 0.14587022740807798\n",
      "Epoch:108/1000\n",
      "Loss on train= 0.023966748267412186\n",
      "Loss on test= 0.022744998335838318\n",
      "acc for Lsat= 0.1092001466287507 \n",
      "acc for Psat= 0.1622139073080487 \n",
      "acc for optim= 0.1475688916113642\n",
      "Epoch:109/1000\n",
      "Loss on train= 0.023866845294833183\n",
      "Loss on test= 0.022460997104644775\n",
      "acc for Lsat= 0.11394964622126685 \n",
      "acc for Psat= 0.1453897762629721 \n",
      "acc for optim= 0.1497202679514885\n",
      "Epoch:110/1000\n",
      "Loss on train= 0.02504056878387928\n",
      "Loss on test= 0.027316192165017128\n",
      "acc for Lsat= 0.1589585264523824 \n",
      "acc for Psat= 0.2095508302251498 \n",
      "acc for optim= 0.153554534415404\n",
      "Epoch:111/1000\n",
      "Loss on train= 0.024719595909118652\n",
      "Loss on test= 0.02085248753428459\n",
      "acc for Lsat= 0.1099239027334584 \n",
      "acc for Psat= 0.15499593797657224 \n",
      "acc for optim= 0.14516307558450436\n",
      "Epoch:112/1000\n",
      "Loss on train= 0.023942554369568825\n",
      "Loss on test= 0.022526675835251808\n",
      "acc for Lsat= 0.12121930420398715 \n",
      "acc for Psat= 0.15265422728326586 \n",
      "acc for optim= 0.14365750658843254\n",
      "Epoch:113/1000\n",
      "Loss on train= 0.024304533377289772\n",
      "Loss on test= 0.023994354531168938\n",
      "acc for Lsat= 0.1226570627341668 \n",
      "acc for Psat= 0.15593902468681334 \n",
      "acc for optim= 0.1498997728029887\n",
      "Epoch:114/1000\n",
      "Loss on train= 0.023709334433078766\n",
      "Loss on test= 0.021177034825086594\n",
      "acc for Lsat= 0.10331222638487815 \n",
      "acc for Psat= 0.14512388938003115 \n",
      "acc for optim= 0.14524783968097632\n",
      "Epoch:115/1000\n",
      "Loss on train= 0.023061275482177734\n",
      "Loss on test= 0.02206668257713318\n",
      "acc for Lsat= 0.1007336656252543 \n",
      "acc for Psat= 0.1541216802265909 \n",
      "acc for optim= 0.14486209758453897\n",
      "Epoch:116/1000\n",
      "Loss on train= 0.0228397436439991\n",
      "Loss on test= 0.021552329882979393\n",
      "acc for Lsat= 0.1001552245269219 \n",
      "acc for Psat= 0.14562057918972438 \n",
      "acc for optim= 0.14578577400081688\n",
      "Epoch:117/1000\n",
      "Loss on train= 0.022749710828065872\n",
      "Loss on test= 0.023352153599262238\n",
      "acc for Lsat= 0.12222992645369637 \n",
      "acc for Psat= 0.14558000961939496 \n",
      "acc for optim= 0.14597761804858844\n",
      "Epoch:118/1000\n",
      "Loss on train= 0.022266807034611702\n",
      "Loss on test= 0.02322741597890854\n",
      "acc for Lsat= 0.10093661554985577 \n",
      "acc for Psat= 0.15468890203369987 \n",
      "acc for optim= 0.1538494452006287\n",
      "Epoch:119/1000\n",
      "Loss on train= 0.022464381530880928\n",
      "Loss on test= 0.02224607579410076\n",
      "acc for Lsat= 0.11013342903720008 \n",
      "acc for Psat= 0.15112947093115914 \n",
      "acc for optim= 0.14619452075825792\n",
      "Epoch:120/1000\n",
      "Loss on train= 0.022380225360393524\n",
      "Loss on test= 0.023210221901535988\n",
      "acc for Lsat= 0.11269622743129733 \n",
      "acc for Psat= 0.2016331712404887 \n",
      "acc for optim= 0.14599605637292068\n",
      "Epoch:121/1000\n",
      "Loss on train= 0.02343597449362278\n",
      "Loss on test= 0.021567044779658318\n",
      "acc for Lsat= 0.10208710746632683 \n",
      "acc for Psat= 0.14345400598314076 \n",
      "acc for optim= 0.1456060221211778\n",
      "Epoch:122/1000\n",
      "Loss on train= 0.022828102111816406\n",
      "Loss on test= 0.02203400246798992\n",
      "acc for Lsat= 0.09502087773548232 \n",
      "acc for Psat= 0.15739807966682648 \n",
      "acc for optim= 0.14814035346110666\n",
      "Epoch:123/1000\n",
      "Loss on train= 0.02218613401055336\n",
      "Loss on test= 0.022263219580054283\n",
      "acc for Lsat= 0.10098302037351664 \n",
      "acc for Psat= 0.14046181639035543 \n",
      "acc for optim= 0.1438274617410369\n",
      "Epoch:124/1000\n",
      "Loss on train= 0.02246272936463356\n",
      "Loss on test= 0.020441440865397453\n",
      "acc for Lsat= 0.0949779989818732 \n",
      "acc for Psat= 0.1406893895732032 \n",
      "acc for optim= 0.14240308111119604\n",
      "Epoch:125/1000\n",
      "Loss on train= 0.022058989852666855\n",
      "Loss on test= 0.021800728514790535\n",
      "acc for Lsat= 0.09967592031591468 \n",
      "acc for Psat= 0.14470048050085704 \n",
      "acc for optim= 0.14634892191323973\n",
      "Epoch:126/1000\n",
      "Loss on train= 0.022249042987823486\n",
      "Loss on test= 0.02197764813899994\n",
      "acc for Lsat= 0.10652353122002549 \n",
      "acc for Psat= 0.15103844321436355 \n",
      "acc for optim= 0.14313641923169296\n",
      "Epoch:127/1000\n",
      "Loss on train= 0.021392419934272766\n",
      "Loss on test= 0.021807309240102768\n",
      "acc for Lsat= 0.09657896365970373 \n",
      "acc for Psat= 0.14108813603719078 \n",
      "acc for optim= 0.14465004164311618\n",
      "Epoch:128/1000\n",
      "Loss on train= 0.022863006219267845\n",
      "Loss on test= 0.022143393754959106\n",
      "acc for Lsat= 0.09364208123750156 \n",
      "acc for Psat= 0.1443112232618862 \n",
      "acc for optim= 0.14494085394673878\n",
      "Epoch:129/1000\n",
      "Loss on train= 0.021049927920103073\n",
      "Loss on test= 0.02019481174647808\n",
      "acc for Lsat= 0.1028514188196924 \n",
      "acc for Psat= 0.1388909015390608 \n",
      "acc for optim= 0.14776858819855582\n",
      "Epoch:130/1000\n",
      "Loss on train= 0.02210281975567341\n",
      "Loss on test= 0.021980388090014458\n",
      "acc for Lsat= 0.10161854972441992 \n",
      "acc for Psat= 0.14194894416464698 \n",
      "acc for optim= 0.14322828265527884\n",
      "Epoch:131/1000\n",
      "Loss on train= 0.02225393056869507\n",
      "Loss on test= 0.02218327671289444\n",
      "acc for Lsat= 0.09990980045662987 \n",
      "acc for Psat= 0.13786600695716011 \n",
      "acc for optim= 0.15259322615133397\n",
      "Epoch:132/1000\n",
      "Loss on train= 0.023419810459017754\n",
      "Loss on test= 0.02317971922457218\n",
      "acc for Lsat= 0.11921216431591247 \n",
      "acc for Psat= 0.1997130314509074 \n",
      "acc for optim= 0.1527798621190919\n",
      "Epoch:133/1000\n",
      "Loss on train= 0.021194780245423317\n",
      "Loss on test= 0.021205434575676918\n",
      "acc for Lsat= 0.09904007812341054 \n",
      "acc for Psat= 0.1459346248043908 \n",
      "acc for optim= 0.14223712488181062\n",
      "Epoch:134/1000\n",
      "Loss on train= 0.021931761875748634\n",
      "Loss on test= 0.020595164969563484\n",
      "acc for Lsat= 0.09278460654119652 \n",
      "acc for Psat= 0.1368866879079077 \n",
      "acc for optim= 0.1438621511682868\n",
      "Epoch:135/1000\n",
      "Loss on train= 0.02166270837187767\n",
      "Loss on test= 0.021643750369548798\n",
      "acc for Lsat= 0.09416081408659618 \n",
      "acc for Psat= 0.13127222127384608 \n",
      "acc for optim= 0.14859243689311874\n",
      "Epoch:136/1000\n",
      "Loss on train= 0.021936597302556038\n",
      "Loss on test= 0.020336762070655823\n",
      "acc for Lsat= 0.09426434785127641 \n",
      "acc for Psat= 0.1362413179543283 \n",
      "acc for optim= 0.13994766206791004\n",
      "Epoch:137/1000\n",
      "Loss on train= 0.021173736080527306\n",
      "Loss on test= 0.020251218229532242\n",
      "acc for Lsat= 0.09696796130802895 \n",
      "acc for Psat= 0.1479615052541097 \n",
      "acc for optim= 0.14260309562087056\n",
      "Epoch:138/1000\n",
      "Loss on train= 0.020408451557159424\n",
      "Loss on test= 0.02274855226278305\n",
      "acc for Lsat= 0.10730708038641347 \n",
      "acc for Psat= 0.1562065217230055 \n",
      "acc for optim= 0.1389602005895641\n",
      "Epoch:139/1000\n",
      "Loss on train= 0.021155890077352524\n",
      "Loss on test= 0.021749082952737808\n",
      "acc for Lsat= 0.12795488246613082 \n",
      "acc for Psat= 0.15075985259479943 \n",
      "acc for optim= 0.14258390913407007\n",
      "Epoch:140/1000\n",
      "Loss on train= 0.02139832079410553\n",
      "Loss on test= 0.022681673988699913\n",
      "acc for Lsat= 0.128538273440467 \n",
      "acc for Psat= 0.1896307047870424 \n",
      "acc for optim= 0.14376020659175182\n",
      "Epoch:141/1000\n",
      "Loss on train= 0.021875739097595215\n",
      "Loss on test= 0.019706973806023598\n",
      "acc for Lsat= 0.11508369412687092 \n",
      "acc for Psat= 0.18219734827677408 \n",
      "acc for optim= 0.1402063087042835\n",
      "Epoch:142/1000\n",
      "Loss on train= 0.021378429606556892\n",
      "Loss on test= 0.020721320062875748\n",
      "acc for Lsat= 0.10275448279248346 \n",
      "acc for Psat= 0.13802246318923103 \n",
      "acc for optim= 0.14130602284438082\n",
      "Epoch:143/1000\n",
      "Loss on train= 0.02177184820175171\n",
      "Loss on test= 0.020904812961816788\n",
      "acc for Lsat= 0.11218032480941877 \n",
      "acc for Psat= 0.14466499222649468 \n",
      "acc for optim= 0.1447294093668461\n",
      "Epoch:144/1000\n",
      "Loss on train= 0.021673649549484253\n",
      "Loss on test= 0.02028501033782959\n",
      "acc for Lsat= 0.09195173316531713 \n",
      "acc for Psat= 0.13941344387001461 \n",
      "acc for optim= 0.14427796949942906\n",
      "Epoch:145/1000\n",
      "Loss on train= 0.020091822370886803\n",
      "Loss on test= 0.01837007887661457\n",
      "acc for Lsat= 0.09457340323262747 \n",
      "acc for Psat= 0.1433001607656479 \n",
      "acc for optim= 0.13998430959052513\n",
      "Epoch:146/1000\n",
      "Loss on train= 0.0203037578612566\n",
      "Loss on test= 0.02097661793231964\n",
      "acc for Lsat= 0.08191491497887506 \n",
      "acc for Psat= 0.1271646367179023 \n",
      "acc for optim= 0.14971337897910014\n",
      "Epoch:147/1000\n",
      "Loss on train= 0.020384076982736588\n",
      "Loss on test= 0.020803382620215416\n",
      "acc for Lsat= 0.0907864444785648 \n",
      "acc for Psat= 0.1267951738503244 \n",
      "acc for optim= 0.13997907323969733\n",
      "Epoch:148/1000\n",
      "Loss on train= 0.021658675745129585\n",
      "Loss on test= 0.020421776920557022\n",
      "acc for Lsat= 0.0923059132364061 \n",
      "acc for Psat= 0.15412618782785203 \n",
      "acc for optim= 0.14433885945214164\n",
      "Epoch:149/1000\n",
      "Loss on train= 0.021408338099718094\n",
      "Loss on test= 0.019645608961582184\n",
      "acc for Lsat= 0.09743589013814927 \n",
      "acc for Psat= 0.13509450885984634 \n",
      "acc for optim= 0.14604590659340225\n",
      "Epoch:150/1000\n",
      "Loss on train= 0.02035694755613804\n",
      "Loss on test= 0.019815221428871155\n",
      "acc for Lsat= 0.08438854739069938 \n",
      "acc for Psat= 0.12824626151058408 \n",
      "acc for optim= 0.14215273279696702\n",
      "Epoch:151/1000\n",
      "Loss on train= 0.02042217366397381\n",
      "Loss on test= 0.0200976449996233\n",
      "acc for Lsat= 0.08673041313886642 \n",
      "acc for Psat= 0.12971431977219053 \n",
      "acc for optim= 0.1388609511156877\n",
      "Epoch:152/1000\n",
      "Loss on train= 0.020900417119264603\n",
      "Loss on test= 0.02116147242486477\n",
      "acc for Lsat= 0.0998276722100046 \n",
      "acc for Psat= 0.1472011324432161 \n",
      "acc for optim= 0.14812516404522788\n",
      "Epoch:153/1000\n",
      "Loss on train= 0.020646339282393456\n",
      "Loss on test= 0.020877914503216743\n",
      "acc for Lsat= 0.099919663141999 \n",
      "acc for Psat= 0.139551298154725 \n",
      "acc for optim= 0.13779240523775418\n",
      "Epoch:154/1000\n",
      "Loss on train= 0.02083447389304638\n",
      "Loss on test= 0.019699402153491974\n",
      "acc for Lsat= 0.09421738990479045 \n",
      "acc for Psat= 0.12962155573897893 \n",
      "acc for optim= 0.14094378275589814\n",
      "Epoch:155/1000\n",
      "Loss on train= 0.020236346870660782\n",
      "Loss on test= 0.01976402848958969\n",
      "acc for Lsat= 0.10134135625428624 \n",
      "acc for Psat= 0.13646719389491613 \n",
      "acc for optim= 0.13920923801552917\n",
      "Epoch:156/1000\n",
      "Loss on train= 0.020282959565520287\n",
      "Loss on test= 0.021007003262639046\n",
      "acc for Lsat= 0.10455360511938733 \n",
      "acc for Psat= 0.15345278514756097 \n",
      "acc for optim= 0.13953373454925086\n",
      "Epoch:157/1000\n",
      "Loss on train= 0.02102513797581196\n",
      "Loss on test= 0.020906850695610046\n",
      "acc for Lsat= 0.0955157630559471 \n",
      "acc for Psat= 0.12999794400400588 \n",
      "acc for optim= 0.14285095723138916\n",
      "Epoch:158/1000\n",
      "Loss on train= 0.019739238545298576\n",
      "Loss on test= 0.01930587738752365\n",
      "acc for Lsat= 0.0871872189972136 \n",
      "acc for Psat= 0.12634074770741993 \n",
      "acc for optim= 0.1411599990601341\n",
      "Epoch:159/1000\n",
      "Loss on train= 0.020366843789815903\n",
      "Loss on test= 0.01992511935532093\n",
      "acc for Lsat= 0.09176650875144536 \n",
      "acc for Psat= 0.13292308929893704 \n",
      "acc for optim= 0.13889597318031724\n",
      "Epoch:160/1000\n",
      "Loss on train= 0.019685564562678337\n",
      "Loss on test= 0.0194240752607584\n",
      "acc for Lsat= 0.09404350684748758 \n",
      "acc for Psat= 0.1659169607692295 \n",
      "acc for optim= 0.13865151261496875\n",
      "Epoch:161/1000\n",
      "Loss on train= 0.020087379962205887\n",
      "Loss on test= 0.019177120178937912\n",
      "acc for Lsat= 0.08406272911363177 \n",
      "acc for Psat= 0.12635355227523382 \n",
      "acc for optim= 0.140354957503991\n",
      "Epoch:162/1000\n",
      "Loss on train= 0.019843144342303276\n",
      "Loss on test= 0.01969281956553459\n",
      "acc for Lsat= 0.08144641170899074 \n",
      "acc for Psat= 0.12513597740067375 \n",
      "acc for optim= 0.14011851304935086\n",
      "Epoch:163/1000\n",
      "Loss on train= 0.019994482398033142\n",
      "Loss on test= 0.018648317083716393\n",
      "acc for Lsat= 0.08707110981146493 \n",
      "acc for Psat= 0.13475908007886675 \n",
      "acc for optim= 0.1412032394980391\n",
      "Epoch:164/1000\n",
      "Loss on train= 0.020250044763088226\n",
      "Loss on test= 0.0190205667167902\n",
      "acc for Lsat= 0.08212178010079596 \n",
      "acc for Psat= 0.14296717809306247 \n",
      "acc for optim= 0.14139362913039\n",
      "Epoch:165/1000\n",
      "Loss on train= 0.019153812900185585\n",
      "Loss on test= 0.01947876065969467\n",
      "acc for Lsat= 0.0967662622531255 \n",
      "acc for Psat= 0.1494011163711548 \n",
      "acc for optim= 0.14118711439271764\n",
      "Epoch:166/1000\n",
      "Loss on train= 0.019500361755490303\n",
      "Loss on test= 0.018943630158901215\n",
      "acc for Lsat= 0.08076579405201806 \n",
      "acc for Psat= 0.12312171541982225 \n",
      "acc for optim= 0.142137037921283\n",
      "Epoch:167/1000\n",
      "Loss on train= 0.019525012001395226\n",
      "Loss on test= 0.01975272037088871\n",
      "acc for Lsat= 0.09143034269412359 \n",
      "acc for Psat= 0.12820999655458662 \n",
      "acc for optim= 0.14260912121584018\n",
      "Epoch:168/1000\n",
      "Loss on train= 0.02007889933884144\n",
      "Loss on test= 0.019319282844662666\n",
      "acc for Lsat= 0.09401397638850743 \n",
      "acc for Psat= 0.1537208186255561 \n",
      "acc for optim= 0.13970810034208825\n",
      "Epoch:169/1000\n",
      "Loss on train= 0.01885353773832321\n",
      "Loss on test= 0.01878396049141884\n",
      "acc for Lsat= 0.08939405298895307 \n",
      "acc for Psat= 0.13931655552652147 \n",
      "acc for optim= 0.13748285772485863\n",
      "Epoch:170/1000\n",
      "Loss on train= 0.019438989460468292\n",
      "Loss on test= 0.019148562103509903\n",
      "acc for Lsat= 0.08347369908458656 \n",
      "acc for Psat= 0.13379717999034457 \n",
      "acc for optim= 0.13597106385148233\n",
      "Epoch:171/1000\n",
      "Loss on train= 0.020121313631534576\n",
      "Loss on test= 0.020902495831251144\n",
      "acc for Lsat= 0.12057145602173275 \n",
      "acc for Psat= 0.20785960720645058 \n",
      "acc for optim= 0.1395458981394768\n",
      "Epoch:172/1000\n",
      "Loss on train= 0.01993696205317974\n",
      "Loss on test= 0.019937077537178993\n",
      "acc for Lsat= 0.08599875155422422 \n",
      "acc for Psat= 0.12473168356551066 \n",
      "acc for optim= 0.13905150840679806\n",
      "Epoch:173/1000\n",
      "Loss on train= 0.019265830516815186\n",
      "Loss on test= 0.02014380507171154\n",
      "acc for Lsat= 0.08066184752517275 \n",
      "acc for Psat= 0.13311790972948073 \n",
      "acc for optim= 0.13746597891052564\n",
      "Epoch:174/1000\n",
      "Loss on train= 0.01935451477766037\n",
      "Loss on test= 0.01775488629937172\n",
      "acc for Lsat= 0.08744272912542024 \n",
      "acc for Psat= 0.1261499222781923 \n",
      "acc for optim= 0.13955343717502222\n",
      "Epoch:175/1000\n",
      "Loss on train= 0.019152376800775528\n",
      "Loss on test= 0.01828721910715103\n",
      "acc for Lsat= 0.08725708838966158 \n",
      "acc for Psat= 0.13972847362359364 \n",
      "acc for optim= 0.13926326162699193\n",
      "Epoch:176/1000\n",
      "Loss on train= 0.01946721412241459\n",
      "Loss on test= 0.01891622692346573\n",
      "acc for Lsat= 0.09423398839102852 \n",
      "acc for Psat= 0.1320285979244444 \n",
      "acc for optim= 0.14227098027865093\n",
      "Epoch:177/1000\n",
      "Loss on train= 0.019116930663585663\n",
      "Loss on test= 0.018780870363116264\n",
      "acc for Lsat= 0.07682990704973539 \n",
      "acc for Psat= 0.13240473833349015 \n",
      "acc for optim= 0.13769379705190662\n",
      "Epoch:178/1000\n",
      "Loss on train= 0.019412770867347717\n",
      "Loss on test= 0.018772946670651436\n",
      "acc for Lsat= 0.08687800144155819 \n",
      "acc for Psat= 0.13380194505055745 \n",
      "acc for optim= 0.13991098950306574\n",
      "Epoch:179/1000\n",
      "Loss on train= 0.018907712772488594\n",
      "Loss on test= 0.017713110893964767\n",
      "acc for Lsat= 0.10313976804415384 \n",
      "acc for Psat= 0.1407112654712465 \n",
      "acc for optim= 0.13919762412293088\n",
      "Epoch:180/1000\n",
      "Loss on train= 0.019103659316897392\n",
      "Loss on test= 0.020133092999458313\n",
      "acc for Lsat= 0.08934577968385483 \n",
      "acc for Psat= 0.1403245002031326 \n",
      "acc for optim= 0.14181466992530556\n",
      "Epoch:181/1000\n",
      "Loss on train= 0.01905275695025921\n",
      "Loss on test= 0.01886558346450329\n",
      "acc for Lsat= 0.0867349613043997 \n",
      "acc for Psat= 0.12675762010945213 \n",
      "acc for optim= 0.1392935127951205\n",
      "Epoch:182/1000\n",
      "Loss on train= 0.01902713067829609\n",
      "Loss on test= 0.018930749967694283\n",
      "acc for Lsat= 0.09489856217470433 \n",
      "acc for Psat= 0.1224827186928855 \n",
      "acc for optim= 0.13678498415069446\n",
      "Epoch:183/1000\n",
      "Loss on train= 0.01854988932609558\n",
      "Loss on test= 0.01869259774684906\n",
      "acc for Lsat= 0.0801133838792642 \n",
      "acc for Psat= 0.13841211133533057 \n",
      "acc for optim= 0.13565113513420024\n",
      "Epoch:184/1000\n",
      "Loss on train= 0.019358014687895775\n",
      "Loss on test= 0.018945135176181793\n",
      "acc for Lsat= 0.08313204927576913 \n",
      "acc for Psat= 0.1183960345056322 \n",
      "acc for optim= 0.1393262463828756\n",
      "Epoch:185/1000\n",
      "Loss on train= 0.018383799120783806\n",
      "Loss on test= 0.017909636721014977\n",
      "acc for Lsat= 0.08446887714995278 \n",
      "acc for Psat= 0.11980381276872423 \n",
      "acc for optim= 0.1411572010980712\n",
      "Epoch:186/1000\n",
      "Loss on train= 0.019243381917476654\n",
      "Loss on test= 0.017941106110811234\n",
      "acc for Lsat= 0.08422101380096542 \n",
      "acc for Psat= 0.12559464739428627 \n",
      "acc for optim= 0.13876231842570833\n",
      "Epoch:187/1000\n",
      "Loss on train= 0.018358375877141953\n",
      "Loss on test= 0.01998768001794815\n",
      "acc for Lsat= 0.10034739854228165 \n",
      "acc for Psat= 0.12827218770980833 \n",
      "acc for optim= 0.13455976698961522\n",
      "Epoch:188/1000\n",
      "Loss on train= 0.018357964232563972\n",
      "Loss on test= 0.018587887287139893\n",
      "acc for Lsat= 0.08648906664715872 \n",
      "acc for Psat= 0.12144188384215036 \n",
      "acc for optim= 0.1422853789395756\n",
      "Epoch:189/1000\n",
      "Loss on train= 0.018767327070236206\n",
      "Loss on test= 0.01788974553346634\n",
      "acc for Lsat= 0.09270917061302397 \n",
      "acc for Psat= 0.11767601188686161 \n",
      "acc for optim= 0.1392691695027881\n",
      "Epoch:190/1000\n",
      "Loss on train= 0.019008083269000053\n",
      "Loss on test= 0.017727216705679893\n",
      "acc for Lsat= 0.08575080008142526 \n",
      "acc for Psat= 0.11755650540192925 \n",
      "acc for optim= 0.14571252564589185\n",
      "Epoch:191/1000\n",
      "Loss on train= 0.018256045877933502\n",
      "Loss on test= 0.018311569467186928\n",
      "acc for Lsat= 0.08202116042375564 \n",
      "acc for Psat= 0.12772739165359073 \n",
      "acc for optim= 0.13467600738836658\n",
      "Epoch:192/1000\n",
      "Loss on train= 0.01772834174335003\n",
      "Loss on test= 0.018723584711551666\n",
      "acc for Lsat= 0.08064581486913894 \n",
      "acc for Psat= 0.1539352410369449 \n",
      "acc for optim= 0.13679047777420947\n",
      "Epoch:193/1000\n",
      "Loss on train= 0.018258575350046158\n",
      "Loss on test= 0.018322652205824852\n",
      "acc for Lsat= 0.09240850773122575 \n",
      "acc for Psat= 0.1404435094859865 \n",
      "acc for optim= 0.13915670147786538\n",
      "Epoch:194/1000\n",
      "Loss on train= 0.018796281889081\n",
      "Loss on test= 0.017484495416283607\n",
      "acc for Lsat= 0.08685086733765074 \n",
      "acc for Psat= 0.12432703425486882 \n",
      "acc for optim= 0.13665212297605142\n",
      "Epoch:195/1000\n",
      "Loss on train= 0.018221648409962654\n",
      "Loss on test= 0.019328802824020386\n",
      "acc for Lsat= 0.08718068632814618 \n",
      "acc for Psat= 0.15481266346242692 \n",
      "acc for optim= 0.13877553624204464\n",
      "Epoch:196/1000\n",
      "Loss on train= 0.018922515213489532\n",
      "Loss on test= 0.01773945428431034\n",
      "acc for Lsat= 0.08119659175475438 \n",
      "acc for Psat= 0.1395265261332194 \n",
      "acc for optim= 0.13349997466947466\n",
      "Epoch:197/1000\n",
      "Loss on train= 0.018510127440094948\n",
      "Loss on test= 0.01812013052403927\n",
      "acc for Lsat= 0.07686139063702689 \n",
      "acc for Psat= 0.12189177142249215 \n",
      "acc for optim= 0.14044857101721894\n",
      "Epoch:198/1000\n",
      "Loss on train= 0.018286630511283875\n",
      "Loss on test= 0.01873946748673916\n",
      "acc for Lsat= 0.10751174162659381 \n",
      "acc for Psat= 0.12862227360407508 \n",
      "acc for optim= 0.13777971524331303\n",
      "Epoch:199/1000\n",
      "Loss on train= 0.018133409321308136\n",
      "Loss on test= 0.017794787883758545\n",
      "acc for Lsat= 0.08842372132672206 \n",
      "acc for Psat= 0.12836380700270336 \n",
      "acc for optim= 0.14465742541684046\n",
      "Epoch:200/1000\n",
      "Loss on train= 0.019085340201854706\n",
      "Loss on test= 0.018122024834156036\n",
      "acc for Lsat= 0.1132207681735357 \n",
      "acc for Psat= 0.14900560312800937 \n",
      "acc for optim= 0.13437309080941812\n",
      "Epoch:201/1000\n",
      "Loss on train= 0.018007837235927582\n",
      "Loss on test= 0.017852958291769028\n",
      "acc for Lsat= 0.08054532441827984 \n",
      "acc for Psat= 0.1404616031381819 \n",
      "acc for optim= 0.1382238475812806\n",
      "Epoch:202/1000\n",
      "Loss on train= 0.018302738666534424\n",
      "Loss on test= 0.017557550221681595\n",
      "acc for Lsat= 0.08452115853627523 \n",
      "acc for Psat= 0.120368150042163 \n",
      "acc for optim= 0.1407628514700466\n",
      "Epoch:203/1000\n",
      "Loss on train= 0.0181852076202631\n",
      "Loss on test= 0.018098361790180206\n",
      "acc for Lsat= 0.07675271696514555 \n",
      "acc for Psat= 0.11733641508552764 \n",
      "acc for optim= 0.1352251326251361\n",
      "Epoch:204/1000\n",
      "Loss on train= 0.01876147836446762\n",
      "Loss on test= 0.017690783366560936\n",
      "acc for Lsat= 0.08031248682075078 \n",
      "acc for Psat= 0.12106131547027162 \n",
      "acc for optim= 0.14165498101049\n",
      "Epoch:205/1000\n",
      "Loss on train= 0.01802080124616623\n",
      "Loss on test= 0.01809667982161045\n",
      "acc for Lsat= 0.07789836037490104 \n",
      "acc for Psat= 0.12150309483210246 \n",
      "acc for optim= 0.137023861043983\n",
      "Epoch:206/1000\n",
      "Loss on train= 0.01830187998712063\n",
      "Loss on test= 0.019526463001966476\n",
      "acc for Lsat= 0.08945951151351135 \n",
      "acc for Psat= 0.15280329220824773 \n",
      "acc for optim= 0.1423816485537423\n",
      "Epoch:207/1000\n",
      "Loss on train= 0.017947010695934296\n",
      "Loss on test= 0.018775060772895813\n",
      "acc for Lsat= 0.12772036956416236 \n",
      "acc for Psat= 0.13772104349401262 \n",
      "acc for optim= 0.13720615524798635\n",
      "Epoch:208/1000\n",
      "Loss on train= 0.017558317631483078\n",
      "Loss on test= 0.018285801634192467\n",
      "acc for Lsat= 0.07951485166947046 \n",
      "acc for Psat= 0.13920255535178713 \n",
      "acc for optim= 0.14251563822229704\n",
      "Epoch:209/1000\n",
      "Loss on train= 0.017722606658935547\n",
      "Loss on test= 0.018647396937012672\n",
      "acc for Lsat= 0.1091050535440445 \n",
      "acc for Psat= 0.16199383636315665 \n",
      "acc for optim= 0.1427070014178753\n",
      "Epoch:210/1000\n",
      "Loss on train= 0.018272167071700096\n",
      "Loss on test= 0.017918143421411514\n",
      "acc for Lsat= 0.08275178488757874 \n",
      "acc for Psat= 0.14863170915179783 \n",
      "acc for optim= 0.1371806677016947\n",
      "Epoch:211/1000\n",
      "Loss on train= 0.01758577674627304\n",
      "Loss on test= 0.017991313710808754\n",
      "acc for Lsat= 0.08259832147094938 \n",
      "acc for Psat= 0.11445407519737877 \n",
      "acc for optim= 0.13652934432029729\n",
      "Epoch:212/1000\n",
      "Loss on train= 0.017244551330804825\n",
      "Loss on test= 0.017255885526537895\n",
      "acc for Lsat= 0.07590364176366064 \n",
      "acc for Psat= 0.11756364968087935 \n",
      "acc for optim= 0.1358284859607617\n",
      "Epoch:213/1000\n",
      "Loss on train= 0.01762664131820202\n",
      "Loss on test= 0.017986943945288658\n",
      "acc for Lsat= 0.08350818637344573 \n",
      "acc for Psat= 0.13342619720432494 \n",
      "acc for optim= 0.1337772948667407\n",
      "Epoch:214/1000\n",
      "Loss on train= 0.017668554559350014\n",
      "Loss on test= 0.016580922529101372\n",
      "acc for Lsat= 0.07747489114602407 \n",
      "acc for Psat= 0.1435585356420941 \n",
      "acc for optim= 0.1350324058905244\n",
      "Epoch:215/1000\n",
      "Loss on train= 0.01820342056453228\n",
      "Loss on test= 0.01767241209745407\n",
      "acc for Lsat= 0.083447287686997 \n",
      "acc for Psat= 0.11528315891822179 \n",
      "acc for optim= 0.14215304735634066\n",
      "Epoch:216/1000\n",
      "Loss on train= 0.017622949555516243\n",
      "Loss on test= 0.018618222326040268\n",
      "acc for Lsat= 0.08059620741340848 \n",
      "acc for Psat= 0.128003618783421 \n",
      "acc for optim= 0.13197198116944897\n",
      "Epoch:217/1000\n",
      "Loss on train= 0.017852531746029854\n",
      "Loss on test= 0.01695862039923668\n",
      "acc for Lsat= 0.07698744725849894 \n",
      "acc for Psat= 0.12476575060023201 \n",
      "acc for optim= 0.13302562322674527\n",
      "Epoch:218/1000\n",
      "Loss on train= 0.018006552010774612\n",
      "Loss on test= 0.017566291615366936\n",
      "acc for Lsat= 0.09432017637623681 \n",
      "acc for Psat= 0.12148350791798698 \n",
      "acc for optim= 0.1383384583517909\n",
      "Epoch:219/1000\n",
      "Loss on train= 0.017892705276608467\n",
      "Loss on test= 0.017481014132499695\n",
      "acc for Lsat= 0.08831218712859684 \n",
      "acc for Psat= 0.12250781986448502 \n",
      "acc for optim= 0.1322901718991084\n",
      "Epoch:220/1000\n",
      "Loss on train= 0.017657510936260223\n",
      "Loss on test= 0.017610488459467888\n",
      "acc for Lsat= 0.07950585683186849 \n",
      "acc for Psat= 0.13085259596506754 \n",
      "acc for optim= 0.135274616525405\n",
      "Epoch:221/1000\n",
      "Loss on train= 0.017681704834103584\n",
      "Loss on test= 0.017136404290795326\n",
      "acc for Lsat= 0.07813903217514356 \n",
      "acc for Psat= 0.11602437065707313 \n",
      "acc for optim= 0.13795314551227628\n",
      "Epoch:222/1000\n",
      "Loss on train= 0.017358532175421715\n",
      "Loss on test= 0.01691845804452896\n",
      "acc for Lsat= 0.0756745274282164 \n",
      "acc for Psat= 0.11687520593404768 \n",
      "acc for optim= 0.13459381227278047\n",
      "Epoch:223/1000\n",
      "Loss on train= 0.017593054100871086\n",
      "Loss on test= 0.01819176971912384\n",
      "acc for Lsat= 0.07900858496626219 \n",
      "acc for Psat= 0.12365586592091457 \n",
      "acc for optim= 0.1322729408223596\n",
      "Epoch:224/1000\n",
      "Loss on train= 0.017588423565030098\n",
      "Loss on test= 0.01751263253390789\n",
      "acc for Lsat= 0.09020136313305961 \n",
      "acc for Psat= 0.12279182374477388 \n",
      "acc for optim= 0.1399157599442535\n",
      "Epoch:225/1000\n",
      "Loss on train= 0.017176207154989243\n",
      "Loss on test= 0.01779721863567829\n",
      "acc for Lsat= 0.07934334261549843 \n",
      "acc for Psat= 0.11790797528293401 \n",
      "acc for optim= 0.13959822571939895\n",
      "Epoch:226/1000\n",
      "Loss on train= 0.01744353584945202\n",
      "Loss on test= 0.016314273700118065\n",
      "acc for Lsat= 0.08016383714146083 \n",
      "acc for Psat= 0.12384420732657114 \n",
      "acc for optim= 0.13360134988195368\n",
      "Epoch:227/1000\n",
      "Loss on train= 0.017128586769104004\n",
      "Loss on test= 0.016604775562882423\n",
      "acc for Lsat= 0.07436421290040016 \n",
      "acc for Psat= 0.11983317401674058 \n",
      "acc for optim= 0.13438206745518577\n",
      "Epoch:228/1000\n",
      "Loss on train= 0.017225036397576332\n",
      "Loss on test= 0.017667412757873535\n",
      "acc for Lsat= 0.09685893737607534 \n",
      "acc for Psat= 0.1247491846481959 \n",
      "acc for optim= 0.1377347520242135\n",
      "Epoch:229/1000\n",
      "Loss on train= 0.016471615061163902\n",
      "Loss on test= 0.016064096242189407\n",
      "acc for Lsat= 0.07562536034319137 \n",
      "acc for Psat= 0.1260782804754045 \n",
      "acc for optim= 0.13678078761117324\n",
      "Epoch:230/1000\n",
      "Loss on train= 0.016804473474621773\n",
      "Loss on test= 0.016635149717330933\n",
      "acc for Lsat= 0.07616128300627073 \n",
      "acc for Psat= 0.11872447861565484 \n",
      "acc for optim= 0.13567215568489496\n",
      "Epoch:231/1000\n",
      "Loss on train= 0.01735462062060833\n",
      "Loss on test= 0.016744455322623253\n",
      "acc for Lsat= 0.07909011799428198 \n",
      "acc for Psat= 0.1174487097395791 \n",
      "acc for optim= 0.13373721040164432\n",
      "Epoch:232/1000\n",
      "Loss on train= 0.016921771690249443\n",
      "Loss on test= 0.017236925661563873\n",
      "acc for Lsat= 0.07892727226846748 \n",
      "acc for Psat= 0.11692047963539759 \n",
      "acc for optim= 0.13210019188829594\n",
      "Epoch:233/1000\n",
      "Loss on train= 0.01755102165043354\n",
      "Loss on test= 0.017338769510388374\n",
      "acc for Lsat= 0.08947411742475297 \n",
      "acc for Psat= 0.13375390105777318 \n",
      "acc for optim= 0.13905004693402181\n",
      "Epoch:234/1000\n",
      "Loss on train= 0.01677503064274788\n",
      "Loss on test= 0.017346875742077827\n",
      "acc for Lsat= 0.08232524643341702 \n",
      "acc for Psat= 0.12491469979286195 \n",
      "acc for optim= 0.13276621813161504\n",
      "Epoch:235/1000\n",
      "Loss on train= 0.016806213185191154\n",
      "Loss on test= 0.017607055604457855\n",
      "acc for Lsat= 0.07962508863872952 \n",
      "acc for Psat= 0.11657162076897087 \n",
      "acc for optim= 0.1438134190936883\n",
      "Epoch:236/1000\n",
      "Loss on train= 0.017504004761576653\n",
      "Loss on test= 0.018247203901410103\n",
      "acc for Lsat= 0.10008604783150885 \n",
      "acc for Psat= 0.1376680836081505 \n",
      "acc for optim= 0.13572121068007414\n",
      "Epoch:237/1000\n",
      "Loss on train= 0.017464520409703255\n",
      "Loss on test= 0.016769908368587494\n",
      "acc for Lsat= 0.08836439665820864 \n",
      "acc for Psat= 0.12024413049221039 \n",
      "acc for optim= 0.13441763938301135\n",
      "Epoch:238/1000\n",
      "Loss on train= 0.01684611104428768\n",
      "Loss on test= 0.0175247173756361\n",
      "acc for Lsat= 0.08888211167520949 \n",
      "acc for Psat= 0.12772127414743106 \n",
      "acc for optim= 0.13423594468169744\n",
      "Epoch:239/1000\n",
      "Loss on train= 0.01702508144080639\n",
      "Loss on test= 0.017320698127150536\n",
      "acc for Lsat= 0.07555773142311308 \n",
      "acc for Psat= 0.12247614098919762 \n",
      "acc for optim= 0.13647567470454508\n",
      "Epoch:240/1000\n",
      "Loss on train= 0.016656748950481415\n",
      "Loss on test= 0.01592228375375271\n",
      "acc for Lsat= 0.07869350992970996 \n",
      "acc for Psat= 0.11082458959685432 \n",
      "acc for optim= 0.13419297207146882\n",
      "Epoch:241/1000\n",
      "Loss on train= 0.01712740771472454\n",
      "Loss on test= 0.01681714504957199\n",
      "acc for Lsat= 0.08921858759389983 \n",
      "acc for Psat= 0.13764246271716224 \n",
      "acc for optim= 0.13400263976719645\n",
      "Epoch:242/1000\n",
      "Loss on train= 0.017346160486340523\n",
      "Loss on test= 0.016888462007045746\n",
      "acc for Lsat= 0.07886980738904742 \n",
      "acc for Psat= 0.11742580301231809 \n",
      "acc for optim= 0.1347418810758326\n",
      "Epoch:243/1000\n",
      "Loss on train= 0.017062775790691376\n",
      "Loss on test= 0.017198938876390457\n",
      "acc for Lsat= 0.08306451994511815 \n",
      "acc for Psat= 0.11598976353804272 \n",
      "acc for optim= 0.13403049004781578\n",
      "Epoch:244/1000\n",
      "Loss on train= 0.01733318902552128\n",
      "Loss on test= 0.018046822398900986\n",
      "acc for Lsat= 0.0926903118689855 \n",
      "acc for Psat= 0.13726944757832424 \n",
      "acc for optim= 0.13401166656985883\n",
      "Epoch:245/1000\n",
      "Loss on train= 0.016398366540670395\n",
      "Loss on test= 0.016231151297688484\n",
      "acc for Lsat= 0.07132288929488922 \n",
      "acc for Psat= 0.11156362626287672 \n",
      "acc for optim= 0.13188787700815333\n",
      "Epoch:246/1000\n",
      "Loss on train= 0.016207676380872726\n",
      "Loss on test= 0.0162831861525774\n",
      "acc for Lsat= 0.08182059692011939 \n",
      "acc for Psat= 0.11194238546821807 \n",
      "acc for optim= 0.13301574403627053\n",
      "Epoch:247/1000\n",
      "Loss on train= 0.016540594398975372\n",
      "Loss on test= 0.017420120537281036\n",
      "acc for Lsat= 0.08640236970451143 \n",
      "acc for Psat= 0.12119499411847857 \n",
      "acc for optim= 0.13551210570666522\n",
      "Epoch:248/1000\n",
      "Loss on train= 0.016899585723876953\n",
      "Loss on test= 0.0171506330370903\n",
      "acc for Lsat= 0.07470528618949984 \n",
      "acc for Psat= 0.1144878759980202 \n",
      "acc for optim= 0.13401868070165315\n",
      "Epoch:249/1000\n",
      "Loss on train= 0.016347821801900864\n",
      "Loss on test= 0.016253018751740456\n",
      "acc for Lsat= 0.07223462379641005 \n",
      "acc for Psat= 0.12016885578632354 \n",
      "acc for optim= 0.13410112692250145\n",
      "Epoch:250/1000\n",
      "Loss on train= 0.01676078327000141\n",
      "Loss on test= 0.016174226999282837\n",
      "acc for Lsat= 0.08487687210241952 \n",
      "acc for Psat= 0.11221164994769627 \n",
      "acc for optim= 0.13545751840704018\n",
      "Epoch:251/1000\n",
      "Loss on train= 0.016230421140789986\n",
      "Loss on test= 0.01660962402820587\n",
      "acc for Lsat= 0.08556932542059156 \n",
      "acc for Psat= 0.12521274950769212 \n",
      "acc for optim= 0.13932320260339315\n",
      "Epoch:252/1000\n",
      "Loss on train= 0.016523240134119987\n",
      "Loss on test= 0.01647142320871353\n",
      "acc for Lsat= 0.08252642047074106 \n",
      "acc for Psat= 0.1166446116235521 \n",
      "acc for optim= 0.13419935682581532\n",
      "Epoch:253/1000\n",
      "Loss on train= 0.016682254150509834\n",
      "Loss on test= 0.016560936346650124\n",
      "acc for Lsat= 0.0820595978034867 \n",
      "acc for Psat= 0.11609906322426267 \n",
      "acc for optim= 0.13090122782935698\n",
      "Epoch:254/1000\n",
      "Loss on train= 0.01662980206310749\n",
      "Loss on test= 0.016535578295588493\n",
      "acc for Lsat= 0.07610485197769272 \n",
      "acc for Psat= 0.11753376358085209 \n",
      "acc for optim= 0.1332877135111226\n",
      "Epoch:255/1000\n",
      "Loss on train= 0.01669846661388874\n",
      "Loss on test= 0.017246311530470848\n",
      "acc for Lsat= 0.08533731450637182 \n",
      "acc for Psat= 0.13613809115356867 \n",
      "acc for optim= 0.13462809258037145\n",
      "Epoch:256/1000\n",
      "Loss on train= 0.016277385875582695\n",
      "Loss on test= 0.016947373747825623\n",
      "acc for Lsat= 0.07799104029933611 \n",
      "acc for Psat= 0.1269428958495458 \n",
      "acc for optim= 0.13739043060276243\n",
      "Epoch:257/1000\n",
      "Loss on train= 0.016539638862013817\n",
      "Loss on test= 0.01708081364631653\n",
      "acc for Lsat= 0.07035441870490709 \n",
      "acc for Psat= 0.10710736148887211 \n",
      "acc for optim= 0.13494346737861634\n",
      "Epoch:258/1000\n",
      "Loss on train= 0.016741516068577766\n",
      "Loss on test= 0.01657973974943161\n",
      "acc for Lsat= 0.0967102590534422 \n",
      "acc for Psat= 0.15833489133252038 \n",
      "acc for optim= 0.1362062095767922\n",
      "Epoch:259/1000\n",
      "Loss on train= 0.017027821391820908\n",
      "Loss on test= 0.015838298946619034\n",
      "acc for Lsat= 0.07694350861840778 \n",
      "acc for Psat= 0.1103846260242992 \n",
      "acc for optim= 0.13223682577825258\n",
      "Epoch:260/1000\n",
      "Loss on train= 0.016409069299697876\n",
      "Loss on test= 0.01696668192744255\n",
      "acc for Lsat= 0.08571643467164701 \n",
      "acc for Psat= 0.10744438519080482 \n",
      "acc for optim= 0.1320958755289515\n",
      "Epoch:261/1000\n",
      "Loss on train= 0.01647101156413555\n",
      "Loss on test= 0.016053520143032074\n",
      "acc for Lsat= 0.0792800416549047 \n",
      "acc for Psat= 0.11412481168905891 \n",
      "acc for optim= 0.1347841872523228\n",
      "Epoch:262/1000\n",
      "Loss on train= 0.0157467033714056\n",
      "Loss on test= 0.016224278137087822\n",
      "acc for Lsat= 0.07720068262683019 \n",
      "acc for Psat= 0.1297091924481922 \n",
      "acc for optim= 0.1336814272734854\n",
      "Epoch:263/1000\n",
      "Loss on train= 0.01650615967810154\n",
      "Loss on test= 0.016383619979023933\n",
      "acc for Lsat= 0.08996607098314496 \n",
      "acc for Psat= 0.11286379098892213 \n",
      "acc for optim= 0.13289746985667283\n",
      "Epoch:264/1000\n",
      "Loss on train= 0.016405509784817696\n",
      "Loss on test= 0.01665096916258335\n",
      "acc for Lsat= 0.078866342206796 \n",
      "acc for Psat= 0.11592794557412464 \n",
      "acc for optim= 0.13659187770552106\n",
      "Epoch:265/1000\n",
      "Loss on train= 0.017059408128261566\n",
      "Loss on test= 0.016971664503216743\n",
      "acc for Lsat= 0.08990483035643897 \n",
      "acc for Psat= 0.12085374891757966 \n",
      "acc for optim= 0.1370718074341615\n",
      "Epoch:266/1000\n",
      "Loss on train= 0.01629776507616043\n",
      "Loss on test= 0.01624501869082451\n",
      "acc for Lsat= 0.08847381856499446 \n",
      "acc for Psat= 0.11938292251692878 \n",
      "acc for optim= 0.13516941952208678\n",
      "Epoch:267/1000\n",
      "Loss on train= 0.016093961894512177\n",
      "Loss on test= 0.016304589807987213\n",
      "acc for Lsat= 0.0808745861053467 \n",
      "acc for Psat= 0.10962648292382557 \n",
      "acc for optim= 0.13523896278606518\n",
      "Epoch:268/1000\n",
      "Loss on train= 0.01645367220044136\n",
      "Loss on test= 0.017001483589410782\n",
      "acc for Lsat= 0.08824875023629931 \n",
      "acc for Psat= 0.12069663140508863 \n",
      "acc for optim= 0.1345636345239149\n",
      "Epoch:269/1000\n",
      "Loss on train= 0.01662207394838333\n",
      "Loss on test= 0.016694428399205208\n",
      "acc for Lsat= 0.07562384323941336 \n",
      "acc for Psat= 0.12934750616550447 \n",
      "acc for optim= 0.14028090536594393\n",
      "Epoch:270/1000\n",
      "Loss on train= 0.015950076282024384\n",
      "Loss on test= 0.016586709767580032\n",
      "acc for Lsat= 0.07722528709305658 \n",
      "acc for Psat= 0.11768558621406555 \n",
      "acc for optim= 0.1305171091730396\n",
      "Epoch:271/1000\n",
      "Loss on train= 0.01620613969862461\n",
      "Loss on test= 0.01702812686562538\n",
      "acc for Lsat= 0.08135203503900104 \n",
      "acc for Psat= 0.1305655837059021 \n",
      "acc for optim= 0.13429207489308384\n",
      "Epoch:272/1000\n",
      "Loss on train= 0.01598614640533924\n",
      "Loss on test= 0.016234401613473892\n",
      "acc for Lsat= 0.07391443790660963 \n",
      "acc for Psat= 0.1075438951452573 \n",
      "acc for optim= 0.13203045821024312\n",
      "Epoch:273/1000\n",
      "Loss on train= 0.01588006503880024\n",
      "Loss on test= 0.016542114317417145\n",
      "acc for Lsat= 0.08479473292827606 \n",
      "acc for Psat= 0.1114613347583347 \n",
      "acc for optim= 0.13926789056923652\n",
      "Epoch:274/1000\n",
      "Loss on train= 0.015752796083688736\n",
      "Loss on test= 0.016681306064128876\n",
      "acc for Lsat= 0.07404230833053588 \n",
      "acc for Psat= 0.11431578480535082 \n",
      "acc for optim= 0.13138700005494883\n",
      "Epoch:275/1000\n",
      "Loss on train= 0.015930024906992912\n",
      "Loss on test= 0.016707057133316994\n",
      "acc for Lsat= 0.07406704657607609 \n",
      "acc for Psat= 0.11889284319347808 \n",
      "acc for optim= 0.13276201064387955\n",
      "Epoch:276/1000\n",
      "Loss on train= 0.01656022109091282\n",
      "Loss on test= 0.01692912168800831\n",
      "acc for Lsat= 0.0852822376622094 \n",
      "acc for Psat= 0.12142218417591517 \n",
      "acc for optim= 0.13624319086472195\n",
      "Epoch:277/1000\n",
      "Loss on train= 0.016575269401073456\n",
      "Loss on test= 0.01646290346980095\n",
      "acc for Lsat= 0.0846645070446862 \n",
      "acc for Psat= 0.1117496030198203 \n",
      "acc for optim= 0.13170631043612957\n",
      "Epoch:278/1000\n",
      "Loss on train= 0.016130007803440094\n",
      "Loss on test= 0.01678897999227047\n",
      "acc for Lsat= 0.07883508064680628 \n",
      "acc for Psat= 0.10704486154847676 \n",
      "acc for optim= 0.13041202862643533\n",
      "Epoch:279/1000\n",
      "Loss on train= 0.015729429200291634\n",
      "Loss on test= 0.015285301953554153\n",
      "acc for Lsat= 0.08473846366008123 \n",
      "acc for Psat= 0.1172655718194114 \n",
      "acc for optim= 0.1328978239455157\n",
      "Epoch:280/1000\n",
      "Loss on train= 0.015705443918704987\n",
      "Loss on test= 0.01640537939965725\n",
      "acc for Lsat= 0.07681089035338827 \n",
      "acc for Psat= 0.1234948241048389 \n",
      "acc for optim= 0.13505247049033645\n",
      "Epoch:281/1000\n",
      "Loss on train= 0.01615486666560173\n",
      "Loss on test= 0.01658109948039055\n",
      "acc for Lsat= 0.08265162607034048 \n",
      "acc for Psat= 0.12637709710333084 \n",
      "acc for optim= 0.13156240441732936\n",
      "Epoch:282/1000\n",
      "Loss on train= 0.015843955799937248\n",
      "Loss on test= 0.016350043937563896\n",
      "acc for Lsat= 0.07315828849871954 \n",
      "acc for Psat= 0.11114423655801348 \n",
      "acc for optim= 0.13493519690301684\n",
      "Epoch:283/1000\n",
      "Loss on train= 0.015939297154545784\n",
      "Loss on test= 0.016005579382181168\n",
      "acc for Lsat= 0.07864542884959115 \n",
      "acc for Psat= 0.11614653269449869 \n",
      "acc for optim= 0.13256883942004705\n",
      "Epoch:284/1000\n",
      "Loss on train= 0.016116701066493988\n",
      "Loss on test= 0.0170219037681818\n",
      "acc for Lsat= 0.08058865339391762 \n",
      "acc for Psat= 0.10804503593179914 \n",
      "acc for optim= 0.13576620022455851\n",
      "Epoch:285/1000\n",
      "Loss on train= 0.015224221162497997\n",
      "Loss on test= 0.01653650961816311\n",
      "acc for Lsat= 0.07528598780433339 \n",
      "acc for Psat= 0.10677227311664156 \n",
      "acc for optim= 0.1347885804664757\n",
      "Epoch:286/1000\n",
      "Loss on train= 0.01613042876124382\n",
      "Loss on test= 0.01612958498299122\n",
      "acc for Lsat= 0.07282383888959884 \n",
      "acc for Psat= 0.12473973731199901 \n",
      "acc for optim= 0.13205363129576048\n",
      "Epoch:287/1000\n",
      "Loss on train= 0.015998007729649544\n",
      "Loss on test= 0.015603401698172092\n",
      "acc for Lsat= 0.07645287911097207 \n",
      "acc for Psat= 0.13401890728208754 \n",
      "acc for optim= 0.13289345589776833\n",
      "Epoch:288/1000\n",
      "Loss on train= 0.016234589740633965\n",
      "Loss on test= 0.015598859637975693\n",
      "acc for Lsat= 0.08813798228899639 \n",
      "acc for Psat= 0.1181239191028807 \n",
      "acc for optim= 0.13462583509584264\n",
      "Epoch:289/1000\n",
      "Loss on train= 0.01562744379043579\n",
      "Loss on test= 0.016667582094669342\n",
      "acc for Lsat= 0.07205485602219902 \n",
      "acc for Psat= 0.1073563489649031 \n",
      "acc for optim= 0.13317199192113346\n",
      "Epoch:290/1000\n",
      "Loss on train= 0.015766581520438194\n",
      "Loss on test= 0.0161807369440794\n",
      "acc for Lsat= 0.07637296997838551 \n",
      "acc for Psat= 0.11727379461129507 \n",
      "acc for optim= 0.13302158688505494\n",
      "Epoch:291/1000\n",
      "Loss on train= 0.016327358782291412\n",
      "Loss on test= 0.01600584387779236\n",
      "acc for Lsat= 0.0764323181576199 \n",
      "acc for Psat= 0.11228134002950456 \n",
      "acc for optim= 0.13132467518250146\n",
      "Epoch:292/1000\n",
      "Loss on train= 0.015334679745137691\n",
      "Loss on test= 0.015828054398298264\n",
      "acc for Lsat= 0.06748853673537573 \n",
      "acc for Psat= 0.10441983714699746 \n",
      "acc for optim= 0.13199714935488174\n",
      "Epoch:293/1000\n",
      "Loss on train= 0.016310080885887146\n",
      "Loss on test= 0.01618345081806183\n",
      "acc for Lsat= 0.0777002332939042 \n",
      "acc for Psat= 0.11311802963415782 \n",
      "acc for optim= 0.13231995333400037\n",
      "Epoch:294/1000\n",
      "Loss on train= 0.015522959642112255\n",
      "Loss on test= 0.014701588079333305\n",
      "acc for Lsat= 0.08705651644203398 \n",
      "acc for Psat= 0.11844568683041468 \n",
      "acc for optim= 0.13123144780596097\n",
      "Epoch:295/1000\n",
      "Loss on train= 0.015149135142564774\n",
      "Loss on test= 0.01556368637830019\n",
      "acc for Lsat= 0.07283244050211377 \n",
      "acc for Psat= 0.10774395101600225 \n",
      "acc for optim= 0.13239510044869449\n",
      "Epoch:296/1000\n",
      "Loss on train= 0.015706080943346024\n",
      "Loss on test= 0.016576256603002548\n",
      "acc for Lsat= 0.07733311404784521 \n",
      "acc for Psat= 0.11799691087669799 \n",
      "acc for optim= 0.13095583274132674\n",
      "Epoch:297/1000\n",
      "Loss on train= 0.015527134761214256\n",
      "Loss on test= 0.01605571247637272\n",
      "acc for Lsat= 0.07737587003244294 \n",
      "acc for Psat= 0.11776418321662481 \n",
      "acc for optim= 0.1378841424981753\n",
      "Epoch:298/1000\n",
      "Loss on train= 0.015869319438934326\n",
      "Loss on test= 0.01562969759106636\n",
      "acc for Lsat= 0.0867498563395606 \n",
      "acc for Psat= 0.1124945663743549 \n",
      "acc for optim= 0.1359261143538687\n",
      "Epoch:299/1000\n",
      "Loss on train= 0.015282717533409595\n",
      "Loss on test= 0.014758463948965073\n",
      "acc for Lsat= 0.08001400861475202 \n",
      "acc for Psat= 0.11030716763602363 \n",
      "acc for optim= 0.13017853003823096\n",
      "Epoch:300/1000\n",
      "Loss on train= 0.015451832674443722\n",
      "Loss on test= 0.015743589028716087\n",
      "acc for Lsat= 0.07466629652513397 \n",
      "acc for Psat= 0.11030156347486711 \n",
      "acc for optim= 0.1324959471821785\n",
      "Epoch:301/1000\n",
      "Loss on train= 0.015365442261099815\n",
      "Loss on test= 0.01673496514558792\n",
      "acc for Lsat= 0.0995462790959411 \n",
      "acc for Psat= 0.11164719363053639 \n",
      "acc for optim= 0.1295619363586108\n",
      "Epoch:302/1000\n",
      "Loss on train= 0.016359003260731697\n",
      "Loss on test= 0.015334725379943848\n",
      "acc for Lsat= 0.08668897251288096 \n",
      "acc for Psat= 0.1143851376242108 \n",
      "acc for optim= 0.13253591747747528\n",
      "Epoch:303/1000\n",
      "Loss on train= 0.015668602660298347\n",
      "Loss on test= 0.01661079376935959\n",
      "acc for Lsat= 0.07562410997019872 \n",
      "acc for Psat= 0.1050911537475056 \n",
      "acc for optim= 0.1353315957718425\n",
      "Epoch:304/1000\n",
      "Loss on train= 0.015693189576268196\n",
      "Loss on test= 0.015670709311962128\n",
      "acc for Lsat= 0.0761978500419193 \n",
      "acc for Psat= 0.10834792256355287 \n",
      "acc for optim= 0.13234682439102066\n",
      "Epoch:305/1000\n",
      "Loss on train= 0.01565023884177208\n",
      "Loss on test= 0.01631574146449566\n",
      "acc for Lsat= 0.08147585640350978 \n",
      "acc for Psat= 0.11983005834950343 \n",
      "acc for optim= 0.13462338546911878\n",
      "Epoch:306/1000\n",
      "Loss on train= 0.015336784534156322\n",
      "Loss on test= 0.01585904136300087\n",
      "acc for Lsat= 0.09015350341796877 \n",
      "acc for Psat= 0.12559775114059446 \n",
      "acc for optim= 0.1300289982722865\n",
      "Epoch:307/1000\n",
      "Loss on train= 0.01559895183891058\n",
      "Loss on test= 0.01646268554031849\n",
      "acc for Lsat= 0.07159399406777488 \n",
      "acc for Psat= 0.11513394746515487 \n",
      "acc for optim= 0.13242027324934802\n",
      "Epoch:308/1000\n",
      "Loss on train= 0.015769246965646744\n",
      "Loss on test= 0.016514422371983528\n",
      "acc for Lsat= 0.07871525734663011 \n",
      "acc for Psat= 0.11205452283223469 \n",
      "acc for optim= 0.13710374443067447\n",
      "Epoch:309/1000\n",
      "Loss on train= 0.01536733191460371\n",
      "Loss on test= 0.01601353846490383\n",
      "acc for Lsat= 0.07825208198693064 \n",
      "acc for Psat= 0.11341850327120889 \n",
      "acc for optim= 0.1330826908763912\n",
      "Epoch:310/1000\n",
      "Loss on train= 0.014951452612876892\n",
      "Loss on test= 0.01567336730659008\n",
      "acc for Lsat= 0.0753280391295751 \n",
      "acc for Psat= 0.10786467360125647 \n",
      "acc for optim= 0.1311436560832792\n",
      "Epoch:311/1000\n",
      "Loss on train= 0.015123700723052025\n",
      "Loss on test= 0.01621684432029724\n",
      "acc for Lsat= 0.07053333111107349 \n",
      "acc for Psat= 0.10151584247748056 \n",
      "acc for optim= 0.1406119215819571\n",
      "Epoch:312/1000\n",
      "Loss on train= 0.015552248805761337\n",
      "Loss on test= 0.01611827127635479\n",
      "acc for Lsat= 0.0845384039812618 \n",
      "acc for Psat= 0.11502943386634189 \n",
      "acc for optim= 0.132028104364872\n",
      "Epoch:313/1000\n",
      "Loss on train= 0.015076139010488987\n",
      "Loss on test= 0.015981731936335564\n",
      "acc for Lsat= 0.08216578894191318 \n",
      "acc for Psat= 0.11030761102835336 \n",
      "acc for optim= 0.1338388663199213\n",
      "Epoch:314/1000\n",
      "Loss on train= 0.014906228519976139\n",
      "Loss on test= 0.01577242836356163\n",
      "acc for Lsat= 0.0796484370198515 \n",
      "acc for Psat= 0.10310962994893391 \n",
      "acc for optim= 0.13418934229347443\n",
      "Epoch:315/1000\n",
      "Loss on train= 0.014924987219274044\n",
      "Loss on test= 0.015715651214122772\n",
      "acc for Lsat= 0.07870904770162372 \n",
      "acc for Psat= 0.10470746590031518 \n",
      "acc for optim= 0.13313026746941936\n",
      "Epoch:316/1000\n",
      "Loss on train= 0.015242621302604675\n",
      "Loss on test= 0.01602252386510372\n",
      "acc for Lsat= 0.09023607042100697 \n",
      "acc for Psat= 0.1135772668653064 \n",
      "acc for optim= 0.13348458111286163\n",
      "Epoch:317/1000\n",
      "Loss on train= 0.014983147382736206\n",
      "Loss on test= 0.016110671684145927\n",
      "acc for Lsat= 0.08752144442664252 \n",
      "acc for Psat= 0.14028152724107104 \n",
      "acc for optim= 0.13169054211013848\n",
      "Epoch:318/1000\n",
      "Loss on train= 0.015257232822477818\n",
      "Loss on test= 0.015422984957695007\n",
      "acc for Lsat= 0.09274295469125111 \n",
      "acc for Psat= 0.12599624660280012 \n",
      "acc for optim= 0.13109678617782064\n",
      "Epoch:319/1000\n",
      "Loss on train= 0.015425112098455429\n",
      "Loss on test= 0.016582338139414787\n",
      "acc for Lsat= 0.07631165517701043 \n",
      "acc for Psat= 0.10940473783347343 \n",
      "acc for optim= 0.13137027248740196\n",
      "Epoch:320/1000\n",
      "Loss on train= 0.015267765149474144\n",
      "Loss on test= 0.01588473841547966\n",
      "acc for Lsat= 0.07621325064036583 \n",
      "acc for Psat= 0.12140682935714724 \n",
      "acc for optim= 0.13168875351548195\n",
      "Epoch:321/1000\n",
      "Loss on train= 0.015578901395201683\n",
      "Loss on test= 0.015476920641958714\n",
      "acc for Lsat= 0.07029593388239541 \n",
      "acc for Psat= 0.11184900287124844 \n",
      "acc for optim= 0.13477066039211222\n",
      "Epoch:322/1000\n",
      "Loss on train= 0.015366330742835999\n",
      "Loss on test= 0.016872268170118332\n",
      "acc for Lsat= 0.08111794036295679 \n",
      "acc for Psat= 0.11674464709228936 \n",
      "acc for optim= 0.13431574776768684\n",
      "Epoch:323/1000\n",
      "Loss on train= 0.015131416730582714\n",
      "Loss on test= 0.016216784715652466\n",
      "acc for Lsat= 0.07595254050360785 \n",
      "acc for Psat= 0.11980832550260757 \n",
      "acc for optim= 0.1329887264718612\n",
      "Epoch:324/1000\n",
      "Loss on train= 0.015625493600964546\n",
      "Loss on test= 0.014971880242228508\n",
      "acc for Lsat= 0.07886032909154891 \n",
      "acc for Psat= 0.10369175920883814 \n",
      "acc for optim= 0.1316459059715271\n",
      "Epoch:325/1000\n",
      "Loss on train= 0.014758065342903137\n",
      "Loss on test= 0.015580909326672554\n",
      "acc for Lsat= 0.08142705592844222 \n",
      "acc for Psat= 0.10886546961135334 \n",
      "acc for optim= 0.1330944896986087\n",
      "Epoch:326/1000\n",
      "Loss on train= 0.015054806135594845\n",
      "Loss on test= 0.016173740848898888\n",
      "acc for Lsat= 0.07976471044951014 \n",
      "acc for Psat= 0.10722746236456765 \n",
      "acc for optim= 0.1316588970522086\n",
      "Epoch:327/1000\n",
      "Loss on train= 0.015224027447402477\n",
      "Loss on test= 0.016401447355747223\n",
      "acc for Lsat= 0.08617211050457424 \n",
      "acc for Psat= 0.1123067546221945 \n",
      "acc for optim= 0.12977619096636767\n",
      "Epoch:328/1000\n",
      "Loss on train= 0.014903740026056767\n",
      "Loss on test= 0.016111290082335472\n",
      "acc for Lsat= 0.08751228451728821 \n",
      "acc for Psat= 0.11205725123484929 \n",
      "acc for optim= 0.12984399961100687\n",
      "Epoch:329/1000\n",
      "Loss on train= 0.01508315559476614\n",
      "Loss on test= 0.015607588924467564\n",
      "acc for Lsat= 0.0748908146388001 \n",
      "acc for Psat= 0.1030261052151521 \n",
      "acc for optim= 0.13220677889055676\n",
      "Epoch:330/1000\n",
      "Loss on train= 0.015009814873337746\n",
      "Loss on test= 0.016164090484380722\n",
      "acc for Lsat= 0.07168727235661615 \n",
      "acc for Psat= 0.1046847954392433 \n",
      "acc for optim= 0.1321993674669001\n",
      "Epoch:331/1000\n",
      "Loss on train= 0.015347528271377087\n",
      "Loss on test= 0.01566137932240963\n",
      "acc for Lsat= 0.07307496832476722 \n",
      "acc for Psat= 0.11011890156401528 \n",
      "acc for optim= 0.13168068507479294\n",
      "Epoch:332/1000\n",
      "Loss on train= 0.014713586308062077\n",
      "Loss on test= 0.016444317996501923\n",
      "acc for Lsat= 0.08191761242018807 \n",
      "acc for Psat= 0.1268425769276089 \n",
      "acc for optim= 0.134859457736214\n",
      "Epoch:333/1000\n",
      "Loss on train= 0.015205632895231247\n",
      "Loss on test= 0.015963353216648102\n",
      "acc for Lsat= 0.07121217822035153 \n",
      "acc for Psat= 0.1099723650349511 \n",
      "acc for optim= 0.13272979524400494\n",
      "Epoch:334/1000\n",
      "Loss on train= 0.015355473384261131\n",
      "Loss on test= 0.016860686242580414\n",
      "acc for Lsat= 0.07031724254290263 \n",
      "acc for Psat= 0.11058326694700453 \n",
      "acc for optim= 0.1326894585457113\n",
      "Epoch:335/1000\n",
      "Loss on train= 0.014827853068709373\n",
      "Loss on test= 0.015022258274257183\n",
      "acc for Lsat= 0.07139475842316945 \n",
      "acc for Psat= 0.1080725345346663 \n",
      "acc for optim= 0.12947717006835674\n",
      "Epoch:336/1000\n",
      "Loss on train= 0.014770030975341797\n",
      "Loss on test= 0.015526645816862583\n",
      "acc for Lsat= 0.07184513765904638 \n",
      "acc for Psat= 0.12256338165866006 \n",
      "acc for optim= 0.12945779193606644\n",
      "Epoch:337/1000\n",
      "Loss on train= 0.014340787194669247\n",
      "Loss on test= 0.016089128330349922\n",
      "acc for Lsat= 0.07643366704384486 \n",
      "acc for Psat= 0.12608853744135964 \n",
      "acc for optim= 0.1368636042707496\n",
      "Epoch:338/1000\n",
      "Loss on train= 0.015067154541611671\n",
      "Loss on test= 0.01542888768017292\n",
      "acc for Lsat= 0.0724500097334385 \n",
      "acc for Psat= 0.10562722269031737 \n",
      "acc for optim= 0.13455740825997461\n",
      "Epoch:339/1000\n",
      "Loss on train= 0.014565506018698215\n",
      "Loss on test= 0.016998371109366417\n",
      "acc for Lsat= 0.0792075557841195 \n",
      "acc for Psat= 0.10759826103846233 \n",
      "acc for optim= 0.13106060392326777\n",
      "Epoch:340/1000\n",
      "Loss on train= 0.014792920090258121\n",
      "Loss on test= 0.015985045582056046\n",
      "acc for Lsat= 0.07191206645220517 \n",
      "acc for Psat= 0.09647928120361435 \n",
      "acc for optim= 0.13588830100165472\n",
      "Epoch:341/1000\n",
      "Loss on train= 0.014480609446763992\n",
      "Loss on test= 0.015072507783770561\n",
      "acc for Lsat= 0.07111150721708934 \n",
      "acc for Psat= 0.10402487069368363 \n",
      "acc for optim= 0.1315161476118697\n",
      "Epoch:342/1000\n",
      "Loss on train= 0.014331149868667126\n",
      "Loss on test= 0.016213271766901016\n",
      "acc for Lsat= 0.06766304175059001 \n",
      "acc for Psat= 0.1047232339779536 \n",
      "acc for optim= 0.131480103938116\n",
      "Epoch:343/1000\n",
      "Loss on train= 0.015198283828794956\n",
      "Loss on test= 0.015557386912405491\n",
      "acc for Lsat= 0.07224295288324356 \n",
      "acc for Psat= 0.10814538697401682 \n",
      "acc for optim= 0.12998616405659252\n",
      "Epoch:344/1000\n",
      "Loss on train= 0.014053469523787498\n",
      "Loss on test= 0.01603437028825283\n",
      "acc for Lsat= 0.06851688110166126 \n",
      "acc for Psat= 0.10014223357041677 \n",
      "acc for optim= 0.13162030246522688\n",
      "Epoch:345/1000\n",
      "Loss on train= 0.015247706323862076\n",
      "Loss on test= 0.015961475670337677\n",
      "acc for Lsat= 0.06865870257218679 \n",
      "acc for Psat= 0.10572901434368558 \n",
      "acc for optim= 0.13423316958877773\n",
      "Epoch:346/1000\n",
      "Loss on train= 0.014705806039273739\n",
      "Loss on test= 0.015955545008182526\n",
      "acc for Lsat= 0.0684680281413926 \n",
      "acc for Psat= 0.10604766375488706 \n",
      "acc for optim= 0.13051732439133856\n",
      "Epoch:347/1000\n",
      "Loss on train= 0.014841759577393532\n",
      "Loss on test= 0.015571892261505127\n",
      "acc for Lsat= 0.08226603012945916 \n",
      "acc for Psat= 0.10816753026511933 \n",
      "acc for optim= 0.13439268006218805\n",
      "Epoch:348/1000\n",
      "Loss on train= 0.014998037368059158\n",
      "Loss on test= 0.01628054305911064\n",
      "acc for Lsat= 0.07266114958458476 \n",
      "acc for Psat= 0.10763717177841399 \n",
      "acc for optim= 0.1301837938113345\n",
      "Epoch:349/1000\n",
      "Loss on train= 0.014405936002731323\n",
      "Loss on test= 0.015911422669887543\n",
      "acc for Lsat= 0.06901409121023284 \n",
      "acc for Psat= 0.10343270268705157 \n",
      "acc for optim= 0.1333767070538468\n",
      "Epoch:350/1000\n",
      "Loss on train= 0.015327578410506248\n",
      "Loss on test= 0.01623011939227581\n",
      "acc for Lsat= 0.07671453886561924 \n",
      "acc for Psat= 0.1089122990767161 \n",
      "acc for optim= 0.1363233919358916\n",
      "Epoch:351/1000\n",
      "Loss on train= 0.014527975581586361\n",
      "Loss on test= 0.015955278649926186\n",
      "acc for Lsat= 0.07071563071674772 \n",
      "acc for Psat= 0.107575065890948 \n",
      "acc for optim= 0.13177915331390172\n",
      "Epoch:352/1000\n",
      "Loss on train= 0.014751321636140347\n",
      "Loss on test= 0.01593613252043724\n",
      "acc for Lsat= 0.07567874176634681 \n",
      "acc for Psat= 0.10715723600652483 \n",
      "acc for optim= 0.13744011306100423\n",
      "Epoch:353/1000\n",
      "Loss on train= 0.015012657269835472\n",
      "Loss on test= 0.015116957947611809\n",
      "acc for Lsat= 0.06910240099661881 \n",
      "acc for Psat= 0.10285630449652672 \n",
      "acc for optim= 0.13210334099001356\n",
      "Epoch:354/1000\n",
      "Loss on train= 0.014558091759681702\n",
      "Loss on test= 0.016348857432603836\n",
      "acc for Lsat= 0.07448046091530058 \n",
      "acc for Psat= 0.10665856136216058 \n",
      "acc for optim= 0.13359148514767485\n",
      "Epoch:355/1000\n",
      "Loss on train= 0.014074885286390781\n",
      "Loss on test= 0.015858866274356842\n",
      "acc for Lsat= 0.073696668446064 \n",
      "acc for Psat= 0.11230586336718663 \n",
      "acc for optim= 0.13100639184315999\n",
      "Epoch:356/1000\n",
      "Loss on train= 0.014055204577744007\n",
      "Loss on test= 0.0165249090641737\n",
      "acc for Lsat= 0.07549312048488192 \n",
      "acc for Psat= 0.11285284790727827 \n",
      "acc for optim= 0.13449097519947423\n",
      "Epoch:357/1000\n",
      "Loss on train= 0.014002128504216671\n",
      "Loss on test= 0.016214804723858833\n",
      "acc for Lsat= 0.07221114875541793 \n",
      "acc for Psat= 0.10277070436212751 \n",
      "acc for optim= 0.13162248581647873\n",
      "Epoch:358/1000\n",
      "Loss on train= 0.014357528649270535\n",
      "Loss on test= 0.016020871698856354\n",
      "acc for Lsat= 0.07641758372386297 \n",
      "acc for Psat= 0.1137314385837979 \n",
      "acc for optim= 0.13271184770597355\n",
      "Epoch:359/1000\n",
      "Loss on train= 0.014266201294958591\n",
      "Loss on test= 0.0158469770103693\n",
      "acc for Lsat= 0.08662013014157614 \n",
      "acc for Psat= 0.12250515090094671 \n",
      "acc for optim= 0.13241117232375677\n",
      "Epoch:360/1000\n",
      "Loss on train= 0.014386472292244434\n",
      "Loss on test= 0.015746213495731354\n",
      "acc for Lsat= 0.077242144714627 \n",
      "acc for Psat= 0.10458121912346945 \n",
      "acc for optim= 0.13064938440091078\n",
      "Epoch:361/1000\n",
      "Loss on train= 0.014318052679300308\n",
      "Loss on test= 0.015416475012898445\n",
      "acc for Lsat= 0.07067522944675551 \n",
      "acc for Psat= 0.10565200514263579 \n",
      "acc for optim= 0.13578140934308372\n",
      "Epoch:362/1000\n",
      "Loss on train= 0.014330274425446987\n",
      "Loss on test= 0.01381917018443346\n",
      "acc for Lsat= 0.06971284978919559 \n",
      "acc for Psat= 0.11250997649298775 \n",
      "acc for optim= 0.13146128522025213\n",
      "Epoch:363/1000\n",
      "Loss on train= 0.014118718914687634\n",
      "Loss on test= 0.015441766940057278\n",
      "acc for Lsat= 0.07825317333141962 \n",
      "acc for Psat= 0.10083578013711507 \n",
      "acc for optim= 0.13133877159820664\n",
      "Epoch:364/1000\n",
      "Loss on train= 0.014415478333830833\n",
      "Loss on test= 0.016841784119606018\n",
      "acc for Lsat= 0.08176095386346181 \n",
      "acc for Psat= 0.11144455571969353 \n",
      "acc for optim= 0.13464194494816992\n",
      "Epoch:365/1000\n",
      "Loss on train= 0.01436038687825203\n",
      "Loss on test= 0.01588454656302929\n",
      "acc for Lsat= 0.08011025355921852 \n",
      "acc for Psat= 0.11260407434569464 \n",
      "acc for optim= 0.13269997822741667\n",
      "Epoch:366/1000\n",
      "Loss on train= 0.014227970503270626\n",
      "Loss on test= 0.016003331169486046\n",
      "acc for Lsat= 0.07259323605232768 \n",
      "acc for Psat= 0.10603481531143187 \n",
      "acc for optim= 0.1379697937104437\n",
      "Epoch:367/1000\n",
      "Loss on train= 0.014275244437158108\n",
      "Loss on test= 0.01614385098218918\n",
      "acc for Lsat= 0.0773243976963891 \n",
      "acc for Psat= 0.11732476353645326 \n",
      "acc for optim= 0.13320894978112646\n",
      "Epoch:368/1000\n",
      "Loss on train= 0.014395535923540592\n",
      "Loss on test= 0.015856929123401642\n",
      "acc for Lsat= 0.071613885793421 \n",
      "acc for Psat= 0.11260286801391178 \n",
      "acc for optim= 0.13299971396724383\n",
      "Epoch:369/1000\n",
      "Loss on train= 0.01404438354074955\n",
      "Loss on test= 0.015874717384576797\n",
      "acc for Lsat= 0.06909746386938626 \n",
      "acc for Psat= 0.11132357650332983 \n",
      "acc for optim= 0.1323105134897762\n",
      "Epoch:370/1000\n",
      "Loss on train= 0.014385509304702282\n",
      "Loss on test= 0.015762625262141228\n",
      "acc for Lsat= 0.08205963795383771 \n",
      "acc for Psat= 0.1154983143011729 \n",
      "acc for optim= 0.13216999607781568\n",
      "Epoch:371/1000\n",
      "Loss on train= 0.014139673672616482\n",
      "Loss on test= 0.015780769288539886\n",
      "acc for Lsat= 0.08038490563631057 \n",
      "acc for Psat= 0.10151918745703167 \n",
      "acc for optim= 0.13391045414739183\n",
      "Epoch:372/1000\n",
      "Loss on train= 0.014294042252004147\n",
      "Loss on test= 0.01569380611181259\n",
      "acc for Lsat= 0.07108167542351616 \n",
      "acc for Psat= 0.09999613712231319 \n",
      "acc for optim= 0.13324195937150052\n",
      "Epoch:373/1000\n",
      "Loss on train= 0.013964620418846607\n",
      "Loss on test= 0.016243061050772667\n",
      "acc for Lsat= 0.08021025955677033 \n",
      "acc for Psat= 0.11804343296421899 \n",
      "acc for optim= 0.13212859506408373\n",
      "Epoch:374/1000\n",
      "Loss on train= 0.014514907263219357\n",
      "Loss on test= 0.015002701431512833\n",
      "acc for Lsat= 0.07039533725215329 \n",
      "acc for Psat= 0.09992913703123729 \n",
      "acc for optim= 0.12991525841255983\n",
      "Epoch:375/1000\n",
      "Loss on train= 0.014627578668296337\n",
      "Loss on test= 0.014520643278956413\n",
      "acc for Lsat= 0.08061576825049187 \n",
      "acc for Psat= 0.10143703387843238 \n",
      "acc for optim= 0.13165241115623047\n",
      "Epoch:376/1000\n",
      "Loss on train= 0.014332014136016369\n",
      "Loss on test= 0.015160053968429565\n",
      "acc for Lsat= 0.06771865230467583 \n",
      "acc for Psat= 0.09793378214041391 \n",
      "acc for optim= 0.13219386293656296\n",
      "Epoch:377/1000\n",
      "Loss on train= 0.014608538709580898\n",
      "Loss on test= 0.015282046049833298\n",
      "acc for Lsat= 0.08182474159532123 \n",
      "acc for Psat= 0.12508380048804815 \n",
      "acc for optim= 0.13250434142020015\n",
      "Epoch:378/1000\n",
      "Loss on train= 0.014297855086624622\n",
      "Loss on test= 0.0159787405282259\n",
      "acc for Lsat= 0.07568291276693344 \n",
      "acc for Psat= 0.11751819782786899 \n",
      "acc for optim= 0.1329173887769381\n",
      "Epoch:379/1000\n",
      "Loss on train= 0.014172637835144997\n",
      "Loss on test= 0.016771111637353897\n",
      "acc for Lsat= 0.07584084586964714 \n",
      "acc for Psat= 0.11101878301964867 \n",
      "acc for optim= 0.1388394459254212\n",
      "Epoch:380/1000\n",
      "Loss on train= 0.01396080944687128\n",
      "Loss on test= 0.015195738524198532\n",
      "acc for Lsat= 0.0715281012157599 \n",
      "acc for Psat= 0.11923529406388599 \n",
      "acc for optim= 0.1322465981874201\n",
      "Epoch:381/1000\n",
      "Loss on train= 0.014265459030866623\n",
      "Loss on test= 0.015440831892192364\n",
      "acc for Lsat= 0.07216356363561419 \n",
      "acc for Psat= 0.10324489474296572 \n",
      "acc for optim= 0.1310872236887614\n",
      "Epoch:382/1000\n",
      "Loss on train= 0.014112782664597034\n",
      "Loss on test= 0.015299477614462376\n",
      "acc for Lsat= 0.07041863004366557 \n",
      "acc for Psat= 0.10320086826880773 \n",
      "acc for optim= 0.13358742172519367\n",
      "Epoch:383/1000\n",
      "Loss on train= 0.013944646343588829\n",
      "Loss on test= 0.01619361713528633\n",
      "acc for Lsat= 0.07292654108670023 \n",
      "acc for Psat= 0.10698380072911581 \n",
      "acc for optim= 0.1319773058510489\n",
      "Epoch:384/1000\n",
      "Loss on train= 0.014238172210752964\n",
      "Loss on test= 0.015135781839489937\n",
      "acc for Lsat= 0.07443664173285167 \n",
      "acc for Psat= 0.10279797977871365 \n",
      "acc for optim= 0.13181259727312458\n",
      "Epoch:385/1000\n",
      "Loss on train= 0.013637003488838673\n",
      "Loss on test= 0.016062309965491295\n",
      "acc for Lsat= 0.0828213741381963 \n",
      "acc for Psat= 0.10976594529218145 \n",
      "acc for optim= 0.13636316508054733\n",
      "Epoch:386/1000\n",
      "Loss on train= 0.014433181844651699\n",
      "Loss on test= 0.015479449182748795\n",
      "acc for Lsat= 0.08102529562181898 \n",
      "acc for Psat= 0.11178597278065154 \n",
      "acc for optim= 0.13407081282801098\n",
      "Epoch:387/1000\n",
      "Loss on train= 0.013966355472803116\n",
      "Loss on test= 0.0167390163987875\n",
      "acc for Lsat= 0.08063031716479196 \n",
      "acc for Psat= 0.1212577157550388 \n",
      "acc for optim= 0.13202935134371122\n",
      "Epoch:388/1000\n",
      "Loss on train= 0.014275952242314816\n",
      "Loss on test= 0.015442905947566032\n",
      "acc for Lsat= 0.07751048058271408 \n",
      "acc for Psat= 0.11768062909444173 \n",
      "acc for optim= 0.13653604479299652\n",
      "Epoch:389/1000\n",
      "Loss on train= 0.013654699549078941\n",
      "Loss on test= 0.014933843165636063\n",
      "acc for Lsat= 0.06966775208711623 \n",
      "acc for Psat= 0.10075215846300126 \n",
      "acc for optim= 0.13352466316686737\n",
      "Epoch:390/1000\n",
      "Loss on train= 0.013676857575774193\n",
      "Loss on test= 0.015375695191323757\n",
      "acc for Lsat= 0.07561622162659963 \n",
      "acc for Psat= 0.09694500383403566 \n",
      "acc for optim= 0.1327365800738335\n",
      "Epoch:391/1000\n",
      "Loss on train= 0.014036736451089382\n",
      "Loss on test= 0.015387419611215591\n",
      "acc for Lsat= 0.06691174821721185 \n",
      "acc for Psat= 0.09949778533644146 \n",
      "acc for optim= 0.1351335749030113\n",
      "Epoch:392/1000\n",
      "Loss on train= 0.013821203261613846\n",
      "Loss on test= 0.01524779200553894\n",
      "acc for Lsat= 0.08842031810846594 \n",
      "acc for Psat= 0.1041782200336456 \n",
      "acc for optim= 0.13023156937625674\n",
      "Epoch:393/1000\n",
      "Loss on train= 0.014045774936676025\n",
      "Loss on test= 0.01612662523984909\n",
      "acc for Lsat= 0.08099147056539853 \n",
      "acc for Psat= 0.11179030570718977 \n",
      "acc for optim= 0.13141297035747104\n",
      "Epoch:394/1000\n",
      "Loss on train= 0.013476273976266384\n",
      "Loss on test= 0.01661817356944084\n",
      "acc for Lsat= 0.06670144287248453 \n",
      "acc for Psat= 0.09967029823197258 \n",
      "acc for optim= 0.1355905834585428\n",
      "Epoch:395/1000\n",
      "Loss on train= 0.014212001115083694\n",
      "Loss on test= 0.015622511506080627\n",
      "acc for Lsat= 0.06907428838312628 \n",
      "acc for Psat= 0.10113892091645135 \n",
      "acc for optim= 0.13096912056207655\n",
      "Epoch:396/1000\n",
      "Loss on train= 0.014181756414473057\n",
      "Loss on test= 0.01577063836157322\n",
      "acc for Lsat= 0.07287065477834809 \n",
      "acc for Psat= 0.09730327261818783 \n",
      "acc for optim= 0.13152443456153076\n",
      "Epoch:397/1000\n",
      "Loss on train= 0.014252450317144394\n",
      "Loss on test= 0.016017504036426544\n",
      "acc for Lsat= 0.06620131184657413 \n",
      "acc for Psat= 0.10196205990182028 \n",
      "acc for optim= 0.13106979102724128\n",
      "Epoch:398/1000\n",
      "Loss on train= 0.013437837362289429\n",
      "Loss on test= 0.01634734682738781\n",
      "acc for Lsat= 0.07467714548110964 \n",
      "acc for Psat= 0.12109482950634425 \n",
      "acc for optim= 0.1384189652072059\n",
      "Epoch:399/1000\n",
      "Loss on train= 0.014138606376945972\n",
      "Loss on test= 0.01563415676355362\n",
      "acc for Lsat= 0.08021556925442484 \n",
      "acc for Psat= 0.10464400963650809 \n",
      "acc for optim= 0.13081697863009242\n",
      "Epoch:400/1000\n",
      "Loss on train= 0.013860476203262806\n",
      "Loss on test= 0.01634766347706318\n",
      "acc for Lsat= 0.06829438118471039 \n",
      "acc for Psat= 0.10979105134805044 \n",
      "acc for optim= 0.13391213483280603\n",
      "Epoch:401/1000\n",
      "Loss on train= 0.013733020052313805\n",
      "Loss on test= 0.015517557971179485\n",
      "acc for Lsat= 0.06657764671577347 \n",
      "acc for Psat= 0.1000177742706405 \n",
      "acc for optim= 0.13512773447566565\n",
      "Epoch:402/1000\n",
      "Loss on train= 0.013636318035423756\n",
      "Loss on test= 0.015329254791140556\n",
      "acc for Lsat= 0.06794968131515715 \n",
      "acc for Psat= 0.10633158236742019 \n",
      "acc for optim= 0.13095770705905227\n",
      "Epoch:403/1000\n",
      "Loss on train= 0.013820304535329342\n",
      "Loss on test= 0.015363648533821106\n",
      "acc for Lsat= 0.0661421543194188 \n",
      "acc for Psat= 0.1031005067957772 \n",
      "acc for optim= 0.13063244625098178\n",
      "Epoch:404/1000\n",
      "Loss on train= 0.014077598229050636\n",
      "Loss on test= 0.016195299103856087\n",
      "acc for Lsat= 0.06492182289560637 \n",
      "acc for Psat= 0.10160681787464353 \n",
      "acc for optim= 0.1332705410818259\n",
      "Epoch:405/1000\n",
      "Loss on train= 0.013708516024053097\n",
      "Loss on test= 0.014961061999201775\n",
      "acc for Lsat= 0.0692030717101362 \n",
      "acc for Psat= 0.10617107682757906 \n",
      "acc for optim= 0.13395030755135748\n",
      "Epoch:406/1000\n",
      "Loss on train= 0.01377299427986145\n",
      "Loss on test= 0.01607205532491207\n",
      "acc for Lsat= 0.07273631509807374 \n",
      "acc for Psat= 0.10525585297081207 \n",
      "acc for optim= 0.1315924478901757\n",
      "Epoch:407/1000\n",
      "Loss on train= 0.014410295523703098\n",
      "Loss on test= 0.016312791034579277\n",
      "acc for Lsat= 0.06942460652854708 \n",
      "acc for Psat= 0.1063716585437457 \n",
      "acc for optim= 0.13362376822365657\n",
      "Epoch:408/1000\n",
      "Loss on train= 0.013912629336118698\n",
      "Loss on test= 0.016404366120696068\n",
      "acc for Lsat= 0.06711445757084422 \n",
      "acc for Psat= 0.10340587132506901 \n",
      "acc for optim= 0.1306389087604152\n",
      "Epoch:409/1000\n",
      "Loss on train= 0.013672340661287308\n",
      "Loss on test= 0.015881039202213287\n",
      "acc for Lsat= 0.07475497329400647 \n",
      "acc for Psat= 0.10906124835213027 \n",
      "acc for optim= 0.1320956726041105\n",
      "Epoch:410/1000\n",
      "Loss on train= 0.014492570422589779\n",
      "Loss on test= 0.015496226027607918\n",
      "acc for Lsat= 0.0729258739285999 \n",
      "acc for Psat= 0.10937778486145869 \n",
      "acc for optim= 0.1297207144813405\n",
      "Epoch:411/1000\n",
      "Loss on train= 0.013782361522316933\n",
      "Loss on test= 0.015574402175843716\n",
      "acc for Lsat= 0.07272196859121322 \n",
      "acc for Psat= 0.1025576631228129 \n",
      "acc for optim= 0.13153094641036459\n",
      "Epoch:412/1000\n",
      "Loss on train= 0.013651936315000057\n",
      "Loss on test= 0.016099577769637108\n",
      "acc for Lsat= 0.07029473168982399 \n",
      "acc for Psat= 0.10333208209938473 \n",
      "acc for optim= 0.13014158796932962\n",
      "Epoch:413/1000\n",
      "Loss on train= 0.0136890122666955\n",
      "Loss on test= 0.01651781238615513\n",
      "acc for Lsat= 0.06919238683250216 \n",
      "acc for Psat= 0.10364765110943053 \n",
      "acc for optim= 0.13524342642890086\n",
      "Epoch:414/1000\n",
      "Loss on train= 0.013674173504114151\n",
      "Loss on test= 0.015388373285531998\n",
      "acc for Lsat= 0.07322017583582136 \n",
      "acc for Psat= 0.09769013954533472 \n",
      "acc for optim= 0.13378631348411243\n",
      "Epoch:415/1000\n",
      "Loss on train= 0.013685166835784912\n",
      "Loss on test= 0.015087359584867954\n",
      "acc for Lsat= 0.07608684913979635 \n",
      "acc for Psat= 0.1104508830441369 \n",
      "acc for optim= 0.13700873479247092\n",
      "Epoch:416/1000\n",
      "Loss on train= 0.013431984931230545\n",
      "Loss on test= 0.01616791822016239\n",
      "acc for Lsat= 0.0688523319032457 \n",
      "acc for Psat= 0.10318706764115228 \n",
      "acc for optim= 0.1333641794820627\n",
      "Epoch:417/1000\n",
      "Loss on train= 0.013256040401756763\n",
      "Loss on test= 0.0161336287856102\n",
      "acc for Lsat= 0.07031895518302916 \n",
      "acc for Psat= 0.09709178772237566 \n",
      "acc for optim= 0.13550654881530336\n",
      "Epoch:418/1000\n",
      "Loss on train= 0.01365624088793993\n",
      "Loss on test= 0.01624308153986931\n",
      "acc for Lsat= 0.08138109826379353 \n",
      "acc for Psat= 0.10297798977957831 \n",
      "acc for optim= 0.13723009559843274\n",
      "Epoch:419/1000\n",
      "Loss on train= 0.0137714808806777\n",
      "Loss on test= 0.01565355807542801\n",
      "acc for Lsat= 0.07352654768360986 \n",
      "acc for Psat= 0.10902655290232764 \n",
      "acc for optim= 0.1371148528324233\n",
      "Epoch:420/1000\n",
      "Loss on train= 0.013543987646698952\n",
      "Loss on test= 0.015145465731620789\n",
      "acc for Lsat= 0.07855981936057407 \n",
      "acc for Psat= 0.11546980771753522 \n",
      "acc for optim= 0.13183077366815676\n",
      "Epoch:421/1000\n",
      "Loss on train= 0.01332467794418335\n",
      "Loss on test= 0.016050118952989578\n",
      "acc for Lsat= 0.07357909878094993 \n",
      "acc for Psat= 0.11083181235525343 \n",
      "acc for optim= 0.13269628629916247\n",
      "Epoch:422/1000\n",
      "Loss on train= 0.013695981353521347\n",
      "Loss on test= 0.01593892090022564\n",
      "acc for Lsat= 0.08299548923969269 \n",
      "acc for Psat= 0.10790018729037708 \n",
      "acc for optim= 0.13259531557559967\n",
      "Epoch:423/1000\n",
      "Loss on train= 0.013366690836846828\n",
      "Loss on test= 0.0157918743789196\n",
      "acc for Lsat= 0.06587711522976557 \n",
      "acc for Psat= 0.09654347664780089 \n",
      "acc for optim= 0.1342365639905135\n",
      "Epoch:424/1000\n",
      "Loss on train= 0.013706647790968418\n",
      "Loss on test= 0.015603040345013142\n",
      "acc for Lsat= 0.07686015599303775 \n",
      "acc for Psat= 0.11987746953964232 \n",
      "acc for optim= 0.13500689218441642\n",
      "Epoch:425/1000\n",
      "Loss on train= 0.013528510928153992\n",
      "Loss on test= 0.015243359841406345\n",
      "acc for Lsat= 0.06870903703901503 \n",
      "acc for Psat= 0.10486703001790576 \n",
      "acc for optim= 0.13267333292298847\n",
      "Epoch:426/1000\n",
      "Loss on train= 0.013492172583937645\n",
      "Loss on test= 0.015941962599754333\n",
      "acc for Lsat= 0.06745871760778957 \n",
      "acc for Psat= 0.0961891159415245 \n",
      "acc for optim= 0.1343858453962538\n",
      "Epoch:427/1000\n",
      "Loss on train= 0.013158369809389114\n",
      "Loss on test= 0.015887359157204628\n",
      "acc for Lsat= 0.07035074250565636 \n",
      "acc for Psat= 0.10315860625770358 \n",
      "acc for optim= 0.13494624843200048\n",
      "Epoch:428/1000\n",
      "Loss on train= 0.01376650296151638\n",
      "Loss on test= 0.015128016471862793\n",
      "acc for Lsat= 0.07467388444476658 \n",
      "acc for Psat= 0.10593367467323937 \n",
      "acc for optim= 0.13447365081972548\n",
      "Epoch:429/1000\n",
      "Loss on train= 0.01358218863606453\n",
      "Loss on test= 0.01596846431493759\n",
      "acc for Lsat= 0.07395366529623668 \n",
      "acc for Psat= 0.10408325758245254 \n",
      "acc for optim= 0.13508518031901784\n",
      "Epoch:430/1000\n",
      "Loss on train= 0.01325212698429823\n",
      "Loss on test= 0.016186527907848358\n",
      "acc for Lsat= 0.07016564615898661 \n",
      "acc for Psat= 0.11435746351877848 \n",
      "acc for optim= 0.13511801701452997\n",
      "Epoch:431/1000\n",
      "Loss on train= 0.013402353972196579\n",
      "Loss on test= 0.016607288271188736\n",
      "acc for Lsat= 0.07808601723776923 \n",
      "acc for Psat= 0.11528609659936695 \n",
      "acc for optim= 0.13403211236000062\n",
      "Epoch:432/1000\n",
      "Loss on train= 0.013603941537439823\n",
      "Loss on test= 0.015218034386634827\n",
      "acc for Lsat= 0.06465839892625808 \n",
      "acc for Psat= 0.10089557733800676 \n",
      "acc for optim= 0.13292443396316633\n",
      "Epoch:433/1000\n",
      "Loss on train= 0.013322667218744755\n",
      "Loss on test= 0.016932714730501175\n",
      "acc for Lsat= 0.06587341204285621 \n",
      "acc for Psat= 0.1050914274321662 \n",
      "acc for optim= 0.1317981919894616\n",
      "Epoch:434/1000\n",
      "Loss on train= 0.01359960064291954\n",
      "Loss on test= 0.015074708499014378\n",
      "acc for Lsat= 0.07169445372290081 \n",
      "acc for Psat= 0.09794334504339429 \n",
      "acc for optim= 0.13926896850268045\n",
      "Epoch:435/1000\n",
      "Loss on train= 0.013462040573358536\n",
      "Loss on test= 0.015438194386661053\n",
      "acc for Lsat= 0.07194102158149085 \n",
      "acc for Psat= 0.111292508078946 \n",
      "acc for optim= 0.13224216008351908\n",
      "Epoch:436/1000\n",
      "Loss on train= 0.013592404313385487\n",
      "Loss on test= 0.015408577397465706\n",
      "acc for Lsat= 0.07104898128244612 \n",
      "acc for Psat= 0.10630471375253465 \n",
      "acc for optim= 0.13248328632778592\n",
      "Epoch:437/1000\n",
      "Loss on train= 0.01317335944622755\n",
      "Loss on test= 0.015386500395834446\n",
      "acc for Lsat= 0.08029612915383444 \n",
      "acc for Psat= 0.09835307333204482 \n",
      "acc for optim= 0.13409917851289113\n",
      "Epoch:438/1000\n",
      "Loss on train= 0.01306740939617157\n",
      "Loss on test= 0.015687057748436928\n",
      "acc for Lsat= 0.07588640600442886 \n",
      "acc for Psat= 0.09780036956071855 \n",
      "acc for optim= 0.13463959130975936\n",
      "Epoch:439/1000\n",
      "Loss on train= 0.013888218440115452\n",
      "Loss on test= 0.015591606497764587\n",
      "acc for Lsat= 0.0656320795830753 \n",
      "acc for Psat= 0.09748441212707096 \n",
      "acc for optim= 0.13664976242515778\n",
      "Epoch:440/1000\n",
      "Loss on train= 0.013797649182379246\n",
      "Loss on test= 0.01639336161315441\n",
      "acc for Lsat= 0.07487744871113035 \n",
      "acc for Psat= 0.10565656953387792 \n",
      "acc for optim= 0.13445320799946786\n",
      "Epoch:441/1000\n",
      "Loss on train= 0.013463199138641357\n",
      "Loss on test= 0.01594661921262741\n",
      "acc for Lsat= 0.07899821864234077 \n",
      "acc for Psat= 0.10615434878402286 \n",
      "acc for optim= 0.1330243169847462\n",
      "Epoch:442/1000\n",
      "Loss on train= 0.014050581492483616\n",
      "Loss on test= 0.0160052552819252\n",
      "acc for Lsat= 0.06767446034484438 \n",
      "acc for Psat= 0.10423988004525502 \n",
      "acc for optim= 0.13816697547833123\n",
      "Epoch:443/1000\n",
      "Loss on train= 0.013068201951682568\n",
      "Loss on test= 0.015234763734042645\n",
      "acc for Lsat= 0.07006094662679567 \n",
      "acc for Psat= 0.09999336020814048 \n",
      "acc for optim= 0.1343911952442593\n",
      "Epoch:444/1000\n",
      "Loss on train= 0.01365260686725378\n",
      "Loss on test= 0.01592407189309597\n",
      "acc for Lsat= 0.0663128309779697 \n",
      "acc for Psat= 0.09885605987575317 \n",
      "acc for optim= 0.13525839795668917\n",
      "Epoch:445/1000\n",
      "Loss on train= 0.013270581141114235\n",
      "Loss on test= 0.01659349538385868\n",
      "acc for Lsat= 0.06833765887551838 \n",
      "acc for Psat= 0.09708432422743901 \n",
      "acc for optim= 0.13708991151716973\n",
      "Epoch:446/1000\n",
      "Loss on train= 0.01319341454654932\n",
      "Loss on test= 0.01614556834101677\n",
      "acc for Lsat= 0.06835159146123462 \n",
      "acc for Psat= 0.10033232387569214 \n",
      "acc for optim= 0.13458142334388362\n",
      "Epoch:447/1000\n",
      "Loss on train= 0.013218411244452\n",
      "Loss on test= 0.015533315017819405\n",
      "acc for Lsat= 0.0721532869670126 \n",
      "acc for Psat= 0.11107733249664306 \n",
      "acc for optim= 0.13545961851874985\n",
      "Epoch:448/1000\n",
      "Loss on train= 0.012904650531709194\n",
      "Loss on test= 0.01627243123948574\n",
      "acc for Lsat= 0.0662275183531973 \n",
      "acc for Psat= 0.09600004156430561 \n",
      "acc for optim= 0.1398703411221504\n",
      "Epoch:449/1000\n",
      "Loss on train= 0.013536500744521618\n",
      "Loss on test= 0.014751706272363663\n",
      "acc for Lsat= 0.07869099295801586 \n",
      "acc for Psat= 0.1069242192639245 \n",
      "acc for optim= 0.1342056948277685\n",
      "Epoch:450/1000\n",
      "Loss on train= 0.013485144823789597\n",
      "Loss on test= 0.015986362472176552\n",
      "acc for Lsat= 0.07011260175042683 \n",
      "acc for Psat= 0.09720736278427973 \n",
      "acc for optim= 0.1347134134835667\n",
      "Epoch:451/1000\n",
      "Loss on train= 0.013391408137977123\n",
      "Loss on test= 0.014844262041151524\n",
      "acc for Lsat= 0.07342455585797628 \n",
      "acc for Psat= 0.10480309095647601 \n",
      "acc for optim= 0.1378135067721208\n",
      "Epoch:452/1000\n",
      "Loss on train= 0.013048538006842136\n",
      "Loss on test= 0.016068018972873688\n",
      "acc for Lsat= 0.06675508519013723 \n",
      "acc for Psat= 0.09567574792438083 \n",
      "acc for optim= 0.1363461247748799\n",
      "Epoch:453/1000\n",
      "Loss on train= 0.012891174294054508\n",
      "Loss on test= 0.016265446320176125\n",
      "acc for Lsat= 0.08777150346173179 \n",
      "acc for Psat= 0.11115219510263868 \n",
      "acc for optim= 0.13707395162847305\n",
      "Epoch:454/1000\n",
      "Loss on train= 0.013481784611940384\n",
      "Loss on test= 0.0164044126868248\n",
      "acc for Lsat= 0.06583127213848962 \n",
      "acc for Psat= 0.09975759469800527 \n",
      "acc for optim= 0.13715135438574683\n",
      "Epoch:455/1000\n",
      "Loss on train= 0.01332248467952013\n",
      "Loss on test= 0.01570660062134266\n",
      "acc for Lsat= 0.0677891453107198 \n",
      "acc for Psat= 0.09873538182841407 \n",
      "acc for optim= 0.13466832952366933\n",
      "Epoch:456/1000\n",
      "Loss on train= 0.013222145847976208\n",
      "Loss on test= 0.016993258148431778\n",
      "acc for Lsat= 0.07600286056598027 \n",
      "acc for Psat= 0.10231035268969008 \n",
      "acc for optim= 0.13343583188123173\n",
      "Epoch:457/1000\n",
      "Loss on train= 0.012867515906691551\n",
      "Loss on test= 0.01658138632774353\n",
      "acc for Lsat= 0.07110480566819509 \n",
      "acc for Psat= 0.1034913899170028 \n",
      "acc for optim= 0.13240501284599301\n",
      "Epoch:458/1000\n",
      "Loss on train= 0.013163379393517971\n",
      "Loss on test= 0.015903541818261147\n",
      "acc for Lsat= 0.06617732499208716 \n",
      "acc for Psat= 0.10086232423782347 \n",
      "acc for optim= 0.13592397719621657\n",
      "Epoch:459/1000\n",
      "Loss on train= 0.012999990954995155\n",
      "Loss on test= 0.015100576914846897\n",
      "acc for Lsat= 0.06500021070241929 \n",
      "acc for Psat= 0.10101958165566126 \n",
      "acc for optim= 0.13296653727690377\n",
      "Epoch:460/1000\n",
      "Loss on train= 0.012960526160895824\n",
      "Loss on test= 0.015495343133807182\n",
      "acc for Lsat= 0.07889478753010433 \n",
      "acc for Psat= 0.11537429259883034 \n",
      "acc for optim= 0.13814632983671293\n",
      "Epoch:461/1000\n",
      "Loss on train= 0.013118591159582138\n",
      "Loss on test= 0.015822285786271095\n",
      "acc for Lsat= 0.06378432909647623 \n",
      "acc for Psat= 0.09961760772599114 \n",
      "acc for optim= 0.13252794535623658\n",
      "Epoch:462/1000\n",
      "Loss on train= 0.013150074519217014\n",
      "Loss on test= 0.015799084678292274\n",
      "acc for Lsat= 0.06617196044988101 \n",
      "acc for Psat= 0.09751580854256947 \n",
      "acc for optim= 0.13932121164268918\n",
      "Epoch:463/1000\n",
      "Loss on train= 0.012795832939445972\n",
      "Loss on test= 0.015839770436286926\n",
      "acc for Lsat= 0.06421075819267168 \n",
      "acc for Psat= 0.09795209997230105 \n",
      "acc for optim= 0.13337081273396809\n",
      "Epoch:464/1000\n",
      "Loss on train= 0.013069037348031998\n",
      "Loss on test= 0.014943752437829971\n",
      "acc for Lsat= 0.0686523988429043 \n",
      "acc for Psat= 0.09594272441334196 \n",
      "acc for optim= 0.13550624731514188\n",
      "Epoch:465/1000\n",
      "Loss on train= 0.012616232968866825\n",
      "Loss on test= 0.014850292354822159\n",
      "acc for Lsat= 0.07148523587319586 \n",
      "acc for Psat= 0.09762901067733765 \n",
      "acc for optim= 0.13981593400239942\n",
      "Epoch:466/1000\n",
      "Loss on train= 0.012705999426543713\n",
      "Loss on test= 0.015661271288990974\n",
      "acc for Lsat= 0.06507373799880346 \n",
      "acc for Psat= 0.09708242548836601 \n",
      "acc for optim= 0.1356164127588272\n",
      "Epoch:467/1000\n",
      "Loss on train= 0.012903138995170593\n",
      "Loss on test= 0.016541235148906708\n",
      "acc for Lsat= 0.07165049562851589 \n",
      "acc for Psat= 0.10035562283462948 \n",
      "acc for optim= 0.13757213503122329\n",
      "Epoch:468/1000\n",
      "Loss on train= 0.012895787134766579\n",
      "Loss on test= 0.015511661767959595\n",
      "acc for Lsat= 0.06591270888845126 \n",
      "acc for Psat= 0.10026838928461075 \n",
      "acc for optim= 0.13455603073040645\n",
      "Epoch:469/1000\n",
      "Loss on train= 0.012821360491216183\n",
      "Loss on test= 0.01575862616300583\n",
      "acc for Lsat= 0.06900216456916598 \n",
      "acc for Psat= 0.0983489390876558 \n",
      "acc for optim= 0.1332814473244879\n",
      "Epoch:470/1000\n",
      "Loss on train= 0.012875890359282494\n",
      "Loss on test= 0.015409733168780804\n",
      "acc for Lsat= 0.06463865405983396 \n",
      "acc for Psat= 0.09659434606631596 \n",
      "acc for optim= 0.13908600194586646\n",
      "Epoch:471/1000\n",
      "Loss on train= 0.013118147850036621\n",
      "Loss on test= 0.015232688747346401\n",
      "acc for Lsat= 0.077299864590168 \n",
      "acc for Psat= 0.10712074057923424 \n",
      "acc for optim= 0.13520835588375726\n",
      "Epoch:472/1000\n",
      "Loss on train= 0.01310406718403101\n",
      "Loss on test= 0.015726303681731224\n",
      "acc for Lsat= 0.06676540772120158 \n",
      "acc for Psat= 0.09807361728615231 \n",
      "acc for optim= 0.13467286013894608\n",
      "Epoch:473/1000\n",
      "Loss on train= 0.013318323530256748\n",
      "Loss on test= 0.016185928136110306\n",
      "acc for Lsat= 0.06203888513975672 \n",
      "acc for Psat= 0.09350624779860178 \n",
      "acc for optim= 0.13857745544777977\n",
      "Epoch:474/1000\n",
      "Loss on train= 0.013073825277388096\n",
      "Loss on test= 0.01573585718870163\n",
      "acc for Lsat= 0.06978945798344083 \n",
      "acc for Psat= 0.10052565998501246 \n",
      "acc for optim= 0.13681414706839454\n",
      "Epoch:475/1000\n",
      "Loss on train= 0.012991460971534252\n",
      "Loss on test= 0.015914643183350563\n",
      "acc for Lsat= 0.0677465885877609 \n",
      "acc for Psat= 0.09644157058662838 \n",
      "acc for optim= 0.1385979488492012\n",
      "Epoch:476/1000\n",
      "Loss on train= 0.012802734039723873\n",
      "Loss on test= 0.016786260530352592\n",
      "acc for Lsat= 0.07694325745105743 \n",
      "acc for Psat= 0.10018839273187848 \n",
      "acc for optim= 0.13519791579908796\n",
      "Epoch:477/1000\n",
      "Loss on train= 0.012885778211057186\n",
      "Loss on test= 0.016118831932544708\n",
      "acc for Lsat= 0.07741835572653347 \n",
      "acc for Psat= 0.10702862160073386 \n",
      "acc for optim= 0.13651614942484433\n",
      "Epoch:478/1000\n",
      "Loss on train= 0.012952533550560474\n",
      "Loss on test= 0.014631529338657856\n",
      "acc for Lsat= 0.06836174610588285 \n",
      "acc for Psat= 0.10101311024692324 \n",
      "acc for optim= 0.13762458968493674\n",
      "Epoch:479/1000\n",
      "Loss on train= 0.01254977285861969\n",
      "Loss on test= 0.016250481829047203\n",
      "acc for Lsat= 0.06995508058203592 \n",
      "acc for Psat= 0.10593909521897632 \n",
      "acc for optim= 0.13752268221643238\n",
      "Epoch:480/1000\n",
      "Loss on train= 0.012460409663617611\n",
      "Loss on test= 0.015249616466462612\n",
      "acc for Lsat= 0.06563841725389162 \n",
      "acc for Psat= 0.10069432804981866 \n",
      "acc for optim= 0.13409555488162572\n",
      "Epoch:481/1000\n",
      "Loss on train= 0.012727716006338596\n",
      "Loss on test= 0.01525727566331625\n",
      "acc for Lsat= 0.06852566417720583 \n",
      "acc for Psat= 0.09674112349748612 \n",
      "acc for optim= 0.1390450569490592\n",
      "Epoch:482/1000\n",
      "Loss on train= 0.012504511512815952\n",
      "Loss on test= 0.01575348526239395\n",
      "acc for Lsat= 0.0764068967766232 \n",
      "acc for Psat= 0.10744380205869676 \n",
      "acc for optim= 0.1376867138677173\n",
      "Epoch:483/1000\n",
      "Loss on train= 0.01303964201360941\n",
      "Loss on test= 0.015480529516935349\n",
      "acc for Lsat= 0.07141273766756058 \n",
      "acc for Psat= 0.1040360551741388 \n",
      "acc for optim= 0.13538419604301452\n",
      "Epoch:484/1000\n",
      "Loss on train= 0.012812620960175991\n",
      "Loss on test= 0.015871362760663033\n",
      "acc for Lsat= 0.06374151979883512 \n",
      "acc for Psat= 0.10246525853872299 \n",
      "acc for optim= 0.1376673898763127\n",
      "Epoch:485/1000\n",
      "Loss on train= 0.012566208839416504\n",
      "Loss on test= 0.015137231908738613\n",
      "acc for Lsat= 0.06375637195176549 \n",
      "acc for Psat= 0.09553047617276508 \n",
      "acc for optim= 0.13798432383272383\n",
      "Epoch:486/1000\n",
      "Loss on train= 0.012538312934339046\n",
      "Loss on test= 0.015682509168982506\n",
      "acc for Lsat= 0.08431366268131467 \n",
      "acc for Psat= 0.1035559399260415 \n",
      "acc for optim= 0.13792140235503514\n",
      "Epoch:487/1000\n",
      "Loss on train= 0.013023353181779385\n",
      "Loss on test= 0.015238393098115921\n",
      "acc for Lsat= 0.06798635448018708 \n",
      "acc for Psat= 0.09787608037392298 \n",
      "acc for optim= 0.13549137016137436\n",
      "Epoch:488/1000\n",
      "Loss on train= 0.012242603115737438\n",
      "Loss on test= 0.016375556588172913\n",
      "acc for Lsat= 0.06942618207799064 \n",
      "acc for Psat= 0.10690918962160745 \n",
      "acc for optim= 0.1372942376467917\n",
      "Epoch:489/1000\n",
      "Loss on train= 0.012942502275109291\n",
      "Loss on test= 0.015595631673932076\n",
      "acc for Lsat= 0.06452912183271514 \n",
      "acc for Psat= 0.10138022585047618 \n",
      "acc for optim= 0.13348955942524804\n",
      "Epoch:490/1000\n",
      "Loss on train= 0.01269765105098486\n",
      "Loss on test= 0.016287911683321\n",
      "acc for Lsat= 0.06428286458055177 \n",
      "acc for Psat= 0.09352255231804317 \n",
      "acc for optim= 0.14078401426474257\n",
      "Epoch:491/1000\n",
      "Loss on train= 0.012523329816758633\n",
      "Loss on test= 0.01648319512605667\n",
      "acc for Lsat= 0.07595037619272867 \n",
      "acc for Psat= 0.1108352897895707 \n",
      "acc for optim= 0.1367829449474812\n",
      "Epoch:492/1000\n",
      "Loss on train= 0.012907451018691063\n",
      "Loss on test= 0.014749501831829548\n",
      "acc for Lsat= 0.07048802748322489 \n",
      "acc for Psat= 0.09399847355153826 \n",
      "acc for optim= 0.13586424200071226\n",
      "Epoch:493/1000\n",
      "Loss on train= 0.012829184532165527\n",
      "Loss on test= 0.01583743467926979\n",
      "acc for Lsat= 0.08146074902680185 \n",
      "acc for Psat= 0.09994936916563246 \n",
      "acc for optim= 0.1376572968231307\n",
      "Epoch:494/1000\n",
      "Loss on train= 0.012855079025030136\n",
      "Loss on test= 0.015227492898702621\n",
      "acc for Lsat= 0.06268979824251598 \n",
      "acc for Psat= 0.09437252895699609 \n",
      "acc for optim= 0.13628635671403672\n",
      "Epoch:495/1000\n",
      "Loss on train= 0.01291379053145647\n",
      "Loss on test= 0.01599741540849209\n",
      "acc for Lsat= 0.06832904857065944 \n",
      "acc for Psat= 0.09751835250192217 \n",
      "acc for optim= 0.13960426209701432\n",
      "Epoch:496/1000\n",
      "Loss on train= 0.012802395969629288\n",
      "Loss on test= 0.01629781909286976\n",
      "acc for Lsat= 0.06738052533732519 \n",
      "acc for Psat= 0.09813426633675892 \n",
      "acc for optim= 0.13435131460428237\n",
      "Epoch:497/1000\n",
      "Loss on train= 0.012769869528710842\n",
      "Loss on test= 0.016832752153277397\n",
      "acc for Lsat= 0.07066314899259142 \n",
      "acc for Psat= 0.10217634720934761 \n",
      "acc for optim= 0.13629153254959317\n",
      "Epoch:498/1000\n",
      "Loss on train= 0.012581190094351768\n",
      "Loss on test= 0.015753911808133125\n",
      "acc for Lsat= 0.07223084767659506 \n",
      "acc for Psat= 0.09877998414966797 \n",
      "acc for optim= 0.1377457689907816\n",
      "Epoch:499/1000\n",
      "Loss on train= 0.012699585407972336\n",
      "Loss on test= 0.015647033229470253\n",
      "acc for Lsat= 0.06644024327397346 \n",
      "acc for Psat= 0.09668267667293547 \n",
      "acc for optim= 0.13535543514622583\n",
      "Epoch:500/1000\n",
      "Loss on train= 0.012488839216530323\n",
      "Loss on test= 0.01587885618209839\n",
      "acc for Lsat= 0.06735977116558288 \n",
      "acc for Psat= 0.09929537243313259 \n",
      "acc for optim= 0.1375079953008228\n",
      "Epoch:501/1000\n",
      "Loss on train= 0.012644853442907333\n",
      "Loss on test= 0.01582617498934269\n",
      "acc for Lsat= 0.06451293312840992 \n",
      "acc for Psat= 0.09969914456208546 \n",
      "acc for optim= 0.13859850731160905\n",
      "Epoch:502/1000\n",
      "Loss on train= 0.012572512030601501\n",
      "Loss on test= 0.015154942870140076\n",
      "acc for Lsat= 0.07036076188087464 \n",
      "acc for Psat= 0.09707176188627878 \n",
      "acc for optim= 0.13785850273238287\n",
      "Epoch:503/1000\n",
      "Loss on train= 0.012273398227989674\n",
      "Loss on test= 0.01638752967119217\n",
      "acc for Lsat= 0.09155861934026083 \n",
      "acc for Psat= 0.09845990505483417 \n",
      "acc for optim= 0.1350694528884358\n",
      "Epoch:504/1000\n",
      "Loss on train= 0.012473148293793201\n",
      "Loss on test= 0.01568623073399067\n",
      "acc for Lsat= 0.07515941427813637 \n",
      "acc for Psat= 0.09732310209009382 \n",
      "acc for optim= 0.13913111421797014\n",
      "Epoch:505/1000\n",
      "Loss on train= 0.012691933661699295\n",
      "Loss on test= 0.015696028247475624\n",
      "acc for Lsat= 0.06590359558661779 \n",
      "acc for Psat= 0.09810276246733135 \n",
      "acc for optim= 0.13642064308126767\n",
      "Epoch:506/1000\n",
      "Loss on train= 0.012926372699439526\n",
      "Loss on test= 0.01684614084661007\n",
      "acc for Lsat= 0.06290101516577934 \n",
      "acc for Psat= 0.10120559690727127 \n",
      "acc for optim= 0.13682115657462013\n",
      "Epoch:507/1000\n",
      "Loss on train= 0.012591672129929066\n",
      "Loss on test= 0.01654728688299656\n",
      "acc for Lsat= 0.07455727466278605 \n",
      "acc for Psat= 0.1036175592078103 \n",
      "acc for optim= 0.1351593189769321\n",
      "Epoch:508/1000\n",
      "Loss on train= 0.012782374396920204\n",
      "Loss on test= 0.016435040161013603\n",
      "acc for Lsat= 0.07066743506325616 \n",
      "acc for Psat= 0.11027412447664474 \n",
      "acc for optim= 0.1386982187628746\n",
      "Epoch:509/1000\n",
      "Loss on train= 0.013118017464876175\n",
      "Loss on test= 0.016754798591136932\n",
      "acc for Lsat= 0.0625631149030394 \n",
      "acc for Psat= 0.09582775226897663 \n",
      "acc for optim= 0.13805414736270902\n",
      "Epoch:510/1000\n",
      "Loss on train= 0.012257370166480541\n",
      "Loss on test= 0.016226761043071747\n",
      "acc for Lsat= 0.0696065429184172 \n",
      "acc for Psat= 0.10140867051151063 \n",
      "acc for optim= 0.13787229748235807\n",
      "Epoch:511/1000\n",
      "Loss on train= 0.012516780756413937\n",
      "Loss on test= 0.01517509575933218\n",
      "acc for Lsat= 0.06778977231846915 \n",
      "acc for Psat= 0.10328380184041128 \n",
      "acc for optim= 0.13925940353009436\n",
      "Epoch:512/1000\n",
      "Loss on train= 0.012689569965004921\n",
      "Loss on test= 0.015673313289880753\n",
      "acc for Lsat= 0.06818099303378 \n",
      "acc for Psat= 0.10347033209270899 \n",
      "acc for optim= 0.13618415022889774\n",
      "Epoch:513/1000\n",
      "Loss on train= 0.012180688790977001\n",
      "Loss on test= 0.015702636912465096\n",
      "acc for Lsat= 0.07238269928428862 \n",
      "acc for Psat= 0.1046177003118727 \n",
      "acc for optim= 0.13679782234960133\n",
      "Epoch:514/1000\n",
      "Loss on train= 0.012043229304254055\n",
      "Loss on test= 0.015345592051744461\n",
      "acc for Lsat= 0.0630618914961815 \n",
      "acc for Psat= 0.0942874944044484 \n",
      "acc for optim= 0.13619459039635126\n",
      "Epoch:515/1000\n",
      "Loss on train= 0.012284412048757076\n",
      "Loss on test= 0.017023615539073944\n",
      "acc for Lsat= 0.06309120166632864 \n",
      "acc for Psat= 0.09775771879487569 \n",
      "acc for optim= 0.13880900856521394\n",
      "Epoch:516/1000\n",
      "Loss on train= 0.012000218965113163\n",
      "Loss on test= 0.015832414850592613\n",
      "acc for Lsat= 0.06356781389978197 \n",
      "acc for Psat= 0.0969178689850701 \n",
      "acc for optim= 0.13560166872209972\n",
      "Epoch:517/1000\n",
      "Loss on train= 0.012325573712587357\n",
      "Loss on test= 0.015896746888756752\n",
      "acc for Lsat= 0.06111302706930372 \n",
      "acc for Psat= 0.0994654996527566 \n",
      "acc for optim= 0.1403067972924974\n",
      "Epoch:518/1000\n",
      "Loss on train= 0.012278211303055286\n",
      "Loss on test= 0.01626427099108696\n",
      "acc for Lsat= 0.07905201415220897 \n",
      "acc for Psat= 0.09700211468670104 \n",
      "acc for optim= 0.13590713143348696\n",
      "Epoch:519/1000\n",
      "Loss on train= 0.013192626647651196\n",
      "Loss on test= 0.015485148876905441\n",
      "acc for Lsat= 0.07223567432827421 \n",
      "acc for Psat= 0.09978968368636237 \n",
      "acc for optim= 0.13961898999081718\n",
      "Epoch:520/1000\n",
      "Loss on train= 0.012400966137647629\n",
      "Loss on test= 0.015729866921901703\n",
      "acc for Lsat= 0.06402564702762498 \n",
      "acc for Psat= 0.09244059837526745 \n",
      "acc for optim= 0.13575315193997492\n",
      "Epoch:521/1000\n",
      "Loss on train= 0.012352156452834606\n",
      "Loss on test= 0.016237780451774597\n",
      "acc for Lsat= 0.07742551349931293 \n",
      "acc for Psat= 0.10857833921909334 \n",
      "acc for optim= 0.13897805106308725\n",
      "Epoch:522/1000\n",
      "Loss on train= 0.013175686821341515\n",
      "Loss on test= 0.01606207899749279\n",
      "acc for Lsat= 0.06556761728392707 \n",
      "acc for Psat= 0.09965899603234399 \n",
      "acc for optim= 0.14030159339308737\n",
      "Epoch:523/1000\n",
      "Loss on train= 0.012140932492911816\n",
      "Loss on test= 0.01638782024383545\n",
      "acc for Lsat= 0.0677452286084493 \n",
      "acc for Psat= 0.10202661156654359 \n",
      "acc for optim= 0.13629914505614174\n",
      "Epoch:524/1000\n",
      "Loss on train= 0.012497266754508018\n",
      "Loss on test= 0.015891198068857193\n",
      "acc for Lsat= 0.06729821537931761 \n",
      "acc for Psat= 0.10035109536515341 \n",
      "acc for optim= 0.13782801710897022\n",
      "Epoch:525/1000\n",
      "Loss on train= 0.012599308043718338\n",
      "Loss on test= 0.015200462192296982\n",
      "acc for Lsat= 0.06332958622111214 \n",
      "acc for Psat= 0.09811482164594862 \n",
      "acc for optim= 0.13781680853830444\n",
      "Epoch:526/1000\n",
      "Loss on train= 0.012818645685911179\n",
      "Loss on test= 0.01585528254508972\n",
      "acc for Lsat= 0.07255139350891113 \n",
      "acc for Psat= 0.1001282383998235 \n",
      "acc for optim= 0.1392070276869668\n",
      "Epoch:527/1000\n",
      "Loss on train= 0.012409650720655918\n",
      "Loss on test= 0.015981534495949745\n",
      "acc for Lsat= 0.07688696218861474 \n",
      "acc for Psat= 0.0968237834672133 \n",
      "acc for optim= 0.13700499799516466\n",
      "Epoch:528/1000\n",
      "Loss on train= 0.012187001295387745\n",
      "Loss on test= 0.015499521046876907\n",
      "acc for Lsat= 0.07152267917990685 \n",
      "acc for Psat= 0.09929822352197437 \n",
      "acc for optim= 0.13697936957081158\n",
      "Epoch:529/1000\n",
      "Loss on train= 0.01251930557191372\n",
      "Loss on test= 0.01571819558739662\n",
      "acc for Lsat= 0.06601725353135002 \n",
      "acc for Psat= 0.09755370616912841 \n",
      "acc for optim= 0.13828010799156296\n",
      "Epoch:530/1000\n",
      "Loss on train= 0.012092340737581253\n",
      "Loss on test= 0.016776982694864273\n",
      "acc for Lsat= 0.0625148450748788 \n",
      "acc for Psat= 0.09598831799295213 \n",
      "acc for optim= 0.139556607686811\n",
      "Epoch:531/1000\n",
      "Loss on train= 0.012642329558730125\n",
      "Loss on test= 0.015411368571221828\n",
      "acc for Lsat= 0.06582712911897234 \n",
      "acc for Psat= 0.09188508076800243 \n",
      "acc for optim= 0.13763293301065763\n",
      "Epoch:532/1000\n",
      "Loss on train= 0.012626299634575844\n",
      "Loss on test= 0.015464838594198227\n",
      "acc for Lsat= 0.07028963963190714 \n",
      "acc for Psat= 0.10156205462084876 \n",
      "acc for optim= 0.13877589719163047\n",
      "Epoch:533/1000\n",
      "Loss on train= 0.012261521071195602\n",
      "Loss on test= 0.015276170335710049\n",
      "acc for Lsat= 0.06621247678995132 \n",
      "acc for Psat= 0.0984732632835706 \n",
      "acc for optim= 0.13882859067784417\n",
      "Epoch:534/1000\n",
      "Loss on train= 0.012331491336226463\n",
      "Loss on test= 0.016325684264302254\n",
      "acc for Lsat= 0.06290497167242898 \n",
      "acc for Psat= 0.09568832003408008 \n",
      "acc for optim= 0.13933999604649017\n",
      "Epoch:535/1000\n",
      "Loss on train= 0.011888284236192703\n",
      "Loss on test= 0.01627303846180439\n",
      "acc for Lsat= 0.06586238882607884 \n",
      "acc for Psat= 0.09879279368453556 \n",
      "acc for optim= 0.14042558835612404\n",
      "Epoch:536/1000\n",
      "Loss on train= 0.01238735020160675\n",
      "Loss on test= 0.01565106399357319\n",
      "acc for Lsat= 0.0678392914434274 \n",
      "acc for Psat= 0.09405565452244548 \n",
      "acc for optim= 0.13912245084842043\n",
      "Epoch:537/1000\n",
      "Loss on train= 0.012050327844917774\n",
      "Loss on test= 0.01516178622841835\n",
      "acc for Lsat= 0.06287301729122798 \n",
      "acc for Psat= 0.0940636020567682 \n",
      "acc for optim= 0.13707462482982213\n",
      "Epoch:538/1000\n",
      "Loss on train= 0.01278548315167427\n",
      "Loss on test= 0.015527273528277874\n",
      "acc for Lsat= 0.06291389349434111 \n",
      "acc for Psat= 0.09485584100087485 \n",
      "acc for optim= 0.1411487099197176\n",
      "Epoch:539/1000\n",
      "Loss on train= 0.011819454841315746\n",
      "Loss on test= 0.0155263626947999\n",
      "acc for Lsat= 0.06500458046793939 \n",
      "acc for Psat= 0.09326275984446206 \n",
      "acc for optim= 0.13734750250975292\n",
      "Epoch:540/1000\n",
      "Loss on train= 0.012083941139280796\n",
      "Loss on test= 0.015970932319760323\n",
      "acc for Lsat= 0.06936346838871639 \n",
      "acc for Psat= 0.09704428116480511 \n",
      "acc for optim= 0.13791870623826982\n",
      "Epoch:541/1000\n",
      "Loss on train= 0.011872837319970131\n",
      "Loss on test= 0.01557353138923645\n",
      "acc for Lsat= 0.06617344501945707 \n",
      "acc for Psat= 0.09662068585554756 \n",
      "acc for optim= 0.1395550361937947\n",
      "Epoch:542/1000\n",
      "Loss on train= 0.011921652592718601\n",
      "Loss on test= 0.01539657637476921\n",
      "acc for Lsat= 0.06868148263957766 \n",
      "acc for Psat= 0.0959526231719388 \n",
      "acc for optim= 0.13766290636526213\n",
      "Epoch:543/1000\n",
      "Loss on train= 0.012248089537024498\n",
      "Loss on test= 0.016199616715312004\n",
      "acc for Lsat= 0.06393396225240496 \n",
      "acc for Psat= 0.09657643569840325 \n",
      "acc for optim= 0.13824245656530063\n",
      "Epoch:544/1000\n",
      "Loss on train= 0.012225350365042686\n",
      "Loss on test= 0.015826135873794556\n",
      "acc for Lsat= 0.06991668873363072 \n",
      "acc for Psat= 0.10452236466937596 \n",
      "acc for optim= 0.14121952851613362\n",
      "Epoch:545/1000\n",
      "Loss on train= 0.011781043373048306\n",
      "Loss on test= 0.01635870337486267\n",
      "acc for Lsat= 0.07733208023839527 \n",
      "acc for Psat= 0.10167843964364794 \n",
      "acc for optim= 0.1388098532954852\n",
      "Epoch:546/1000\n",
      "Loss on train= 0.011810055933892727\n",
      "Loss on test= 0.016036737710237503\n",
      "acc for Lsat= 0.06602535396814346 \n",
      "acc for Psat= 0.09922356721427705 \n",
      "acc for optim= 0.13859434699018794\n",
      "Epoch:547/1000\n",
      "Loss on train= 0.011834630742669106\n",
      "Loss on test= 0.016654683277010918\n",
      "acc for Lsat= 0.06718230379952322 \n",
      "acc for Psat= 0.09917686714066401 \n",
      "acc for optim= 0.139106633928087\n",
      "Epoch:548/1000\n",
      "Loss on train= 0.011654305271804333\n",
      "Loss on test= 0.01722494512796402\n",
      "acc for Lsat= 0.06396689232852723 \n",
      "acc for Psat= 0.09831681615776487 \n",
      "acc for optim= 0.14186614304780962\n",
      "Epoch:549/1000\n",
      "Loss on train= 0.011973968707025051\n",
      "Loss on test= 0.016217654570937157\n",
      "acc for Lsat= 0.06859316759639317 \n",
      "acc for Psat= 0.10059010634819666 \n",
      "acc for optim= 0.13715057637956407\n",
      "Epoch:550/1000\n",
      "Loss on train= 0.012604748830199242\n",
      "Loss on test= 0.01633474975824356\n",
      "acc for Lsat= 0.07288613451851739 \n",
      "acc for Psat= 0.09439125541183685 \n",
      "acc for optim= 0.14089940190315248\n",
      "Epoch:551/1000\n",
      "Loss on train= 0.012151152826845646\n",
      "Loss on test= 0.015597192570567131\n",
      "acc for Lsat= 0.06331124603748321 \n",
      "acc for Psat= 0.09560360776053535 \n",
      "acc for optim= 0.14181824392742579\n",
      "Epoch:552/1000\n",
      "Loss on train= 0.011843141168355942\n",
      "Loss on test= 0.01632501557469368\n",
      "acc for Lsat= 0.06705808242162067 \n",
      "acc for Psat= 0.09531729171673456 \n",
      "acc for optim= 0.1406343448493216\n",
      "Epoch:553/1000\n",
      "Loss on train= 0.012064460664987564\n",
      "Loss on test= 0.015346328727900982\n",
      "acc for Lsat= 0.06827473060952292 \n",
      "acc for Psat= 0.09840080522828631 \n",
      "acc for optim= 0.1362512813674079\n",
      "Epoch:554/1000\n",
      "Loss on train= 0.01179554220288992\n",
      "Loss on test= 0.015874870121479034\n",
      "acc for Lsat= 0.07174935191869737 \n",
      "acc for Psat= 0.09945415291521285 \n",
      "acc for optim= 0.13650297646721204\n",
      "Epoch:555/1000\n",
      "Loss on train= 0.012406829744577408\n",
      "Loss on test= 0.014619039371609688\n",
      "acc for Lsat= 0.06863619304365583 \n",
      "acc for Psat= 0.10090789397557576 \n",
      "acc for optim= 0.13999977078702713\n",
      "Epoch:556/1000\n",
      "Loss on train= 0.012024608440697193\n",
      "Loss on test= 0.015345068648457527\n",
      "acc for Lsat= 0.0648908687962426 \n",
      "acc for Psat= 0.09686037318574056 \n",
      "acc for optim= 0.13731582421395516\n",
      "Epoch:557/1000\n",
      "Loss on train= 0.01168113574385643\n",
      "Loss on test= 0.01690490171313286\n",
      "acc for Lsat= 0.06817707295219104 \n",
      "acc for Psat= 0.09702426758077409 \n",
      "acc for optim= 0.1382073389987151\n",
      "Epoch:558/1000\n",
      "Loss on train= 0.011537551879882812\n",
      "Loss on test= 0.016711704432964325\n",
      "acc for Lsat= 0.06335359944237604 \n",
      "acc for Psat= 0.09589537200000552 \n",
      "acc for optim= 0.13775013089179994\n",
      "Epoch:559/1000\n",
      "Loss on train= 0.011518820188939571\n",
      "Loss on test= 0.015034796670079231\n",
      "acc for Lsat= 0.06435197012292014 \n",
      "acc for Psat= 0.10040175335274802 \n",
      "acc for optim= 0.13971096177895861\n",
      "Epoch:560/1000\n",
      "Loss on train= 0.011799151077866554\n",
      "Loss on test= 0.01644633710384369\n",
      "acc for Lsat= 0.0623067835966746 \n",
      "acc for Psat= 0.09490046087238524 \n",
      "acc for optim= 0.13892333259185155\n",
      "Epoch:561/1000\n",
      "Loss on train= 0.012011881917715073\n",
      "Loss on test= 0.015525932423770428\n",
      "acc for Lsat= 0.07090172949764464 \n",
      "acc for Psat= 0.10589241716596814 \n",
      "acc for optim= 0.13811694333950678\n",
      "Epoch:562/1000\n",
      "Loss on train= 0.012225243262946606\n",
      "Loss on test= 0.015546034090220928\n",
      "acc for Lsat= 0.07574380586544673 \n",
      "acc for Psat= 0.10017793774604797 \n",
      "acc for optim= 0.1401678224404653\n",
      "Epoch:563/1000\n",
      "Loss on train= 0.01194061990827322\n",
      "Loss on test= 0.016497457399964333\n",
      "acc for Lsat= 0.06987693011760712 \n",
      "acc for Psat= 0.10225223369068569 \n",
      "acc for optim= 0.14015762300954926\n",
      "Epoch:564/1000\n",
      "Loss on train= 0.011898702010512352\n",
      "Loss on test= 0.015624874271452427\n",
      "acc for Lsat= 0.07105435298548805 \n",
      "acc for Psat= 0.10614292290475634 \n",
      "acc for optim= 0.13976463899016378\n",
      "Epoch:565/1000\n",
      "Loss on train= 0.012375853024423122\n",
      "Loss on test= 0.017110012471675873\n",
      "acc for Lsat= 0.09720814542637932 \n",
      "acc for Psat= 0.10981213947137197 \n",
      "acc for optim= 0.1376517618695895\n",
      "Epoch:566/1000\n",
      "Loss on train= 0.012226930819451809\n",
      "Loss on test= 0.016980573534965515\n",
      "acc for Lsat= 0.06435342331727345 \n",
      "acc for Psat= 0.09841393729050955 \n",
      "acc for optim= 0.1385345906847053\n",
      "Epoch:567/1000\n",
      "Loss on train= 0.011565386317670345\n",
      "Loss on test= 0.016449589282274246\n",
      "acc for Lsat= 0.06684548573361503 \n",
      "acc for Psat= 0.09557391802469889 \n",
      "acc for optim= 0.14109716448518964\n",
      "Epoch:568/1000\n",
      "Loss on train= 0.01200028695166111\n",
      "Loss on test= 0.016928868368268013\n",
      "acc for Lsat= 0.07859160999457042 \n",
      "acc for Psat= 0.10370902518431346 \n",
      "acc for optim= 0.1394526412089666\n",
      "Epoch:569/1000\n",
      "Loss on train= 0.011722991243004799\n",
      "Loss on test= 0.015983732417225838\n",
      "acc for Lsat= 0.0706292986869812 \n",
      "acc for Psat= 0.10040801829761928 \n",
      "acc for optim= 0.14013714525434706\n",
      "Epoch:570/1000\n",
      "Loss on train= 0.011737477965652943\n",
      "Loss on test= 0.016177508980035782\n",
      "acc for Lsat= 0.066095213426484 \n",
      "acc for Psat= 0.0965623441669676 \n",
      "acc for optim= 0.14192905972401298\n",
      "Epoch:571/1000\n",
      "Loss on train= 0.012010020203888416\n",
      "Loss on test= 0.015938516706228256\n",
      "acc for Lsat= 0.06373110396994483 \n",
      "acc for Psat= 0.10358982582887015 \n",
      "acc for optim= 0.13975341932641136\n",
      "Epoch:572/1000\n",
      "Loss on train= 0.012157502584159374\n",
      "Loss on test= 0.01570657640695572\n",
      "acc for Lsat= 0.062282541311449474 \n",
      "acc for Psat= 0.0953789754046334 \n",
      "acc for optim= 0.14055507464541328\n",
      "Epoch:573/1000\n",
      "Loss on train= 0.0113675557076931\n",
      "Loss on test= 0.016490066424012184\n",
      "acc for Lsat= 0.07404200368457371 \n",
      "acc for Psat= 0.10192599826388889 \n",
      "acc for optim= 0.14209305495023727\n",
      "Epoch:574/1000\n",
      "Loss on train= 0.01133906003087759\n",
      "Loss on test= 0.01680445857346058\n",
      "acc for Lsat= 0.06617169032494227 \n",
      "acc for Psat= 0.09871764911545648 \n",
      "acc for optim= 0.14224525549345549\n",
      "Epoch:575/1000\n",
      "Loss on train= 0.011780709959566593\n",
      "Loss on test= 0.01621762104332447\n",
      "acc for Lsat= 0.06875797013441723 \n",
      "acc for Psat= 0.09918475747108457 \n",
      "acc for optim= 0.13846764117479327\n",
      "Epoch:576/1000\n",
      "Loss on train= 0.012490765191614628\n",
      "Loss on test= 0.015452222898602486\n",
      "acc for Lsat= 0.06647993433806632 \n",
      "acc for Psat= 0.09611502786477409 \n",
      "acc for optim= 0.13935936623149447\n",
      "Epoch:577/1000\n",
      "Loss on train= 0.011738636530935764\n",
      "Loss on test= 0.016312643885612488\n",
      "acc for Lsat= 0.061953735848267875 \n",
      "acc for Psat= 0.09620277881622313 \n",
      "acc for optim= 0.14155376007159554\n",
      "Epoch:578/1000\n",
      "Loss on train= 0.011497112922370434\n",
      "Loss on test= 0.0167319867759943\n",
      "acc for Lsat= 0.06694089497129123 \n",
      "acc for Psat= 0.09936424990495046 \n",
      "acc for optim= 0.14164774434434044\n",
      "Epoch:579/1000\n",
      "Loss on train= 0.011669119819998741\n",
      "Loss on test= 0.01590840145945549\n",
      "acc for Lsat= 0.06345668029454019 \n",
      "acc for Psat= 0.09280813733736673 \n",
      "acc for optim= 0.14033343974086973\n",
      "Epoch:580/1000\n",
      "Loss on train= 0.011851788498461246\n",
      "Loss on test= 0.016470380127429962\n",
      "acc for Lsat= 0.06835901240507761 \n",
      "acc for Psat= 0.09584273199240369 \n",
      "acc for optim= 0.13922617220216327\n",
      "Epoch:581/1000\n",
      "Loss on train= 0.011589677073061466\n",
      "Loss on test= 0.01624666340649128\n",
      "acc for Lsat= 0.06685871904095014 \n",
      "acc for Psat= 0.09600369450118808 \n",
      "acc for optim= 0.1401231644882096\n",
      "Epoch:582/1000\n",
      "Loss on train= 0.011475802399218082\n",
      "Loss on test= 0.01604301482439041\n",
      "acc for Lsat= 0.06484531122777197 \n",
      "acc for Psat= 0.10285228126578862 \n",
      "acc for optim= 0.1362816029952632\n",
      "Epoch:583/1000\n",
      "Loss on train= 0.011452976614236832\n",
      "Loss on test= 0.016503073275089264\n",
      "acc for Lsat= 0.06630317072073619 \n",
      "acc for Psat= 0.09874873426225451 \n",
      "acc for optim= 0.14212106466293334\n",
      "Epoch:584/1000\n",
      "Loss on train= 0.011540361680090427\n",
      "Loss on test= 0.015776999294757843\n",
      "acc for Lsat= 0.06305194455716345 \n",
      "acc for Psat= 0.09358880503310098 \n",
      "acc for optim= 0.13765898231003015\n",
      "Epoch:585/1000\n",
      "Loss on train= 0.012375494465231895\n",
      "Loss on test= 0.015244383364915848\n",
      "acc for Lsat= 0.06651529586977428 \n",
      "acc for Psat= 0.09883263607819875 \n",
      "acc for optim= 0.1383881011770831\n",
      "Epoch:586/1000\n",
      "Loss on train= 0.01114573609083891\n",
      "Loss on test= 0.015534419566392899\n",
      "acc for Lsat= 0.07939181990093656 \n",
      "acc for Psat= 0.10849472482999165 \n",
      "acc for optim= 0.1385166140066253\n",
      "Epoch:587/1000\n",
      "Loss on train= 0.011424403637647629\n",
      "Loss on test= 0.01687467470765114\n",
      "acc for Lsat= 0.06035279093517197 \n",
      "acc for Psat= 0.0972853634092543 \n",
      "acc for optim= 0.14006873004966314\n",
      "Epoch:588/1000\n",
      "Loss on train= 0.01130373403429985\n",
      "Loss on test= 0.015685928985476494\n",
      "acc for Lsat= 0.06801647295554479 \n",
      "acc for Psat= 0.09600379765033722 \n",
      "acc for optim= 0.14129973037375343\n",
      "Epoch:589/1000\n",
      "Loss on train= 0.011467230506241322\n",
      "Loss on test= 0.016452930867671967\n",
      "acc for Lsat= 0.06691009981764688 \n",
      "acc for Psat= 0.09483935568067761 \n",
      "acc for optim= 0.14034440343578658\n",
      "Epoch:590/1000\n",
      "Loss on train= 0.011507261544466019\n",
      "Loss on test= 0.016636034473776817\n",
      "acc for Lsat= 0.07184031969971127 \n",
      "acc for Psat= 0.10154431892765893 \n",
      "acc for optim= 0.14059291697210732\n",
      "Epoch:591/1000\n",
      "Loss on train= 0.011388958431780338\n",
      "Loss on test= 0.0167233869433403\n",
      "acc for Lsat= 0.062331434753206044 \n",
      "acc for Psat= 0.09375656661060122 \n",
      "acc for optim= 0.13908363067441518\n",
      "Epoch:592/1000\n",
      "Loss on train= 0.011698901653289795\n",
      "Loss on test= 0.0157838836312294\n",
      "acc for Lsat= 0.062348879625399894 \n",
      "acc for Psat= 0.0957619572679202 \n",
      "acc for optim= 0.14179291774829225\n",
      "Epoch:593/1000\n",
      "Loss on train= 0.011765164323151112\n",
      "Loss on test= 0.016327539458870888\n",
      "acc for Lsat= 0.06531584536035856 \n",
      "acc for Psat= 0.09989326761828529 \n",
      "acc for optim= 0.14161117416289118\n",
      "Epoch:594/1000\n",
      "Loss on train= 0.011298851110041142\n",
      "Loss on test= 0.015440713614225388\n",
      "acc for Lsat= 0.06662036958667966 \n",
      "acc for Psat= 0.10244158539507126 \n",
      "acc for optim= 0.14090886703795855\n",
      "Epoch:595/1000\n",
      "Loss on train= 0.011560670100152493\n",
      "Loss on test= 0.016089757904410362\n",
      "acc for Lsat= 0.061162045432461634 \n",
      "acc for Psat= 0.09442294008202025 \n",
      "acc for optim= 0.14065571981999608\n",
      "Epoch:596/1000\n",
      "Loss on train= 0.011826339177787304\n",
      "Loss on test= 0.01673506759107113\n",
      "acc for Lsat= 0.06228893945614497 \n",
      "acc for Psat= 0.09496420522530874 \n",
      "acc for optim= 0.13872461054060195\n",
      "Epoch:597/1000\n",
      "Loss on train= 0.011509976349771023\n",
      "Loss on test= 0.016172129660844803\n",
      "acc for Lsat= 0.06631564249595007 \n",
      "acc for Psat= 0.1015395419465171 \n",
      "acc for optim= 0.1417248891459571\n",
      "Epoch:598/1000\n",
      "Loss on train= 0.011678391136229038\n",
      "Loss on test= 0.01564943790435791\n",
      "acc for Lsat= 0.06417131937212414 \n",
      "acc for Psat= 0.1006544535358747 \n",
      "acc for optim= 0.13872376812828913\n",
      "Epoch:599/1000\n",
      "Loss on train= 0.011412447318434715\n",
      "Loss on test= 0.01609770767390728\n",
      "acc for Lsat= 0.06158433937364156 \n",
      "acc for Psat= 0.1005615888370408 \n",
      "acc for optim= 0.140856756352716\n",
      "Epoch:600/1000\n",
      "Loss on train= 0.01133502647280693\n",
      "Loss on test= 0.01598496176302433\n",
      "acc for Lsat= 0.0638954635295603 \n",
      "acc for Psat= 0.10003271533383262 \n",
      "acc for optim= 0.14199329482184517\n",
      "Epoch:601/1000\n",
      "Loss on train= 0.011735936626791954\n",
      "Loss on test= 0.016712822020053864\n",
      "acc for Lsat= 0.06565584101610714 \n",
      "acc for Psat= 0.0976202792591519 \n",
      "acc for optim= 0.14096825420856474\n",
      "Epoch:602/1000\n",
      "Loss on train= 0.011561309918761253\n",
      "Loss on test= 0.016921687871217728\n",
      "acc for Lsat= 0.06350145339965821 \n",
      "acc for Psat= 0.09711096783479055 \n",
      "acc for optim= 0.13809051074915457\n",
      "Epoch:603/1000\n",
      "Loss on train= 0.011920810677111149\n",
      "Loss on test= 0.015236266888678074\n",
      "acc for Lsat= 0.0640111422373189 \n",
      "acc for Psat= 0.09848016401131948 \n",
      "acc for optim= 0.13848128914833072\n",
      "Epoch:604/1000\n",
      "Loss on train= 0.011495392769575119\n",
      "Loss on test= 0.015313666313886642\n",
      "acc for Lsat= 0.0657799404528406 \n",
      "acc for Psat= 0.09502626326349045 \n",
      "acc for optim= 0.14011890954441492\n",
      "Epoch:605/1000\n",
      "Loss on train= 0.011897324584424496\n",
      "Loss on test= 0.01696007512509823\n",
      "acc for Lsat= 0.0641615586148368 \n",
      "acc for Psat= 0.09815467844406763 \n",
      "acc for optim= 0.14177100575632517\n",
      "Epoch:606/1000\n",
      "Loss on train= 0.011520322412252426\n",
      "Loss on test= 0.015812629833817482\n",
      "acc for Lsat= 0.062336535337898465 \n",
      "acc for Psat= 0.09531262293457984 \n",
      "acc for optim= 0.1375861156317923\n",
      "Epoch:607/1000\n",
      "Loss on train= 0.011350101791322231\n",
      "Loss on test= 0.016419649124145508\n",
      "acc for Lsat= 0.0666050339738528 \n",
      "acc for Psat= 0.09389800462457867 \n",
      "acc for optim= 0.14014878173669176\n",
      "Epoch:608/1000\n",
      "Loss on train= 0.011789190582931042\n",
      "Loss on test= 0.016676539555191994\n",
      "acc for Lsat= 0.061540852983792624 \n",
      "acc for Psat= 0.0971374751793014 \n",
      "acc for optim= 0.1420824303395218\n",
      "Epoch:609/1000\n",
      "Loss on train= 0.011341657489538193\n",
      "Loss on test= 0.016436954960227013\n",
      "acc for Lsat= 0.0652018396390809 \n",
      "acc for Psat= 0.09653681665658952 \n",
      "acc for optim= 0.13909489843580458\n",
      "Epoch:610/1000\n",
      "Loss on train= 0.011565575376152992\n",
      "Loss on test= 0.016097260639071465\n",
      "acc for Lsat= 0.07310884015427695 \n",
      "acc for Psat= 0.09824965794881185 \n",
      "acc for optim= 0.13882443234324454\n",
      "Epoch:611/1000\n",
      "Loss on train= 0.011480889283120632\n",
      "Loss on test= 0.016883298754692078\n",
      "acc for Lsat= 0.06404342477520307 \n",
      "acc for Psat= 0.09411329974730809 \n",
      "acc for optim= 0.13794062808156016\n",
      "Epoch:612/1000\n",
      "Loss on train= 0.011620789766311646\n",
      "Loss on test= 0.01590638980269432\n",
      "acc for Lsat= 0.06777475757731331 \n",
      "acc for Psat= 0.09631846621632577 \n",
      "acc for optim= 0.13921807896759777\n",
      "Epoch:613/1000\n",
      "Loss on train= 0.011701361276209354\n",
      "Loss on test= 0.0159614160656929\n",
      "acc for Lsat= 0.06667130755053625 \n",
      "acc for Psat= 0.09738463345501158 \n",
      "acc for optim= 0.13752544017301668\n",
      "Epoch:614/1000\n",
      "Loss on train= 0.011520332656800747\n",
      "Loss on test= 0.01615355908870697\n",
      "acc for Lsat= 0.06471909234921137 \n",
      "acc for Psat= 0.10024141271909076 \n",
      "acc for optim= 0.14179101636012395\n",
      "Epoch:615/1000\n",
      "Loss on train= 0.01120214443653822\n",
      "Loss on test= 0.015827925875782967\n",
      "acc for Lsat= 0.06381453540590074 \n",
      "acc for Psat= 0.09906787276268005 \n",
      "acc for optim= 0.13964780295888582\n",
      "Epoch:616/1000\n",
      "Loss on train= 0.011615588329732418\n",
      "Loss on test= 0.016267500817775726\n",
      "acc for Lsat= 0.06742623911963569 \n",
      "acc for Psat= 0.0996804585059484 \n",
      "acc for optim= 0.14182489340504012\n",
      "Epoch:617/1000\n",
      "Loss on train= 0.011538079008460045\n",
      "Loss on test= 0.016107965260744095\n",
      "acc for Lsat= 0.06039562390910255 \n",
      "acc for Psat= 0.09913009603818257 \n",
      "acc for optim= 0.14257706850767138\n",
      "Epoch:618/1000\n",
      "Loss on train= 0.011562942527234554\n",
      "Loss on test= 0.016388539224863052\n",
      "acc for Lsat= 0.07060338051782714 \n",
      "acc for Psat= 0.11312351690398323 \n",
      "acc for optim= 0.13991286497977046\n",
      "Epoch:619/1000\n",
      "Loss on train= 0.011611326597630978\n",
      "Loss on test= 0.015711840242147446\n",
      "acc for Lsat= 0.07065128174920876 \n",
      "acc for Psat= 0.09452357093493141 \n",
      "acc for optim= 0.14169998632536993\n",
      "Epoch:620/1000\n",
      "Loss on train= 0.011501476168632507\n",
      "Loss on test= 0.015668367967009544\n",
      "acc for Lsat= 0.06764232052697075 \n",
      "acc for Psat= 0.10813799268669552 \n",
      "acc for optim= 0.14000470985968907\n",
      "Epoch:621/1000\n",
      "Loss on train= 0.011137201450765133\n",
      "Loss on test= 0.016724055632948875\n",
      "acc for Lsat= 0.05923116579651831 \n",
      "acc for Psat= 0.09102059106032051 \n",
      "acc for optim= 0.13989188108179304\n",
      "Epoch:622/1000\n",
      "Loss on train= 0.011649820022284985\n",
      "Loss on test= 0.01567796804010868\n",
      "acc for Lsat= 0.06118390013774236 \n",
      "acc for Psat= 0.09422705620527268 \n",
      "acc for optim= 0.1391940163241492\n",
      "Epoch:623/1000\n",
      "Loss on train= 0.011557101272046566\n",
      "Loss on test= 0.016117367893457413\n",
      "acc for Lsat= 0.06569108532534707 \n",
      "acc for Psat= 0.09850954976346758 \n",
      "acc for optim= 0.14078394737508562\n",
      "Epoch:624/1000\n",
      "Loss on train= 0.01124742440879345\n",
      "Loss on test= 0.016586536541581154\n",
      "acc for Lsat= 0.0636048367453946 \n",
      "acc for Psat= 0.09439472373988894 \n",
      "acc for optim= 0.13866909328434204\n",
      "Epoch:625/1000\n",
      "Loss on train= 0.01113443449139595\n",
      "Loss on test= 0.016123050823807716\n",
      "acc for Lsat= 0.06338138083616893 \n",
      "acc for Psat= 0.09811791015995874 \n",
      "acc for optim= 0.1409661917222871\n",
      "Epoch:626/1000\n",
      "Loss on train= 0.010806395672261715\n",
      "Loss on test= 0.017331643030047417\n",
      "acc for Lsat= 0.065163382059998 \n",
      "acc for Psat= 0.09668733312024008 \n",
      "acc for optim= 0.1418669627772437\n",
      "Epoch:627/1000\n",
      "Loss on train= 0.011695470660924911\n",
      "Loss on test= 0.015898950397968292\n",
      "acc for Lsat= 0.0680261398355166 \n",
      "acc for Psat= 0.09810143129693136 \n",
      "acc for optim= 0.14057133297125496\n",
      "Epoch:628/1000\n",
      "Loss on train= 0.01144169270992279\n",
      "Loss on test= 0.01622425764799118\n",
      "acc for Lsat= 0.0633837522731887 \n",
      "acc for Psat= 0.0978054874473148 \n",
      "acc for optim= 0.13808278491099676\n",
      "Epoch:629/1000\n",
      "Loss on train= 0.011256859637796879\n",
      "Loss on test= 0.016803713515400887\n",
      "acc for Lsat= 0.06572983496718937 \n",
      "acc for Psat= 0.0991469297144148 \n",
      "acc for optim= 0.14288095459342004\n",
      "Epoch:630/1000\n",
      "Loss on train= 0.011109502986073494\n",
      "Loss on test= 0.01556784100830555\n",
      "acc for Lsat= 0.06336428440279432 \n",
      "acc for Psat= 0.0972939262787501 \n",
      "acc for optim= 0.1399668523006969\n",
      "Epoch:631/1000\n",
      "Loss on train= 0.011263309977948666\n",
      "Loss on test= 0.015440229326486588\n",
      "acc for Lsat= 0.06377534452411864 \n",
      "acc for Psat= 0.10162596354881923 \n",
      "acc for optim= 0.14055837392807008\n",
      "Epoch:632/1000\n",
      "Loss on train= 0.01118573360145092\n",
      "Loss on test= 0.01597844436764717\n",
      "acc for Lsat= 0.07146677010589175 \n",
      "acc for Psat= 0.1092727157804701 \n",
      "acc for optim= 0.14116781701644263\n",
      "Epoch:633/1000\n",
      "Loss on train= 0.011363430880010128\n",
      "Loss on test= 0.01563144102692604\n",
      "acc for Lsat= 0.07244831141498353 \n",
      "acc for Psat= 0.10720880627632141 \n",
      "acc for optim= 0.13872604982720482\n",
      "Epoch:634/1000\n",
      "Loss on train= 0.01123890932649374\n",
      "Loss on test= 0.015870878472924232\n",
      "acc for Lsat= 0.061992781692081035 \n",
      "acc for Psat= 0.09502420276403425 \n",
      "acc for optim= 0.13967996372116936\n",
      "Epoch:635/1000\n",
      "Loss on train= 0.011228404939174652\n",
      "Loss on test= 0.01711864024400711\n",
      "acc for Lsat= 0.06940506017870372 \n",
      "acc for Psat= 0.09705672992600337 \n",
      "acc for optim= 0.1406438395380974\n",
      "Epoch:636/1000\n",
      "Loss on train= 0.011381799355149269\n",
      "Loss on test= 0.015701599419116974\n",
      "acc for Lsat= 0.0646392624411318 \n",
      "acc for Psat= 0.09923453645573724 \n",
      "acc for optim= 0.14009388420316907\n",
      "Epoch:637/1000\n",
      "Loss on train= 0.01112337689846754\n",
      "Loss on test= 0.01650341786444187\n",
      "acc for Lsat= 0.07013821519083446 \n",
      "acc for Psat= 0.09787083516518275 \n",
      "acc for optim= 0.1423110963569747\n",
      "Epoch:638/1000\n",
      "Loss on train= 0.011406157165765762\n",
      "Loss on test= 0.015788691118359566\n",
      "acc for Lsat= 0.06330518490738338 \n",
      "acc for Psat= 0.10233765045801797 \n",
      "acc for optim= 0.13957996004157597\n",
      "Epoch:639/1000\n",
      "Loss on train= 0.01127710472792387\n",
      "Loss on test= 0.015812592580914497\n",
      "acc for Lsat= 0.07230963673856523 \n",
      "acc for Psat= 0.09441262930631637 \n",
      "acc for optim= 0.1383876798881425\n",
      "Epoch:640/1000\n",
      "Loss on train= 0.011451643891632557\n",
      "Loss on test= 0.016268260776996613\n",
      "acc for Lsat= 0.06927222079700894 \n",
      "acc for Psat= 0.09520842615101072 \n",
      "acc for optim= 0.14347777432865566\n",
      "Epoch:641/1000\n",
      "Loss on train= 0.011327339336276054\n",
      "Loss on test= 0.016818158328533173\n",
      "acc for Lsat= 0.06757744683159722 \n",
      "acc for Psat= 0.09658598626653353 \n",
      "acc for optim= 0.13915392408768337\n",
      "Epoch:642/1000\n",
      "Loss on train= 0.011693029664456844\n",
      "Loss on test= 0.017148496583104134\n",
      "acc for Lsat= 0.06821418495641814 \n",
      "acc for Psat= 0.09378134426143434 \n",
      "acc for optim= 0.14124171601401436\n",
      "Epoch:643/1000\n",
      "Loss on train= 0.01124462578445673\n",
      "Loss on test= 0.016044847667217255\n",
      "acc for Lsat= 0.06372903784116109 \n",
      "acc for Psat= 0.09541864991188048 \n",
      "acc for optim= 0.14180713180038662\n",
      "Epoch:644/1000\n",
      "Loss on train= 0.01116611622273922\n",
      "Loss on test= 0.017072271555662155\n",
      "acc for Lsat= 0.07102293959922261 \n",
      "acc for Psat= 0.09326073527336122 \n",
      "acc for optim= 0.14135826494958664\n",
      "Epoch:645/1000\n",
      "Loss on train= 0.011231047101318836\n",
      "Loss on test= 0.015993287786841393\n",
      "acc for Lsat= 0.06640919860866334 \n",
      "acc for Psat= 0.09703696072101592 \n",
      "acc for optim= 0.13846855188409482\n",
      "Epoch:646/1000\n",
      "Loss on train= 0.011262144893407822\n",
      "Loss on test= 0.016091667115688324\n",
      "acc for Lsat= 0.06344309134615793 \n",
      "acc for Psat= 0.09921150902907054 \n",
      "acc for optim= 0.14187664455837676\n",
      "Epoch:647/1000\n",
      "Loss on train= 0.011267591267824173\n",
      "Loss on test= 0.015566757880151272\n",
      "acc for Lsat= 0.06262338625060188 \n",
      "acc for Psat= 0.09815096507469814 \n",
      "acc for optim= 0.1403975945379999\n",
      "Epoch:648/1000\n",
      "Loss on train= 0.010924026370048523\n",
      "Loss on test= 0.016251331195235252\n",
      "acc for Lsat= 0.07192986889017955 \n",
      "acc for Psat= 0.09644555714395313 \n",
      "acc for optim= 0.14053214821550583\n",
      "Epoch:649/1000\n",
      "Loss on train= 0.010874186642467976\n",
      "Loss on test= 0.015845170244574547\n",
      "acc for Lsat= 0.06664310950371954 \n",
      "acc for Psat= 0.09338220689031812 \n",
      "acc for optim= 0.14074688686264883\n",
      "Epoch:650/1000\n",
      "Loss on train= 0.011361213400959969\n",
      "Loss on test= 0.015544737689197063\n",
      "acc for Lsat= 0.06626611037386788 \n",
      "acc for Psat= 0.09414550960063933 \n",
      "acc for optim= 0.14106502185265224\n",
      "Epoch:651/1000\n",
      "Loss on train= 0.011422851122915745\n",
      "Loss on test= 0.016747651621699333\n",
      "acc for Lsat= 0.06496394044823117 \n",
      "acc for Psat= 0.09637177189191184 \n",
      "acc for optim= 0.14126386824581358\n",
      "Epoch:652/1000\n",
      "Loss on train= 0.011479055508971214\n",
      "Loss on test= 0.015383089892566204\n",
      "acc for Lsat= 0.06529819584555095 \n",
      "acc for Psat= 0.09605923841396967 \n",
      "acc for optim= 0.13932790011167526\n",
      "Epoch:653/1000\n",
      "Loss on train= 0.010771708562970161\n",
      "Loss on test= 0.016323640942573547\n",
      "acc for Lsat= 0.06413696375158098 \n",
      "acc for Psat= 0.09701020154688095 \n",
      "acc for optim= 0.14173829721079934\n",
      "Epoch:654/1000\n",
      "Loss on train= 0.010906444862484932\n",
      "Loss on test= 0.016201520338654518\n",
      "acc for Lsat= 0.06428370761374633 \n",
      "acc for Psat= 0.09910705619388156 \n",
      "acc for optim= 0.14606238529086113\n",
      "Epoch:655/1000\n",
      "Loss on train= 0.011097266338765621\n",
      "Loss on test= 0.015928929671645164\n",
      "acc for Lsat= 0.06732217421134314 \n",
      "acc for Psat= 0.10145686368147531 \n",
      "acc for optim= 0.14068884087933434\n",
      "Epoch:656/1000\n",
      "Loss on train= 0.011089738458395004\n",
      "Loss on test= 0.016464730724692345\n",
      "acc for Lsat= 0.06575352334313923 \n",
      "acc for Psat= 0.0963127745522393 \n",
      "acc for optim= 0.14200049506293402\n",
      "Epoch:657/1000\n",
      "Loss on train= 0.011658947914838791\n",
      "Loss on test= 0.016465328633785248\n",
      "acc for Lsat= 0.06555002447631623 \n",
      "acc for Psat= 0.09752945635053846 \n",
      "acc for optim= 0.1431533695095115\n",
      "Epoch:658/1000\n",
      "Loss on train= 0.011201233603060246\n",
      "Loss on test= 0.015367425046861172\n",
      "acc for Lsat= 0.06245673216051525 \n",
      "acc for Psat= 0.09795520669884154 \n",
      "acc for optim= 0.1422860387298796\n",
      "Epoch:659/1000\n",
      "Loss on train= 0.011464598588645458\n",
      "Loss on test= 0.016115404665470123\n",
      "acc for Lsat= 0.06518826915158166 \n",
      "acc for Psat= 0.10423084563679165 \n",
      "acc for optim= 0.14137141340308718\n",
      "Epoch:660/1000\n",
      "Loss on train= 0.010968003422021866\n",
      "Loss on test= 0.015805432572960854\n",
      "acc for Lsat= 0.0643226663271586 \n",
      "acc for Psat= 0.09561805228392284 \n",
      "acc for optim= 0.14076891988515855\n",
      "Epoch:661/1000\n",
      "Loss on train= 0.01109684631228447\n",
      "Loss on test= 0.016719801351428032\n",
      "acc for Lsat= 0.06232805119620428 \n",
      "acc for Psat= 0.09917334881093769 \n",
      "acc for optim= 0.1411853070060412\n",
      "Epoch:662/1000\n",
      "Loss on train= 0.011261160485446453\n",
      "Loss on test= 0.01705176755785942\n",
      "acc for Lsat= 0.0641876424352328 \n",
      "acc for Psat= 0.09481902884112464 \n",
      "acc for optim= 0.13925702853335273\n",
      "Epoch:663/1000\n",
      "Loss on train= 0.011162572540342808\n",
      "Loss on test= 0.016602005809545517\n",
      "acc for Lsat= 0.06224982870949639 \n",
      "acc for Psat= 0.09421296086576249 \n",
      "acc for optim= 0.1411117182837592\n",
      "Epoch:664/1000\n",
      "Loss on train= 0.011041991412639618\n",
      "Loss on test= 0.01660042256116867\n",
      "acc for Lsat= 0.06379611972305509 \n",
      "acc for Psat= 0.09880372898446188 \n",
      "acc for optim= 0.14013918654786214\n",
      "Epoch:665/1000\n",
      "Loss on train= 0.010993980802595615\n",
      "Loss on test= 0.016599543392658234\n",
      "acc for Lsat= 0.06365533040629494 \n",
      "acc for Psat= 0.09672707782851325 \n",
      "acc for optim= 0.13993076094322732\n",
      "Epoch:666/1000\n",
      "Loss on train= 0.010807685554027557\n",
      "Loss on test= 0.016454795375466347\n",
      "acc for Lsat= 0.06389367348617976 \n",
      "acc for Psat= 0.09474300112989215 \n",
      "acc for optim= 0.1410210702154372\n",
      "Epoch:667/1000\n",
      "Loss on train= 0.010956831276416779\n",
      "Loss on test= 0.016235820949077606\n",
      "acc for Lsat= 0.06437963644663493 \n",
      "acc for Psat= 0.09717214306195576 \n",
      "acc for optim= 0.1399790575106939\n",
      "Epoch:668/1000\n",
      "Loss on train= 0.011217381805181503\n",
      "Loss on test= 0.01682029664516449\n",
      "acc for Lsat= 0.06363754570484162 \n",
      "acc for Psat= 0.0958970387776693 \n",
      "acc for optim= 0.14171319885386366\n",
      "Epoch:669/1000\n",
      "Loss on train= 0.0109305614605546\n",
      "Loss on test= 0.015667922794818878\n",
      "acc for Lsat= 0.06505781445238326 \n",
      "acc for Psat= 0.09986440125438902 \n",
      "acc for optim= 0.14270208477973936\n",
      "Epoch:670/1000\n",
      "Loss on train= 0.01083225104957819\n",
      "Loss on test= 0.017561890184879303\n",
      "acc for Lsat= 0.06395046582652462 \n",
      "acc for Psat= 0.10085795455508761 \n",
      "acc for optim= 0.14297277281681697\n",
      "Epoch:671/1000\n",
      "Loss on train= 0.01124048326164484\n",
      "Loss on test= 0.017035599797964096\n",
      "acc for Lsat= 0.0632422909140587 \n",
      "acc for Psat= 0.09671314160029093 \n",
      "acc for optim= 0.14139134577578968\n",
      "Epoch:672/1000\n",
      "Loss on train= 0.010693714022636414\n",
      "Loss on test= 0.016708459705114365\n",
      "acc for Lsat= 0.06324265806211365 \n",
      "acc for Psat= 0.09279013988044527 \n",
      "acc for optim= 0.14031972106960086\n",
      "Epoch:673/1000\n",
      "Loss on train= 0.011043491773307323\n",
      "Loss on test= 0.016376178711652756\n",
      "acc for Lsat= 0.06260011808739768 \n",
      "acc for Psat= 0.09655937386883631 \n",
      "acc for optim= 0.14193790993756716\n",
      "Epoch:674/1000\n",
      "Loss on train= 0.011103566735982895\n",
      "Loss on test= 0.016016129404306412\n",
      "acc for Lsat= 0.06637910339567397 \n",
      "acc for Psat= 0.09776351501544317 \n",
      "acc for optim= 0.1403316842185126\n",
      "Epoch:675/1000\n",
      "Loss on train= 0.010845786891877651\n",
      "Loss on test= 0.01591213047504425\n",
      "acc for Lsat= 0.06498049961196052 \n",
      "acc for Psat= 0.09595936454004711 \n",
      "acc for optim= 0.1415121171209547\n",
      "Epoch:676/1000\n",
      "Loss on train= 0.011849614791572094\n",
      "Loss on test= 0.017209989950060844\n",
      "acc for Lsat= 0.06122169726424747 \n",
      "acc for Psat= 0.0960750886135631 \n",
      "acc for optim= 0.14048296138644217\n",
      "Epoch:677/1000\n",
      "Loss on train= 0.01095212996006012\n",
      "Loss on test= 0.016489528119564056\n",
      "acc for Lsat= 0.061355161832438575 \n",
      "acc for Psat= 0.09606854418913523 \n",
      "acc for optim= 0.1449385071794192\n",
      "Epoch:678/1000\n",
      "Loss on train= 0.011170024052262306\n",
      "Loss on test= 0.016080161556601524\n",
      "acc for Lsat= 0.06471456074052387 \n",
      "acc for Psat= 0.09892872439490424 \n",
      "acc for optim= 0.14234971569644084\n",
      "Epoch:679/1000\n",
      "Loss on train= 0.011205216869711876\n",
      "Loss on test= 0.016808541491627693\n",
      "acc for Lsat= 0.06239631788598167 \n",
      "acc for Psat= 0.09589973025851778 \n",
      "acc for optim= 0.14117048035065335\n",
      "Epoch:680/1000\n",
      "Loss on train= 0.011026793159544468\n",
      "Loss on test= 0.016172213479876518\n",
      "acc for Lsat= 0.06310388313399422 \n",
      "acc for Psat= 0.09626293033361435 \n",
      "acc for optim= 0.14202242179049387\n",
      "Epoch:681/1000\n",
      "Loss on train= 0.010732068680226803\n",
      "Loss on test= 0.016807513311505318\n",
      "acc for Lsat= 0.06777403967248069 \n",
      "acc for Psat= 0.09872160322136349 \n",
      "acc for optim= 0.14018542667229975\n",
      "Epoch:682/1000\n",
      "Loss on train= 0.01100662350654602\n",
      "Loss on test= 0.016155390068888664\n",
      "acc for Lsat= 0.06768708477417629 \n",
      "acc for Psat= 0.10031566288736132 \n",
      "acc for optim= 0.14093168560001584\n",
      "Epoch:683/1000\n",
      "Loss on train= 0.010865203104913235\n",
      "Loss on test= 0.0158225167542696\n",
      "acc for Lsat= 0.06503366132577261 \n",
      "acc for Psat= 0.09947564270761276 \n",
      "acc for optim= 0.14285677555534576\n",
      "Epoch:684/1000\n",
      "Loss on train= 0.010835792869329453\n",
      "Loss on test= 0.01721530221402645\n",
      "acc for Lsat= 0.06677752013007801 \n",
      "acc for Psat= 0.09616132179896038 \n",
      "acc for optim= 0.14099655648072562\n",
      "Epoch:685/1000\n",
      "Loss on train= 0.010870927944779396\n",
      "Loss on test= 0.016099734231829643\n",
      "acc for Lsat= 0.06260351811846097 \n",
      "acc for Psat= 0.09317231989569134 \n",
      "acc for optim= 0.14148216611809197\n",
      "Epoch:686/1000\n",
      "Loss on train= 0.010683746077120304\n",
      "Loss on test= 0.016224805265665054\n",
      "acc for Lsat= 0.06318248510360719 \n",
      "acc for Psat= 0.100721655620469 \n",
      "acc for optim= 0.14067079375187555\n",
      "Epoch:687/1000\n",
      "Loss on train= 0.01110935304313898\n",
      "Loss on test= 0.015838030725717545\n",
      "acc for Lsat= 0.0672838588555654 \n",
      "acc for Psat= 0.09360051022635564 \n",
      "acc for optim= 0.14466404898299112\n",
      "Epoch:688/1000\n",
      "Loss on train= 0.010754278860986233\n",
      "Loss on test= 0.01611035130918026\n",
      "acc for Lsat= 0.06303988645474115 \n",
      "acc for Psat= 0.09535473866595162 \n",
      "acc for optim= 0.1430013972851965\n",
      "Epoch:689/1000\n",
      "Loss on train= 0.01134074293076992\n",
      "Loss on test= 0.016353130340576172\n",
      "acc for Lsat= 0.0650446762641271 \n",
      "acc for Psat= 0.09148155583275691 \n",
      "acc for optim= 0.14249711318148506\n",
      "Epoch:690/1000\n",
      "Loss on train= 0.011261493898928165\n",
      "Loss on test= 0.01592242904007435\n",
      "acc for Lsat= 0.06276259041494794 \n",
      "acc for Psat= 0.09236827045679091 \n",
      "acc for optim= 0.13968492555949422\n",
      "Epoch:691/1000\n",
      "Loss on train= 0.01076381467282772\n",
      "Loss on test= 0.015202326700091362\n",
      "acc for Lsat= 0.06202136956983143 \n",
      "acc for Psat= 0.0937651546465026 \n",
      "acc for optim= 0.14070087124903996\n",
      "Epoch:692/1000\n",
      "Loss on train= 0.011061636731028557\n",
      "Loss on test= 0.015864379703998566\n",
      "acc for Lsat= 0.06460752338171005 \n",
      "acc for Psat= 0.09427848044368957 \n",
      "acc for optim= 0.14151319166024526\n",
      "Epoch:693/1000\n",
      "Loss on train= 0.01066712848842144\n",
      "Loss on test= 0.01673119328916073\n",
      "acc for Lsat= 0.06356473598215315 \n",
      "acc for Psat= 0.09344944390985702 \n",
      "acc for optim= 0.14184211914738018\n",
      "Epoch:694/1000\n",
      "Loss on train= 0.010865267366170883\n",
      "Loss on test= 0.016247807070612907\n",
      "acc for Lsat= 0.07211596394578616 \n",
      "acc for Psat= 0.09538688543770048 \n",
      "acc for optim= 0.14169425566991173\n",
      "Epoch:695/1000\n",
      "Loss on train= 0.011055453680455685\n",
      "Loss on test= 0.016810135915875435\n",
      "acc for Lsat= 0.06347318755255806 \n",
      "acc for Psat= 0.09798728028933205 \n",
      "acc for optim= 0.1407216681374444\n",
      "Epoch:696/1000\n",
      "Loss on train= 0.0109234768897295\n",
      "Loss on test= 0.016183558851480484\n",
      "acc for Lsat= 0.07395098027255799 \n",
      "acc for Psat= 0.1078400307231479 \n",
      "acc for optim= 0.14292023744848037\n",
      "Epoch:697/1000\n",
      "Loss on train= 0.011472386308014393\n",
      "Loss on test= 0.01591225527226925\n",
      "acc for Lsat= 0.06481688155068292 \n",
      "acc for Psat= 0.09634631673494973 \n",
      "acc for optim= 0.14249882929854926\n",
      "Epoch:698/1000\n",
      "Loss on train= 0.01075578760355711\n",
      "Loss on test= 0.01592322438955307\n",
      "acc for Lsat= 0.06461438006824917 \n",
      "acc for Psat= 0.09650359451770783 \n",
      "acc for optim= 0.14008916657831935\n",
      "Epoch:699/1000\n",
      "Loss on train= 0.010721361264586449\n",
      "Loss on test= 0.016852708533406258\n",
      "acc for Lsat= 0.06412621811032296 \n",
      "acc for Psat= 0.09415653944015505 \n",
      "acc for optim= 0.14088943815893595\n",
      "Epoch:700/1000\n",
      "Loss on train= 0.010367400012910366\n",
      "Loss on test= 0.016141004860401154\n",
      "acc for Lsat= 0.06647503657473458 \n",
      "acc for Psat= 0.09942484994729359 \n",
      "acc for optim= 0.14194964551263387\n",
      "Epoch:701/1000\n",
      "Loss on train= 0.010853966698050499\n",
      "Loss on test= 0.017035730183124542\n",
      "acc for Lsat= 0.06601630904608302 \n",
      "acc for Psat= 0.09415570580297045 \n",
      "acc for optim= 0.14165045320987701\n",
      "Epoch:702/1000\n",
      "Loss on train= 0.010663184337317944\n",
      "Loss on test= 0.015391652472317219\n",
      "acc for Lsat= 0.061740573247273764 \n",
      "acc for Psat= 0.09403256922960282 \n",
      "acc for optim= 0.14395500090387137\n",
      "Epoch:703/1000\n",
      "Loss on train= 0.010738957673311234\n",
      "Loss on test= 0.01647479087114334\n",
      "acc for Lsat= 0.06604255851772096 \n",
      "acc for Psat= 0.09560745507478714 \n",
      "acc for optim= 0.14231738390194046\n",
      "Epoch:704/1000\n",
      "Loss on train= 0.01078047789633274\n",
      "Loss on test= 0.016409730538725853\n",
      "acc for Lsat= 0.06506727851099438 \n",
      "acc for Psat= 0.09610765675703686 \n",
      "acc for optim= 0.1456542334622807\n",
      "Epoch:705/1000\n",
      "Loss on train= 0.010995556600391865\n",
      "Loss on test= 0.017112188041210175\n",
      "acc for Lsat= 0.06826495693789587 \n",
      "acc for Psat= 0.09704464375972749 \n",
      "acc for optim= 0.14146007820963857\n",
      "Epoch:706/1000\n",
      "Loss on train= 0.010822350159287453\n",
      "Loss on test= 0.016710542142391205\n",
      "acc for Lsat= 0.07264515293969048 \n",
      "acc for Psat= 0.09757018155521817 \n",
      "acc for optim= 0.14396455470058653\n",
      "Epoch:707/1000\n",
      "Loss on train= 0.010577881708741188\n",
      "Loss on test= 0.016477562487125397\n",
      "acc for Lsat= 0.06975891358322567 \n",
      "acc for Psat= 0.09946889181931814 \n",
      "acc for optim= 0.14474734548065396\n",
      "Epoch:708/1000\n",
      "Loss on train= 0.011107252910733223\n",
      "Loss on test= 0.016170701012015343\n",
      "acc for Lsat= 0.06389771136972638 \n",
      "acc for Psat= 0.09257612559530473 \n",
      "acc for optim= 0.1416106041106913\n",
      "Epoch:709/1000\n",
      "Loss on train= 0.010793996974825859\n",
      "Loss on test= 0.01617899350821972\n",
      "acc for Lsat= 0.06271328205863634 \n",
      "acc for Psat= 0.09787092490328683 \n",
      "acc for optim= 0.14528988641169335\n",
      "Epoch:710/1000\n",
      "Loss on train= 0.01094471849501133\n",
      "Loss on test= 0.01673646830022335\n",
      "acc for Lsat= 0.06569599840376111 \n",
      "acc for Psat= 0.0938805381457011 \n",
      "acc for optim= 0.14395565754837458\n",
      "Epoch:711/1000\n",
      "Loss on train= 0.010793770663440228\n",
      "Loss on test= 0.0163793433457613\n",
      "acc for Lsat= 0.061614329781797185 \n",
      "acc for Psat= 0.09932815697458054 \n",
      "acc for optim= 0.14175329340828785\n",
      "Epoch:712/1000\n",
      "Loss on train= 0.010598931461572647\n",
      "Loss on test= 0.017010174691677094\n",
      "acc for Lsat= 0.06381963640451432 \n",
      "acc for Psat= 0.10026265316539341 \n",
      "acc for optim= 0.1456435203552246\n",
      "Epoch:713/1000\n",
      "Loss on train= 0.01076040230691433\n",
      "Loss on test= 0.015465800650417805\n",
      "acc for Lsat= 0.06120054837730196 \n",
      "acc for Psat= 0.09401766823397742 \n",
      "acc for optim= 0.14324529651138518\n",
      "Epoch:714/1000\n",
      "Loss on train= 0.01036067120730877\n",
      "Loss on test= 0.01656820997595787\n",
      "acc for Lsat= 0.06277922309107249 \n",
      "acc for Psat= 0.09599548776944479 \n",
      "acc for optim= 0.1419526709450616\n",
      "Epoch:715/1000\n",
      "Loss on train= 0.010624916292726994\n",
      "Loss on test= 0.016553334891796112\n",
      "acc for Lsat= 0.06298258826136588 \n",
      "acc for Psat= 0.09194793850183487 \n",
      "acc for optim= 0.14288706762923137\n",
      "Epoch:716/1000\n",
      "Loss on train= 0.010998777113854885\n",
      "Loss on test= 0.016151506453752518\n",
      "acc for Lsat= 0.0666103336546156 \n",
      "acc for Psat= 0.09849970589081447 \n",
      "acc for optim= 0.14107723120186064\n",
      "Epoch:717/1000\n",
      "Loss on train= 0.010853578336536884\n",
      "Loss on test= 0.01723792962729931\n",
      "acc for Lsat= 0.0648549508717325 \n",
      "acc for Psat= 0.09240877098507352 \n",
      "acc for optim= 0.14134366669588616\n",
      "Epoch:718/1000\n",
      "Loss on train= 0.010587717406451702\n",
      "Loss on test= 0.016985366120934486\n",
      "acc for Lsat= 0.06238617077469825 \n",
      "acc for Psat= 0.09356597794426812 \n",
      "acc for optim= 0.14239022003279792\n",
      "Epoch:719/1000\n",
      "Loss on train= 0.010599356144666672\n",
      "Loss on test= 0.015877582132816315\n",
      "acc for Lsat= 0.06921718799405628 \n",
      "acc for Psat= 0.095142805410756 \n",
      "acc for optim= 0.141510103808509\n",
      "Epoch:720/1000\n",
      "Loss on train= 0.010520914569497108\n",
      "Loss on test= 0.016409318894147873\n",
      "acc for Lsat= 0.06635848118199243 \n",
      "acc for Psat= 0.09918293919828203 \n",
      "acc for optim= 0.14066512973772158\n",
      "Epoch:721/1000\n",
      "Loss on train= 0.01045915111899376\n",
      "Loss on test= 0.016162609681487083\n",
      "acc for Lsat= 0.0639016823636161 \n",
      "acc for Psat= 0.09300784054729676 \n",
      "acc for optim= 0.14149355772468777\n",
      "Epoch:722/1000\n",
      "Loss on train= 0.0107616251334548\n",
      "Loss on test= 0.01563822291791439\n",
      "acc for Lsat= 0.07182179076804056 \n",
      "acc for Psat= 0.09756644235716924 \n",
      "acc for optim= 0.14308436148696477\n",
      "Epoch:723/1000\n",
      "Loss on train= 0.010735741816461086\n",
      "Loss on test= 0.016962073743343353\n",
      "acc for Lsat= 0.0667116965684626 \n",
      "acc for Psat= 0.0937298243244489 \n",
      "acc for optim= 0.1422733200920953\n",
      "Epoch:724/1000\n",
      "Loss on train= 0.01062270998954773\n",
      "Loss on test= 0.01603352464735508\n",
      "acc for Lsat= 0.06949323962132135 \n",
      "acc for Psat= 0.09534492327107325 \n",
      "acc for optim= 0.14224990258614223\n",
      "Epoch:725/1000\n",
      "Loss on train= 0.011055312119424343\n",
      "Loss on test= 0.01651654951274395\n",
      "acc for Lsat= 0.0682426537076632 \n",
      "acc for Psat= 0.09798565953969957 \n",
      "acc for optim= 0.14379449834426242\n",
      "Epoch:726/1000\n",
      "Loss on train= 0.010959941893815994\n",
      "Loss on test= 0.01598265767097473\n",
      "acc for Lsat= 0.06391792429818048 \n",
      "acc for Psat= 0.09121206899483998 \n",
      "acc for optim= 0.1424572436345948\n",
      "Epoch:727/1000\n",
      "Loss on train= 0.010466372594237328\n",
      "Loss on test= 0.016390962526202202\n",
      "acc for Lsat= 0.06644849561982685 \n",
      "acc for Psat= 0.09415356616179149 \n",
      "acc for optim= 0.1443940046760771\n",
      "Epoch:728/1000\n",
      "Loss on train= 0.010779871605336666\n",
      "Loss on test= 0.016550512984395027\n",
      "acc for Lsat= 0.06342974768744575 \n",
      "acc for Psat= 0.09518269664711422 \n",
      "acc for optim= 0.14485700958304937\n",
      "Epoch:729/1000\n",
      "Loss on train= 0.01044688280671835\n",
      "Loss on test= 0.015449012629687786\n",
      "acc for Lsat= 0.06048386693000794 \n",
      "acc for Psat= 0.09302790429857043 \n",
      "acc for optim= 0.1413416776392195\n",
      "Epoch:730/1000\n",
      "Loss on train= 0.010457160882651806\n",
      "Loss on test= 0.016525333747267723\n",
      "acc for Lsat= 0.06112163431114621 \n",
      "acc for Psat= 0.0935826137661934 \n",
      "acc for optim= 0.1440904948446486\n",
      "Epoch:731/1000\n",
      "Loss on train= 0.010353105142712593\n",
      "Loss on test= 0.016727028414607048\n",
      "acc for Lsat= 0.06903654022349251 \n",
      "acc for Psat= 0.09792882485522164 \n",
      "acc for optim= 0.14191643959946104\n",
      "Epoch:732/1000\n",
      "Loss on train= 0.010799353010952473\n",
      "Loss on test= 0.01661788299679756\n",
      "acc for Lsat= 0.07858182324303521 \n",
      "acc for Psat= 0.10587183336416882 \n",
      "acc for optim= 0.14190820753574374\n",
      "Epoch:733/1000\n",
      "Loss on train= 0.01052598375827074\n",
      "Loss on test= 0.01747835986316204\n",
      "acc for Lsat= 0.06563127057419882 \n",
      "acc for Psat= 0.09508625434504614 \n",
      "acc for optim= 0.14273431963390776\n",
      "Epoch:734/1000\n",
      "Loss on train= 0.010793759487569332\n",
      "Loss on test= 0.017404472455382347\n",
      "acc for Lsat= 0.06532327002949184 \n",
      "acc for Psat= 0.09419176065259509 \n",
      "acc for optim= 0.14178646951913831\n",
      "Epoch:735/1000\n",
      "Loss on train= 0.010906193405389786\n",
      "Loss on test= 0.015831466764211655\n",
      "acc for Lsat= 0.062356655465231996 \n",
      "acc for Psat= 0.09520821058087878 \n",
      "acc for optim= 0.1410177124871148\n",
      "Epoch:736/1000\n",
      "Loss on train= 0.010546102188527584\n",
      "Loss on test= 0.016771424561738968\n",
      "acc for Lsat= 0.06352604875961938 \n",
      "acc for Psat= 0.09671763132015865 \n",
      "acc for optim= 0.14235648148589664\n",
      "Epoch:737/1000\n",
      "Loss on train= 0.010720939375460148\n",
      "Loss on test= 0.017506256699562073\n",
      "acc for Lsat= 0.06649649548861716 \n",
      "acc for Psat= 0.09565436906284758 \n",
      "acc for optim= 0.14278975741730793\n",
      "Epoch:738/1000\n",
      "Loss on train= 0.010807744227349758\n",
      "Loss on test= 0.017187848687171936\n",
      "acc for Lsat= 0.06358228176832198 \n",
      "acc for Psat= 0.09453435391187667 \n",
      "acc for optim= 0.14062734908527802\n",
      "Epoch:739/1000\n",
      "Loss on train= 0.010317841544747353\n",
      "Loss on test= 0.01627529039978981\n",
      "acc for Lsat= 0.06818338193827206 \n",
      "acc for Psat= 0.0947386903895272 \n",
      "acc for optim= 0.14433305147621367\n",
      "Epoch:740/1000\n",
      "Loss on train= 0.01075283344835043\n",
      "Loss on test= 0.016370834782719612\n",
      "acc for Lsat= 0.0626495255364312 \n",
      "acc for Psat= 0.09143625381920072 \n",
      "acc for optim= 0.1420454778605037\n",
      "Epoch:741/1000\n",
      "Loss on train= 0.010922566056251526\n",
      "Loss on test= 0.017109058797359467\n",
      "acc for Lsat= 0.05991547496782409 \n",
      "acc for Psat= 0.09117932253413728 \n",
      "acc for optim= 0.14238090241948764\n",
      "Epoch:742/1000\n",
      "Loss on train= 0.010502695105969906\n",
      "Loss on test= 0.01670491136610508\n",
      "acc for Lsat= 0.07146760059727562 \n",
      "acc for Psat= 0.09766153660085465 \n",
      "acc for optim= 0.14075904968712064\n",
      "Epoch:743/1000\n",
      "Loss on train= 0.010685687884688377\n",
      "Loss on test= 0.0171283520758152\n",
      "acc for Lsat= 0.06926761915286382 \n",
      "acc for Psat= 0.09615777366691165 \n",
      "acc for optim= 0.14229356679651475\n",
      "Epoch:744/1000\n",
      "Loss on train= 0.01033370103687048\n",
      "Loss on test= 0.016533834859728813\n",
      "acc for Lsat= 0.0675210287173589 \n",
      "acc for Psat= 0.09305673738320669 \n",
      "acc for optim= 0.1419575511581368\n",
      "Epoch:745/1000\n",
      "Loss on train= 0.010421568527817726\n",
      "Loss on test= 0.016221309080719948\n",
      "acc for Lsat= 0.06381472862429088 \n",
      "acc for Psat= 0.0993016968170802 \n",
      "acc for optim= 0.1428353909817007\n",
      "Epoch:746/1000\n",
      "Loss on train= 0.010215342976152897\n",
      "Loss on test= 0.016958534717559814\n",
      "acc for Lsat= 0.06993650512562859 \n",
      "acc for Psat= 0.09738753537336985 \n",
      "acc for optim= 0.1417380192213588\n",
      "Epoch:747/1000\n",
      "Loss on train= 0.010712909512221813\n",
      "Loss on test= 0.01733885519206524\n",
      "acc for Lsat= 0.06502375288142098 \n",
      "acc for Psat= 0.0952798949347602 \n",
      "acc for optim= 0.14462508989704978\n",
      "Epoch:748/1000\n",
      "Loss on train= 0.01038631983101368\n",
      "Loss on test= 0.017637738958001137\n",
      "acc for Lsat= 0.06533652179770999 \n",
      "acc for Psat= 0.0992103652821647 \n",
      "acc for optim= 0.1440581990612878\n",
      "Epoch:749/1000\n",
      "Loss on train= 0.010180450044572353\n",
      "Loss on test= 0.01660948246717453\n",
      "acc for Lsat= 0.06425408464339043 \n",
      "acc for Psat= 0.096183985306157 \n",
      "acc for optim= 0.14253092093600164\n",
      "Epoch:750/1000\n",
      "Loss on train= 0.010449630208313465\n",
      "Loss on test= 0.015953468158841133\n",
      "acc for Lsat= 0.06251288404067358 \n",
      "acc for Psat= 0.0926586397820049 \n",
      "acc for optim= 0.14415313717391756\n",
      "Epoch:751/1000\n",
      "Loss on train= 0.010492397472262383\n",
      "Loss on test= 0.01632157526910305\n",
      "acc for Lsat= 0.06372403129935264 \n",
      "acc for Psat= 0.09219581964943142 \n",
      "acc for optim= 0.1426384150981903\n",
      "Epoch:752/1000\n",
      "Loss on train= 0.010232102125883102\n",
      "Loss on test= 0.0166897252202034\n",
      "acc for Lsat= 0.06679109401173061 \n",
      "acc for Psat= 0.0976794992884 \n",
      "acc for optim= 0.1422972384426329\n",
      "Epoch:753/1000\n",
      "Loss on train= 0.010704834945499897\n",
      "Loss on test= 0.016936130821704865\n",
      "acc for Lsat= 0.06178880631923675 \n",
      "acc for Psat= 0.09197194642490812 \n",
      "acc for optim= 0.14369745237959752\n",
      "Epoch:754/1000\n",
      "Loss on train= 0.010789886116981506\n",
      "Loss on test= 0.01637011021375656\n",
      "acc for Lsat= 0.06467539461122619 \n",
      "acc for Psat= 0.09352156983481513 \n",
      "acc for optim= 0.14343946079413097\n",
      "Epoch:755/1000\n",
      "Loss on train= 0.010695341043174267\n",
      "Loss on test= 0.016715731471776962\n",
      "acc for Lsat= 0.06772833880450992 \n",
      "acc for Psat= 0.10590883062945473 \n",
      "acc for optim= 0.1440331925948461\n",
      "Epoch:756/1000\n",
      "Loss on train= 0.01015501469373703\n",
      "Loss on test= 0.016258062794804573\n",
      "acc for Lsat= 0.06312453299760817 \n",
      "acc for Psat= 0.10028829591141808 \n",
      "acc for optim= 0.1435201636619038\n",
      "Epoch:757/1000\n",
      "Loss on train= 0.010624752379953861\n",
      "Loss on test= 0.016985153779387474\n",
      "acc for Lsat= 0.06579060869084463 \n",
      "acc for Psat= 0.10172160532739428 \n",
      "acc for optim= 0.14360938337114124\n",
      "Epoch:758/1000\n",
      "Loss on train= 0.010259310714900494\n",
      "Loss on test= 0.01567007042467594\n",
      "acc for Lsat= 0.06381142147713237 \n",
      "acc for Psat= 0.09295881473355821 \n",
      "acc for optim= 0.1417005818751123\n",
      "Epoch:759/1000\n",
      "Loss on train= 0.010368376038968563\n",
      "Loss on test= 0.01673859730362892\n",
      "acc for Lsat= 0.06504742726683616 \n",
      "acc for Psat= 0.1011276145776113 \n",
      "acc for optim= 0.14370379315482243\n",
      "Epoch:760/1000\n",
      "Loss on train= 0.009901723824441433\n",
      "Loss on test= 0.016750464215874672\n",
      "acc for Lsat= 0.062099951008955635 \n",
      "acc for Psat= 0.0943441376917892 \n",
      "acc for optim= 0.14290732343991597\n",
      "Epoch:761/1000\n",
      "Loss on train= 0.010358517058193684\n",
      "Loss on test= 0.016243495047092438\n",
      "acc for Lsat= 0.065889060166147 \n",
      "acc for Psat= 0.10007688005765278 \n",
      "acc for optim= 0.14447819756136998\n",
      "Epoch:762/1000\n",
      "Loss on train= 0.010216999799013138\n",
      "Loss on test= 0.01565544493496418\n",
      "acc for Lsat= 0.06404930709136857 \n",
      "acc for Psat= 0.09873751948277157 \n",
      "acc for optim= 0.14393068502346673\n",
      "Epoch:763/1000\n",
      "Loss on train= 0.010661620646715164\n",
      "Loss on test= 0.01650760881602764\n",
      "acc for Lsat= 0.06220463232861625 \n",
      "acc for Psat= 0.09268391132354738 \n",
      "acc for optim= 0.14512375063366356\n",
      "Epoch:764/1000\n",
      "Loss on train= 0.010431485250592232\n",
      "Loss on test= 0.016617394983768463\n",
      "acc for Lsat= 0.0647619963520103 \n",
      "acc for Psat= 0.09484186536735958 \n",
      "acc for optim= 0.1440249052312639\n",
      "Epoch:765/1000\n",
      "Loss on train= 0.01024302002042532\n",
      "Loss on test= 0.01603028178215027\n",
      "acc for Lsat= 0.06912033028072781 \n",
      "acc for Psat= 0.0927253622147772 \n",
      "acc for optim= 0.1416047347916497\n",
      "Epoch:766/1000\n",
      "Loss on train= 0.010475332848727703\n",
      "Loss on test= 0.01653960905969143\n",
      "acc for Lsat= 0.07237011740605037 \n",
      "acc for Psat= 0.10839540726608698 \n",
      "acc for optim= 0.14346826242076025\n",
      "Epoch:767/1000\n",
      "Loss on train= 0.010313006117939949\n",
      "Loss on test= 0.01688637211918831\n",
      "acc for Lsat= 0.07032503460844357 \n",
      "acc for Psat= 0.09571812020407781 \n",
      "acc for optim= 0.14125191436873538\n",
      "Epoch:768/1000\n",
      "Loss on train= 0.010304320603609085\n",
      "Loss on test= 0.01559319719672203\n",
      "acc for Lsat= 0.06317488882276748 \n",
      "acc for Psat= 0.09421053495672015 \n",
      "acc for optim= 0.1430048613084687\n",
      "Epoch:769/1000\n",
      "Loss on train= 0.01073919516056776\n",
      "Loss on test= 0.01609083078801632\n",
      "acc for Lsat= 0.06363923392362064 \n",
      "acc for Psat= 0.09518484390444228 \n",
      "acc for optim= 0.14340652045276425\n",
      "Epoch:770/1000\n",
      "Loss on train= 0.010498604737222195\n",
      "Loss on test= 0.01681317389011383\n",
      "acc for Lsat= 0.06358938159214125 \n",
      "acc for Psat= 0.09459046307537289 \n",
      "acc for optim= 0.1440413342581855\n",
      "Epoch:771/1000\n",
      "Loss on train= 0.010390696115791798\n",
      "Loss on test= 0.016037331894040108\n",
      "acc for Lsat= 0.06762924078438017 \n",
      "acc for Psat= 0.09584337936507333 \n",
      "acc for optim= 0.14436381641361448\n",
      "Epoch:772/1000\n",
      "Loss on train= 0.010101034305989742\n",
      "Loss on test= 0.016012907028198242\n",
      "acc for Lsat= 0.06653955164882872 \n",
      "acc for Psat= 0.09338027205732134 \n",
      "acc for optim= 0.1423141787449519\n",
      "Epoch:773/1000\n",
      "Loss on train= 0.010382578708231449\n",
      "Loss on test= 0.017217328771948814\n",
      "acc for Lsat= 0.06696110831366646 \n",
      "acc for Psat= 0.09346706834104325 \n",
      "acc for optim= 0.14078462024529775\n",
      "Epoch:774/1000\n",
      "Loss on train= 0.010007080622017384\n",
      "Loss on test= 0.01621553674340248\n",
      "acc for Lsat= 0.06505822075737848 \n",
      "acc for Psat= 0.09527863843573464 \n",
      "acc for optim= 0.14230344163046946\n",
      "Epoch:775/1000\n",
      "Loss on train= 0.010022991336882114\n",
      "Loss on test= 0.017218776047229767\n",
      "acc for Lsat= 0.06605441321929297 \n",
      "acc for Psat= 0.10047377198934554 \n",
      "acc for optim= 0.14458790123462675\n",
      "Epoch:776/1000\n",
      "Loss on train= 0.010147352702915668\n",
      "Loss on test= 0.01669594831764698\n",
      "acc for Lsat= 0.06595961393581495 \n",
      "acc for Psat= 0.09896343797445298 \n",
      "acc for optim= 0.14463215702109866\n",
      "Epoch:777/1000\n",
      "Loss on train= 0.01008022390305996\n",
      "Loss on test= 0.017490386962890625\n",
      "acc for Lsat= 0.0707597628235817 \n",
      "acc for Psat= 0.10066425502300264 \n",
      "acc for optim= 0.14268247245086566\n",
      "Epoch:778/1000\n",
      "Loss on train= 0.009964576922357082\n",
      "Loss on test= 0.016927242279052734\n",
      "acc for Lsat= 0.0681606463260121 \n",
      "acc for Psat= 0.0951340658797158 \n",
      "acc for optim= 0.14488926662339108\n",
      "Epoch:779/1000\n",
      "Loss on train= 0.010293482802808285\n",
      "Loss on test= 0.01691862940788269\n",
      "acc for Lsat= 0.0634356393582291 \n",
      "acc for Psat= 0.09096953512893784 \n",
      "acc for optim= 0.14455828881926006\n",
      "Epoch:780/1000\n",
      "Loss on train= 0.010339812375605106\n",
      "Loss on test= 0.015568535774946213\n",
      "acc for Lsat= 0.06465773979822795 \n",
      "acc for Psat= 0.097010271747907 \n",
      "acc for optim= 0.14453681583205863\n",
      "Epoch:781/1000\n",
      "Loss on train= 0.010291344486176968\n",
      "Loss on test= 0.016126079484820366\n",
      "acc for Lsat= 0.06393275393380059 \n",
      "acc for Psat= 0.09253608898984061 \n",
      "acc for optim= 0.14460019121567408\n",
      "Epoch:782/1000\n",
      "Loss on train= 0.010224773548543453\n",
      "Loss on test= 0.017106859013438225\n",
      "acc for Lsat= 0.06549994150797527 \n",
      "acc for Psat= 0.0955910677711169 \n",
      "acc for optim= 0.1452001043491893\n",
      "Epoch:783/1000\n",
      "Loss on train= 0.009924454614520073\n",
      "Loss on test= 0.016201229766011238\n",
      "acc for Lsat= 0.062050821383794155 \n",
      "acc for Psat= 0.09550239824586444 \n",
      "acc for optim= 0.14408070660299724\n",
      "Epoch:784/1000\n",
      "Loss on train= 0.010225371457636356\n",
      "Loss on test= 0.01694704219698906\n",
      "acc for Lsat= 0.06354339867830278 \n",
      "acc for Psat= 0.09442939460277558 \n",
      "acc for optim= 0.14177262302902005\n",
      "Epoch:785/1000\n",
      "Loss on train= 0.01022866927087307\n",
      "Loss on test= 0.016230622306466103\n",
      "acc for Lsat= 0.06324699545900027 \n",
      "acc for Psat= 0.09300291289885838 \n",
      "acc for optim= 0.14426688734028076\n",
      "Epoch:786/1000\n",
      "Loss on train= 0.010425516404211521\n",
      "Loss on test= 0.016987884417176247\n",
      "acc for Lsat= 0.06635174138678444 \n",
      "acc for Psat= 0.09012497911850612 \n",
      "acc for optim= 0.143576797677411\n",
      "Epoch:787/1000\n",
      "Loss on train= 0.0103465486317873\n",
      "Loss on test= 0.01711387187242508\n",
      "acc for Lsat= 0.0625241527126895 \n",
      "acc for Psat= 0.09086510356929565 \n",
      "acc for optim= 0.14519297629594802\n",
      "Epoch:788/1000\n",
      "Loss on train= 0.01046585664153099\n",
      "Loss on test= 0.016895359382033348\n",
      "acc for Lsat= 0.06527993389301828 \n",
      "acc for Psat= 0.09249053820967673 \n",
      "acc for optim= 0.14397791938649285\n",
      "Epoch:789/1000\n",
      "Loss on train= 0.009973745793104172\n",
      "Loss on test= 0.01688873954117298\n",
      "acc for Lsat= 0.06765125725004409 \n",
      "acc for Psat= 0.09712723228666517 \n",
      "acc for optim= 0.14366145961814458\n",
      "Epoch:790/1000\n",
      "Loss on train= 0.010230423882603645\n",
      "Loss on test= 0.016167936846613884\n",
      "acc for Lsat= 0.06382826450798246 \n",
      "acc for Psat= 0.09183615545431772 \n",
      "acc for optim= 0.14524921725193657\n",
      "Epoch:791/1000\n",
      "Loss on train= 0.010638215579092503\n",
      "Loss on test= 0.016342148184776306\n",
      "acc for Lsat= 0.06346231947342555 \n",
      "acc for Psat= 0.09839231189754273 \n",
      "acc for optim= 0.14282008359829587\n",
      "Epoch:792/1000\n",
      "Loss on train= 0.010318147018551826\n",
      "Loss on test= 0.016441727057099342\n",
      "acc for Lsat= 0.06249940759605831 \n",
      "acc for Psat= 0.0981133250726594 \n",
      "acc for optim= 0.1444233876135614\n",
      "Epoch:793/1000\n",
      "Loss on train= 0.010434858500957489\n",
      "Loss on test= 0.01616200990974903\n",
      "acc for Lsat= 0.0644514681564437 \n",
      "acc for Psat= 0.09217834058735105 \n",
      "acc for optim= 0.14272117101483872\n",
      "Epoch:794/1000\n",
      "Loss on train= 0.01012127473950386\n",
      "Loss on test= 0.01688513532280922\n",
      "acc for Lsat= 0.06140573885705734 \n",
      "acc for Psat= 0.09520315395461186 \n",
      "acc for optim= 0.14712504943211874\n",
      "Epoch:795/1000\n",
      "Loss on train= 0.01013878919184208\n",
      "Loss on test= 0.017354276031255722\n",
      "acc for Lsat= 0.06591028918822607 \n",
      "acc for Psat= 0.09521061811182234 \n",
      "acc for optim= 0.1416261217660374\n",
      "Epoch:796/1000\n",
      "Loss on train= 0.010650428012013435\n",
      "Loss on test= 0.015664324164390564\n",
      "acc for Lsat= 0.06333539552158779 \n",
      "acc for Psat= 0.09450201425287458 \n",
      "acc for optim= 0.1443903475999832\n",
      "Epoch:797/1000\n",
      "Loss on train= 0.010491670109331608\n",
      "Loss on test= 0.016248902305960655\n",
      "acc for Lsat= 0.06402501728799609 \n",
      "acc for Psat= 0.09778209560447271 \n",
      "acc for optim= 0.14325319925944008\n",
      "Epoch:798/1000\n",
      "Loss on train= 0.010175079107284546\n",
      "Loss on test= 0.016559014096856117\n",
      "acc for Lsat= 0.06572325908475453 \n",
      "acc for Psat= 0.09248677144447962 \n",
      "acc for optim= 0.14500790437062583\n",
      "Epoch:799/1000\n",
      "Loss on train= 0.010039820335805416\n",
      "Loss on test= 0.016740163788199425\n",
      "acc for Lsat= 0.06279751790894401 \n",
      "acc for Psat= 0.09421415312422647 \n",
      "acc for optim= 0.14539192219575245\n",
      "Epoch:800/1000\n",
      "Loss on train= 0.009770771488547325\n",
      "Loss on test= 0.017563052475452423\n",
      "acc for Lsat= 0.07115502357482911 \n",
      "acc for Psat= 0.0967772662639618 \n",
      "acc for optim= 0.14465094274944731\n",
      "Epoch:801/1000\n",
      "Loss on train= 0.009923138655722141\n",
      "Loss on test= 0.015602800995111465\n",
      "acc for Lsat= 0.06221244260668753 \n",
      "acc for Psat= 0.09180312305688858 \n",
      "acc for optim= 0.14385878344376885\n",
      "Epoch:802/1000\n",
      "Loss on train= 0.010083706118166447\n",
      "Loss on test= 0.01755618304014206\n",
      "acc for Lsat= 0.06574364693628416 \n",
      "acc for Psat= 0.09205602920717665 \n",
      "acc for optim= 0.14385096745358572\n",
      "Epoch:803/1000\n",
      "Loss on train= 0.01004054956138134\n",
      "Loss on test= 0.016174491494894028\n",
      "acc for Lsat= 0.06744293669859569 \n",
      "acc for Psat= 0.08957441614733803 \n",
      "acc for optim= 0.14583528820011352\n",
      "Epoch:804/1000\n",
      "Loss on train= 0.010343879461288452\n",
      "Loss on test= 0.01645559072494507\n",
      "acc for Lsat= 0.06857062346405454 \n",
      "acc for Psat= 0.09945519947343402 \n",
      "acc for optim= 0.1442666353450881\n",
      "Epoch:805/1000\n",
      "Loss on train= 0.01010983157902956\n",
      "Loss on test= 0.01671811193227768\n",
      "acc for Lsat= 0.06425000015232299 \n",
      "acc for Psat= 0.09567071828577253 \n",
      "acc for optim= 0.14318971567683747\n",
      "Epoch:806/1000\n",
      "Loss on train= 0.010083714500069618\n",
      "Loss on test= 0.01678401418030262\n",
      "acc for Lsat= 0.0624477941956785 \n",
      "acc for Psat= 0.09401350352499221 \n",
      "acc for optim= 0.14371459070179196\n",
      "Epoch:807/1000\n",
      "Loss on train= 0.01020005065947771\n",
      "Loss on test= 0.016790982335805893\n",
      "acc for Lsat= 0.06721569928858015 \n",
      "acc for Psat= 0.09328208118677139 \n",
      "acc for optim= 0.14347764237059488\n",
      "Epoch:808/1000\n",
      "Loss on train= 0.00983246136456728\n",
      "Loss on test= 0.0164389219135046\n",
      "acc for Lsat= 0.060932858039935436 \n",
      "acc for Psat= 0.09337301610244646 \n",
      "acc for optim= 0.1446087862054507\n",
      "Epoch:809/1000\n",
      "Loss on train= 0.009967721067368984\n",
      "Loss on test= 0.016785405576229095\n",
      "acc for Lsat= 0.06498247360189754 \n",
      "acc for Psat= 0.09953388538625503 \n",
      "acc for optim= 0.14634180019299192\n",
      "Epoch:810/1000\n",
      "Loss on train= 0.01062189880758524\n",
      "Loss on test= 0.01640498824417591\n",
      "acc for Lsat= 0.06646852592627207 \n",
      "acc for Psat= 0.09520002239280276 \n",
      "acc for optim= 0.1436607451902496\n",
      "Epoch:811/1000\n",
      "Loss on train= 0.010348356328904629\n",
      "Loss on test= 0.017063045874238014\n",
      "acc for Lsat= 0.061730363468329115 \n",
      "acc for Psat= 0.09326494087775548 \n",
      "acc for optim= 0.14272849609454474\n",
      "Epoch:812/1000\n",
      "Loss on train= 0.010654199868440628\n",
      "Loss on test= 0.017639363184571266\n",
      "acc for Lsat= 0.06766472376055188 \n",
      "acc for Psat= 0.09629022611512077 \n",
      "acc for optim= 0.1425155413647493\n",
      "Epoch:813/1000\n",
      "Loss on train= 0.010425897315144539\n",
      "Loss on test= 0.01661907322704792\n",
      "acc for Lsat= 0.0628860804769728 \n",
      "acc for Psat= 0.09611976510948604 \n",
      "acc for optim= 0.1446480933162901\n",
      "Epoch:814/1000\n",
      "Loss on train= 0.010247847065329552\n",
      "Loss on test= 0.016308985650539398\n",
      "acc for Lsat= 0.06509853485557768 \n",
      "acc for Psat= 0.09451072331931858 \n",
      "acc for optim= 0.1435673983560668\n",
      "Epoch:815/1000\n",
      "Loss on train= 0.010432299226522446\n",
      "Loss on test= 0.016719073057174683\n",
      "acc for Lsat= 0.062290061182445955 \n",
      "acc for Psat= 0.09391828800241152 \n",
      "acc for optim= 0.14404377738634747\n",
      "Epoch:816/1000\n",
      "Loss on train= 0.01028471440076828\n",
      "Loss on test= 0.016169145703315735\n",
      "acc for Lsat= 0.06725317993097836 \n",
      "acc for Psat= 0.09347041183047823 \n",
      "acc for optim= 0.14219766027397582\n",
      "Epoch:817/1000\n",
      "Loss on train= 0.010135074146091938\n",
      "Loss on test= 0.016726670786738396\n",
      "acc for Lsat= 0.06634625345468521 \n",
      "acc for Psat= 0.09625263429350323 \n",
      "acc for optim= 0.14409235765536627\n",
      "Epoch:818/1000\n",
      "Loss on train= 0.009736024774610996\n",
      "Loss on test= 0.017261795699596405\n",
      "acc for Lsat= 0.07124105642239252 \n",
      "acc for Psat= 0.10220526456832885 \n",
      "acc for optim= 0.14561250574058957\n",
      "Epoch:819/1000\n",
      "Loss on train= 0.010124439373612404\n",
      "Loss on test= 0.015317870303988457\n",
      "acc for Lsat= 0.06380710254112879 \n",
      "acc for Psat= 0.10128732025623322 \n",
      "acc for optim= 0.1444417402148247\n",
      "Epoch:820/1000\n",
      "Loss on train= 0.010283693671226501\n",
      "Loss on test= 0.016349855810403824\n",
      "acc for Lsat= 0.06530861258506776 \n",
      "acc for Psat= 0.09401422606574167 \n",
      "acc for optim= 0.14319103343619244\n",
      "Epoch:821/1000\n",
      "Loss on train= 0.01002980675548315\n",
      "Loss on test= 0.01741766557097435\n",
      "acc for Lsat= 0.06481168932384915 \n",
      "acc for Psat= 0.09372496638033126 \n",
      "acc for optim= 0.14612485335932834\n",
      "Epoch:822/1000\n",
      "Loss on train= 0.009814852848649025\n",
      "Loss on test= 0.015845181420445442\n",
      "acc for Lsat= 0.06743680006927914 \n",
      "acc for Psat= 0.09495372705989415 \n",
      "acc for optim= 0.1420624789264467\n",
      "Epoch:823/1000\n",
      "Loss on train= 0.010066413320600986\n",
      "Loss on test= 0.01577034220099449\n",
      "acc for Lsat= 0.06276622108287282 \n",
      "acc for Psat= 0.09151361915800307 \n",
      "acc for optim= 0.1461989839871725\n",
      "Epoch:824/1000\n",
      "Loss on train= 0.010148563422262669\n",
      "Loss on test= 0.01655181311070919\n",
      "acc for Lsat= 0.06376609048909612 \n",
      "acc for Psat= 0.09544323450989194 \n",
      "acc for optim= 0.14445845021141898\n",
      "Epoch:825/1000\n",
      "Loss on train= 0.009556185454130173\n",
      "Loss on test= 0.016897091642022133\n",
      "acc for Lsat= 0.0631089511844847 \n",
      "acc for Psat= 0.09297632169392374 \n",
      "acc for optim= 0.14572638306352834\n",
      "Epoch:826/1000\n",
      "Loss on train= 0.010264541022479534\n",
      "Loss on test= 0.01689085364341736\n",
      "acc for Lsat= 0.062429937885867225 \n",
      "acc for Psat= 0.09153103861543868 \n",
      "acc for optim= 0.14589772125085196\n",
      "Epoch:827/1000\n",
      "Loss on train= 0.010470235720276833\n",
      "Loss on test= 0.01630045846104622\n",
      "acc for Lsat= 0.06305515559183225 \n",
      "acc for Psat= 0.09284657546215587 \n",
      "acc for optim= 0.14147014915943146\n",
      "Epoch:828/1000\n",
      "Loss on train= 0.009982841089367867\n",
      "Loss on test= 0.016957618296146393\n",
      "acc for Lsat= 0.06539553354183832 \n",
      "acc for Psat= 0.0908049628138542 \n",
      "acc for optim= 0.14164977835284337\n",
      "Epoch:829/1000\n",
      "Loss on train= 0.009973800741136074\n",
      "Loss on test= 0.016742179170250893\n",
      "acc for Lsat= 0.06747072239716849 \n",
      "acc for Psat= 0.0965747892856598 \n",
      "acc for optim= 0.14367554552025266\n",
      "Epoch:830/1000\n",
      "Loss on train= 0.01005797740072012\n",
      "Loss on test= 0.017005395144224167\n",
      "acc for Lsat= 0.06373845140139262 \n",
      "acc for Psat= 0.09266785962714089 \n",
      "acc for optim= 0.14522645556264452\n",
      "Epoch:831/1000\n",
      "Loss on train= 0.010050342418253422\n",
      "Loss on test= 0.01547156646847725\n",
      "acc for Lsat= 0.06876617504490748 \n",
      "acc for Psat= 0.09552415758371353 \n",
      "acc for optim= 0.1440095273984803\n",
      "Epoch:832/1000\n",
      "Loss on train= 0.010008989833295345\n",
      "Loss on test= 0.017676113173365593\n",
      "acc for Lsat= 0.06315009180042479 \n",
      "acc for Psat= 0.09230738199419444 \n",
      "acc for optim= 0.14308807253837585\n",
      "Epoch:833/1000\n",
      "Loss on train= 0.010065755806863308\n",
      "Loss on test= 0.017379360273480415\n",
      "acc for Lsat= 0.06831092172198824 \n",
      "acc for Psat= 0.09566967619789972 \n",
      "acc for optim= 0.14564436508549583\n",
      "Epoch:834/1000\n",
      "Loss on train= 0.009883249178528786\n",
      "Loss on test= 0.017311736941337585\n",
      "acc for Lsat= 0.06329540072215928 \n",
      "acc for Psat= 0.09158532751931085 \n",
      "acc for optim= 0.14336674743228492\n",
      "Epoch:835/1000\n",
      "Loss on train= 0.009588148444890976\n",
      "Loss on test= 0.016659142449498177\n",
      "acc for Lsat= 0.06231373060080741 \n",
      "acc for Psat= 0.09194395252399976 \n",
      "acc for optim= 0.1434329660402404\n",
      "Epoch:836/1000\n",
      "Loss on train= 0.010432463139295578\n",
      "Loss on test= 0.01626308262348175\n",
      "acc for Lsat= 0.06355644083685344 \n",
      "acc for Psat= 0.0923765379521582 \n",
      "acc for optim= 0.14477537009451125\n",
      "Epoch:837/1000\n",
      "Loss on train= 0.00987547729164362\n",
      "Loss on test= 0.015808381140232086\n",
      "acc for Lsat= 0.0631877621014913 \n",
      "acc for Psat= 0.09336391819847953 \n",
      "acc for optim= 0.14659499625364944\n",
      "Epoch:838/1000\n",
      "Loss on train= 0.009757989086210728\n",
      "Loss on test= 0.016812337562441826\n",
      "acc for Lsat= 0.06581591251823637 \n",
      "acc for Psat= 0.09645509454939101 \n",
      "acc for optim= 0.14527624133560393\n",
      "Epoch:839/1000\n",
      "Loss on train= 0.010536352172493935\n",
      "Loss on test= 0.017399819567799568\n",
      "acc for Lsat= 0.06952317853768668 \n",
      "acc for Psat= 0.09579884492688709 \n",
      "acc for optim= 0.14476369536585276\n",
      "Epoch:840/1000\n",
      "Loss on train= 0.010114922188222408\n",
      "Loss on test= 0.016780845820903778\n",
      "acc for Lsat= 0.06449470089541541 \n",
      "acc for Psat= 0.0921840849849913 \n",
      "acc for optim= 0.14521003945006264\n",
      "Epoch:841/1000\n",
      "Loss on train= 0.00974720437079668\n",
      "Loss on test= 0.016704365611076355\n",
      "acc for Lsat= 0.06268600043323305 \n",
      "acc for Psat= 0.09200815674331453 \n",
      "acc for optim= 0.14532442241907118\n",
      "Epoch:842/1000\n",
      "Loss on train= 0.00980925839394331\n",
      "Loss on test= 0.01625731587409973\n",
      "acc for Lsat= 0.06240937254495091 \n",
      "acc for Psat= 0.09015407769216432 \n",
      "acc for optim= 0.1423870944314533\n",
      "Epoch:843/1000\n",
      "Loss on train= 0.009685692377388477\n",
      "Loss on test= 0.01592603512108326\n",
      "acc for Lsat= 0.0638667717576027 \n",
      "acc for Psat= 0.09001973668734234 \n",
      "acc for optim= 0.1458028524286217\n",
      "Epoch:844/1000\n",
      "Loss on train= 0.010187659412622452\n",
      "Loss on test= 0.017102601006627083\n",
      "acc for Lsat= 0.06491298096047507 \n",
      "acc for Psat= 0.09502091540230645 \n",
      "acc for optim= 0.1444508171743817\n",
      "Epoch:845/1000\n",
      "Loss on train= 0.010269654914736748\n",
      "Loss on test= 0.016521502286195755\n",
      "acc for Lsat= 0.06155017481909857 \n",
      "acc for Psat= 0.09307318644391166 \n",
      "acc for optim= 0.142911818706327\n",
      "Epoch:846/1000\n",
      "Loss on train= 0.010132378898561\n",
      "Loss on test= 0.017479268833994865\n",
      "acc for Lsat= 0.06263692925373714 \n",
      "acc for Psat= 0.10215317010879515 \n",
      "acc for optim= 0.145698929991987\n",
      "Epoch:847/1000\n",
      "Loss on train= 0.009827340953052044\n",
      "Loss on test= 0.01745452731847763\n",
      "acc for Lsat= 0.06126527173651589 \n",
      "acc for Psat= 0.09306455353895822 \n",
      "acc for optim= 0.14636885391341314\n",
      "Epoch:848/1000\n",
      "Loss on train= 0.009742630645632744\n",
      "Loss on test= 0.01653805747628212\n",
      "acc for Lsat= 0.0630211192700598 \n",
      "acc for Psat= 0.09238481505049598 \n",
      "acc for optim= 0.14780246367057168\n",
      "Epoch:849/1000\n",
      "Loss on train= 0.009843065403401852\n",
      "Loss on test= 0.015315350145101547\n",
      "acc for Lsat= 0.06301584045092265 \n",
      "acc for Psat= 0.0996248154176606 \n",
      "acc for optim= 0.14377526359425652\n",
      "Epoch:850/1000\n",
      "Loss on train= 0.009981287643313408\n",
      "Loss on test= 0.017663804814219475\n",
      "acc for Lsat= 0.06762323561641904 \n",
      "acc for Psat= 0.1014718974630038 \n",
      "acc for optim= 0.14356121818224588\n",
      "Epoch:851/1000\n",
      "Loss on train= 0.009646630845963955\n",
      "Loss on test= 0.017505042254924774\n",
      "acc for Lsat= 0.061629900294873446 \n",
      "acc for Psat= 0.09361322878135575 \n",
      "acc for optim= 0.1442580637004641\n",
      "Epoch:852/1000\n",
      "Loss on train= 0.010013402439653873\n",
      "Loss on test= 0.016584403812885284\n",
      "acc for Lsat= 0.06305703413155345 \n",
      "acc for Psat= 0.0951110965675778 \n",
      "acc for optim= 0.14404356413417393\n",
      "Epoch:853/1000\n",
      "Loss on train= 0.009857035242021084\n",
      "Loss on test= 0.01676059141755104\n",
      "acc for Lsat= 0.06258976153201529 \n",
      "acc for Psat= 0.09203623963726892 \n",
      "acc for optim= 0.14642382827070022\n",
      "Epoch:854/1000\n",
      "Loss on train= 0.009739420376718044\n",
      "Loss on test= 0.016001753509044647\n",
      "acc for Lsat= 0.06173632939656575 \n",
      "acc for Psat= 0.09097155845827526 \n",
      "acc for optim= 0.14484648207823436\n",
      "Epoch:855/1000\n",
      "Loss on train= 0.009651612490415573\n",
      "Loss on test= 0.017043227329850197\n",
      "acc for Lsat= 0.06707304567098617 \n",
      "acc for Psat= 0.10048494338989257 \n",
      "acc for optim= 0.14594144001603127\n",
      "Epoch:856/1000\n",
      "Loss on train= 0.010101978667080402\n",
      "Loss on test= 0.01590782403945923\n",
      "acc for Lsat= 0.06708727296855715 \n",
      "acc for Psat= 0.09871499207284716 \n",
      "acc for optim= 0.1430067723823918\n",
      "Epoch:857/1000\n",
      "Loss on train= 0.009871645830571651\n",
      "Loss on test= 0.017694657668471336\n",
      "acc for Lsat= 0.061594476136896344 \n",
      "acc for Psat= 0.09298820677730772 \n",
      "acc for optim= 0.14463381701045563\n",
      "Epoch:858/1000\n",
      "Loss on train= 0.00999780185520649\n",
      "Loss on test= 0.01650003157556057\n",
      "acc for Lsat= 0.06519640220536127 \n",
      "acc for Psat= 0.09230153610308965 \n",
      "acc for optim= 0.14719415042135453\n",
      "Epoch:859/1000\n",
      "Loss on train= 0.00977207813411951\n",
      "Loss on test= 0.016852887347340584\n",
      "acc for Lsat= 0.06662828259997898 \n",
      "acc for Psat= 0.09416149788432653 \n",
      "acc for optim= 0.14552644573979912\n",
      "Epoch:860/1000\n",
      "Loss on train= 0.010136143304407597\n",
      "Loss on test= 0.01725444570183754\n",
      "acc for Lsat= 0.06366480166713398 \n",
      "acc for Psat= 0.09072883658938939 \n",
      "acc for optim= 0.14253506114085518\n",
      "Epoch:861/1000\n",
      "Loss on train= 0.009849011898040771\n",
      "Loss on test= 0.017146974802017212\n",
      "acc for Lsat= 0.06818473057614431 \n",
      "acc for Psat= 0.095849157207542 \n",
      "acc for optim= 0.1440257504582405\n",
      "Epoch:862/1000\n",
      "Loss on train= 0.009616740047931671\n",
      "Loss on test= 0.017445119097828865\n",
      "acc for Lsat= 0.06220799982547761 \n",
      "acc for Psat= 0.09013564901219472 \n",
      "acc for optim= 0.14415365805228553\n",
      "Epoch:863/1000\n",
      "Loss on train= 0.009594249539077282\n",
      "Loss on test= 0.018016699701547623\n",
      "acc for Lsat= 0.06214127358463076 \n",
      "acc for Psat= 0.09030428296989865 \n",
      "acc for optim= 0.14428860727283688\n",
      "Epoch:864/1000\n",
      "Loss on train= 0.0097159119322896\n",
      "Loss on test= 0.01618395932018757\n",
      "acc for Lsat= 0.06241772083772553 \n",
      "acc for Psat= 0.09217022044791116 \n",
      "acc for optim= 0.14501924845907424\n",
      "Epoch:865/1000\n",
      "Loss on train= 0.009582381695508957\n",
      "Loss on test= 0.01651328057050705\n",
      "acc for Lsat= 0.06221567955282 \n",
      "acc for Psat= 0.09243529107835555 \n",
      "acc for optim= 0.14460173083676234\n",
      "Epoch:866/1000\n",
      "Loss on train= 0.009889858774840832\n",
      "Loss on test= 0.016688361763954163\n",
      "acc for Lsat= 0.07064085420635012 \n",
      "acc for Psat= 0.09998977035284042 \n",
      "acc for optim= 0.14498617913987902\n",
      "Epoch:867/1000\n",
      "Loss on train= 0.009910563938319683\n",
      "Loss on test= 0.01576707325875759\n",
      "acc for Lsat= 0.06315622867809402 \n",
      "acc for Psat= 0.09599931687116622 \n",
      "acc for optim= 0.14412822806172904\n",
      "Epoch:868/1000\n",
      "Loss on train= 0.010107277892529964\n",
      "Loss on test= 0.01763291470706463\n",
      "acc for Lsat= 0.06863155571950805 \n",
      "acc for Psat= 0.09288039538595413 \n",
      "acc for optim= 0.14583139088418748\n",
      "Epoch:869/1000\n",
      "Loss on train= 0.010127871297299862\n",
      "Loss on test= 0.017070861533284187\n",
      "acc for Lsat= 0.06332978026734458 \n",
      "acc for Psat= 0.09566147757901086 \n",
      "acc for optim= 0.14312362405988907\n",
      "Epoch:870/1000\n",
      "Loss on train= 0.009556942619383335\n",
      "Loss on test= 0.016578543931245804\n",
      "acc for Lsat= 0.07005931072764926 \n",
      "acc for Psat= 0.10278977851072946 \n",
      "acc for optim= 0.14623514115810388\n",
      "Epoch:871/1000\n",
      "Loss on train= 0.009789172559976578\n",
      "Loss on test= 0.01706855557858944\n",
      "acc for Lsat= 0.06363552063703537 \n",
      "acc for Psat= 0.09595241513517166 \n",
      "acc for optim= 0.14845670395427282\n",
      "Epoch:872/1000\n",
      "Loss on train= 0.010042546316981316\n",
      "Loss on test= 0.016507014632225037\n",
      "acc for Lsat= 0.0632363980015119 \n",
      "acc for Psat= 0.09289983229504692 \n",
      "acc for optim= 0.14409705632262762\n",
      "Epoch:873/1000\n",
      "Loss on train= 0.009649342857301235\n",
      "Loss on test= 0.015960687771439552\n",
      "acc for Lsat= 0.06360794049170283 \n",
      "acc for Psat= 0.09370811680952709 \n",
      "acc for optim= 0.14521425134605837\n",
      "Epoch:874/1000\n",
      "Loss on train= 0.009841913357377052\n",
      "Loss on test= 0.017743321135640144\n",
      "acc for Lsat= 0.06616171581877603 \n",
      "acc for Psat= 0.0986036330461502 \n",
      "acc for optim= 0.14275424314869778\n",
      "Epoch:875/1000\n",
      "Loss on train= 0.009633679874241352\n",
      "Loss on test= 0.017695227637887\n",
      "acc for Lsat= 0.06493763021296925 \n",
      "acc for Psat= 0.09657351440853543 \n",
      "acc for optim= 0.14455932560894225\n",
      "Epoch:876/1000\n",
      "Loss on train= 0.009350894019007683\n",
      "Loss on test= 0.017050758004188538\n",
      "acc for Lsat= 0.06294084828760889 \n",
      "acc for Psat= 0.09308399624294705 \n",
      "acc for optim= 0.14558919535742865\n",
      "Epoch:877/1000\n",
      "Loss on train= 0.009589467197656631\n",
      "Loss on test= 0.016310682520270348\n",
      "acc for Lsat= 0.06815460357401108 \n",
      "acc for Psat= 0.09094466169675192 \n",
      "acc for optim= 0.14517107158899312\n",
      "Epoch:878/1000\n",
      "Loss on train= 0.010008268058300018\n",
      "Loss on test= 0.016866449266672134\n",
      "acc for Lsat= 0.06829794206553036 \n",
      "acc for Psat= 0.09436987489461897 \n",
      "acc for optim= 0.1453807748026318\n",
      "Epoch:879/1000\n",
      "Loss on train= 0.009307043626904488\n",
      "Loss on test= 0.017247747629880905\n",
      "acc for Lsat= 0.062107501841253715 \n",
      "acc for Psat= 0.09239569563004704 \n",
      "acc for optim= 0.1463171049952507\n",
      "Epoch:880/1000\n",
      "Loss on train= 0.010051090270280838\n",
      "Loss on test= 0.01696624606847763\n",
      "acc for Lsat= 0.0629835800992118 \n",
      "acc for Psat= 0.09812647518184453 \n",
      "acc for optim= 0.1481531045503087\n",
      "Epoch:881/1000\n",
      "Loss on train= 0.009647120721638203\n",
      "Loss on test= 0.017313744872808456\n",
      "acc for Lsat= 0.06280696408616171 \n",
      "acc for Psat= 0.09101268094446925 \n",
      "acc for optim= 0.14593241926696565\n",
      "Epoch:882/1000\n",
      "Loss on train= 0.009743661619722843\n",
      "Loss on test= 0.017309017479419708\n",
      "acc for Lsat= 0.06508434257573553 \n",
      "acc for Psat= 0.09293682244088913 \n",
      "acc for optim= 0.14526077061891557\n",
      "Epoch:883/1000\n",
      "Loss on train= 0.009773281402885914\n",
      "Loss on test= 0.01700598932802677\n",
      "acc for Lsat= 0.06541952184504933 \n",
      "acc for Psat= 0.09273772603935665 \n",
      "acc for optim= 0.14760250300168995\n",
      "Epoch:884/1000\n",
      "Loss on train= 0.009867832995951176\n",
      "Loss on test= 0.016340306028723717\n",
      "acc for Lsat= 0.06312143868870206 \n",
      "acc for Psat= 0.09258881683150925 \n",
      "acc for optim= 0.1456457171175215\n",
      "Epoch:885/1000\n",
      "Loss on train= 0.010248175822198391\n",
      "Loss on test= 0.017640551552176476\n",
      "acc for Lsat= 0.06690662817822562 \n",
      "acc for Psat= 0.09157439561353789 \n",
      "acc for optim= 0.1467527939213647\n",
      "Epoch:886/1000\n",
      "Loss on train= 0.009869479574263096\n",
      "Loss on test= 0.01767953298985958\n",
      "acc for Lsat= 0.06668866003553073 \n",
      "acc for Psat= 0.09585613509019214 \n",
      "acc for optim= 0.14411814328696992\n",
      "Epoch:887/1000\n",
      "Loss on train= 0.009685683995485306\n",
      "Loss on test= 0.017859896644949913\n",
      "acc for Lsat= 0.06345490126146211 \n",
      "acc for Psat= 0.09531780911816493 \n",
      "acc for optim= 0.14676889959308834\n",
      "Epoch:888/1000\n",
      "Loss on train= 0.009810291230678558\n",
      "Loss on test= 0.01685275323688984\n",
      "acc for Lsat= 0.06246875392066108 \n",
      "acc for Psat= 0.09122146219015122 \n",
      "acc for optim= 0.1452345018585523\n",
      "Epoch:889/1000\n",
      "Loss on train= 0.00972056481987238\n",
      "Loss on test= 0.017400143668055534\n",
      "acc for Lsat= 0.06488418694999483 \n",
      "acc for Psat= 0.09511755448248652 \n",
      "acc for optim= 0.14694462915261586\n",
      "Epoch:890/1000\n",
      "Loss on train= 0.009840662591159344\n",
      "Loss on test= 0.017021015286445618\n",
      "acc for Lsat= 0.06346822794940736 \n",
      "acc for Psat= 0.09213775661256578 \n",
      "acc for optim= 0.1433609273698595\n",
      "Epoch:891/1000\n",
      "Loss on train= 0.009628272615373135\n",
      "Loss on test= 0.016761260107159615\n",
      "acc for Lsat= 0.06430524157153236 \n",
      "acc for Psat= 0.09177792287535136 \n",
      "acc for optim= 0.1475196441014608\n",
      "Epoch:892/1000\n",
      "Loss on train= 0.009721287526190281\n",
      "Loss on test= 0.017530543729662895\n",
      "acc for Lsat= 0.06379800339539846 \n",
      "acc for Psat= 0.09374314165777628 \n",
      "acc for optim= 0.1462532247106234\n",
      "Epoch:893/1000\n",
      "Loss on train= 0.009687998332083225\n",
      "Loss on test= 0.017777545377612114\n",
      "acc for Lsat= 0.06474498766991826 \n",
      "acc for Psat= 0.0924789220922523 \n",
      "acc for optim= 0.14616691983408397\n",
      "Epoch:894/1000\n",
      "Loss on train= 0.009246556088328362\n",
      "Loss on test= 0.016603566706180573\n",
      "acc for Lsat= 0.06336427869068252 \n",
      "acc for Psat= 0.0949331608083513 \n",
      "acc for optim= 0.14463833719491956\n",
      "Epoch:895/1000\n",
      "Loss on train= 0.010035702027380466\n",
      "Loss on test= 0.018168993294239044\n",
      "acc for Lsat= 0.06275751259591844 \n",
      "acc for Psat= 0.09305327269766064 \n",
      "acc for optim= 0.1468093756172392\n",
      "Epoch:896/1000\n",
      "Loss on train= 0.009743202477693558\n",
      "Loss on test= 0.016655325889587402\n",
      "acc for Lsat= 0.06460201500190628 \n",
      "acc for Psat= 0.09300900581810209 \n",
      "acc for optim= 0.14594374563958915\n",
      "Epoch:897/1000\n",
      "Loss on train= 0.009689412079751492\n",
      "Loss on test= 0.01720309816300869\n",
      "acc for Lsat= 0.06457900885078643 \n",
      "acc for Psat= 0.09629727568891312 \n",
      "acc for optim= 0.14698218173450892\n",
      "Epoch:898/1000\n",
      "Loss on train= 0.00979608204215765\n",
      "Loss on test= 0.017298340797424316\n",
      "acc for Lsat= 0.06605425526698432 \n",
      "acc for Psat= 0.0999835204746988 \n",
      "acc for optim= 0.14756166934967044\n",
      "Epoch:899/1000\n",
      "Loss on train= 0.00992195587605238\n",
      "Loss on test= 0.016821982339024544\n",
      "acc for Lsat= 0.0644286334514618 \n",
      "acc for Psat= 0.09392181601789264 \n",
      "acc for optim= 0.14615702794657814\n",
      "Epoch:900/1000\n",
      "Loss on train= 0.009946329519152641\n",
      "Loss on test= 0.017889102920889854\n",
      "acc for Lsat= 0.06332449283864763 \n",
      "acc for Psat= 0.09253224366241032 \n",
      "acc for optim= 0.14502493407991196\n",
      "Epoch:901/1000\n",
      "Loss on train= 0.009726297110319138\n",
      "Loss on test= 0.016474701464176178\n",
      "acc for Lsat= 0.06242991106377707 \n",
      "acc for Psat= 0.09042130998439259 \n",
      "acc for optim= 0.1476386351717843\n",
      "Epoch:902/1000\n",
      "Loss on train= 0.01018314529210329\n",
      "Loss on test= 0.0172449741512537\n",
      "acc for Lsat= 0.06459449215067757 \n",
      "acc for Psat= 0.09681969036658605 \n",
      "acc for optim= 0.14638072003920874\n",
      "Epoch:903/1000\n",
      "Loss on train= 0.009471914730966091\n",
      "Loss on test= 0.01702272705733776\n",
      "acc for Lsat= 0.06544627762503094 \n",
      "acc for Psat= 0.09420929004748663 \n",
      "acc for optim= 0.1447925857371754\n",
      "Epoch:904/1000\n",
      "Loss on train= 0.00961889699101448\n",
      "Loss on test= 0.016102023422718048\n",
      "acc for Lsat= 0.06214689902133413 \n",
      "acc for Psat= 0.090645582228899 \n",
      "acc for optim= 0.1452192174063789\n",
      "Epoch:905/1000\n",
      "Loss on train= 0.009725553914904594\n",
      "Loss on test= 0.017249440774321556\n",
      "acc for Lsat= 0.0631897466050254 \n",
      "acc for Psat= 0.09230779293510649 \n",
      "acc for optim= 0.14809117317199705\n",
      "Epoch:906/1000\n",
      "Loss on train= 0.009573361836373806\n",
      "Loss on test= 0.01711166836321354\n",
      "acc for Lsat= 0.06839398741722108 \n",
      "acc for Psat= 0.1026203536325031 \n",
      "acc for optim= 0.1471365307768186\n",
      "Epoch:907/1000\n",
      "Loss on train= 0.009563961066305637\n",
      "Loss on test= 0.01701853983104229\n",
      "acc for Lsat= 0.07071203291416167 \n",
      "acc for Psat= 0.10166797637939454 \n",
      "acc for optim= 0.14450714124573605\n",
      "Epoch:908/1000\n",
      "Loss on train= 0.009407044388353825\n",
      "Loss on test= 0.01633438840508461\n",
      "acc for Lsat= 0.06360684442851278 \n",
      "acc for Psat= 0.0937594238254759 \n",
      "acc for optim= 0.14390844073560505\n",
      "Epoch:909/1000\n",
      "Loss on train= 0.009845403023064137\n",
      "Loss on test= 0.01713567227125168\n",
      "acc for Lsat= 0.06541200992133882 \n",
      "acc for Psat= 0.09141162816021178 \n",
      "acc for optim= 0.1473452594545152\n",
      "Epoch:910/1000\n",
      "Loss on train= 0.009813866578042507\n",
      "Loss on test= 0.018271099776029587\n",
      "acc for Lsat= 0.061909457627269945 \n",
      "acc for Psat= 0.09239315978354877 \n",
      "acc for optim= 0.14648920098940532\n",
      "Epoch:911/1000\n",
      "Loss on train= 0.00960553903132677\n",
      "Loss on test= 0.01708727329969406\n",
      "acc for Lsat= 0.06094731705056297 \n",
      "acc for Psat= 0.09017653548055225 \n",
      "acc for optim= 0.14687846716907293\n",
      "Epoch:912/1000\n",
      "Loss on train= 0.009931525215506554\n",
      "Loss on test= 0.017828498035669327\n",
      "acc for Lsat= 0.06275202333927155 \n",
      "acc for Psat= 0.09248236633009381 \n",
      "acc for optim= 0.14385913527674143\n",
      "Epoch:913/1000\n",
      "Loss on train= 0.009187421761453152\n",
      "Loss on test= 0.016633274033665657\n",
      "acc for Lsat= 0.0657918700741397 \n",
      "acc for Psat= 0.09340391415688727 \n",
      "acc for optim= 0.14553694592581856\n",
      "Epoch:914/1000\n",
      "Loss on train= 0.009692409075796604\n",
      "Loss on test= 0.01776883751153946\n",
      "acc for Lsat= 0.0646905796395408 \n",
      "acc for Psat= 0.09423821303579541 \n",
      "acc for optim= 0.14769826597637598\n",
      "Epoch:915/1000\n",
      "Loss on train= 0.009780237451195717\n",
      "Loss on test= 0.016711007803678513\n",
      "acc for Lsat= 0.06501650156246291 \n",
      "acc for Psat= 0.09211247182554667 \n",
      "acc for optim= 0.14616107311513687\n",
      "Epoch:916/1000\n",
      "Loss on train= 0.009407241828739643\n",
      "Loss on test= 0.0168205164372921\n",
      "acc for Lsat= 0.06263437544306119 \n",
      "acc for Psat= 0.09006877442200979 \n",
      "acc for optim= 0.1478886216878891\n",
      "Epoch:917/1000\n",
      "Loss on train= 0.009840666316449642\n",
      "Loss on test= 0.016357919201254845\n",
      "acc for Lsat= 0.061750611828433145 \n",
      "acc for Psat= 0.09093538017736541 \n",
      "acc for optim= 0.14686501423517864\n",
      "Epoch:918/1000\n",
      "Loss on train= 0.009145073592662811\n",
      "Loss on test= 0.018181411549448967\n",
      "acc for Lsat= 0.06501916779412163 \n",
      "acc for Psat= 0.09329150385326808 \n",
      "acc for optim= 0.14763490839136972\n",
      "Epoch:919/1000\n",
      "Loss on train= 0.009909829124808311\n",
      "Loss on test= 0.017173659056425095\n",
      "acc for Lsat= 0.06408478857742415 \n",
      "acc for Psat= 0.09287517335679797 \n",
      "acc for optim= 0.14495793879032137\n",
      "Epoch:920/1000\n",
      "Loss on train= 0.009644079953432083\n",
      "Loss on test= 0.01722077652812004\n",
      "acc for Lsat= 0.06241446882486343 \n",
      "acc for Psat= 0.09371546871132322 \n",
      "acc for optim= 0.1458409604099062\n",
      "Epoch:921/1000\n",
      "Loss on train= 0.00971272960305214\n",
      "Loss on test= 0.016248159110546112\n",
      "acc for Lsat= 0.06285988878872659 \n",
      "acc for Psat= 0.09490500042835871 \n",
      "acc for optim= 0.14625485026174123\n",
      "Epoch:922/1000\n",
      "Loss on train= 0.010041547007858753\n",
      "Loss on test= 0.016713008284568787\n",
      "acc for Lsat= 0.06530428959263695 \n",
      "acc for Psat= 0.09354933218823541 \n",
      "acc for optim= 0.14561198337210549\n",
      "Epoch:923/1000\n",
      "Loss on train= 0.008998239412903786\n",
      "Loss on test= 0.017719479277729988\n",
      "acc for Lsat= 0.0671583773361312 \n",
      "acc for Psat= 0.09362715259194374 \n",
      "acc for optim= 0.14678934216499326\n",
      "Epoch:924/1000\n",
      "Loss on train= 0.00980533566325903\n",
      "Loss on test= 0.0178222618997097\n",
      "acc for Lsat= 0.06403131352530585 \n",
      "acc for Psat= 0.09719451500309839 \n",
      "acc for optim= 0.147162775364187\n",
      "Epoch:925/1000\n",
      "Loss on train= 0.009552150033414364\n",
      "Loss on test= 0.017643308266997337\n",
      "acc for Lsat= 0.06432430404755803 \n",
      "acc for Psat= 0.09696705473793878 \n",
      "acc for optim= 0.14589792324437034\n",
      "Epoch:926/1000\n",
      "Loss on train= 0.009697755798697472\n",
      "Loss on test= 0.017619524151086807\n",
      "acc for Lsat= 0.06530546231402291 \n",
      "acc for Psat= 0.09248618417316012 \n",
      "acc for optim= 0.14573249253961773\n",
      "Epoch:927/1000\n",
      "Loss on train= 0.00971045158803463\n",
      "Loss on test= 0.016466040164232254\n",
      "acc for Lsat= 0.06660745955175823 \n",
      "acc for Psat= 0.09525795082251232 \n",
      "acc for optim= 0.14415080696344373\n",
      "Epoch:928/1000\n",
      "Loss on train= 0.010168180800974369\n",
      "Loss on test= 0.016462398692965508\n",
      "acc for Lsat= 0.0653357908129692 \n",
      "acc for Psat= 0.10029536518785688 \n",
      "acc for optim= 0.14670726921823288\n",
      "Epoch:929/1000\n",
      "Loss on train= 0.009494217112660408\n",
      "Loss on test= 0.016692407429218292\n",
      "acc for Lsat= 0.0634918643368615 \n",
      "acc for Psat= 0.09317165613174438 \n",
      "acc for optim= 0.14613784667518404\n",
      "Epoch:930/1000\n",
      "Loss on train= 0.009422710165381432\n",
      "Loss on test= 0.015745580196380615\n",
      "acc for Lsat= 0.06315738517377112 \n",
      "acc for Psat= 0.09052770833174388 \n",
      "acc for optim= 0.14503490494357213\n",
      "Epoch:931/1000\n",
      "Loss on train= 0.009545903652906418\n",
      "Loss on test= 0.01623130589723587\n",
      "acc for Lsat= 0.0625666340192159 \n",
      "acc for Psat= 0.08987323972913955 \n",
      "acc for optim= 0.1462283290094799\n",
      "Epoch:932/1000\n",
      "Loss on train= 0.009364435449242592\n",
      "Loss on test= 0.017899906262755394\n",
      "acc for Lsat= 0.06549753894408544 \n",
      "acc for Psat= 0.09409429927666983 \n",
      "acc for optim= 0.1474883526563644\n",
      "Epoch:933/1000\n",
      "Loss on train= 0.009476829320192337\n",
      "Loss on test= 0.016832228749990463\n",
      "acc for Lsat= 0.061965081592400856 \n",
      "acc for Psat= 0.09441720644632975 \n",
      "acc for optim= 0.1452044645945231\n",
      "Epoch:934/1000\n",
      "Loss on train= 0.00958275143057108\n",
      "Loss on test= 0.017067665234208107\n",
      "acc for Lsat= 0.06470801399813758 \n",
      "acc for Psat= 0.09143609967496662 \n",
      "acc for optim= 0.14723193446795146\n",
      "Epoch:935/1000\n",
      "Loss on train= 0.009728902950882912\n",
      "Loss on test= 0.016995664685964584\n",
      "acc for Lsat= 0.06354935533470578 \n",
      "acc for Psat= 0.08938365479310353 \n",
      "acc for optim= 0.14607775774266984\n",
      "Epoch:936/1000\n",
      "Loss on train= 0.009645689278841019\n",
      "Loss on test= 0.017074571922421455\n",
      "acc for Lsat= 0.06319148639837902 \n",
      "acc for Psat= 0.09260321425067053 \n",
      "acc for optim= 0.14637638860278657\n",
      "Epoch:937/1000\n",
      "Loss on train= 0.009714890271425247\n",
      "Loss on test= 0.016313612461090088\n",
      "acc for Lsat= 0.06389217293924757 \n",
      "acc for Psat= 0.09203752345508999 \n",
      "acc for optim= 0.14738481955395807\n",
      "Epoch:938/1000\n",
      "Loss on train= 0.00973778311163187\n",
      "Loss on test= 0.016249556094408035\n",
      "acc for Lsat= 0.06389887432257334 \n",
      "acc for Psat= 0.09493619518147575 \n",
      "acc for optim= 0.14509437382221224\n",
      "Epoch:939/1000\n",
      "Loss on train= 0.009489878080785275\n",
      "Loss on test= 0.01690533570945263\n",
      "acc for Lsat= 0.06438927832576963 \n",
      "acc for Psat= 0.09425938079754512 \n",
      "acc for optim= 0.14590444390972454\n",
      "Epoch:940/1000\n",
      "Loss on train= 0.009443766437470913\n",
      "Loss on test= 0.016873590648174286\n",
      "acc for Lsat= 0.0654383963180913 \n",
      "acc for Psat= 0.089779109093878 \n",
      "acc for optim= 0.14687418739000957\n",
      "Epoch:941/1000\n",
      "Loss on train= 0.009496851824223995\n",
      "Loss on test= 0.017854992300271988\n",
      "acc for Lsat= 0.06424038542641534 \n",
      "acc for Psat= 0.09236482489440176 \n",
      "acc for optim= 0.14506390690803528\n",
      "Epoch:942/1000\n",
      "Loss on train= 0.009774116799235344\n",
      "Loss on test= 0.018435297533869743\n",
      "acc for Lsat= 0.0632053520116541 \n",
      "acc for Psat= 0.09245617389678956 \n",
      "acc for optim= 0.14785969985855946\n",
      "Epoch:943/1000\n",
      "Loss on train= 0.009728885255753994\n",
      "Loss on test= 0.01782127283513546\n",
      "acc for Lsat= 0.06156075298786164 \n",
      "acc for Psat= 0.09296629122561878 \n",
      "acc for optim= 0.1453183624479506\n",
      "Epoch:944/1000\n",
      "Loss on train= 0.009489777497947216\n",
      "Loss on test= 0.017611367627978325\n",
      "acc for Lsat= 0.06262882086965772 \n",
      "acc for Psat= 0.09108279198408127 \n",
      "acc for optim= 0.14580390916930305\n",
      "Epoch:945/1000\n",
      "Loss on train= 0.009556081146001816\n",
      "Loss on test= 0.017248447984457016\n",
      "acc for Lsat= 0.06267847749922009 \n",
      "acc for Psat= 0.09686145832141242 \n",
      "acc for optim= 0.1454814263516002\n",
      "Epoch:946/1000\n",
      "Loss on train= 0.009687003679573536\n",
      "Loss on test= 0.017349768429994583\n",
      "acc for Lsat= 0.062195297578970576 \n",
      "acc for Psat= 0.09220033172104093 \n",
      "acc for optim= 0.14463657115896542\n",
      "Epoch:947/1000\n",
      "Loss on train= 0.009729008190333843\n",
      "Loss on test= 0.01619628258049488\n",
      "acc for Lsat= 0.06307460682259665 \n",
      "acc for Psat= 0.0909815048178037 \n",
      "acc for optim= 0.14962891257471508\n",
      "Epoch:948/1000\n",
      "Loss on train= 0.009627409279346466\n",
      "Loss on test= 0.01710948720574379\n",
      "acc for Lsat= 0.06628752946853639 \n",
      "acc for Psat= 0.09420868257681529 \n",
      "acc for optim= 0.14393146203623874\n",
      "Epoch:949/1000\n",
      "Loss on train= 0.00959987472742796\n",
      "Loss on test= 0.016435077413916588\n",
      "acc for Lsat= 0.06251625816027323 \n",
      "acc for Psat= 0.09083458615673913 \n",
      "acc for optim= 0.14532775779565177\n",
      "Epoch:950/1000\n",
      "Loss on train= 0.009421799331903458\n",
      "Loss on test= 0.015452721156179905\n",
      "acc for Lsat= 0.06384306632810169 \n",
      "acc for Psat= 0.09080108114414745 \n",
      "acc for optim= 0.14663239667812986\n",
      "Epoch:951/1000\n",
      "Loss on train= 0.009275602176785469\n",
      "Loss on test= 0.017390094697475433\n",
      "acc for Lsat= 0.06347744348976347 \n",
      "acc for Psat= 0.09330989950233035 \n",
      "acc for optim= 0.14535532726181877\n",
      "Epoch:952/1000\n",
      "Loss on train= 0.009667380712926388\n",
      "Loss on test= 0.016699831932783127\n",
      "acc for Lsat= 0.06267333560519749 \n",
      "acc for Psat= 0.09173497325844235 \n",
      "acc for optim= 0.14629425605138144\n",
      "Epoch:953/1000\n",
      "Loss on train= 0.009766303934156895\n",
      "Loss on test= 0.017361313104629517\n",
      "acc for Lsat= 0.0639682905541526 \n",
      "acc for Psat= 0.08851065403885312 \n",
      "acc for optim= 0.146681296494272\n",
      "Epoch:954/1000\n",
      "Loss on train= 0.009415263310074806\n",
      "Loss on test= 0.016155757009983063\n",
      "acc for Lsat= 0.06234189752075408 \n",
      "acc for Psat= 0.09235765909155208 \n",
      "acc for optim= 0.14606303804450566\n",
      "Epoch:955/1000\n",
      "Loss on train= 0.009649483487010002\n",
      "Loss on test= 0.01711449958384037\n",
      "acc for Lsat= 0.06292365011241702 \n",
      "acc for Psat= 0.09411218563715616 \n",
      "acc for optim= 0.14618913630644484\n",
      "Epoch:956/1000\n",
      "Loss on train= 0.00951374601572752\n",
      "Loss on test= 0.017059775069355965\n",
      "acc for Lsat= 0.06170103996992112 \n",
      "acc for Psat= 0.09352321310175789 \n",
      "acc for optim= 0.14613941080040405\n",
      "Epoch:957/1000\n",
      "Loss on train= 0.00961126759648323\n",
      "Loss on test= 0.017310569062829018\n",
      "acc for Lsat= 0.06315356178416145 \n",
      "acc for Psat= 0.09447657184468375 \n",
      "acc for optim= 0.1451624708043204\n",
      "Epoch:958/1000\n",
      "Loss on train= 0.0097568454220891\n",
      "Loss on test= 0.017862387001514435\n",
      "acc for Lsat= 0.06410424593422148 \n",
      "acc for Psat= 0.0929000003470315 \n",
      "acc for optim= 0.14563462204403346\n",
      "Epoch:959/1000\n",
      "Loss on train= 0.009307676926255226\n",
      "Loss on test= 0.017142735421657562\n",
      "acc for Lsat= 0.06563171694676082 \n",
      "acc for Psat= 0.0924195859167311 \n",
      "acc for optim= 0.1450188448031743\n",
      "Epoch:960/1000\n",
      "Loss on train= 0.009533672593533993\n",
      "Loss on test= 0.01742200180888176\n",
      "acc for Lsat= 0.06386020721660719 \n",
      "acc for Psat= 0.0982692685392168 \n",
      "acc for optim= 0.14492006351550418\n",
      "Epoch:961/1000\n",
      "Loss on train= 0.0092318682000041\n",
      "Loss on test= 0.016881251707673073\n",
      "acc for Lsat= 0.06475164410140778 \n",
      "acc for Psat= 0.09191049428449738 \n",
      "acc for optim= 0.1452417925000191\n",
      "Epoch:962/1000\n",
      "Loss on train= 0.009694863110780716\n",
      "Loss on test= 0.017130481079220772\n",
      "acc for Lsat= 0.06488127675321367 \n",
      "acc for Psat= 0.0956595124469863 \n",
      "acc for optim= 0.14820182191001047\n",
      "Epoch:963/1000\n",
      "Loss on train= 0.009693469852209091\n",
      "Loss on test= 0.017610758543014526\n",
      "acc for Lsat= 0.06527022454473708 \n",
      "acc for Psat= 0.09623097860150867 \n",
      "acc for optim= 0.14437624646557704\n",
      "Epoch:964/1000\n",
      "Loss on train= 0.00921161100268364\n",
      "Loss on test= 0.017275184392929077\n",
      "acc for Lsat= 0.06497785333130095 \n",
      "acc for Psat= 0.09249167053235902 \n",
      "acc for optim= 0.14471975962320965\n",
      "Epoch:965/1000\n",
      "Loss on train= 0.009504674933850765\n",
      "Loss on test= 0.017478208988904953\n",
      "acc for Lsat= 0.0631071494685279 \n",
      "acc for Psat= 0.09127898497713939 \n",
      "acc for optim= 0.1455578154987759\n",
      "Epoch:966/1000\n",
      "Loss on train= 0.010052978061139584\n",
      "Loss on test= 0.016903942450881004\n",
      "acc for Lsat= 0.06552153428395589 \n",
      "acc for Psat= 0.09220767501327726 \n",
      "acc for optim= 0.14492733478546146\n",
      "Epoch:967/1000\n",
      "Loss on train= 0.009552482515573502\n",
      "Loss on test= 0.016546033322811127\n",
      "acc for Lsat= 0.06446581648455725 \n",
      "acc for Psat= 0.09331646213928858 \n",
      "acc for optim= 0.1455612579981486\n",
      "Epoch:968/1000\n",
      "Loss on train= 0.009324397891759872\n",
      "Loss on test= 0.017971055582165718\n",
      "acc for Lsat= 0.062191110683812036 \n",
      "acc for Psat= 0.08881141212251452 \n",
      "acc for optim= 0.1462461661961344\n",
      "Epoch:969/1000\n",
      "Loss on train= 0.009494300000369549\n",
      "Loss on test= 0.01666489988565445\n",
      "acc for Lsat= 0.06546564764446683 \n",
      "acc for Psat= 0.09103612866666583 \n",
      "acc for optim= 0.14528597924444409\n",
      "Epoch:970/1000\n",
      "Loss on train= 0.009913239628076553\n",
      "Loss on test= 0.017136838287115097\n",
      "acc for Lsat= 0.06642738646931118 \n",
      "acc for Psat= 0.0935478437278006 \n",
      "acc for optim= 0.14578991399870977\n",
      "Epoch:971/1000\n",
      "Loss on train= 0.00939318910241127\n",
      "Loss on test= 0.016843311488628387\n",
      "acc for Lsat= 0.06313356575038696 \n",
      "acc for Psat= 0.09516778687636057 \n",
      "acc for optim= 0.14627358035908808\n",
      "Epoch:972/1000\n",
      "Loss on train= 0.009539145044982433\n",
      "Loss on test= 0.01681019738316536\n",
      "acc for Lsat= 0.06282870223124822 \n",
      "acc for Psat= 0.0941281982594066 \n",
      "acc for optim= 0.14434926956892014\n",
      "Epoch:973/1000\n",
      "Loss on train= 0.009730752557516098\n",
      "Loss on test= 0.016255680471658707\n",
      "acc for Lsat= 0.06257093126575151 \n",
      "acc for Psat= 0.0903656272424592 \n",
      "acc for optim= 0.1449837062093947\n",
      "Epoch:974/1000\n",
      "Loss on train= 0.009371021762490273\n",
      "Loss on test= 0.016047589480876923\n",
      "acc for Lsat= 0.062261275202035914 \n",
      "acc for Psat= 0.09350759751266904 \n",
      "acc for optim= 0.14669843630658258\n",
      "Epoch:975/1000\n",
      "Loss on train= 0.00937590841203928\n",
      "Loss on test= 0.017014026641845703\n",
      "acc for Lsat= 0.06250055813127094 \n",
      "acc for Psat= 0.08956970738040076 \n",
      "acc for optim= 0.14614574743641748\n",
      "Epoch:976/1000\n",
      "Loss on train= 0.009983870200812817\n",
      "Loss on test= 0.016891038045287132\n",
      "acc for Lsat= 0.06335759527153438 \n",
      "acc for Psat= 0.09719874660174051 \n",
      "acc for optim= 0.14811707552936343\n",
      "Epoch:977/1000\n",
      "Loss on train= 0.009258311241865158\n",
      "Loss on test= 0.01602538675069809\n",
      "acc for Lsat= 0.06315264453490575 \n",
      "acc for Psat= 0.09074047994282511 \n",
      "acc for optim= 0.14501791281832588\n",
      "Epoch:978/1000\n",
      "Loss on train= 0.009440465830266476\n",
      "Loss on test= 0.01612548530101776\n",
      "acc for Lsat= 0.06965347131093341 \n",
      "acc for Psat= 0.10048238138357798 \n",
      "acc for optim= 0.14682260586155785\n",
      "Epoch:979/1000\n",
      "Loss on train= 0.009456069208681583\n",
      "Loss on test= 0.017897486686706543\n",
      "acc for Lsat= 0.06632700595590804 \n",
      "acc for Psat= 0.09319536156124537 \n",
      "acc for optim= 0.14745212511883843\n",
      "Epoch:980/1000\n",
      "Loss on train= 0.009110577404499054\n",
      "Loss on test= 0.017032328993082047\n",
      "acc for Lsat= 0.0649614368047979 \n",
      "acc for Psat= 0.09093397657076516 \n",
      "acc for optim= 0.14655422899458143\n",
      "Epoch:981/1000\n",
      "Loss on train= 0.009586279280483723\n",
      "Loss on test= 0.01707080565392971\n",
      "acc for Lsat= 0.06137784040636486 \n",
      "acc for Psat= 0.09278791596492131 \n",
      "acc for optim= 0.14611069510380426\n",
      "Epoch:982/1000\n",
      "Loss on train= 0.009555462747812271\n",
      "Loss on test= 0.016237668693065643\n",
      "acc for Lsat= 0.06468139141798018 \n",
      "acc for Psat= 0.09919052703513039 \n",
      "acc for optim= 0.14667620079384905\n",
      "Epoch:983/1000\n",
      "Loss on train= 0.00933939591050148\n",
      "Loss on test= 0.017821721732616425\n",
      "acc for Lsat= 0.0651046473118994 \n",
      "acc for Psat= 0.09736470265520944 \n",
      "acc for optim= 0.14712110161781314\n",
      "Epoch:984/1000\n",
      "Loss on train= 0.009659036993980408\n",
      "Loss on test= 0.017273426055908203\n",
      "acc for Lsat= 0.06224254435963101 \n",
      "acc for Psat= 0.09342288722594579 \n",
      "acc for optim= 0.14795475055774052\n",
      "Epoch:985/1000\n",
      "Loss on train= 0.009098004549741745\n",
      "Loss on test= 0.01606416329741478\n",
      "acc for Lsat= 0.0651594771279229 \n",
      "acc for Psat= 0.09163406358824835 \n",
      "acc for optim= 0.14542806877030268\n",
      "Epoch:986/1000\n",
      "Loss on train= 0.009391690604388714\n",
      "Loss on test= 0.01878727227449417\n",
      "acc for Lsat= 0.06228445925646358 \n",
      "acc for Psat= 0.0924388724896643 \n",
      "acc for optim= 0.14756311277548473\n",
      "Epoch:987/1000\n",
      "Loss on train= 0.009051795117557049\n",
      "Loss on test= 0.017747309058904648\n",
      "acc for Lsat= 0.06252861403756672 \n",
      "acc for Psat= 0.09153900576962366 \n",
      "acc for optim= 0.14637411369217768\n",
      "Epoch:988/1000\n",
      "Loss on train= 0.009248263202607632\n",
      "Loss on test= 0.016959428787231445\n",
      "acc for Lsat= 0.0627658862206671 \n",
      "acc for Psat= 0.0905509673886829 \n",
      "acc for optim= 0.14631706492768393\n",
      "Epoch:989/1000\n",
      "Loss on train= 0.0093887560069561\n",
      "Loss on test= 0.017510291188955307\n",
      "acc for Lsat= 0.06623453050851821 \n",
      "acc for Psat= 0.10873990688059065 \n",
      "acc for optim= 0.14472673535346983\n",
      "Epoch:990/1000\n",
      "Loss on train= 0.00927072111517191\n",
      "Loss on test= 0.016880307346582413\n",
      "acc for Lsat= 0.06274695611662334 \n",
      "acc for Psat= 0.09304045604334939 \n",
      "acc for optim= 0.14819946446352536\n",
      "Epoch:991/1000\n",
      "Loss on train= 0.009457395412027836\n",
      "Loss on test= 0.016862506046891212\n",
      "acc for Lsat= 0.06445122775104312 \n",
      "acc for Psat= 0.09088407572772769 \n",
      "acc for optim= 0.14534949461619062\n",
      "Epoch:992/1000\n",
      "Loss on train= 0.009211624972522259\n",
      "Loss on test= 0.017202487215399742\n",
      "acc for Lsat= 0.0640899024075932 \n",
      "acc for Psat= 0.08908067577415044 \n",
      "acc for optim= 0.14704985635148157\n",
      "Epoch:993/1000\n",
      "Loss on train= 0.009260066784918308\n",
      "Loss on test= 0.01726609654724598\n",
      "acc for Lsat= 0.0629993552962939 \n",
      "acc for Psat= 0.09152876204914516 \n",
      "acc for optim= 0.1489655637078815\n",
      "Epoch:994/1000\n",
      "Loss on train= 0.009580792859196663\n",
      "Loss on test= 0.016287345439195633\n",
      "acc for Lsat= 0.06609935528702207 \n",
      "acc for Psat= 0.09439483806490898 \n",
      "acc for optim= 0.14736322429445053\n",
      "Epoch:995/1000\n",
      "Loss on train= 0.009148010984063148\n",
      "Loss on test= 0.016568714752793312\n",
      "acc for Lsat= 0.06251649608214696 \n",
      "acc for Psat= 0.09274964241517916 \n",
      "acc for optim= 0.14799054794841343\n",
      "Epoch:996/1000\n",
      "Loss on train= 0.009126108139753342\n",
      "Loss on test= 0.01822255738079548\n",
      "acc for Lsat= 0.0644713196489546 \n",
      "acc for Psat= 0.09206826330886947 \n",
      "acc for optim= 0.1453179864419831\n",
      "Epoch:997/1000\n",
      "Loss on train= 0.00931390468031168\n",
      "Loss on test= 0.016761429607868195\n",
      "acc for Lsat= 0.061345814830727034 \n",
      "acc for Psat= 0.09083449136879712 \n",
      "acc for optim= 0.1486337342196041\n",
      "Epoch:998/1000\n",
      "Loss on train= 0.009424158371984959\n",
      "Loss on test= 0.017907187342643738\n",
      "acc for Lsat= 0.06251804075307316 \n",
      "acc for Psat= 0.09248551660113864 \n",
      "acc for optim= 0.14605032073126897\n",
      "Epoch:999/1000\n",
      "Loss on train= 0.009222891181707382\n",
      "Loss on test= 0.01735765114426613\n",
      "acc for Lsat= 0.06612422698073916 \n",
      "acc for Psat= 0.0940733150475555 \n",
      "acc for optim= 0.14614248772462213\n",
      "Epoch:1000/1000\n",
      "Loss on train= 0.009172063320875168\n",
      "Loss on test= 0.017311399802565575\n",
      "acc for Lsat= 0.06753504508071476 \n",
      "acc for Psat= 0.09208499573998981 \n",
      "acc for optim= 0.14581121852000553\n",
      "Fold 3\n",
      "Epoch:1/1000\n",
      "Loss on train= 0.20916999876499176\n",
      "Loss on test= 0.10308241099119186\n",
      "acc for Lsat= 0.4054900585570269 \n",
      "acc for Psat= 0.5012390083736844 \n",
      "acc for optim= 0.1801133029990726\n",
      "Epoch:2/1000\n",
      "Loss on train= 0.08814908564090729\n",
      "Loss on test= 0.061878468841314316\n",
      "acc for Lsat= 0.27661383483144975 \n",
      "acc for Psat= 0.4536885059542126 \n",
      "acc for optim= 0.1530184285508262\n",
      "Epoch:3/1000\n",
      "Loss on train= 0.06806832551956177\n",
      "Loss on test= 0.0568566657602787\n",
      "acc for Lsat= 0.26914249592357214 \n",
      "acc for Psat= 0.3795048023263613 \n",
      "acc for optim= 0.15092881959345605\n",
      "Epoch:4/1000\n",
      "Loss on train= 0.0632285475730896\n",
      "Loss on test= 0.059316448867321014\n",
      "acc for Lsat= 0.27040939446952605 \n",
      "acc for Psat= 0.38279894524150426 \n",
      "acc for optim= 0.15455912123951648\n",
      "Epoch:5/1000\n",
      "Loss on train= 0.0588977113366127\n",
      "Loss on test= 0.052637726068496704\n",
      "acc for Lsat= 0.260548683669832 \n",
      "acc for Psat= 0.33568973822726156 \n",
      "acc for optim= 0.1528727534744475\n",
      "Epoch:6/1000\n",
      "Loss on train= 0.05648303031921387\n",
      "Loss on test= 0.04750820994377136\n",
      "acc for Lsat= 0.26870684888627794 \n",
      "acc for Psat= 0.3531770925968885 \n",
      "acc for optim= 0.1518727128704389\n",
      "Epoch:7/1000\n",
      "Loss on train= 0.0539894662797451\n",
      "Loss on test= 0.04432567209005356\n",
      "acc for Lsat= 0.2506349275509516 \n",
      "acc for Psat= 0.32211182260264953 \n",
      "acc for optim= 0.15144300949242384\n",
      "Epoch:8/1000\n",
      "Loss on train= 0.053876351565122604\n",
      "Loss on test= 0.0492427796125412\n",
      "acc for Lsat= 0.28752146859963734 \n",
      "acc for Psat= 0.37603484607405124 \n",
      "acc for optim= 0.15738745563560064\n",
      "Epoch:9/1000\n",
      "Loss on train= 0.051196835935115814\n",
      "Loss on test= 0.0460994616150856\n",
      "acc for Lsat= 0.25793819361262854 \n",
      "acc for Psat= 0.2991556886583567 \n",
      "acc for optim= 0.15579848997294907\n",
      "Epoch:10/1000\n",
      "Loss on train= 0.04966292157769203\n",
      "Loss on test= 0.048287197947502136\n",
      "acc for Lsat= 0.3116630461480882 \n",
      "acc for Psat= 0.338309604757362 \n",
      "acc for optim= 0.1519376269645161\n",
      "Epoch:11/1000\n",
      "Loss on train= 0.05126955360174179\n",
      "Loss on test= 0.04624287039041519\n",
      "acc for Lsat= 0.2643570979436239 \n",
      "acc for Psat= 0.30881467908620835 \n",
      "acc for optim= 0.16434136197591823\n",
      "Epoch:12/1000\n",
      "Loss on train= 0.05089925229549408\n",
      "Loss on test= 0.044536132365465164\n",
      "acc for Lsat= 0.2540810356537502 \n",
      "acc for Psat= 0.3203014102247026 \n",
      "acc for optim= 0.1561028727226787\n",
      "Epoch:13/1000\n",
      "Loss on train= 0.047795578837394714\n",
      "Loss on test= 0.041255805641412735\n",
      "acc for Lsat= 0.25050632821189034 \n",
      "acc for Psat= 0.31008958667516706 \n",
      "acc for optim= 0.16607000062035188\n",
      "Epoch:14/1000\n",
      "Loss on train= 0.04622899368405342\n",
      "Loss on test= 0.04425787925720215\n",
      "acc for Lsat= 0.25146536860201096 \n",
      "acc for Psat= 0.2919676168925232 \n",
      "acc for optim= 0.16179520034541686\n",
      "Epoch:15/1000\n",
      "Loss on train= 0.0454435795545578\n",
      "Loss on test= 0.03853241726756096\n",
      "acc for Lsat= 0.2693544692463345 \n",
      "acc for Psat= 0.31275520175695415 \n",
      "acc for optim= 0.15339702177378867\n",
      "Epoch:16/1000\n",
      "Loss on train= 0.049963440746068954\n",
      "Loss on test= 0.03988545760512352\n",
      "acc for Lsat= 0.24504872759183247 \n",
      "acc for Psat= 0.2993414872222477 \n",
      "acc for optim= 0.16408991239861487\n",
      "Epoch:17/1000\n",
      "Loss on train= 0.04540976509451866\n",
      "Loss on test= 0.0340358130633831\n",
      "acc for Lsat= 0.24776814977327988 \n",
      "acc for Psat= 0.28302992044223685 \n",
      "acc for optim= 0.15341962410344015\n",
      "Epoch:18/1000\n",
      "Loss on train= 0.04555131122469902\n",
      "Loss on test= 0.0358472615480423\n",
      "acc for Lsat= 0.24638342029518556 \n",
      "acc for Psat= 0.28723500354422465 \n",
      "acc for optim= 0.15430453121662138\n",
      "Epoch:19/1000\n",
      "Loss on train= 0.04326619952917099\n",
      "Loss on test= 0.03871374949812889\n",
      "acc for Lsat= 0.2645022610823313 \n",
      "acc for Psat= 0.31791606114970317 \n",
      "acc for optim= 0.15474402523703049\n",
      "Epoch:20/1000\n",
      "Loss on train= 0.04272651672363281\n",
      "Loss on test= 0.039643798023462296\n",
      "acc for Lsat= 0.2563234822617637 \n",
      "acc for Psat= 0.2824498626920912 \n",
      "acc for optim= 0.15408352580335408\n",
      "Epoch:21/1000\n",
      "Loss on train= 0.04268675670027733\n",
      "Loss on test= 0.03797883912920952\n",
      "acc for Lsat= 0.24689601163069408 \n",
      "acc for Psat= 0.32615869409508175 \n",
      "acc for optim= 0.16683069401317172\n",
      "Epoch:22/1000\n",
      "Loss on train= 0.04329931363463402\n",
      "Loss on test= 0.03559918701648712\n",
      "acc for Lsat= 0.2703099759088622 \n",
      "acc for Psat= 0.27334232777357104 \n",
      "acc for optim= 0.1528758179810312\n",
      "Epoch:23/1000\n",
      "Loss on train= 0.04309150204062462\n",
      "Loss on test= 0.036192793399095535\n",
      "acc for Lsat= 0.2565692967838712 \n",
      "acc for Psat= 0.2656166339086161 \n",
      "acc for optim= 0.155563967095481\n",
      "Epoch:24/1000\n",
      "Loss on train= 0.04182424768805504\n",
      "Loss on test= 0.03647610917687416\n",
      "acc for Lsat= 0.2531842187047004 \n",
      "acc for Psat= 0.27254133125146224 \n",
      "acc for optim= 0.15684437817997401\n",
      "Epoch:25/1000\n",
      "Loss on train= 0.04197154939174652\n",
      "Loss on test= 0.03785952925682068\n",
      "acc for Lsat= 0.2520325515005324 \n",
      "acc for Psat= 0.34756879475381636 \n",
      "acc for optim= 0.15497843937741385\n",
      "Epoch:26/1000\n",
      "Loss on train= 0.04127253592014313\n",
      "Loss on test= 0.03314598277211189\n",
      "acc for Lsat= 0.23391743898391726 \n",
      "acc for Psat= 0.3319696575403214 \n",
      "acc for optim= 0.15325283921427194\n",
      "Epoch:27/1000\n",
      "Loss on train= 0.042063888162374496\n",
      "Loss on test= 0.034136705100536346\n",
      "acc for Lsat= 0.2873041982452074 \n",
      "acc for Psat= 0.26037322282791137 \n",
      "acc for optim= 0.1521538164880541\n",
      "Epoch:28/1000\n",
      "Loss on train= 0.041222307831048965\n",
      "Loss on test= 0.03536674380302429\n",
      "acc for Lsat= 0.2645170080992911 \n",
      "acc for Psat= 0.26138261357943215 \n",
      "acc for optim= 0.15176130632559456\n",
      "Epoch:29/1000\n",
      "Loss on train= 0.04078473895788193\n",
      "Loss on test= 0.03457567095756531\n",
      "acc for Lsat= 0.24619933068752287 \n",
      "acc for Psat= 0.31083971195750776 \n",
      "acc for optim= 0.1534523334768083\n",
      "Epoch:30/1000\n",
      "Loss on train= 0.03865739330649376\n",
      "Loss on test= 0.038987983018159866\n",
      "acc for Lsat= 0.3207399080197016 \n",
      "acc for Psat= 0.2563595245074895 \n",
      "acc for optim= 0.15124598037865425\n",
      "Epoch:31/1000\n",
      "Loss on train= 0.04029861465096474\n",
      "Loss on test= 0.03782368823885918\n",
      "acc for Lsat= 0.262276628613472 \n",
      "acc for Psat= 0.33259594937165576 \n",
      "acc for optim= 0.15212022463480632\n",
      "Epoch:32/1000\n",
      "Loss on train= 0.039185646921396255\n",
      "Loss on test= 0.03326092287898064\n",
      "acc for Lsat= 0.2533903767665227 \n",
      "acc for Psat= 0.24915579540861982 \n",
      "acc for optim= 0.14935196836789447\n",
      "Epoch:33/1000\n",
      "Loss on train= 0.039664819836616516\n",
      "Loss on test= 0.037282783538103104\n",
      "acc for Lsat= 0.2931247558858659 \n",
      "acc for Psat= 0.30318095982074733 \n",
      "acc for optim= 0.15263619505696824\n",
      "Epoch:34/1000\n",
      "Loss on train= 0.0400543175637722\n",
      "Loss on test= 0.03267653286457062\n",
      "acc for Lsat= 0.22794428136613634 \n",
      "acc for Psat= 0.24957529066337475 \n",
      "acc for optim= 0.15559611005915536\n",
      "Epoch:35/1000\n",
      "Loss on train= 0.038420841097831726\n",
      "Loss on test= 0.032005421817302704\n",
      "acc for Lsat= 0.22522682249546047 \n",
      "acc for Psat= 0.25588989324039885 \n",
      "acc for optim= 0.14993685699171488\n",
      "Epoch:36/1000\n",
      "Loss on train= 0.036395248025655746\n",
      "Loss on test= 0.03059225343167782\n",
      "acc for Lsat= 0.22618883003791168 \n",
      "acc for Psat= 0.2685881432559755 \n",
      "acc for optim= 0.14958142373296948\n",
      "Epoch:37/1000\n",
      "Loss on train= 0.03652903810143471\n",
      "Loss on test= 0.032983891665935516\n",
      "acc for Lsat= 0.24447569515970016 \n",
      "acc for Psat= 0.23429706298435735 \n",
      "acc for optim= 0.1566806975338194\n",
      "Epoch:38/1000\n",
      "Loss on train= 0.039045289158821106\n",
      "Loss on test= 0.033608242869377136\n",
      "acc for Lsat= 0.2214976062377294 \n",
      "acc for Psat= 0.28319072508149673 \n",
      "acc for optim= 0.1711640977197223\n",
      "Epoch:39/1000\n",
      "Loss on train= 0.03567667305469513\n",
      "Loss on test= 0.03171635791659355\n",
      "acc for Lsat= 0.23087852795918787 \n",
      "acc for Psat= 0.28146476944287613 \n",
      "acc for optim= 0.15092616826295857\n",
      "Epoch:40/1000\n",
      "Loss on train= 0.03758980706334114\n",
      "Loss on test= 0.03305858373641968\n",
      "acc for Lsat= 0.26115923921267187 \n",
      "acc for Psat= 0.22924583007891972 \n",
      "acc for optim= 0.1522494085133076\n",
      "Epoch:41/1000\n",
      "Loss on train= 0.039390578866004944\n",
      "Loss on test= 0.030593493953347206\n",
      "acc for Lsat= 0.23026542713244755 \n",
      "acc for Psat= 0.23818651868237387 \n",
      "acc for optim= 0.14940947509474226\n",
      "Epoch:42/1000\n",
      "Loss on train= 0.034877508878707886\n",
      "Loss on test= 0.03070438653230667\n",
      "acc for Lsat= 0.24019816418488815 \n",
      "acc for Psat= 0.22538945140937963 \n",
      "acc for optim= 0.15058562639686798\n",
      "Epoch:43/1000\n",
      "Loss on train= 0.03472962602972984\n",
      "Loss on test= 0.030540019273757935\n",
      "acc for Lsat= 0.22743600590361485 \n",
      "acc for Psat= 0.2555106557077832 \n",
      "acc for optim= 0.147441137333711\n",
      "Epoch:44/1000\n",
      "Loss on train= 0.03540908172726631\n",
      "Loss on test= 0.03219497203826904\n",
      "acc for Lsat= 0.24619350400235918 \n",
      "acc for Psat= 0.33933849136034644 \n",
      "acc for optim= 0.1529657029443317\n",
      "Epoch:45/1000\n",
      "Loss on train= 0.03546600043773651\n",
      "Loss on test= 0.030117355287075043\n",
      "acc for Lsat= 0.23220420711570317 \n",
      "acc for Psat= 0.23017993801169925 \n",
      "acc for optim= 0.14899164189894995\n",
      "Epoch:46/1000\n",
      "Loss on train= 0.03437325730919838\n",
      "Loss on test= 0.029756691306829453\n",
      "acc for Lsat= 0.22969133771128122 \n",
      "acc for Psat= 0.2373302901784579 \n",
      "acc for optim= 0.1524676472776466\n",
      "Epoch:47/1000\n",
      "Loss on train= 0.035321637988090515\n",
      "Loss on test= 0.02967984415590763\n",
      "acc for Lsat= 0.2239947878652149 \n",
      "acc for Psat= 0.2320401746365759 \n",
      "acc for optim= 0.14764189364181624\n",
      "Epoch:48/1000\n",
      "Loss on train= 0.034067537635564804\n",
      "Loss on test= 0.03463764116168022\n",
      "acc for Lsat= 0.24841215411822004 \n",
      "acc for Psat= 0.3028893023729324 \n",
      "acc for optim= 0.16618105471134187\n",
      "Epoch:49/1000\n",
      "Loss on train= 0.03582921624183655\n",
      "Loss on test= 0.02989487536251545\n",
      "acc for Lsat= 0.22844196508328116 \n",
      "acc for Psat= 0.23575326767232685 \n",
      "acc for optim= 0.1523970142006874\n",
      "Epoch:50/1000\n",
      "Loss on train= 0.03493600711226463\n",
      "Loss on test= 0.029171667993068695\n",
      "acc for Lsat= 0.2265061994393667 \n",
      "acc for Psat= 0.23352720273865588 \n",
      "acc for optim= 0.15519561105304294\n",
      "Epoch:51/1000\n",
      "Loss on train= 0.03449000418186188\n",
      "Loss on test= 0.02855107933282852\n",
      "acc for Lsat= 0.2286918451388677 \n",
      "acc for Psat= 0.24411527183320786 \n",
      "acc for optim= 0.146815211739805\n",
      "Epoch:52/1000\n",
      "Loss on train= 0.034038130193948746\n",
      "Loss on test= 0.03038785234093666\n",
      "acc for Lsat= 0.23097492853800455 \n",
      "acc for Psat= 0.2201315397189723 \n",
      "acc for optim= 0.1501787684030003\n",
      "Epoch:53/1000\n",
      "Loss on train= 0.03466401621699333\n",
      "Loss on test= 0.02934895269572735\n",
      "acc for Lsat= 0.22178624106778042 \n",
      "acc for Psat= 0.22364816980229485 \n",
      "acc for optim= 0.1523002314898703\n",
      "Epoch:54/1000\n",
      "Loss on train= 0.03274114429950714\n",
      "Loss on test= 0.027939459308981895\n",
      "acc for Lsat= 0.21406330102019835 \n",
      "acc for Psat= 0.21678712339036993 \n",
      "acc for optim= 0.1480453982949257\n",
      "Epoch:55/1000\n",
      "Loss on train= 0.03211953118443489\n",
      "Loss on test= 0.03376404941082001\n",
      "acc for Lsat= 0.24585479299227395 \n",
      "acc for Psat= 0.3172639979256524 \n",
      "acc for optim= 0.1504397249884076\n",
      "Epoch:56/1000\n",
      "Loss on train= 0.03622990474104881\n",
      "Loss on test= 0.03039168007671833\n",
      "acc for Lsat= 0.24735454668601353 \n",
      "acc for Psat= 0.2160329230129719 \n",
      "acc for optim= 0.15476336975892385\n",
      "Epoch:57/1000\n",
      "Loss on train= 0.03363248333334923\n",
      "Loss on test= 0.02707812748849392\n",
      "acc for Lsat= 0.23048212462001372 \n",
      "acc for Psat= 0.21603267076942656 \n",
      "acc for optim= 0.15067967101931573\n",
      "Epoch:58/1000\n",
      "Loss on train= 0.0326215960085392\n",
      "Loss on test= 0.027226522564888\n",
      "acc for Lsat= 0.2217241535584132 \n",
      "acc for Psat= 0.24478940135902827 \n",
      "acc for optim= 0.14696750624312294\n",
      "Epoch:59/1000\n",
      "Loss on train= 0.03210670128464699\n",
      "Loss on test= 0.029027799144387245\n",
      "acc for Lsat= 0.2198948631683986 \n",
      "acc for Psat= 0.2333417572908931 \n",
      "acc for optim= 0.14795676585700776\n",
      "Epoch:60/1000\n",
      "Loss on train= 0.03221532329916954\n",
      "Loss on test= 0.02674308978021145\n",
      "acc for Lsat= 0.21579479790396164 \n",
      "acc for Psat= 0.22070453349086974 \n",
      "acc for optim= 0.14893810351689654\n",
      "Epoch:61/1000\n",
      "Loss on train= 0.034554656594991684\n",
      "Loss on test= 0.031073102727532387\n",
      "acc for Lsat= 0.23089863989088266 \n",
      "acc for Psat= 0.21170365462700524 \n",
      "acc for optim= 0.15753914254407092\n",
      "Epoch:62/1000\n",
      "Loss on train= 0.03372262045741081\n",
      "Loss on test= 0.029052291065454483\n",
      "acc for Lsat= 0.23425397707356352 \n",
      "acc for Psat= 0.22862253884474437 \n",
      "acc for optim= 0.14957623249954646\n",
      "Epoch:63/1000\n",
      "Loss on train= 0.03254517912864685\n",
      "Loss on test= 0.026912661269307137\n",
      "acc for Lsat= 0.23591566648748186 \n",
      "acc for Psat= 0.24255713621775307 \n",
      "acc for optim= 0.15421025405327476\n",
      "Epoch:64/1000\n",
      "Loss on train= 0.032335925847291946\n",
      "Loss on test= 0.025273898616433144\n",
      "acc for Lsat= 0.2163689798778958 \n",
      "acc for Psat= 0.24424591296248968 \n",
      "acc for optim= 0.14730463127295174\n",
      "Epoch:65/1000\n",
      "Loss on train= 0.031979482620954514\n",
      "Loss on test= 0.027394752949476242\n",
      "acc for Lsat= 0.2165435825785001 \n",
      "acc for Psat= 0.22136874149243038 \n",
      "acc for optim= 0.14901082151465941\n",
      "Epoch:66/1000\n",
      "Loss on train= 0.03182303532958031\n",
      "Loss on test= 0.027104603126645088\n",
      "acc for Lsat= 0.21821260187360977 \n",
      "acc for Psat= 0.24738014870219763 \n",
      "acc for optim= 0.14563163204325572\n",
      "Epoch:67/1000\n",
      "Loss on train= 0.033020224422216415\n",
      "Loss on test= 0.02790147438645363\n",
      "acc for Lsat= 0.21350272049506502 \n",
      "acc for Psat= 0.21057096934980818 \n",
      "acc for optim= 0.1508689762817489\n",
      "Epoch:68/1000\n",
      "Loss on train= 0.03169439733028412\n",
      "Loss on test= 0.029737817123532295\n",
      "acc for Lsat= 0.2509539494394428 \n",
      "acc for Psat= 0.20083982133203082 \n",
      "acc for optim= 0.15223425908221136\n",
      "Epoch:69/1000\n",
      "Loss on train= 0.031675878912210464\n",
      "Loss on test= 0.02568409964442253\n",
      "acc for Lsat= 0.21180291291740205 \n",
      "acc for Psat= 0.22197010119756067 \n",
      "acc for optim= 0.14894551386435825\n",
      "Epoch:70/1000\n",
      "Loss on train= 0.030666129663586617\n",
      "Loss on test= 0.026924869045615196\n",
      "acc for Lsat= 0.22601565884219277 \n",
      "acc for Psat= 0.2570391959614224 \n",
      "acc for optim= 0.14823514454894593\n",
      "Epoch:71/1000\n",
      "Loss on train= 0.03238823637366295\n",
      "Loss on test= 0.02808244712650776\n",
      "acc for Lsat= 0.2075997430417273 \n",
      "acc for Psat= 0.20694833662774828 \n",
      "acc for optim= 0.14968850620918805\n",
      "Epoch:72/1000\n",
      "Loss on train= 0.029860615730285645\n",
      "Loss on test= 0.02520260587334633\n",
      "acc for Lsat= 0.21382328304979537 \n",
      "acc for Psat= 0.21403686437341904 \n",
      "acc for optim= 0.15189124792814254\n",
      "Epoch:73/1000\n",
      "Loss on train= 0.03181961178779602\n",
      "Loss on test= 0.02645907737314701\n",
      "acc for Lsat= 0.21986511184109586 \n",
      "acc for Psat= 0.2023283157083723 \n",
      "acc for optim= 0.14698846042156222\n",
      "Epoch:74/1000\n",
      "Loss on train= 0.03031742572784424\n",
      "Loss on test= 0.0250744316726923\n",
      "acc for Lsat= 0.20486667553583782 \n",
      "acc for Psat= 0.21311596714788014 \n",
      "acc for optim= 0.1478404912683699\n",
      "Epoch:75/1000\n",
      "Loss on train= 0.030141906812787056\n",
      "Loss on test= 0.028559528291225433\n",
      "acc for Lsat= 0.21380786001682284 \n",
      "acc for Psat= 0.2700456976890564 \n",
      "acc for optim= 0.14883521149555842\n",
      "Epoch:76/1000\n",
      "Loss on train= 0.029988324269652367\n",
      "Loss on test= 0.025756031274795532\n",
      "acc for Lsat= 0.19911852180957798 \n",
      "acc for Psat= 0.2233581357532077 \n",
      "acc for optim= 0.15043660534752737\n",
      "Epoch:77/1000\n",
      "Loss on train= 0.028738945722579956\n",
      "Loss on test= 0.025607118383049965\n",
      "acc for Lsat= 0.19448703808916937 \n",
      "acc for Psat= 0.1957836134566201 \n",
      "acc for optim= 0.14996091590987312\n",
      "Epoch:78/1000\n",
      "Loss on train= 0.02963314764201641\n",
      "Loss on test= 0.026152441278100014\n",
      "acc for Lsat= 0.2061234484116236 \n",
      "acc for Psat= 0.22426528731981912 \n",
      "acc for optim= 0.14886864688661364\n",
      "Epoch:79/1000\n",
      "Loss on train= 0.030369563028216362\n",
      "Loss on test= 0.025873519480228424\n",
      "acc for Lsat= 0.198275923066669 \n",
      "acc for Psat= 0.19376833654112283 \n",
      "acc for optim= 0.1512132677767012\n",
      "Epoch:80/1000\n",
      "Loss on train= 0.030454551801085472\n",
      "Loss on test= 0.025382263585925102\n",
      "acc for Lsat= 0.19975278808010943 \n",
      "acc for Psat= 0.1951757286985715 \n",
      "acc for optim= 0.14783291121323905\n",
      "Epoch:81/1000\n",
      "Loss on train= 0.029899030923843384\n",
      "Loss on test= 0.023668453097343445\n",
      "acc for Lsat= 0.20202882720364465 \n",
      "acc for Psat= 0.1998916616042455 \n",
      "acc for optim= 0.14786787033081056\n",
      "Epoch:82/1000\n",
      "Loss on train= 0.02929576486349106\n",
      "Loss on test= 0.026984473690390587\n",
      "acc for Lsat= 0.21127025683720907 \n",
      "acc for Psat= 0.21385196480486127 \n",
      "acc for optim= 0.1472626070181529\n",
      "Epoch:83/1000\n",
      "Loss on train= 0.028957881033420563\n",
      "Loss on test= 0.024348918348550797\n",
      "acc for Lsat= 0.1991279027528233 \n",
      "acc for Psat= 0.18437829928265675 \n",
      "acc for optim= 0.148296194192436\n",
      "Epoch:84/1000\n",
      "Loss on train= 0.028607523068785667\n",
      "Loss on test= 0.023060645908117294\n",
      "acc for Lsat= 0.19897437029414708 \n",
      "acc for Psat= 0.2027230991257562 \n",
      "acc for optim= 0.14817666734258333\n",
      "Epoch:85/1000\n",
      "Loss on train= 0.02961607463657856\n",
      "Loss on test= 0.02971731685101986\n",
      "acc for Lsat= 0.22878758112589517 \n",
      "acc for Psat= 0.23555214901765187 \n",
      "acc for optim= 0.15120067149400712\n",
      "Epoch:86/1000\n",
      "Loss on train= 0.03062111884355545\n",
      "Loss on test= 0.024711523205041885\n",
      "acc for Lsat= 0.20510138869285582 \n",
      "acc for Psat= 0.18173897448513243 \n",
      "acc for optim= 0.15604081518120239\n",
      "Epoch:87/1000\n",
      "Loss on train= 0.028572766110301018\n",
      "Loss on test= 0.025995412841439247\n",
      "acc for Lsat= 0.1877817413873143 \n",
      "acc for Psat= 0.1894375615649753 \n",
      "acc for optim= 0.14866377015908555\n",
      "Epoch:88/1000\n",
      "Loss on train= 0.02802509255707264\n",
      "Loss on test= 0.025930343195796013\n",
      "acc for Lsat= 0.190088898771339 \n",
      "acc for Psat= 0.22898591425683762 \n",
      "acc for optim= 0.14928090771039326\n",
      "Epoch:89/1000\n",
      "Loss on train= 0.028314627707004547\n",
      "Loss on test= 0.02953970432281494\n",
      "acc for Lsat= 0.23161385787857902 \n",
      "acc for Psat= 0.2178296837541792 \n",
      "acc for optim= 0.14909365839428368\n",
      "Epoch:90/1000\n",
      "Loss on train= 0.02783147059381008\n",
      "Loss on test= 0.025084352120757103\n",
      "acc for Lsat= 0.1787815140353309 \n",
      "acc for Psat= 0.19770924746990204 \n",
      "acc for optim= 0.15237704002194932\n",
      "Epoch:91/1000\n",
      "Loss on train= 0.02683202736079693\n",
      "Loss on test= 0.022788573056459427\n",
      "acc for Lsat= 0.16282132930225798 \n",
      "acc for Psat= 0.17640320956706995 \n",
      "acc for optim= 0.14492757005823986\n",
      "Epoch:92/1000\n",
      "Loss on train= 0.0267815999686718\n",
      "Loss on test= 0.022782912477850914\n",
      "acc for Lsat= 0.15847353173626794 \n",
      "acc for Psat= 0.1666296703947915 \n",
      "acc for optim= 0.1461042058136728\n",
      "Epoch:93/1000\n",
      "Loss on train= 0.02672421932220459\n",
      "Loss on test= 0.024844981729984283\n",
      "acc for Lsat= 0.1739641964435577 \n",
      "acc for Psat= 0.20676464604006875 \n",
      "acc for optim= 0.15805033379130892\n",
      "Epoch:94/1000\n",
      "Loss on train= 0.026477215811610222\n",
      "Loss on test= 0.0231705904006958\n",
      "acc for Lsat= 0.1593994355863995 \n",
      "acc for Psat= 0.16545957360002733 \n",
      "acc for optim= 0.1510855375892586\n",
      "Epoch:95/1000\n",
      "Loss on train= 0.02588547393679619\n",
      "Loss on test= 0.025403983891010284\n",
      "acc for Lsat= 0.15418863991896314 \n",
      "acc for Psat= 0.17386372023158597 \n",
      "acc for optim= 0.14736159957117506\n",
      "Epoch:96/1000\n",
      "Loss on train= 0.025208203122019768\n",
      "Loss on test= 0.02574123628437519\n",
      "acc for Lsat= 0.1537418458196852 \n",
      "acc for Psat= 0.158947524925073 \n",
      "acc for optim= 0.14968276255660587\n",
      "Epoch:97/1000\n",
      "Loss on train= 0.026363633573055267\n",
      "Loss on test= 0.022091545164585114\n",
      "acc for Lsat= 0.15653117845455805 \n",
      "acc for Psat= 0.1649782250324885 \n",
      "acc for optim= 0.15468221306800845\n",
      "Epoch:98/1000\n",
      "Loss on train= 0.026184212416410446\n",
      "Loss on test= 0.022690095007419586\n",
      "acc for Lsat= 0.153125990430514 \n",
      "acc for Psat= 0.15717670884397295 \n",
      "acc for optim= 0.14785383426480825\n",
      "Epoch:99/1000\n",
      "Loss on train= 0.024964213371276855\n",
      "Loss on test= 0.021223576739430428\n",
      "acc for Lsat= 0.1417792759007878 \n",
      "acc for Psat= 0.15426576137542722 \n",
      "acc for optim= 0.14416043625937566\n",
      "Epoch:100/1000\n",
      "Loss on train= 0.025466782972216606\n",
      "Loss on test= 0.023965971544384956\n",
      "acc for Lsat= 0.13987289932039046 \n",
      "acc for Psat= 0.16467781431145137 \n",
      "acc for optim= 0.14942160265313256\n",
      "Epoch:101/1000\n",
      "Loss on train= 0.02498452551662922\n",
      "Loss on test= 0.02060629427433014\n",
      "acc for Lsat= 0.14729044917556974 \n",
      "acc for Psat= 0.1585059705707762 \n",
      "acc for optim= 0.14705132726165984\n",
      "Epoch:102/1000\n",
      "Loss on train= 0.02525344304740429\n",
      "Loss on test= 0.023710057139396667\n",
      "acc for Lsat= 0.18991064959102205 \n",
      "acc for Psat= 0.18543673919306858 \n",
      "acc for optim= 0.1457292440864775\n",
      "Epoch:103/1000\n",
      "Loss on train= 0.02494065836071968\n",
      "Loss on test= 0.022174671292304993\n",
      "acc for Lsat= 0.15711170004473793 \n",
      "acc for Psat= 0.19678069618013172 \n",
      "acc for optim= 0.1442239193452729\n",
      "Epoch:104/1000\n",
      "Loss on train= 0.024601714685559273\n",
      "Loss on test= 0.020943425595760345\n",
      "acc for Lsat= 0.13085779017872282 \n",
      "acc for Psat= 0.14655277099874287 \n",
      "acc for optim= 0.14801168777048587\n",
      "Epoch:105/1000\n",
      "Loss on train= 0.02414301224052906\n",
      "Loss on test= 0.021719565615057945\n",
      "acc for Lsat= 0.16226432886388567 \n",
      "acc for Psat= 0.17778916425175137 \n",
      "acc for optim= 0.1443126935097906\n",
      "Epoch:106/1000\n",
      "Loss on train= 0.024394748732447624\n",
      "Loss on test= 0.020670071244239807\n",
      "acc for Lsat= 0.1289308425452974 \n",
      "acc for Psat= 0.142383421295219 \n",
      "acc for optim= 0.1426550312174691\n",
      "Epoch:107/1000\n",
      "Loss on train= 0.023933131247758865\n",
      "Loss on test= 0.022929545491933823\n",
      "acc for Lsat= 0.17094725370407104 \n",
      "acc for Psat= 0.24508393870459663 \n",
      "acc for optim= 0.1447115349272887\n",
      "Epoch:108/1000\n",
      "Loss on train= 0.025406496599316597\n",
      "Loss on test= 0.018834518268704414\n",
      "acc for Lsat= 0.12350605643457839 \n",
      "acc for Psat= 0.14862194524870975 \n",
      "acc for optim= 0.14449081354671053\n",
      "Epoch:109/1000\n",
      "Loss on train= 0.024944473057985306\n",
      "Loss on test= 0.02005205862224102\n",
      "acc for Lsat= 0.1357488613989618 \n",
      "acc for Psat= 0.14720950159761642 \n",
      "acc for optim= 0.14450680613517763\n",
      "Epoch:110/1000\n",
      "Loss on train= 0.023851437494158745\n",
      "Loss on test= 0.02255638875067234\n",
      "acc for Lsat= 0.1179596644308832 \n",
      "acc for Psat= 0.14221685147947738 \n",
      "acc for optim= 0.14432919190989602\n",
      "Epoch:111/1000\n",
      "Loss on train= 0.022910337895154953\n",
      "Loss on test= 0.019685937091708183\n",
      "acc for Lsat= 0.12486994746658536 \n",
      "acc for Psat= 0.15535740918583338 \n",
      "acc for optim= 0.14790476601984767\n",
      "Epoch:112/1000\n",
      "Loss on train= 0.024410782381892204\n",
      "Loss on test= 0.02342800982296467\n",
      "acc for Lsat= 0.14292138665914533 \n",
      "acc for Psat= 0.14773848487271202 \n",
      "acc for optim= 0.14333302146858637\n",
      "Epoch:113/1000\n",
      "Loss on train= 0.0238074641674757\n",
      "Loss on test= 0.021251587197184563\n",
      "acc for Lsat= 0.13499359422259863 \n",
      "acc for Psat= 0.17414528230826062 \n",
      "acc for optim= 0.1450298266278373\n",
      "Epoch:114/1000\n",
      "Loss on train= 0.023545807227492332\n",
      "Loss on test= 0.019490191712975502\n",
      "acc for Lsat= 0.11356144663360385 \n",
      "acc for Psat= 0.13644913335641226 \n",
      "acc for optim= 0.1440637521445751\n",
      "Epoch:115/1000\n",
      "Loss on train= 0.02268143743276596\n",
      "Loss on test= 0.01905147172510624\n",
      "acc for Lsat= 0.1172669851117664 \n",
      "acc for Psat= 0.14265303644869065 \n",
      "acc for optim= 0.14217305473155448\n",
      "Epoch:116/1000\n",
      "Loss on train= 0.022287797182798386\n",
      "Loss on test= 0.020229825749993324\n",
      "acc for Lsat= 0.12010009868277442 \n",
      "acc for Psat= 0.14396178589926825 \n",
      "acc for optim= 0.14624377787113194\n",
      "Epoch:117/1000\n",
      "Loss on train= 0.022321002557873726\n",
      "Loss on test= 0.019017599523067474\n",
      "acc for Lsat= 0.13162205037143496 \n",
      "acc for Psat= 0.14439542897873453 \n",
      "acc for optim= 0.14498494325412645\n",
      "Epoch:118/1000\n",
      "Loss on train= 0.022649014368653297\n",
      "Loss on test= 0.018879836425185204\n",
      "acc for Lsat= 0.11987212548653285 \n",
      "acc for Psat= 0.17814228468471102 \n",
      "acc for optim= 0.14197034239768985\n",
      "Epoch:119/1000\n",
      "Loss on train= 0.02234840579330921\n",
      "Loss on test= 0.01934162899851799\n",
      "acc for Lsat= 0.1134210619661543 \n",
      "acc for Psat= 0.13572710222668116 \n",
      "acc for optim= 0.1480344007412593\n",
      "Epoch:120/1000\n",
      "Loss on train= 0.022016985341906548\n",
      "Loss on test= 0.019990194588899612\n",
      "acc for Lsat= 0.12926219618982737 \n",
      "acc for Psat= 0.15166164603498247 \n",
      "acc for optim= 0.1437197316851881\n",
      "Epoch:121/1000\n",
      "Loss on train= 0.02218436636030674\n",
      "Loss on test= 0.01778605580329895\n",
      "acc for Lsat= 0.10812939835919273 \n",
      "acc for Psat= 0.13080082999335396 \n",
      "acc for optim= 0.1457080605957243\n",
      "Epoch:122/1000\n",
      "Loss on train= 0.022598441690206528\n",
      "Loss on test= 0.019784945994615555\n",
      "acc for Lsat= 0.11260343806611167 \n",
      "acc for Psat= 0.14474969506263735 \n",
      "acc for optim= 0.14425976889000997\n",
      "Epoch:123/1000\n",
      "Loss on train= 0.022121334448456764\n",
      "Loss on test= 0.019493650645017624\n",
      "acc for Lsat= 0.11355499260955387 \n",
      "acc for Psat= 0.12909518049822913 \n",
      "acc for optim= 0.1408319483200709\n",
      "Epoch:124/1000\n",
      "Loss on train= 0.021777288988232613\n",
      "Loss on test= 0.018882689997553825\n",
      "acc for Lsat= 0.10497904833820132 \n",
      "acc for Psat= 0.13639819969733558 \n",
      "acc for optim= 0.14447650619679026\n",
      "Epoch:125/1000\n",
      "Loss on train= 0.022322271019220352\n",
      "Loss on test= 0.019614584743976593\n",
      "acc for Lsat= 0.10484333684047063 \n",
      "acc for Psat= 0.13079267144203188 \n",
      "acc for optim= 0.14665810747279062\n",
      "Epoch:126/1000\n",
      "Loss on train= 0.02200421877205372\n",
      "Loss on test= 0.01829461008310318\n",
      "acc for Lsat= 0.115982901222176 \n",
      "acc for Psat= 0.2061710947089725 \n",
      "acc for optim= 0.1447855307824082\n",
      "Epoch:127/1000\n",
      "Loss on train= 0.021640794351696968\n",
      "Loss on test= 0.019263239577412605\n",
      "acc for Lsat= 0.11044902321365145 \n",
      "acc for Psat= 0.13537845065196355 \n",
      "acc for optim= 0.14006965764694745\n",
      "Epoch:128/1000\n",
      "Loss on train= 0.02094685100018978\n",
      "Loss on test= 0.01814437471330166\n",
      "acc for Lsat= 0.10421681354443232 \n",
      "acc for Psat= 0.1260980053908295 \n",
      "acc for optim= 0.14285033659802543\n",
      "Epoch:129/1000\n",
      "Loss on train= 0.021663477644324303\n",
      "Loss on test= 0.018800396472215652\n",
      "acc for Lsat= 0.09948588328229056 \n",
      "acc for Psat= 0.13960860967636105 \n",
      "acc for optim= 0.1420506293574969\n",
      "Epoch:130/1000\n",
      "Loss on train= 0.021178752183914185\n",
      "Loss on test= 0.019437626004219055\n",
      "acc for Lsat= 0.11535390979713865 \n",
      "acc for Psat= 0.16527241932021247 \n",
      "acc for optim= 0.1434594141112434\n",
      "Epoch:131/1000\n",
      "Loss on train= 0.021961813792586327\n",
      "Loss on test= 0.019047606736421585\n",
      "acc for Lsat= 0.10558107213841544 \n",
      "acc for Psat= 0.14430102474159667 \n",
      "acc for optim= 0.1415147266454167\n",
      "Epoch:132/1000\n",
      "Loss on train= 0.021287642419338226\n",
      "Loss on test= 0.019389042630791664\n",
      "acc for Lsat= 0.12248929407861499 \n",
      "acc for Psat= 0.16516586608356898 \n",
      "acc for optim= 0.1439657160805331\n",
      "Epoch:133/1000\n",
      "Loss on train= 0.021516112610697746\n",
      "Loss on test= 0.018448226153850555\n",
      "acc for Lsat= 0.10926806761158837 \n",
      "acc for Psat= 0.1440043287144767 \n",
      "acc for optim= 0.13931062221527102\n",
      "Epoch:134/1000\n",
      "Loss on train= 0.021143976598978043\n",
      "Loss on test= 0.018255621194839478\n",
      "acc for Lsat= 0.10506486180755828 \n",
      "acc for Psat= 0.12760842740535738 \n",
      "acc for optim= 0.14271527561876504\n",
      "Epoch:135/1000\n",
      "Loss on train= 0.020461199805140495\n",
      "Loss on test= 0.018473152071237564\n",
      "acc for Lsat= 0.0982015925976965 \n",
      "acc for Psat= 0.14024612969822353 \n",
      "acc for optim= 0.1397202982670731\n",
      "Epoch:136/1000\n",
      "Loss on train= 0.019964411854743958\n",
      "Loss on test= 0.017757004126906395\n",
      "acc for Lsat= 0.09556845608684753 \n",
      "acc for Psat= 0.12258642166852951 \n",
      "acc for optim= 0.14123900433381398\n",
      "Epoch:137/1000\n",
      "Loss on train= 0.021248681470751762\n",
      "Loss on test= 0.01736844703555107\n",
      "acc for Lsat= 0.0979866380492846 \n",
      "acc for Psat= 0.12389863282442093 \n",
      "acc for optim= 0.1421649619936943\n",
      "Epoch:138/1000\n",
      "Loss on train= 0.020720239728689194\n",
      "Loss on test= 0.018406379967927933\n",
      "acc for Lsat= 0.1032258419526948 \n",
      "acc for Psat= 0.12341414690017699 \n",
      "acc for optim= 0.13898824850718178\n",
      "Epoch:139/1000\n",
      "Loss on train= 0.02069304697215557\n",
      "Loss on test= 0.01912878453731537\n",
      "acc for Lsat= 0.1281394339270062 \n",
      "acc for Psat= 0.13850147773822147 \n",
      "acc for optim= 0.14313169502549702\n",
      "Epoch:140/1000\n",
      "Loss on train= 0.020302530378103256\n",
      "Loss on test= 0.019338134676218033\n",
      "acc for Lsat= 0.11282718115382723 \n",
      "acc for Psat= 0.12448818269703124 \n",
      "acc for optim= 0.13896217863592839\n",
      "Epoch:141/1000\n",
      "Loss on train= 0.02056601457297802\n",
      "Loss on test= 0.018389709293842316\n",
      "acc for Lsat= 0.13643677681684496 \n",
      "acc for Psat= 0.1658100578520033 \n",
      "acc for optim= 0.1407054740521643\n",
      "Epoch:142/1000\n",
      "Loss on train= 0.020035380497574806\n",
      "Loss on test= 0.017657747492194176\n",
      "acc for Lsat= 0.10364347191320525 \n",
      "acc for Psat= 0.1310548178023762 \n",
      "acc for optim= 0.13691938163505657\n",
      "Epoch:143/1000\n",
      "Loss on train= 0.02096429653465748\n",
      "Loss on test= 0.01776837557554245\n",
      "acc for Lsat= 0.11518966721163856 \n",
      "acc for Psat= 0.1541963219642639 \n",
      "acc for optim= 0.1410624103413688\n",
      "Epoch:144/1000\n",
      "Loss on train= 0.020213617011904716\n",
      "Loss on test= 0.017280716449022293\n",
      "acc for Lsat= 0.09791459689537685 \n",
      "acc for Psat= 0.11428835143645605 \n",
      "acc for optim= 0.13835817451278368\n",
      "Epoch:145/1000\n",
      "Loss on train= 0.020389752462506294\n",
      "Loss on test= 0.018572229892015457\n",
      "acc for Lsat= 0.12554016146394942 \n",
      "acc for Psat= 0.1542394992378023 \n",
      "acc for optim= 0.14472805509964623\n",
      "Epoch:146/1000\n",
      "Loss on train= 0.02049591951072216\n",
      "Loss on test= 0.01796935684978962\n",
      "acc for Lsat= 0.09868122223350735 \n",
      "acc for Psat= 0.12244392136732737 \n",
      "acc for optim= 0.14164814775188764\n",
      "Epoch:147/1000\n",
      "Loss on train= 0.020165428519248962\n",
      "Loss on test= 0.016292760148644447\n",
      "acc for Lsat= 0.09228051039907668 \n",
      "acc for Psat= 0.13291692866219418 \n",
      "acc for optim= 0.13934803091817433\n",
      "Epoch:148/1000\n",
      "Loss on train= 0.0203778725117445\n",
      "Loss on test= 0.018450060859322548\n",
      "acc for Lsat= 0.10345773531330954 \n",
      "acc for Psat= 0.12771059655480912 \n",
      "acc for optim= 0.13839401875933013\n",
      "Epoch:149/1000\n",
      "Loss on train= 0.02024797350168228\n",
      "Loss on test= 0.017463404685258865\n",
      "acc for Lsat= 0.108945970568392 \n",
      "acc for Psat= 0.13969019452730813 \n",
      "acc for optim= 0.13741360836558872\n",
      "Epoch:150/1000\n",
      "Loss on train= 0.020382821559906006\n",
      "Loss on test= 0.01710248365998268\n",
      "acc for Lsat= 0.0970262936419911 \n",
      "acc for Psat= 0.11433000018199285 \n",
      "acc for optim= 0.14315148509211012\n",
      "Epoch:151/1000\n",
      "Loss on train= 0.01952536217868328\n",
      "Loss on test= 0.01701621524989605\n",
      "acc for Lsat= 0.10440355853901967 \n",
      "acc for Psat= 0.1453058246109221 \n",
      "acc for optim= 0.13730899608797498\n",
      "Epoch:152/1000\n",
      "Loss on train= 0.019636251032352448\n",
      "Loss on test= 0.018093064427375793\n",
      "acc for Lsat= 0.1006855873597993 \n",
      "acc for Psat= 0.135215139720175 \n",
      "acc for optim= 0.14177284141381583\n",
      "Epoch:153/1000\n",
      "Loss on train= 0.01972712017595768\n",
      "Loss on test= 0.018150249496102333\n",
      "acc for Lsat= 0.12052356385522417 \n",
      "acc for Psat= 0.12088541736205421 \n",
      "acc for optim= 0.1357418976724148\n",
      "Epoch:154/1000\n",
      "Loss on train= 0.020543474704027176\n",
      "Loss on test= 0.017712794244289398\n",
      "acc for Lsat= 0.11169921242528494 \n",
      "acc for Psat= 0.11751975739995639 \n",
      "acc for optim= 0.14000331875350738\n",
      "Epoch:155/1000\n",
      "Loss on train= 0.019463451579213142\n",
      "Loss on test= 0.01734805852174759\n",
      "acc for Lsat= 0.09646422035164304 \n",
      "acc for Psat= 0.11792761716577739 \n",
      "acc for optim= 0.138123806214167\n",
      "Epoch:156/1000\n",
      "Loss on train= 0.019402574747800827\n",
      "Loss on test= 0.016604730859398842\n",
      "acc for Lsat= 0.11322921878761716 \n",
      "acc for Psat= 0.13170759545432198 \n",
      "acc for optim= 0.13951708889669842\n",
      "Epoch:157/1000\n",
      "Loss on train= 0.01966441422700882\n",
      "Loss on test= 0.017838293686509132\n",
      "acc for Lsat= 0.10273635205295352 \n",
      "acc for Psat= 0.12202915284368727 \n",
      "acc for optim= 0.14138783257868556\n",
      "Epoch:158/1000\n",
      "Loss on train= 0.02095785364508629\n",
      "Loss on test= 0.0183827243745327\n",
      "acc for Lsat= 0.10354924185408484 \n",
      "acc for Psat= 0.11942444824510151 \n",
      "acc for optim= 0.13737943892677626\n",
      "Epoch:159/1000\n",
      "Loss on train= 0.01903102546930313\n",
      "Loss on test= 0.018665466457605362\n",
      "acc for Lsat= 0.12537431021531423 \n",
      "acc for Psat= 0.18540462321705287 \n",
      "acc for optim= 0.13981822969184982\n",
      "Epoch:160/1000\n",
      "Loss on train= 0.01918804831802845\n",
      "Loss on test= 0.01712711527943611\n",
      "acc for Lsat= 0.11234039200676811 \n",
      "acc for Psat= 0.11947203725576402 \n",
      "acc for optim= 0.134825791625513\n",
      "Epoch:161/1000\n",
      "Loss on train= 0.019660139456391335\n",
      "Loss on test= 0.0160610880702734\n",
      "acc for Lsat= 0.09243832677602767 \n",
      "acc for Psat= 0.11055311751034526 \n",
      "acc for optim= 0.1384279303252697\n",
      "Epoch:162/1000\n",
      "Loss on train= 0.018845880404114723\n",
      "Loss on test= 0.016931496560573578\n",
      "acc for Lsat= 0.09645389103227192 \n",
      "acc for Psat= 0.1391714072889752 \n",
      "acc for optim= 0.13959781005978586\n",
      "Epoch:163/1000\n",
      "Loss on train= 0.01875239424407482\n",
      "Loss on test= 0.015835167840123177\n",
      "acc for Lsat= 0.09540087050861783 \n",
      "acc for Psat= 0.12355913453631932 \n",
      "acc for optim= 0.13618743088510302\n",
      "Epoch:164/1000\n",
      "Loss on train= 0.01936713606119156\n",
      "Loss on test= 0.015402454882860184\n",
      "acc for Lsat= 0.10503569063213135 \n",
      "acc for Psat= 0.11652602553367616 \n",
      "acc for optim= 0.1388387395275964\n",
      "Epoch:165/1000\n",
      "Loss on train= 0.01902754046022892\n",
      "Loss on test= 0.016598843038082123\n",
      "acc for Lsat= 0.10439826067950989 \n",
      "acc for Psat= 0.11862789657380847 \n",
      "acc for optim= 0.13974856725997395\n",
      "Epoch:166/1000\n",
      "Loss on train= 0.019468136131763458\n",
      "Loss on test= 0.017532669007778168\n",
      "acc for Lsat= 0.1242547028594547 \n",
      "acc for Psat= 0.1308971219592624 \n",
      "acc for optim= 0.1368940841820505\n",
      "Epoch:167/1000\n",
      "Loss on train= 0.01919001154601574\n",
      "Loss on test= 0.016585901379585266\n",
      "acc for Lsat= 0.09261077592770257 \n",
      "acc for Psat= 0.11519015961223175 \n",
      "acc for optim= 0.1400596799949805\n",
      "Epoch:168/1000\n",
      "Loss on train= 0.019354520365595818\n",
      "Loss on test= 0.016995633020997047\n",
      "acc for Lsat= 0.09435705608791776 \n",
      "acc for Psat= 0.11490865283542205 \n",
      "acc for optim= 0.13825149229831168\n",
      "Epoch:169/1000\n",
      "Loss on train= 0.019519425928592682\n",
      "Loss on test= 0.015277809463441372\n",
      "acc for Lsat= 0.09555815094047122 \n",
      "acc for Psat= 0.11047842817174064 \n",
      "acc for optim= 0.13559582514895335\n",
      "Epoch:170/1000\n",
      "Loss on train= 0.01837426796555519\n",
      "Loss on test= 0.01619233377277851\n",
      "acc for Lsat= 0.08901138802369436 \n",
      "acc for Psat= 0.11189665322502457 \n",
      "acc for optim= 0.13599497800072033\n",
      "Epoch:171/1000\n",
      "Loss on train= 0.0189271941781044\n",
      "Loss on test= 0.01666303537786007\n",
      "acc for Lsat= 0.10190671814812553 \n",
      "acc for Psat= 0.13097362286514708 \n",
      "acc for optim= 0.138322680691878\n",
      "Epoch:172/1000\n",
      "Loss on train= 0.019295774400234222\n",
      "Loss on test= 0.015820631757378578\n",
      "acc for Lsat= 0.09940141985813776 \n",
      "acc for Psat= 0.1245091199874878 \n",
      "acc for optim= 0.13827898452679316\n",
      "Epoch:173/1000\n",
      "Loss on train= 0.018864581361413002\n",
      "Loss on test= 0.016222938895225525\n",
      "acc for Lsat= 0.08944027506642872 \n",
      "acc for Psat= 0.10851589573754206 \n",
      "acc for optim= 0.13598241292768054\n",
      "Epoch:174/1000\n",
      "Loss on train= 0.019033988937735558\n",
      "Loss on test= 0.016691947355866432\n",
      "acc for Lsat= 0.10755161924494638 \n",
      "acc for Psat= 0.13113086687193976 \n",
      "acc for optim= 0.1372704720331563\n",
      "Epoch:175/1000\n",
      "Loss on train= 0.018860073760151863\n",
      "Loss on test= 0.01799323596060276\n",
      "acc for Lsat= 0.10670061210791268 \n",
      "acc for Psat= 0.1497188995281855 \n",
      "acc for optim= 0.13672464829352168\n",
      "Epoch:176/1000\n",
      "Loss on train= 0.018348870798945427\n",
      "Loss on test= 0.015276120975613594\n",
      "acc for Lsat= 0.09243657622072433 \n",
      "acc for Psat= 0.11220907641367778 \n",
      "acc for optim= 0.13717143014073374\n",
      "Epoch:177/1000\n",
      "Loss on train= 0.01805632933974266\n",
      "Loss on test= 0.016761763021349907\n",
      "acc for Lsat= 0.10051682624551986 \n",
      "acc for Psat= 0.15357919898298056 \n",
      "acc for optim= 0.13420027345418933\n",
      "Epoch:178/1000\n",
      "Loss on train= 0.01872325874865055\n",
      "Loss on test= 0.016893655061721802\n",
      "acc for Lsat= 0.11695278949207728 \n",
      "acc for Psat= 0.12973439378870857 \n",
      "acc for optim= 0.13740178007218573\n",
      "Epoch:179/1000\n",
      "Loss on train= 0.018829552456736565\n",
      "Loss on test= 0.015291798859834671\n",
      "acc for Lsat= 0.09103822343879275 \n",
      "acc for Psat= 0.12887772040234674 \n",
      "acc for optim= 0.13757615842752985\n",
      "Epoch:180/1000\n",
      "Loss on train= 0.0192918311804533\n",
      "Loss on test= 0.016827614977955818\n",
      "acc for Lsat= 0.10654219769769245 \n",
      "acc for Psat= 0.13315069874127705 \n",
      "acc for optim= 0.13475323766469954\n",
      "Epoch:181/1000\n",
      "Loss on train= 0.018336713314056396\n",
      "Loss on test= 0.015985174104571342\n",
      "acc for Lsat= 0.08450428537196583 \n",
      "acc for Psat= 0.10686131790280341 \n",
      "acc for optim= 0.13474406976666717\n",
      "Epoch:182/1000\n",
      "Loss on train= 0.018848033621907234\n",
      "Loss on test= 0.016691135242581367\n",
      "acc for Lsat= 0.11367567810747357 \n",
      "acc for Psat= 0.1310003982649909 \n",
      "acc for optim= 0.1360814726187123\n",
      "Epoch:183/1000\n",
      "Loss on train= 0.01816547103226185\n",
      "Loss on test= 0.016074173152446747\n",
      "acc for Lsat= 0.10658384640183716 \n",
      "acc for Psat= 0.1092122755944729 \n",
      "acc for optim= 0.14229786040054426\n",
      "Epoch:184/1000\n",
      "Loss on train= 0.0188316460698843\n",
      "Loss on test= 0.016294199973344803\n",
      "acc for Lsat= 0.08558471591936218 \n",
      "acc for Psat= 0.1087704694105519 \n",
      "acc for optim= 0.1372290237703257\n",
      "Epoch:185/1000\n",
      "Loss on train= 0.01846781186759472\n",
      "Loss on test= 0.01583888940513134\n",
      "acc for Lsat= 0.08966205169757208 \n",
      "acc for Psat= 0.10818674779600568 \n",
      "acc for optim= 0.13231315207150246\n",
      "Epoch:186/1000\n",
      "Loss on train= 0.01803148165345192\n",
      "Loss on test= 0.01532176323235035\n",
      "acc for Lsat= 0.08844824698236253 \n",
      "acc for Psat= 0.11235176424185436 \n",
      "acc for optim= 0.1439344636268086\n",
      "Epoch:187/1000\n",
      "Loss on train= 0.01781703718006611\n",
      "Loss on test= 0.01539347879588604\n",
      "acc for Lsat= 0.08839241845740213 \n",
      "acc for Psat= 0.11043261256482867 \n",
      "acc for optim= 0.1335723773472839\n",
      "Epoch:188/1000\n",
      "Loss on train= 0.018220197409391403\n",
      "Loss on test= 0.01610216125845909\n",
      "acc for Lsat= 0.10171481139130062 \n",
      "acc for Psat= 0.14011664224995507 \n",
      "acc for optim= 0.1338924213416047\n",
      "Epoch:189/1000\n",
      "Loss on train= 0.01825752481818199\n",
      "Loss on test= 0.015394083224236965\n",
      "acc for Lsat= 0.08334464016887874 \n",
      "acc for Psat= 0.10484473928809164 \n",
      "acc for optim= 0.13827195233768885\n",
      "Epoch:190/1000\n",
      "Loss on train= 0.017962608486413956\n",
      "Loss on test= 0.015709228813648224\n",
      "acc for Lsat= 0.08966962628894383 \n",
      "acc for Psat= 0.11588759455415938 \n",
      "acc for optim= 0.1345312343703376\n",
      "Epoch:191/1000\n",
      "Loss on train= 0.018556207418441772\n",
      "Loss on test= 0.015829037874937057\n",
      "acc for Lsat= 0.0878376035226716 \n",
      "acc for Psat= 0.10746575718124708 \n",
      "acc for optim= 0.13817031532526014\n",
      "Epoch:192/1000\n",
      "Loss on train= 0.018065864220261574\n",
      "Loss on test= 0.01511429250240326\n",
      "acc for Lsat= 0.08649494581752354 \n",
      "acc for Psat= 0.1107060819864273 \n",
      "acc for optim= 0.13301141845683256\n",
      "Epoch:193/1000\n",
      "Loss on train= 0.01815296709537506\n",
      "Loss on test= 0.015330963768064976\n",
      "acc for Lsat= 0.09876498017046186 \n",
      "acc for Psat= 0.1320135709312227 \n",
      "acc for optim= 0.13341106184654766\n",
      "Epoch:194/1000\n",
      "Loss on train= 0.018451794981956482\n",
      "Loss on test= 0.015570675022900105\n",
      "acc for Lsat= 0.09292661729786132 \n",
      "acc for Psat= 0.11722409559620751 \n",
      "acc for optim= 0.13601399635275205\n",
      "Epoch:195/1000\n",
      "Loss on train= 0.017435533925890923\n",
      "Loss on test= 0.015729881823062897\n",
      "acc for Lsat= 0.10493074340952768 \n",
      "acc for Psat= 0.12359997034072877 \n",
      "acc for optim= 0.13575516479710736\n",
      "Epoch:196/1000\n",
      "Loss on train= 0.018410785123705864\n",
      "Loss on test= 0.014797489158809185\n",
      "acc for Lsat= 0.0927778939406077 \n",
      "acc for Psat= 0.12490016420682269 \n",
      "acc for optim= 0.13479677107599047\n",
      "Epoch:197/1000\n",
      "Loss on train= 0.017508380115032196\n",
      "Loss on test= 0.015987930819392204\n",
      "acc for Lsat= 0.095224220222897 \n",
      "acc for Psat= 0.12125343845950232 \n",
      "acc for optim= 0.13751561823818417\n",
      "Epoch:198/1000\n",
      "Loss on train= 0.017807386815547943\n",
      "Loss on test= 0.015236933715641499\n",
      "acc for Lsat= 0.08522755089733335 \n",
      "acc for Psat= 0.10161921514405145 \n",
      "acc for optim= 0.13562014814880163\n",
      "Epoch:199/1000\n",
      "Loss on train= 0.017433414235711098\n",
      "Loss on test= 0.015542198903858662\n",
      "acc for Lsat= 0.09574897107150819 \n",
      "acc for Psat= 0.12375777529345619 \n",
      "acc for optim= 0.1343750393225087\n",
      "Epoch:200/1000\n",
      "Loss on train= 0.017772596329450607\n",
      "Loss on test= 0.015063794329762459\n",
      "acc for Lsat= 0.08970620003011491 \n",
      "acc for Psat= 0.1096849552459187 \n",
      "acc for optim= 0.13354307661453882\n",
      "Epoch:201/1000\n",
      "Loss on train= 0.018079416826367378\n",
      "Loss on test= 0.014559601433575153\n",
      "acc for Lsat= 0.08981340693102942 \n",
      "acc for Psat= 0.11297898259427812 \n",
      "acc for optim= 0.13351153942445915\n",
      "Epoch:202/1000\n",
      "Loss on train= 0.017575586214661598\n",
      "Loss on test= 0.014575873501598835\n",
      "acc for Lsat= 0.10161800450748865 \n",
      "acc for Psat= 0.09990967131323286 \n",
      "acc for optim= 0.13898237687018186\n",
      "Epoch:203/1000\n",
      "Loss on train= 0.017317553982138634\n",
      "Loss on test= 0.014831907115876675\n",
      "acc for Lsat= 0.08563094486792883 \n",
      "acc for Psat= 0.11150343053870732 \n",
      "acc for optim= 0.13519342847996288\n",
      "Epoch:204/1000\n",
      "Loss on train= 0.017351025715470314\n",
      "Loss on test= 0.015347268432378769\n",
      "acc for Lsat= 0.09457629546523093 \n",
      "acc for Psat= 0.10840824196736018 \n",
      "acc for optim= 0.13232946818073593\n",
      "Epoch:205/1000\n",
      "Loss on train= 0.01747829280793667\n",
      "Loss on test= 0.014743451029062271\n",
      "acc for Lsat= 0.08651066240337162 \n",
      "acc for Psat= 0.10530111243327459 \n",
      "acc for optim= 0.1397025265627437\n",
      "Epoch:206/1000\n",
      "Loss on train= 0.01695884019136429\n",
      "Loss on test= 0.015722151845693588\n",
      "acc for Lsat= 0.09223236905203927 \n",
      "acc for Psat= 0.13054763409826492 \n",
      "acc for optim= 0.13428797763254907\n",
      "Epoch:207/1000\n",
      "Loss on train= 0.01722332276403904\n",
      "Loss on test= 0.015594909898936749\n",
      "acc for Lsat= 0.09690199577146108 \n",
      "acc for Psat= 0.12843635612063936 \n",
      "acc for optim= 0.1330522277702888\n",
      "Epoch:208/1000\n",
      "Loss on train= 0.0173873919993639\n",
      "Loss on test= 0.014770797453820705\n",
      "acc for Lsat= 0.09761976765261755 \n",
      "acc for Psat= 0.12050909797350566 \n",
      "acc for optim= 0.13462882741457885\n",
      "Epoch:209/1000\n",
      "Loss on train= 0.01751752197742462\n",
      "Loss on test= 0.014704578556120396\n",
      "acc for Lsat= 0.08382670978705088 \n",
      "acc for Psat= 0.1110243240992228 \n",
      "acc for optim= 0.13422658741474153\n",
      "Epoch:210/1000\n",
      "Loss on train= 0.01745603047311306\n",
      "Loss on test= 0.015309108421206474\n",
      "acc for Lsat= 0.09093139370282491 \n",
      "acc for Psat= 0.13402347134219272 \n",
      "acc for optim= 0.14020808239777885\n",
      "Epoch:211/1000\n",
      "Loss on train= 0.017168743535876274\n",
      "Loss on test= 0.01487697008997202\n",
      "acc for Lsat= 0.08070767637756134 \n",
      "acc for Psat= 0.10896312908993827 \n",
      "acc for optim= 0.13467277155982124\n",
      "Epoch:212/1000\n",
      "Loss on train= 0.016745613887906075\n",
      "Loss on test= 0.015512807294726372\n",
      "acc for Lsat= 0.10328965882460278 \n",
      "acc for Psat= 0.11093275977505578 \n",
      "acc for optim= 0.1325922211011251\n",
      "Epoch:213/1000\n",
      "Loss on train= 0.01732337288558483\n",
      "Loss on test= 0.015296746045351028\n",
      "acc for Lsat= 0.08399828043248919 \n",
      "acc for Psat= 0.11704610768291686 \n",
      "acc for optim= 0.13750676222973401\n",
      "Epoch:214/1000\n",
      "Loss on train= 0.01809144765138626\n",
      "Loss on test= 0.016126930713653564\n",
      "acc for Lsat= 0.0975319431680772 \n",
      "acc for Psat= 0.11518570730048751 \n",
      "acc for optim= 0.13446701409088244\n",
      "Epoch:215/1000\n",
      "Loss on train= 0.017620602622628212\n",
      "Loss on test= 0.015681536868214607\n",
      "acc for Lsat= 0.09634846664137312 \n",
      "acc for Psat= 0.15141419370969136 \n",
      "acc for optim= 0.13185794721874924\n",
      "Epoch:216/1000\n",
      "Loss on train= 0.01759926788508892\n",
      "Loss on test= 0.013852371834218502\n",
      "acc for Lsat= 0.08368284040027195 \n",
      "acc for Psat= 0.10463015387455622 \n",
      "acc for optim= 0.13374934353762202\n",
      "Epoch:217/1000\n",
      "Loss on train= 0.017270348966121674\n",
      "Loss on test= 0.016185337677598\n",
      "acc for Lsat= 0.11175217736098503 \n",
      "acc for Psat= 0.1171303500731786 \n",
      "acc for optim= 0.13381745740771292\n",
      "Epoch:218/1000\n",
      "Loss on train= 0.017123019322752953\n",
      "Loss on test= 0.015066582709550858\n",
      "acc for Lsat= 0.09953850507736207 \n",
      "acc for Psat= 0.11927537620067596 \n",
      "acc for optim= 0.14014626915256181\n",
      "Epoch:219/1000\n",
      "Loss on train= 0.017572719603776932\n",
      "Loss on test= 0.014757235534489155\n",
      "acc for Lsat= 0.09253337813748254 \n",
      "acc for Psat= 0.12909060617287954 \n",
      "acc for optim= 0.12968746258152855\n",
      "Epoch:220/1000\n",
      "Loss on train= 0.017157575115561485\n",
      "Loss on test= 0.014628516510128975\n",
      "acc for Lsat= 0.08623292214340635 \n",
      "acc for Psat= 0.10192042804426615 \n",
      "acc for optim= 0.13348695184621545\n",
      "Epoch:221/1000\n",
      "Loss on train= 0.01668170839548111\n",
      "Loss on test= 0.014933173544704914\n",
      "acc for Lsat= 0.08046884602970547 \n",
      "acc for Psat= 0.09945778333478503 \n",
      "acc for optim= 0.13461999470988908\n",
      "Epoch:222/1000\n",
      "Loss on train= 0.016302207484841347\n",
      "Loss on test= 0.014501543715596199\n",
      "acc for Lsat= 0.07979252967569563 \n",
      "acc for Psat= 0.1120860407749812 \n",
      "acc for optim= 0.13330360030134517\n",
      "Epoch:223/1000\n",
      "Loss on train= 0.016053730621933937\n",
      "Loss on test= 0.014811362139880657\n",
      "acc for Lsat= 0.08659269635876019 \n",
      "acc for Psat= 0.10830720961093904 \n",
      "acc for optim= 0.1326942313876417\n",
      "Epoch:224/1000\n",
      "Loss on train= 0.01711331680417061\n",
      "Loss on test= 0.01519869640469551\n",
      "acc for Lsat= 0.10416778177022934 \n",
      "acc for Psat= 0.09962434156073463 \n",
      "acc for optim= 0.1340718137307299\n",
      "Epoch:225/1000\n",
      "Loss on train= 0.017008785158395767\n",
      "Loss on test= 0.014264623634517193\n",
      "acc for Lsat= 0.10806390610006121 \n",
      "acc for Psat= 0.1210258576605055 \n",
      "acc for optim= 0.13125616659720737\n",
      "Epoch:226/1000\n",
      "Loss on train= 0.016812412068247795\n",
      "Loss on test= 0.013950739987194538\n",
      "acc for Lsat= 0.08721873660882312 \n",
      "acc for Psat= 0.12254095905356938 \n",
      "acc for optim= 0.13139976710081103\n",
      "Epoch:227/1000\n",
      "Loss on train= 0.016864832490682602\n",
      "Loss on test= 0.015239061787724495\n",
      "acc for Lsat= 0.08016641288995742 \n",
      "acc for Psat= 0.10094950844844183 \n",
      "acc for optim= 0.13895827059944468\n",
      "Epoch:228/1000\n",
      "Loss on train= 0.016466472297906876\n",
      "Loss on test= 0.014377276413142681\n",
      "acc for Lsat= 0.08605333094795548 \n",
      "acc for Psat= 0.10327476635575294 \n",
      "acc for optim= 0.13126742107172804\n",
      "Epoch:229/1000\n",
      "Loss on train= 0.017276685684919357\n",
      "Loss on test= 0.014613769017159939\n",
      "acc for Lsat= 0.08146036813656488 \n",
      "acc for Psat= 0.11394156217575072 \n",
      "acc for optim= 0.1330214012000296\n",
      "Epoch:230/1000\n",
      "Loss on train= 0.016275040805339813\n",
      "Loss on test= 0.013587520457804203\n",
      "acc for Lsat= 0.08037167953120339 \n",
      "acc for Psat= 0.11409386826886071 \n",
      "acc for optim= 0.1333140455186367\n",
      "Epoch:231/1000\n",
      "Loss on train= 0.016646189615130424\n",
      "Loss on test= 0.014580760151147842\n",
      "acc for Lsat= 0.07800687352816264 \n",
      "acc for Psat= 0.0944542838467492 \n",
      "acc for optim= 0.13441130336787965\n",
      "Epoch:232/1000\n",
      "Loss on train= 0.016358010470867157\n",
      "Loss on test= 0.01498591061681509\n",
      "acc for Lsat= 0.08858709418111377 \n",
      "acc for Psat= 0.12559660192992952 \n",
      "acc for optim= 0.13553340343965425\n",
      "Epoch:233/1000\n",
      "Loss on train= 0.01623755320906639\n",
      "Loss on test= 0.0158007200807333\n",
      "acc for Lsat= 0.12898561855157217 \n",
      "acc for Psat= 0.11336399929391014 \n",
      "acc for optim= 0.13109051022264692\n",
      "Epoch:234/1000\n",
      "Loss on train= 0.017041539773344994\n",
      "Loss on test= 0.015378696843981743\n",
      "acc for Lsat= 0.09629695382383135 \n",
      "acc for Psat= 0.14102575282255808 \n",
      "acc for optim= 0.13276429834465187\n",
      "Epoch:235/1000\n",
      "Loss on train= 0.016960015520453453\n",
      "Loss on test= 0.014031910337507725\n",
      "acc for Lsat= 0.08283804257710775 \n",
      "acc for Psat= 0.09639427951640554 \n",
      "acc for optim= 0.13145063875450028\n",
      "Epoch:236/1000\n",
      "Loss on train= 0.01605033688247204\n",
      "Loss on test= 0.015373814851045609\n",
      "acc for Lsat= 0.08199149982796776 \n",
      "acc for Psat= 0.10695430570178563 \n",
      "acc for optim= 0.130691601211826\n",
      "Epoch:237/1000\n",
      "Loss on train= 0.016193540766835213\n",
      "Loss on test= 0.015475775115191936\n",
      "acc for Lsat= 0.09059250586562688 \n",
      "acc for Psat= 0.1118713883890046 \n",
      "acc for optim= 0.13358796143697368\n",
      "Epoch:238/1000\n",
      "Loss on train= 0.0163850300014019\n",
      "Loss on test= 0.013669397681951523\n",
      "acc for Lsat= 0.07703646048903465 \n",
      "acc for Psat= 0.09361260508497556 \n",
      "acc for optim= 0.1310629006061289\n",
      "Epoch:239/1000\n",
      "Loss on train= 0.0163106732070446\n",
      "Loss on test= 0.013679144904017448\n",
      "acc for Lsat= 0.0831568087140719 \n",
      "acc for Psat= 0.1124466064903471 \n",
      "acc for optim= 0.129248935646481\n",
      "Epoch:240/1000\n",
      "Loss on train= 0.016313083469867706\n",
      "Loss on test= 0.014673639088869095\n",
      "acc for Lsat= 0.09207770990000831 \n",
      "acc for Psat= 0.11216544558604562 \n",
      "acc for optim= 0.13102667451732686\n",
      "Epoch:241/1000\n",
      "Loss on train= 0.015741998329758644\n",
      "Loss on test= 0.014555443078279495\n",
      "acc for Lsat= 0.08956913501024247 \n",
      "acc for Psat= 0.10646129681004418 \n",
      "acc for optim= 0.1336481183146437\n",
      "Epoch:242/1000\n",
      "Loss on train= 0.016599051654338837\n",
      "Loss on test= 0.014443889260292053\n",
      "acc for Lsat= 0.08750097412202093 \n",
      "acc for Psat= 0.10504678355322944 \n",
      "acc for optim= 0.1292871490534809\n",
      "Epoch:243/1000\n",
      "Loss on train= 0.016074446961283684\n",
      "Loss on test= 0.01577877439558506\n",
      "acc for Lsat= 0.08094873428344729 \n",
      "acc for Psat= 0.10164651075998943 \n",
      "acc for optim= 0.1334785962684287\n",
      "Epoch:244/1000\n",
      "Loss on train= 0.015817608684301376\n",
      "Loss on test= 0.014726374298334122\n",
      "acc for Lsat= 0.07840381678607729 \n",
      "acc for Psat= 0.09648596263594096 \n",
      "acc for optim= 0.1314401713406874\n",
      "Epoch:245/1000\n",
      "Loss on train= 0.01601230911910534\n",
      "Loss on test= 0.014194387942552567\n",
      "acc for Lsat= 0.08193334705299803 \n",
      "acc for Psat= 0.09864618678887686 \n",
      "acc for optim= 0.13339692056179048\n",
      "Epoch:246/1000\n",
      "Loss on train= 0.01622840017080307\n",
      "Loss on test= 0.015009679831564426\n",
      "acc for Lsat= 0.08080849316385058 \n",
      "acc for Psat= 0.09973554975456661 \n",
      "acc for optim= 0.13143660525480907\n",
      "Epoch:247/1000\n",
      "Loss on train= 0.01674722321331501\n",
      "Loss on test= 0.014849651604890823\n",
      "acc for Lsat= 0.09362686955266529 \n",
      "acc for Psat= 0.1137686401605606 \n",
      "acc for optim= 0.13970620425211058\n",
      "Epoch:248/1000\n",
      "Loss on train= 0.01646289974451065\n",
      "Loss on test= 0.013887022621929646\n",
      "acc for Lsat= 0.08298797127273345 \n",
      "acc for Psat= 0.0949120951195558 \n",
      "acc for optim= 0.13273206883006625\n",
      "Epoch:249/1000\n",
      "Loss on train= 0.015433565713465214\n",
      "Loss on test= 0.01425206195563078\n",
      "acc for Lsat= 0.08416183292865753 \n",
      "acc for Psat= 0.11571182641718122 \n",
      "acc for optim= 0.13091983290182224\n",
      "Epoch:250/1000\n",
      "Loss on train= 0.016032256186008453\n",
      "Loss on test= 0.013977216556668282\n",
      "acc for Lsat= 0.08093250940243403 \n",
      "acc for Psat= 0.10797029154168235 \n",
      "acc for optim= 0.1301939110789034\n",
      "Epoch:251/1000\n",
      "Loss on train= 0.015736816450953484\n",
      "Loss on test= 0.013369136489927769\n",
      "acc for Lsat= 0.07741985734966066 \n",
      "acc for Psat= 0.09804730150434707 \n",
      "acc for optim= 0.13059285668035348\n",
      "Epoch:252/1000\n",
      "Loss on train= 0.01583719439804554\n",
      "Loss on test= 0.016030482947826385\n",
      "acc for Lsat= 0.09118596431281832 \n",
      "acc for Psat= 0.11330444200171365 \n",
      "acc for optim= 0.13160623879068428\n",
      "Epoch:253/1000\n",
      "Loss on train= 0.01581861823797226\n",
      "Loss on test= 0.014046783559024334\n",
      "acc for Lsat= 0.08225921202037072 \n",
      "acc for Psat= 0.10413376241922379 \n",
      "acc for optim= 0.13185451808902954\n",
      "Epoch:254/1000\n",
      "Loss on train= 0.01566154696047306\n",
      "Loss on test= 0.014775182120501995\n",
      "acc for Lsat= 0.10034557465049956 \n",
      "acc for Psat= 0.1003637906577852 \n",
      "acc for optim= 0.12836543805897238\n",
      "Epoch:255/1000\n",
      "Loss on train= 0.015825144946575165\n",
      "Loss on test= 0.014139888808131218\n",
      "acc for Lsat= 0.07732851654291152 \n",
      "acc for Psat= 0.09834692892101075 \n",
      "acc for optim= 0.1369943550063504\n",
      "Epoch:256/1000\n",
      "Loss on train= 0.0156954824924469\n",
      "Loss on test= 0.013845231384038925\n",
      "acc for Lsat= 0.08222728189494874 \n",
      "acc for Psat= 0.1134518944554859 \n",
      "acc for optim= 0.13254199818604523\n",
      "Epoch:257/1000\n",
      "Loss on train= 0.015462377108633518\n",
      "Loss on test= 0.014169678092002869\n",
      "acc for Lsat= 0.07421010235945384 \n",
      "acc for Psat= 0.10482560876342983 \n",
      "acc for optim= 0.13213958607779608\n",
      "Epoch:258/1000\n",
      "Loss on train= 0.015698976814746857\n",
      "Loss on test= 0.013507221825420856\n",
      "acc for Lsat= 0.0793581703470813 \n",
      "acc for Psat= 0.11105001717805864 \n",
      "acc for optim= 0.12983784605231552\n",
      "Epoch:259/1000\n",
      "Loss on train= 0.015617134980857372\n",
      "Loss on test= 0.014067824929952621\n",
      "acc for Lsat= 0.07805725816223356 \n",
      "acc for Psat= 0.09267755150794983 \n",
      "acc for optim= 0.13205150937040647\n",
      "Epoch:260/1000\n",
      "Loss on train= 0.015328171662986279\n",
      "Loss on test= 0.014138261787593365\n",
      "acc for Lsat= 0.07768520149919722 \n",
      "acc for Psat= 0.11081672459840776 \n",
      "acc for optim= 0.13769826756583323\n",
      "Epoch:261/1000\n",
      "Loss on train= 0.016255462542176247\n",
      "Loss on test= 0.01433358620852232\n",
      "acc for Lsat= 0.08803975135087967 \n",
      "acc for Psat= 0.09426413716541397 \n",
      "acc for optim= 0.12812153370016152\n",
      "Epoch:262/1000\n",
      "Loss on train= 0.015150118619203568\n",
      "Loss on test= 0.014069687575101852\n",
      "acc for Lsat= 0.0781921688053343 \n",
      "acc for Psat= 0.09526499658823012 \n",
      "acc for optim= 0.13303122619787852\n",
      "Epoch:263/1000\n",
      "Loss on train= 0.01568482629954815\n",
      "Loss on test= 0.01427343301475048\n",
      "acc for Lsat= 0.09490045391851001 \n",
      "acc for Psat= 0.126871132850647 \n",
      "acc for optim= 0.13102677576243876\n",
      "Epoch:264/1000\n",
      "Loss on train= 0.01542994100600481\n",
      "Loss on test= 0.014341155998408794\n",
      "acc for Lsat= 0.07802219440539679 \n",
      "acc for Psat= 0.12181749310758377 \n",
      "acc for optim= 0.1284164738530914\n",
      "Epoch:265/1000\n",
      "Loss on train= 0.014842403121292591\n",
      "Loss on test= 0.014581071212887764\n",
      "acc for Lsat= 0.08028969417015713 \n",
      "acc for Psat= 0.11122220390372806 \n",
      "acc for optim= 0.13102632645103662\n",
      "Epoch:266/1000\n",
      "Loss on train= 0.015437070280313492\n",
      "Loss on test= 0.01388949528336525\n",
      "acc for Lsat= 0.0923430128229989 \n",
      "acc for Psat= 0.10997604429721833 \n",
      "acc for optim= 0.13273214639888867\n",
      "Epoch:267/1000\n",
      "Loss on train= 0.015569159761071205\n",
      "Loss on test= 0.014622724615037441\n",
      "acc for Lsat= 0.08793256332476934 \n",
      "acc for Psat= 0.11606658432218765 \n",
      "acc for optim= 0.1295611736882064\n",
      "Epoch:268/1000\n",
      "Loss on train= 0.01486868504434824\n",
      "Loss on test= 0.014344210736453533\n",
      "acc for Lsat= 0.0836988788512018 \n",
      "acc for Psat= 0.10283245543638866 \n",
      "acc for optim= 0.13073033311714732\n",
      "Epoch:269/1000\n",
      "Loss on train= 0.015949120745062828\n",
      "Loss on test= 0.013736936263740063\n",
      "acc for Lsat= 0.08983973231580523 \n",
      "acc for Psat= 0.11174846473667355 \n",
      "acc for optim= 0.13234888087544175\n",
      "Epoch:270/1000\n",
      "Loss on train= 0.01543455757200718\n",
      "Loss on test= 0.014072466641664505\n",
      "acc for Lsat= 0.08495386176639133 \n",
      "acc for Psat= 0.10984404351976183 \n",
      "acc for optim= 0.13171854847007325\n",
      "Epoch:271/1000\n",
      "Loss on train= 0.01582172140479088\n",
      "Loss on test= 0.015279382467269897\n",
      "acc for Lsat= 0.08158931682507198 \n",
      "acc for Psat= 0.11474795407719081 \n",
      "acc for optim= 0.12789894977791444\n",
      "Epoch:272/1000\n",
      "Loss on train= 0.01507802214473486\n",
      "Loss on test= 0.013632435351610184\n",
      "acc for Lsat= 0.09153804894950655 \n",
      "acc for Psat= 0.11360535489188302 \n",
      "acc for optim= 0.13374111817942722\n",
      "Epoch:273/1000\n",
      "Loss on train= 0.015238964930176735\n",
      "Loss on test= 0.013711572624742985\n",
      "acc for Lsat= 0.08581232908699249 \n",
      "acc for Psat= 0.1063123173183865 \n",
      "acc for optim= 0.12995540366715028\n",
      "Epoch:274/1000\n",
      "Loss on train= 0.0150520708411932\n",
      "Loss on test= 0.013659103773534298\n",
      "acc for Lsat= 0.07698639945851433 \n",
      "acc for Psat= 0.09957761201593612 \n",
      "acc for optim= 0.12987356131068534\n",
      "Epoch:275/1000\n",
      "Loss on train= 0.014879760332405567\n",
      "Loss on test= 0.013681041076779366\n",
      "acc for Lsat= 0.08227472172843085 \n",
      "acc for Psat= 0.11708353327380287 \n",
      "acc for optim= 0.12782580144185987\n",
      "Epoch:276/1000\n",
      "Loss on train= 0.014872447587549686\n",
      "Loss on test= 0.013992471620440483\n",
      "acc for Lsat= 0.07426407610376677 \n",
      "acc for Psat= 0.09674677186542087 \n",
      "acc for optim= 0.13426728513505723\n",
      "Epoch:277/1000\n",
      "Loss on train= 0.014745685271918774\n",
      "Loss on test= 0.014151091687381268\n",
      "acc for Lsat= 0.07782557904720305 \n",
      "acc for Psat= 0.10499945713414087 \n",
      "acc for optim= 0.1304108418110344\n",
      "Epoch:278/1000\n",
      "Loss on train= 0.015313874930143356\n",
      "Loss on test= 0.013977820985019207\n",
      "acc for Lsat= 0.07815495497650571 \n",
      "acc for Psat= 0.10322212676207225 \n",
      "acc for optim= 0.1293655581358406\n",
      "Epoch:279/1000\n",
      "Loss on train= 0.014959435909986496\n",
      "Loss on test= 0.012803155928850174\n",
      "acc for Lsat= 0.07643284218178854 \n",
      "acc for Psat= 0.09509714957740573 \n",
      "acc for optim= 0.13190353558295304\n",
      "Epoch:280/1000\n",
      "Loss on train= 0.015887700021266937\n",
      "Loss on test= 0.01444685272872448\n",
      "acc for Lsat= 0.08968732969628442 \n",
      "acc for Psat= 0.09642377446095149 \n",
      "acc for optim= 0.12923469433767928\n",
      "Epoch:281/1000\n",
      "Loss on train= 0.015301969833672047\n",
      "Loss on test= 0.013828147202730179\n",
      "acc for Lsat= 0.0891038469142384 \n",
      "acc for Psat= 0.1115419410996967 \n",
      "acc for optim= 0.13019645590749052\n",
      "Epoch:282/1000\n",
      "Loss on train= 0.015289261005818844\n",
      "Loss on test= 0.013402465730905533\n",
      "acc for Lsat= 0.07664525368147425 \n",
      "acc for Psat= 0.09811210872398485 \n",
      "acc for optim= 0.12922808701793354\n",
      "Epoch:283/1000\n",
      "Loss on train= 0.015594441443681717\n",
      "Loss on test= 0.013250484131276608\n",
      "acc for Lsat= 0.08220819764667087 \n",
      "acc for Psat= 0.10385080509715611 \n",
      "acc for optim= 0.13036765843215917\n",
      "Epoch:284/1000\n",
      "Loss on train= 0.015335649251937866\n",
      "Loss on test= 0.013487566262483597\n",
      "acc for Lsat= 0.07246255245473648 \n",
      "acc for Psat= 0.10860673156049516 \n",
      "acc for optim= 0.12932387706306248\n",
      "Epoch:285/1000\n",
      "Loss on train= 0.014687404967844486\n",
      "Loss on test= 0.013899102807044983\n",
      "acc for Lsat= 0.07495469881428612 \n",
      "acc for Psat= 0.09764456641342906 \n",
      "acc for optim= 0.12768061115509932\n",
      "Epoch:286/1000\n",
      "Loss on train= 0.01520885806530714\n",
      "Loss on test= 0.01404101774096489\n",
      "acc for Lsat= 0.09154352347056072 \n",
      "acc for Psat= 0.10206890371110705 \n",
      "acc for optim= 0.1290217327329123\n",
      "Epoch:287/1000\n",
      "Loss on train= 0.015151655301451683\n",
      "Loss on test= 0.014016957953572273\n",
      "acc for Lsat= 0.08353649427493413 \n",
      "acc for Psat= 0.10110957208606931 \n",
      "acc for optim= 0.1299343383146657\n",
      "Epoch:288/1000\n",
      "Loss on train= 0.014946702867746353\n",
      "Loss on test= 0.013712129555642605\n",
      "acc for Lsat= 0.07394921200142966 \n",
      "acc for Psat= 0.0983033618165387 \n",
      "acc for optim= 0.1286501356122446\n",
      "Epoch:289/1000\n",
      "Loss on train= 0.015143903903663158\n",
      "Loss on test= 0.01544786337763071\n",
      "acc for Lsat= 0.09001303878095415 \n",
      "acc for Psat= 0.10070913417471779 \n",
      "acc for optim= 0.12917766202655107\n",
      "Epoch:290/1000\n",
      "Loss on train= 0.014527758583426476\n",
      "Loss on test= 0.01427331380546093\n",
      "acc for Lsat= 0.08651535991165374 \n",
      "acc for Psat= 0.1374165561464098 \n",
      "acc for optim= 0.12994003860900802\n",
      "Epoch:291/1000\n",
      "Loss on train= 0.014486477710306644\n",
      "Loss on test= 0.01467396691441536\n",
      "acc for Lsat= 0.09396210064490634 \n",
      "acc for Psat= 0.12024568650457594 \n",
      "acc for optim= 0.12865068573090765\n",
      "Epoch:292/1000\n",
      "Loss on train= 0.014746456407010555\n",
      "Loss on test= 0.013956108130514622\n",
      "acc for Lsat= 0.08664877663056055 \n",
      "acc for Psat= 0.09816621724102233 \n",
      "acc for optim= 0.13060491759743953\n",
      "Epoch:293/1000\n",
      "Loss on train= 0.015006798319518566\n",
      "Loss on test= 0.013378010131418705\n",
      "acc for Lsat= 0.07826755974027845 \n",
      "acc for Psat= 0.10398566044039198 \n",
      "acc for optim= 0.12946109385747048\n",
      "Epoch:294/1000\n",
      "Loss on train= 0.014572825282812119\n",
      "Loss on test= 0.013958520255982876\n",
      "acc for Lsat= 0.08831822789377637 \n",
      "acc for Psat= 0.12088178065088057 \n",
      "acc for optim= 0.127363850403991\n",
      "Epoch:295/1000\n",
      "Loss on train= 0.01503880973905325\n",
      "Loss on test= 0.015007002279162407\n",
      "acc for Lsat= 0.07821953213877147 \n",
      "acc for Psat= 0.10055985185835097 \n",
      "acc for optim= 0.1264923165216007\n",
      "Epoch:296/1000\n",
      "Loss on train= 0.014540115371346474\n",
      "Loss on test= 0.013865794986486435\n",
      "acc for Lsat= 0.08293922543525696 \n",
      "acc for Psat= 0.10464194715023041 \n",
      "acc for optim= 0.1343882082651059\n",
      "Epoch:297/1000\n",
      "Loss on train= 0.01507537066936493\n",
      "Loss on test= 0.01438087411224842\n",
      "acc for Lsat= 0.10223418457640543 \n",
      "acc for Psat= 0.1068752470943663 \n",
      "acc for optim= 0.12990559519579015\n",
      "Epoch:298/1000\n",
      "Loss on train= 0.015337849035859108\n",
      "Loss on test= 0.013067723251879215\n",
      "acc for Lsat= 0.07367433384060859 \n",
      "acc for Psat= 0.09993421203560299 \n",
      "acc for optim= 0.1308611539606419\n",
      "Epoch:299/1000\n",
      "Loss on train= 0.014569759368896484\n",
      "Loss on test= 0.013145389966666698\n",
      "acc for Lsat= 0.07051160161693891 \n",
      "acc for Psat= 0.09068544374571905 \n",
      "acc for optim= 0.12597781099223843\n",
      "Epoch:300/1000\n",
      "Loss on train= 0.014595023356378078\n",
      "Loss on test= 0.013135679066181183\n",
      "acc for Lsat= 0.07651632676521937 \n",
      "acc for Psat= 0.09522657543420793 \n",
      "acc for optim= 0.1274281537781159\n",
      "Epoch:301/1000\n",
      "Loss on train= 0.014427793212234974\n",
      "Loss on test= 0.013367433100938797\n",
      "acc for Lsat= 0.07385686172379387 \n",
      "acc for Psat= 0.09771776729159885 \n",
      "acc for optim= 0.1289889611924688\n",
      "Epoch:302/1000\n",
      "Loss on train= 0.014073321595788002\n",
      "Loss on test= 0.014264515601098537\n",
      "acc for Lsat= 0.07466221981578403 \n",
      "acc for Psat= 0.09957866619030635 \n",
      "acc for optim= 0.1269112819702261\n",
      "Epoch:303/1000\n",
      "Loss on train= 0.014449483714997768\n",
      "Loss on test= 0.014134402386844158\n",
      "acc for Lsat= 0.08490715258651309 \n",
      "acc for Psat= 0.09838700890541076 \n",
      "acc for optim= 0.129610228476425\n",
      "Epoch:304/1000\n",
      "Loss on train= 0.015104498714208603\n",
      "Loss on test= 0.013176915235817432\n",
      "acc for Lsat= 0.07602673653099272 \n",
      "acc for Psat= 0.09237118628289966 \n",
      "acc for optim= 0.12943505987318024\n",
      "Epoch:305/1000\n",
      "Loss on train= 0.014371397905051708\n",
      "Loss on test= 0.013285845518112183\n",
      "acc for Lsat= 0.08475066406859291 \n",
      "acc for Psat= 0.09931318677133985 \n",
      "acc for optim= 0.13004751508641574\n",
      "Epoch:306/1000\n",
      "Loss on train= 0.01460715476423502\n",
      "Loss on test= 0.012869071215391159\n",
      "acc for Lsat= 0.08279772169060177 \n",
      "acc for Psat= 0.10538885858323839 \n",
      "acc for optim= 0.12789598191027632\n",
      "Epoch:307/1000\n",
      "Loss on train= 0.01466111745685339\n",
      "Loss on test= 0.01441045943647623\n",
      "acc for Lsat= 0.08158621432052719 \n",
      "acc for Psat= 0.10400963011715146 \n",
      "acc for optim= 0.13579705030553868\n",
      "Epoch:308/1000\n",
      "Loss on train= 0.014920474030077457\n",
      "Loss on test= 0.012934465892612934\n",
      "acc for Lsat= 0.08047634412844977 \n",
      "acc for Psat= 0.09767291405134731 \n",
      "acc for optim= 0.1285532160351674\n",
      "Epoch:309/1000\n",
      "Loss on train= 0.014439959079027176\n",
      "Loss on test= 0.014601215720176697\n",
      "acc for Lsat= 0.07961941179302004 \n",
      "acc for Psat= 0.10869174997011821 \n",
      "acc for optim= 0.12652937523606753\n",
      "Epoch:310/1000\n",
      "Loss on train= 0.014310269616544247\n",
      "Loss on test= 0.01389166060835123\n",
      "acc for Lsat= 0.09879918810394074 \n",
      "acc for Psat= 0.1254613608121872 \n",
      "acc for optim= 0.13281886362367207\n",
      "Epoch:311/1000\n",
      "Loss on train= 0.014725464396178722\n",
      "Loss on test= 0.014391185715794563\n",
      "acc for Lsat= 0.08161340339316263 \n",
      "acc for Psat= 0.09918414884143406 \n",
      "acc for optim= 0.13007208535240755\n",
      "Epoch:312/1000\n",
      "Loss on train= 0.014019987545907497\n",
      "Loss on test= 0.014397693797945976\n",
      "acc for Lsat= 0.07755033357275858 \n",
      "acc for Psat= 0.09411231958203845 \n",
      "acc for optim= 0.1312689290071527\n",
      "Epoch:313/1000\n",
      "Loss on train= 0.014349869452416897\n",
      "Loss on test= 0.013556028716266155\n",
      "acc for Lsat= 0.07940041629804506 \n",
      "acc for Psat= 0.08874941567579905 \n",
      "acc for optim= 0.12638786287991227\n",
      "Epoch:314/1000\n",
      "Loss on train= 0.01398436352610588\n",
      "Loss on test= 0.014324218034744263\n",
      "acc for Lsat= 0.07727299729983012 \n",
      "acc for Psat= 0.09998323685593075 \n",
      "acc for optim= 0.12789893622199694\n",
      "Epoch:315/1000\n",
      "Loss on train= 0.01441640593111515\n",
      "Loss on test= 0.012553808279335499\n",
      "acc for Lsat= 0.07212743129995133 \n",
      "acc for Psat= 0.10263753781716028 \n",
      "acc for optim= 0.1305124572167794\n",
      "Epoch:316/1000\n",
      "Loss on train= 0.014394140802323818\n",
      "Loss on test= 0.01334165409207344\n",
      "acc for Lsat= 0.07645606878730984 \n",
      "acc for Psat= 0.09991762985785801 \n",
      "acc for optim= 0.12855898934519955\n",
      "Epoch:317/1000\n",
      "Loss on train= 0.01445902232080698\n",
      "Loss on test= 0.013972361572086811\n",
      "acc for Lsat= 0.0769724268052313 \n",
      "acc for Psat= 0.09625015465749634 \n",
      "acc for optim= 0.12856764006945823\n",
      "Epoch:318/1000\n",
      "Loss on train= 0.014430823735892773\n",
      "Loss on test= 0.01313304528594017\n",
      "acc for Lsat= 0.0767909730474154 \n",
      "acc for Psat= 0.11454332686132856 \n",
      "acc for optim= 0.12764641063080892\n",
      "Epoch:319/1000\n",
      "Loss on train= 0.014089804142713547\n",
      "Loss on test= 0.013866874389350414\n",
      "acc for Lsat= 0.09300737745232052 \n",
      "acc for Psat= 0.12215919858879515 \n",
      "acc for optim= 0.12993530842165152\n",
      "Epoch:320/1000\n",
      "Loss on train= 0.014359498396515846\n",
      "Loss on test= 0.015468943864107132\n",
      "acc for Lsat= 0.08475143429305819 \n",
      "acc for Psat= 0.11913128031624688 \n",
      "acc for optim= 0.12980208935009108\n",
      "Epoch:321/1000\n",
      "Loss on train= 0.014727232046425343\n",
      "Loss on test= 0.013672511093318462\n",
      "acc for Lsat= 0.09322034683492447 \n",
      "acc for Psat= 0.1352274571855863 \n",
      "acc for optim= 0.12762106895032854\n",
      "Epoch:322/1000\n",
      "Loss on train= 0.013928746804594994\n",
      "Loss on test= 0.013258678838610649\n",
      "acc for Lsat= 0.07865381042162577 \n",
      "acc for Psat= 0.10403897298706903 \n",
      "acc for optim= 0.12541292363570794\n",
      "Epoch:323/1000\n",
      "Loss on train= 0.014071483165025711\n",
      "Loss on test= 0.013284657150506973\n",
      "acc for Lsat= 0.08224276519483992 \n",
      "acc for Psat= 0.09728903041945564 \n",
      "acc for optim= 0.12810102631855344\n",
      "Epoch:324/1000\n",
      "Loss on train= 0.013893724419176579\n",
      "Loss on test= 0.013591866940259933\n",
      "acc for Lsat= 0.08269905116822986 \n",
      "acc for Psat= 0.09578007227844663 \n",
      "acc for optim= 0.12863036306678421\n",
      "Epoch:325/1000\n",
      "Loss on train= 0.013860703445971012\n",
      "Loss on test= 0.013566256500780582\n",
      "acc for Lsat= 0.07295442115929392 \n",
      "acc for Psat= 0.0985758395658599 \n",
      "acc for optim= 0.1341892905533314\n",
      "Epoch:326/1000\n",
      "Loss on train= 0.014149748720228672\n",
      "Loss on test= 0.01310662366449833\n",
      "acc for Lsat= 0.09197341650724408 \n",
      "acc for Psat= 0.11200134174691306 \n",
      "acc for optim= 0.1304458841060599\n",
      "Epoch:327/1000\n",
      "Loss on train= 0.013740593567490578\n",
      "Loss on test= 0.013768111355602741\n",
      "acc for Lsat= 0.08488087819682227 \n",
      "acc for Psat= 0.12687873277399275 \n",
      "acc for optim= 0.12791218327151402\n",
      "Epoch:328/1000\n",
      "Loss on train= 0.014335328713059425\n",
      "Loss on test= 0.013587836176156998\n",
      "acc for Lsat= 0.0764361419611507 \n",
      "acc for Psat= 0.11348870893319447 \n",
      "acc for optim= 0.1274372268700972\n",
      "Epoch:329/1000\n",
      "Loss on train= 0.01407397910952568\n",
      "Loss on test= 0.013243302702903748\n",
      "acc for Lsat= 0.07487687153948676 \n",
      "acc for Psat= 0.10244376162687938 \n",
      "acc for optim= 0.12837055064737796\n",
      "Epoch:330/1000\n",
      "Loss on train= 0.014096572063863277\n",
      "Loss on test= 0.013393803499639034\n",
      "acc for Lsat= 0.07110432634751002 \n",
      "acc for Psat= 0.09739397863547007 \n",
      "acc for optim= 0.12626922595211199\n",
      "Epoch:331/1000\n",
      "Loss on train= 0.014120987616479397\n",
      "Loss on test= 0.013857490383088589\n",
      "acc for Lsat= 0.07125067503915893 \n",
      "acc for Psat= 0.09357460439205169 \n",
      "acc for optim= 0.1292613538189067\n",
      "Epoch:332/1000\n",
      "Loss on train= 0.013845757581293583\n",
      "Loss on test= 0.01380647998303175\n",
      "acc for Lsat= 0.07633994354142083 \n",
      "acc for Psat= 0.10403188268343608 \n",
      "acc for optim= 0.13123867764241165\n",
      "Epoch:333/1000\n",
      "Loss on train= 0.01367878820747137\n",
      "Loss on test= 0.012761037796735764\n",
      "acc for Lsat= 0.0777550923327605 \n",
      "acc for Psat= 0.10119044896629122 \n",
      "acc for optim= 0.1277588043581798\n",
      "Epoch:334/1000\n",
      "Loss on train= 0.01339380070567131\n",
      "Loss on test= 0.013026162981987\n",
      "acc for Lsat= 0.07357707487212288 \n",
      "acc for Psat= 0.10514655858278273 \n",
      "acc for optim= 0.13476420756843355\n",
      "Epoch:335/1000\n",
      "Loss on train= 0.013763033784925938\n",
      "Loss on test= 0.01358064729720354\n",
      "acc for Lsat= 0.07074046515756183 \n",
      "acc for Psat= 0.09204453486535283 \n",
      "acc for optim= 0.13016788899484605\n",
      "Epoch:336/1000\n",
      "Loss on train= 0.013993699103593826\n",
      "Loss on test= 0.013467035256326199\n",
      "acc for Lsat= 0.07854572037855784 \n",
      "acc for Psat= 0.09494848102331163 \n",
      "acc for optim= 0.13158283421976702\n",
      "Epoch:337/1000\n",
      "Loss on train= 0.013763794675469398\n",
      "Loss on test= 0.014149271883070469\n",
      "acc for Lsat= 0.0789650617374314 \n",
      "acc for Psat= 0.1025253997908698 \n",
      "acc for optim= 0.12914925379057726\n",
      "Epoch:338/1000\n",
      "Loss on train= 0.013789590448141098\n",
      "Loss on test= 0.013332990929484367\n",
      "acc for Lsat= 0.07859043412738376 \n",
      "acc for Psat= 0.10358266664875879 \n",
      "acc for optim= 0.13305011248836912\n",
      "Epoch:339/1000\n",
      "Loss on train= 0.013638115487992764\n",
      "Loss on test= 0.012630178593099117\n",
      "acc for Lsat= 0.06826373752620485 \n",
      "acc for Psat= 0.09633441898557875 \n",
      "acc for optim= 0.12954046908352113\n",
      "Epoch:340/1000\n",
      "Loss on train= 0.014228720217943192\n",
      "Loss on test= 0.01365726813673973\n",
      "acc for Lsat= 0.08432050028608903 \n",
      "acc for Psat= 0.10530048774348363 \n",
      "acc for optim= 0.12952510580006574\n",
      "Epoch:341/1000\n",
      "Loss on train= 0.013755420222878456\n",
      "Loss on test= 0.013219280168414116\n",
      "acc for Lsat= 0.07452981529964342 \n",
      "acc for Psat= 0.09076068898042042 \n",
      "acc for optim= 0.13658266572488675\n",
      "Epoch:342/1000\n",
      "Loss on train= 0.014054455794394016\n",
      "Loss on test= 0.013986216858029366\n",
      "acc for Lsat= 0.08710995780097114 \n",
      "acc for Psat= 0.09737182011206945 \n",
      "acc for optim= 0.12958777063112292\n",
      "Epoch:343/1000\n",
      "Loss on train= 0.01355280913412571\n",
      "Loss on test= 0.012929407879710197\n",
      "acc for Lsat= 0.07400505592425664 \n",
      "acc for Psat= 0.09584916813506021 \n",
      "acc for optim= 0.12842580208347903\n",
      "Epoch:344/1000\n",
      "Loss on train= 0.013817143626511097\n",
      "Loss on test= 0.013819307088851929\n",
      "acc for Lsat= 0.06910661417577002 \n",
      "acc for Psat= 0.08845381016532579 \n",
      "acc for optim= 0.12847384889092706\n",
      "Epoch:345/1000\n",
      "Loss on train= 0.013535412959754467\n",
      "Loss on test= 0.01375248096883297\n",
      "acc for Lsat= 0.07947819944885043 \n",
      "acc for Psat= 0.105445686644978 \n",
      "acc for optim= 0.12672205936008446\n",
      "Epoch:346/1000\n",
      "Loss on train= 0.013508045114576817\n",
      "Loss on test= 0.01360877975821495\n",
      "acc for Lsat= 0.06928831301629543 \n",
      "acc for Psat= 0.08891644494401084 \n",
      "acc for optim= 0.13094483740213844\n",
      "Epoch:347/1000\n",
      "Loss on train= 0.013326975516974926\n",
      "Loss on test= 0.013553434051573277\n",
      "acc for Lsat= 0.07057758735285864 \n",
      "acc for Psat= 0.0974586500061883 \n",
      "acc for optim= 0.1290974956781914\n",
      "Epoch:348/1000\n",
      "Loss on train= 0.013965779915452003\n",
      "Loss on test= 0.013474957086145878\n",
      "acc for Lsat= 0.08153254406319724 \n",
      "acc for Psat= 0.10179171694649589 \n",
      "acc for optim= 0.12604114415719067\n",
      "Epoch:349/1000\n",
      "Loss on train= 0.013340604491531849\n",
      "Loss on test= 0.013066680170595646\n",
      "acc for Lsat= 0.0723182669116391 \n",
      "acc for Psat= 0.09156464685996374 \n",
      "acc for optim= 0.12836690764460298\n",
      "Epoch:350/1000\n",
      "Loss on train= 0.013771679252386093\n",
      "Loss on test= 0.013406258076429367\n",
      "acc for Lsat= 0.07458967268466948 \n",
      "acc for Psat= 0.08762635009156333 \n",
      "acc for optim= 0.13084419433337946\n",
      "Epoch:351/1000\n",
      "Loss on train= 0.013904908671975136\n",
      "Loss on test= 0.013877319172024727\n",
      "acc for Lsat= 0.07251340424021085 \n",
      "acc for Psat= 0.0946035333805614 \n",
      "acc for optim= 0.1325848141685128\n",
      "Epoch:352/1000\n",
      "Loss on train= 0.013501236215233803\n",
      "Loss on test= 0.01279725506901741\n",
      "acc for Lsat= 0.07363568097352982 \n",
      "acc for Psat= 0.10711031688584222 \n",
      "acc for optim= 0.1287491633867224\n",
      "Epoch:353/1000\n",
      "Loss on train= 0.013691517524421215\n",
      "Loss on test= 0.013947957195341587\n",
      "acc for Lsat= 0.08714574890004263 \n",
      "acc for Psat= 0.10955550571282706 \n",
      "acc for optim= 0.1297398131340742\n",
      "Epoch:354/1000\n",
      "Loss on train= 0.014131068252027035\n",
      "Loss on test= 0.013736661523580551\n",
      "acc for Lsat= 0.0810109512673484 \n",
      "acc for Psat= 0.11532348924212983 \n",
      "acc for optim= 0.13447492025378677\n",
      "Epoch:355/1000\n",
      "Loss on train= 0.013361305929720402\n",
      "Loss on test= 0.013083911500871181\n",
      "acc for Lsat= 0.08748814364274343 \n",
      "acc for Psat= 0.10614189472463396 \n",
      "acc for optim= 0.12931036268516136\n",
      "Epoch:356/1000\n",
      "Loss on train= 0.013405458070337772\n",
      "Loss on test= 0.013350624591112137\n",
      "acc for Lsat= 0.0736096725695663 \n",
      "acc for Psat= 0.09600261863734988 \n",
      "acc for optim= 0.12958933539274667\n",
      "Epoch:357/1000\n",
      "Loss on train= 0.013543394394218922\n",
      "Loss on test= 0.01379106193780899\n",
      "acc for Lsat= 0.08162830074628194 \n",
      "acc for Psat= 0.11379262970553504 \n",
      "acc for optim= 0.130839728936553\n",
      "Epoch:358/1000\n",
      "Loss on train= 0.013659612275660038\n",
      "Loss on test= 0.01298457570374012\n",
      "acc for Lsat= 0.06991262700822619 \n",
      "acc for Psat= 0.09133265589674314 \n",
      "acc for optim= 0.12926040234872038\n",
      "Epoch:359/1000\n",
      "Loss on train= 0.013694765977561474\n",
      "Loss on test= 0.013198534026741982\n",
      "acc for Lsat= 0.07277940726942486 \n",
      "acc for Psat= 0.0886513527896669 \n",
      "acc for optim= 0.12867598715755674\n",
      "Epoch:360/1000\n",
      "Loss on train= 0.013880208134651184\n",
      "Loss on test= 0.013931934721767902\n",
      "acc for Lsat= 0.07119629581769309 \n",
      "acc for Psat= 0.09756820731692845 \n",
      "acc for optim= 0.1320655902640687\n",
      "Epoch:361/1000\n",
      "Loss on train= 0.01393592357635498\n",
      "Loss on test= 0.013505118899047375\n",
      "acc for Lsat= 0.08322057442532646 \n",
      "acc for Psat= 0.09654107582237986 \n",
      "acc for optim= 0.1258798359996743\n",
      "Epoch:362/1000\n",
      "Loss on train= 0.013372346758842468\n",
      "Loss on test= 0.01380283385515213\n",
      "acc for Lsat= 0.07228639440404044 \n",
      "acc for Psat= 0.09961002998881871 \n",
      "acc for optim= 0.1298775549978018\n",
      "Epoch:363/1000\n",
      "Loss on train= 0.013633028604090214\n",
      "Loss on test= 0.01407518982887268\n",
      "acc for Lsat= 0.07718614273601107 \n",
      "acc for Psat= 0.10158181190490724 \n",
      "acc for optim= 0.12745677890876927\n",
      "Epoch:364/1000\n",
      "Loss on train= 0.013431788422167301\n",
      "Loss on test= 0.01336245983839035\n",
      "acc for Lsat= 0.07263149834341472 \n",
      "acc for Psat= 0.09318934571411874 \n",
      "acc for optim= 0.13407239706979857\n",
      "Epoch:365/1000\n",
      "Loss on train= 0.013331210240721703\n",
      "Loss on test= 0.012833134271204472\n",
      "acc for Lsat= 0.08415513253874247 \n",
      "acc for Psat= 0.09317942162354788 \n",
      "acc for optim= 0.13086350686434245\n",
      "Epoch:366/1000\n",
      "Loss on train= 0.013719018548727036\n",
      "Loss on test= 0.014493096619844437\n",
      "acc for Lsat= 0.07195592688189613 \n",
      "acc for Psat= 0.10219481504625745 \n",
      "acc for optim= 0.1350010290327999\n",
      "Epoch:367/1000\n",
      "Loss on train= 0.013546676374971867\n",
      "Loss on test= 0.013488889671862125\n",
      "acc for Lsat= 0.08253076018558608 \n",
      "acc for Psat= 0.09964902798334757 \n",
      "acc for optim= 0.12894993228061746\n",
      "Epoch:368/1000\n",
      "Loss on train= 0.013405000790953636\n",
      "Loss on test= 0.013057970441877842\n",
      "acc for Lsat= 0.07478609250651465 \n",
      "acc for Psat= 0.09339319037066565 \n",
      "acc for optim= 0.12914186384942797\n",
      "Epoch:369/1000\n",
      "Loss on train= 0.012930184602737427\n",
      "Loss on test= 0.013043436221778393\n",
      "acc for Lsat= 0.06909697097208765 \n",
      "acc for Psat= 0.08864405999581018 \n",
      "acc for optim= 0.12737108610777392\n",
      "Epoch:370/1000\n",
      "Loss on train= 0.013509992510080338\n",
      "Loss on test= 0.012944704852998257\n",
      "acc for Lsat= 0.07702129259705544 \n",
      "acc for Psat= 0.09019972880681355 \n",
      "acc for optim= 0.13176161835177075\n",
      "Epoch:371/1000\n",
      "Loss on train= 0.013199695385992527\n",
      "Loss on test= 0.013004550710320473\n",
      "acc for Lsat= 0.06930912310878437 \n",
      "acc for Psat= 0.08623027230302492 \n",
      "acc for optim= 0.12990496346933972\n",
      "Epoch:372/1000\n",
      "Loss on train= 0.013413766399025917\n",
      "Loss on test= 0.013167466968297958\n",
      "acc for Lsat= 0.0846924086411794 \n",
      "acc for Psat= 0.0932874172925949 \n",
      "acc for optim= 0.12771783671859238\n",
      "Epoch:373/1000\n",
      "Loss on train= 0.013083693571388721\n",
      "Loss on test= 0.013153585605323315\n",
      "acc for Lsat= 0.07257617397440803 \n",
      "acc for Psat= 0.08925789197285969 \n",
      "acc for optim= 0.12680970389596447\n",
      "Epoch:374/1000\n",
      "Loss on train= 0.013179288245737553\n",
      "Loss on test= 0.01285197027027607\n",
      "acc for Lsat= 0.07216881795061961 \n",
      "acc for Psat= 0.09282005661063725 \n",
      "acc for optim= 0.13177729207608435\n",
      "Epoch:375/1000\n",
      "Loss on train= 0.013410013169050217\n",
      "Loss on test= 0.0131335174664855\n",
      "acc for Lsat= 0.0722924033800761 \n",
      "acc for Psat= 0.09248666713635127 \n",
      "acc for optim= 0.12900944892317054\n",
      "Epoch:376/1000\n",
      "Loss on train= 0.013485702686011791\n",
      "Loss on test= 0.013180953450500965\n",
      "acc for Lsat= 0.082846723165777 \n",
      "acc for Psat= 0.12228330042627121 \n",
      "acc for optim= 0.1280948444786999\n",
      "Epoch:377/1000\n",
      "Loss on train= 0.013763215392827988\n",
      "Loss on test= 0.01328001543879509\n",
      "acc for Lsat= 0.07288435788618194 \n",
      "acc for Psat= 0.10251584268278546 \n",
      "acc for optim= 0.1314382562931213\n",
      "Epoch:378/1000\n",
      "Loss on train= 0.013055035844445229\n",
      "Loss on test= 0.01220647618174553\n",
      "acc for Lsat= 0.08213537848658031 \n",
      "acc for Psat= 0.09699180225531259 \n",
      "acc for optim= 0.1303147619797124\n",
      "Epoch:379/1000\n",
      "Loss on train= 0.013480905443429947\n",
      "Loss on test= 0.013761747628450394\n",
      "acc for Lsat= 0.07100003643168344 \n",
      "acc for Psat= 0.0998857824338807 \n",
      "acc for optim= 0.13168551543106638\n",
      "Epoch:380/1000\n",
      "Loss on train= 0.013646471314132214\n",
      "Loss on test= 0.013326931744813919\n",
      "acc for Lsat= 0.08049638205104402 \n",
      "acc for Psat= 0.08918579270442326 \n",
      "acc for optim= 0.13123240285139118\n",
      "Epoch:381/1000\n",
      "Loss on train= 0.013102947734296322\n",
      "Loss on test= 0.012450087815523148\n",
      "acc for Lsat= 0.06931140489048428 \n",
      "acc for Psat= 0.09162725682059923 \n",
      "acc for optim= 0.13318000278539127\n",
      "Epoch:382/1000\n",
      "Loss on train= 0.013026499189436436\n",
      "Loss on test= 0.01282917708158493\n",
      "acc for Lsat= 0.06950782603687711 \n",
      "acc for Psat= 0.08718738348947631 \n",
      "acc for optim= 0.13160506611069042\n",
      "Epoch:383/1000\n",
      "Loss on train= 0.013172977603971958\n",
      "Loss on test= 0.013238918967545033\n",
      "acc for Lsat= 0.07208329819970662 \n",
      "acc for Psat= 0.09541161076890098 \n",
      "acc for optim= 0.12964278308467733\n",
      "Epoch:384/1000\n",
      "Loss on train= 0.013593833893537521\n",
      "Loss on test= 0.012565789744257927\n",
      "acc for Lsat= 0.07720335092809465 \n",
      "acc for Psat= 0.10498194777303271 \n",
      "acc for optim= 0.12979972326817613\n",
      "Epoch:385/1000\n",
      "Loss on train= 0.013389387167990208\n",
      "Loss on test= 0.013657089322805405\n",
      "acc for Lsat= 0.06958927503890461 \n",
      "acc for Psat= 0.08403293958140744 \n",
      "acc for optim= 0.1314638817475902\n",
      "Epoch:386/1000\n",
      "Loss on train= 0.013887573033571243\n",
      "Loss on test= 0.013843282125890255\n",
      "acc for Lsat= 0.06682906473676364 \n",
      "acc for Psat= 0.09643483112255731 \n",
      "acc for optim= 0.1379438110937675\n",
      "Epoch:387/1000\n",
      "Loss on train= 0.01283035334199667\n",
      "Loss on test= 0.01423056423664093\n",
      "acc for Lsat= 0.07287663784292009 \n",
      "acc for Psat= 0.10266093081898159 \n",
      "acc for optim= 0.13106219680565928\n",
      "Epoch:388/1000\n",
      "Loss on train= 0.012736218981444836\n",
      "Loss on test= 0.013375869020819664\n",
      "acc for Lsat= 0.06638460920916664 \n",
      "acc for Psat= 0.09287748767269982 \n",
      "acc for optim= 0.12984079721694192\n",
      "Epoch:389/1000\n",
      "Loss on train= 0.01325481291860342\n",
      "Loss on test= 0.012996160425245762\n",
      "acc for Lsat= 0.06666344478726387 \n",
      "acc for Psat= 0.10195335729254616 \n",
      "acc for optim= 0.12919570785015821\n",
      "Epoch:390/1000\n",
      "Loss on train= 0.013485584408044815\n",
      "Loss on test= 0.012850068509578705\n",
      "acc for Lsat= 0.07155011039641168 \n",
      "acc for Psat= 0.09421720314357016 \n",
      "acc for optim= 0.13153119357302784\n",
      "Epoch:391/1000\n",
      "Loss on train= 0.01289801299571991\n",
      "Loss on test= 0.012845205143094063\n",
      "acc for Lsat= 0.08334569765461815 \n",
      "acc for Psat= 0.09568115373452504 \n",
      "acc for optim= 0.12798070932686742\n",
      "Epoch:392/1000\n",
      "Loss on train= 0.013325664214789867\n",
      "Loss on test= 0.01358601450920105\n",
      "acc for Lsat= 0.06828117883867688 \n",
      "acc for Psat= 0.08934240225288603 \n",
      "acc for optim= 0.13345420480602319\n",
      "Epoch:393/1000\n",
      "Loss on train= 0.012753264047205448\n",
      "Loss on test= 0.013373457826673985\n",
      "acc for Lsat= 0.06822151036726104 \n",
      "acc for Psat= 0.08998052279154461 \n",
      "acc for optim= 0.13024654963778126\n",
      "Epoch:394/1000\n",
      "Loss on train= 0.01316805835813284\n",
      "Loss on test= 0.013740558177232742\n",
      "acc for Lsat= 0.09239138683511153 \n",
      "acc for Psat= 0.09983053807583121 \n",
      "acc for optim= 0.132408001470483\n",
      "Epoch:395/1000\n",
      "Loss on train= 0.013380768708884716\n",
      "Loss on test= 0.012649199925363064\n",
      "acc for Lsat= 0.07197973099019793 \n",
      "acc for Psat= 0.08812308402525054 \n",
      "acc for optim= 0.13164376119255194\n",
      "Epoch:396/1000\n",
      "Loss on train= 0.013176427222788334\n",
      "Loss on test= 0.013574719429016113\n",
      "acc for Lsat= 0.07897359513574177 \n",
      "acc for Psat= 0.10561019447114732 \n",
      "acc for optim= 0.12889134120713508\n",
      "Epoch:397/1000\n",
      "Loss on train= 0.012655258178710938\n",
      "Loss on test= 0.013717623427510262\n",
      "acc for Lsat= 0.0850651459561454 \n",
      "acc for Psat= 0.10101957660582331 \n",
      "acc for optim= 0.12825818742728892\n",
      "Epoch:398/1000\n",
      "Loss on train= 0.013041439466178417\n",
      "Loss on test= 0.012680293060839176\n",
      "acc for Lsat= 0.07764231711626053 \n",
      "acc for Psat= 0.11210460497273338 \n",
      "acc for optim= 0.1304143830616441\n",
      "Epoch:399/1000\n",
      "Loss on train= 0.01304706558585167\n",
      "Loss on test= 0.012932175770401955\n",
      "acc for Lsat= 0.08156349642409216 \n",
      "acc for Psat= 0.10017343428399827 \n",
      "acc for optim= 0.13076816772421204\n",
      "Epoch:400/1000\n",
      "Loss on train= 0.013121736235916615\n",
      "Loss on test= 0.013969019055366516\n",
      "acc for Lsat= 0.07447295139233272 \n",
      "acc for Psat= 0.09884593023194208 \n",
      "acc for optim= 0.13114850574897396\n",
      "Epoch:401/1000\n",
      "Loss on train= 0.01303455512970686\n",
      "Loss on test= 0.012496988289058208\n",
      "acc for Lsat= 0.07697653141286637 \n",
      "acc for Psat= 0.10497876389159096 \n",
      "acc for optim= 0.12996297471432222\n",
      "Epoch:402/1000\n",
      "Loss on train= 0.012895038351416588\n",
      "Loss on test= 0.014062334783375263\n",
      "acc for Lsat= 0.07033490108119117 \n",
      "acc for Psat= 0.0961957327193684 \n",
      "acc for optim= 0.13238558702998693\n",
      "Epoch:403/1000\n",
      "Loss on train= 0.012851056642830372\n",
      "Loss on test= 0.01407554280012846\n",
      "acc for Lsat= 0.07711337059736251 \n",
      "acc for Psat= 0.11058251592848038 \n",
      "acc for optim= 0.13465899297864073\n",
      "Epoch:404/1000\n",
      "Loss on train= 0.01310814917087555\n",
      "Loss on test= 0.014156765304505825\n",
      "acc for Lsat= 0.07595598979128731 \n",
      "acc for Psat= 0.10300797985659706 \n",
      "acc for optim= 0.1285993166371352\n",
      "Epoch:405/1000\n",
      "Loss on train= 0.012877682223916054\n",
      "Loss on test= 0.013390850275754929\n",
      "acc for Lsat= 0.07047373205423355 \n",
      "acc for Psat= 0.08817223144902124 \n",
      "acc for optim= 0.13013511932351524\n",
      "Epoch:406/1000\n",
      "Loss on train= 0.012340023182332516\n",
      "Loss on test= 0.014313098043203354\n",
      "acc for Lsat= 0.07218297885523903 \n",
      "acc for Psat= 0.0878943411840333 \n",
      "acc for optim= 0.13117374351455102\n",
      "Epoch:407/1000\n",
      "Loss on train= 0.012761027552187443\n",
      "Loss on test= 0.013679086230695248\n",
      "acc for Lsat= 0.06919122007158068 \n",
      "acc for Psat= 0.09617988450659645 \n",
      "acc for optim= 0.13082185958822568\n",
      "Epoch:408/1000\n",
      "Loss on train= 0.012994020245969296\n",
      "Loss on test= 0.012898226268589497\n",
      "acc for Lsat= 0.07385816044277616 \n",
      "acc for Psat= 0.09195910352799627 \n",
      "acc for optim= 0.12777390319129658\n",
      "Epoch:409/1000\n",
      "Loss on train= 0.01261614914983511\n",
      "Loss on test= 0.012296085245907307\n",
      "acc for Lsat= 0.07129870835277768 \n",
      "acc for Psat= 0.08758666705754069 \n",
      "acc for optim= 0.12769224389145772\n",
      "Epoch:410/1000\n",
      "Loss on train= 0.012235129252076149\n",
      "Loss on test= 0.013327299617230892\n",
      "acc for Lsat= 0.06918663697110282 \n",
      "acc for Psat= 0.09049570221039983 \n",
      "acc for optim= 0.12923676766869097\n",
      "Epoch:411/1000\n",
      "Loss on train= 0.01262748148292303\n",
      "Loss on test= 0.012701669707894325\n",
      "acc for Lsat= 0.07222570247120327 \n",
      "acc for Psat= 0.09498615033096737 \n",
      "acc for optim= 0.12832965864282517\n",
      "Epoch:412/1000\n",
      "Loss on train= 0.01231880858540535\n",
      "Loss on test= 0.013563652522861958\n",
      "acc for Lsat= 0.08246263464291888 \n",
      "acc for Psat= 0.10107743110921648 \n",
      "acc for optim= 0.12968307065053114\n",
      "Epoch:413/1000\n",
      "Loss on train= 0.012487465515732765\n",
      "Loss on test= 0.013027539476752281\n",
      "acc for Lsat= 0.07988689045111337 \n",
      "acc for Psat= 0.11064979483683904 \n",
      "acc for optim= 0.12944028551379835\n",
      "Epoch:414/1000\n",
      "Loss on train= 0.012223863042891026\n",
      "Loss on test= 0.013438286259770393\n",
      "acc for Lsat= 0.0725616174439589 \n",
      "acc for Psat= 0.09077427047822212 \n",
      "acc for optim= 0.12819476366922672\n",
      "Epoch:415/1000\n",
      "Loss on train= 0.012594235129654408\n",
      "Loss on test= 0.013175721280276775\n",
      "acc for Lsat= 0.07312049567699434 \n",
      "acc for Psat= 0.091848281191455 \n",
      "acc for optim= 0.1325127169903782\n",
      "Epoch:416/1000\n",
      "Loss on train= 0.012360934168100357\n",
      "Loss on test= 0.013211973942816257\n",
      "acc for Lsat= 0.06903911580642066 \n",
      "acc for Psat= 0.08952921943532095 \n",
      "acc for optim= 0.13305621964650025\n",
      "Epoch:417/1000\n",
      "Loss on train= 0.012338531203567982\n",
      "Loss on test= 0.011774856597185135\n",
      "acc for Lsat= 0.06928059193823072 \n",
      "acc for Psat= 0.09337698767582575 \n",
      "acc for optim= 0.12785873122953087\n",
      "Epoch:418/1000\n",
      "Loss on train= 0.012636839412152767\n",
      "Loss on test= 0.01306232437491417\n",
      "acc for Lsat= 0.0692934450176027 \n",
      "acc for Psat= 0.08938041379054389 \n",
      "acc for optim= 0.12900891370243497\n",
      "Epoch:419/1000\n",
      "Loss on train= 0.012894629500806332\n",
      "Loss on test= 0.012682202272117138\n",
      "acc for Lsat= 0.07178695748249692 \n",
      "acc for Psat= 0.10670433027876748 \n",
      "acc for optim= 0.1293293739772505\n",
      "Epoch:420/1000\n",
      "Loss on train= 0.013062303885817528\n",
      "Loss on test= 0.01303957775235176\n",
      "acc for Lsat= 0.06849065522352855 \n",
      "acc for Psat= 0.09227360495262678 \n",
      "acc for optim= 0.1290231876974253\n",
      "Epoch:421/1000\n",
      "Loss on train= 0.01282833144068718\n",
      "Loss on test= 0.01255439966917038\n",
      "acc for Lsat= 0.0766194952858819 \n",
      "acc for Psat= 0.09804958204428356 \n",
      "acc for optim= 0.12907132109006242\n",
      "Epoch:422/1000\n",
      "Loss on train= 0.012610974721610546\n",
      "Loss on test= 0.012808969244360924\n",
      "acc for Lsat= 0.06823412477970123 \n",
      "acc for Psat= 0.09286513129870097 \n",
      "acc for optim= 0.13033638406131\n",
      "Epoch:423/1000\n",
      "Loss on train= 0.01263097021728754\n",
      "Loss on test= 0.012383085675537586\n",
      "acc for Lsat= 0.06854020986292096 \n",
      "acc for Psat= 0.09278586076365576 \n",
      "acc for optim= 0.13266043404324188\n",
      "Epoch:424/1000\n",
      "Loss on train= 0.012500702403485775\n",
      "Loss on test= 0.013753365725278854\n",
      "acc for Lsat= 0.07533440821700627 \n",
      "acc for Psat= 0.0910184885892603 \n",
      "acc for optim= 0.12861025923242173\n",
      "Epoch:425/1000\n",
      "Loss on train= 0.012397084385156631\n",
      "Loss on test= 0.012840219773352146\n",
      "acc for Lsat= 0.06946822802225748 \n",
      "acc for Psat= 0.09922605835729176 \n",
      "acc for optim= 0.1318770975495378\n",
      "Epoch:426/1000\n",
      "Loss on train= 0.012263950891792774\n",
      "Loss on test= 0.013191555626690388\n",
      "acc for Lsat= 0.07683670951260461 \n",
      "acc for Psat= 0.09660697496599621 \n",
      "acc for optim= 0.13004331961274146\n",
      "Epoch:427/1000\n",
      "Loss on train= 0.012269689701497555\n",
      "Loss on test= 0.013316597789525986\n",
      "acc for Lsat= 0.06727399354179701 \n",
      "acc for Psat= 0.0944824210471577 \n",
      "acc for optim= 0.13052259244852596\n",
      "Epoch:428/1000\n",
      "Loss on train= 0.012353888712823391\n",
      "Loss on test= 0.013855835422873497\n",
      "acc for Lsat= 0.07621882971790102 \n",
      "acc for Psat= 0.09405493835608165 \n",
      "acc for optim= 0.13028519310885006\n",
      "Epoch:429/1000\n",
      "Loss on train= 0.012332738377153873\n",
      "Loss on test= 0.014156728982925415\n",
      "acc for Lsat= 0.06414103772905139 \n",
      "acc for Psat= 0.0864205952319834 \n",
      "acc for optim= 0.13077726624906064\n",
      "Epoch:430/1000\n",
      "Loss on train= 0.012054848484694958\n",
      "Loss on test= 0.013758694753050804\n",
      "acc for Lsat= 0.07829396095540787 \n",
      "acc for Psat= 0.10131510496139526 \n",
      "acc for optim= 0.1293146157119837\n",
      "Epoch:431/1000\n",
      "Loss on train= 0.01252624113112688\n",
      "Loss on test= 0.012738668359816074\n",
      "acc for Lsat= 0.07052240106794569 \n",
      "acc for Psat= 0.09042923011713559 \n",
      "acc for optim= 0.13008291613724496\n",
      "Epoch:432/1000\n",
      "Loss on train= 0.012380235828459263\n",
      "Loss on test= 0.01306933630257845\n",
      "acc for Lsat= 0.06737471835480796 \n",
      "acc for Psat= 0.08415007996890281 \n",
      "acc for optim= 0.12708861028982532\n",
      "Epoch:433/1000\n",
      "Loss on train= 0.012413263320922852\n",
      "Loss on test= 0.012858928181231022\n",
      "acc for Lsat= 0.06743934576710063 \n",
      "acc for Psat= 0.08681503824061819 \n",
      "acc for optim= 0.12955629035002658\n",
      "Epoch:434/1000\n",
      "Loss on train= 0.01223534345626831\n",
      "Loss on test= 0.013426108285784721\n",
      "acc for Lsat= 0.074534915222062 \n",
      "acc for Psat= 0.09947520411676833 \n",
      "acc for optim= 0.12882336994840038\n",
      "Epoch:435/1000\n",
      "Loss on train= 0.012658662162721157\n",
      "Loss on test= 0.013348559848964214\n",
      "acc for Lsat= 0.07861434966325759 \n",
      "acc for Psat= 0.10174852808316548 \n",
      "acc for optim= 0.13236306702925096\n",
      "Epoch:436/1000\n",
      "Loss on train= 0.011760084889829159\n",
      "Loss on test= 0.013089698739349842\n",
      "acc for Lsat= 0.06277536327640215 \n",
      "acc for Psat= 0.08649708024329611 \n",
      "acc for optim= 0.13176916456884807\n",
      "Epoch:437/1000\n",
      "Loss on train= 0.012064800597727299\n",
      "Loss on test= 0.013511499390006065\n",
      "acc for Lsat= 0.10184547586573495 \n",
      "acc for Psat= 0.10887586043940652 \n",
      "acc for optim= 0.13220344550855873\n",
      "Epoch:438/1000\n",
      "Loss on train= 0.012403040193021297\n",
      "Loss on test= 0.013069800101220608\n",
      "acc for Lsat= 0.07275813503397836 \n",
      "acc for Psat= 0.08689976831277212 \n",
      "acc for optim= 0.13077195909702116\n",
      "Epoch:439/1000\n",
      "Loss on train= 0.01237715408205986\n",
      "Loss on test= 0.013657690025866032\n",
      "acc for Lsat= 0.06994660480154885 \n",
      "acc for Psat= 0.10779495686292646 \n",
      "acc for optim= 0.12883013730040854\n",
      "Epoch:440/1000\n",
      "Loss on train= 0.012008875608444214\n",
      "Loss on test= 0.013248583301901817\n",
      "acc for Lsat= 0.06938833743333817 \n",
      "acc for Psat= 0.08343007274799878 \n",
      "acc for optim= 0.13232586511100333\n",
      "Epoch:441/1000\n",
      "Loss on train= 0.012290761806070805\n",
      "Loss on test= 0.013516666367650032\n",
      "acc for Lsat= 0.06776514417595335 \n",
      "acc for Psat= 0.08869762693842252 \n",
      "acc for optim= 0.13017933567365012\n",
      "Epoch:442/1000\n",
      "Loss on train= 0.012010795064270496\n",
      "Loss on test= 0.013836936093866825\n",
      "acc for Lsat= 0.07959513598018221 \n",
      "acc for Psat= 0.1039927108420266 \n",
      "acc for optim= 0.13237294865151247\n",
      "Epoch:443/1000\n",
      "Loss on train= 0.01185767911374569\n",
      "Loss on test= 0.013220389373600483\n",
      "acc for Lsat= 0.07028082923756705 \n",
      "acc for Psat= 0.09112128160066076 \n",
      "acc for optim= 0.13298788459764588\n",
      "Epoch:444/1000\n",
      "Loss on train= 0.012153232470154762\n",
      "Loss on test= 0.013126576319336891\n",
      "acc for Lsat= 0.06902958038780424 \n",
      "acc for Psat= 0.08523913659155369 \n",
      "acc for optim= 0.12999768666923045\n",
      "Epoch:445/1000\n",
      "Loss on train= 0.012089609168469906\n",
      "Loss on test= 0.013371024280786514\n",
      "acc for Lsat= 0.06846067756414415 \n",
      "acc for Psat= 0.08704607768191232 \n",
      "acc for optim= 0.12781151475436572\n",
      "Epoch:446/1000\n",
      "Loss on train= 0.012238061986863613\n",
      "Loss on test= 0.013930052518844604\n",
      "acc for Lsat= 0.07127782106399536 \n",
      "acc for Psat= 0.09355727086464564 \n",
      "acc for optim= 0.12937841325377422\n",
      "Epoch:447/1000\n",
      "Loss on train= 0.01270037516951561\n",
      "Loss on test= 0.012754596769809723\n",
      "acc for Lsat= 0.06639100114504497 \n",
      "acc for Psat= 0.08409248536659614 \n",
      "acc for optim= 0.1294088718584842\n",
      "Epoch:448/1000\n",
      "Loss on train= 0.011724397540092468\n",
      "Loss on test= 0.01358624268323183\n",
      "acc for Lsat= 0.08225558797518412 \n",
      "acc for Psat= 0.09472262437144913 \n",
      "acc for optim= 0.1300720312115219\n",
      "Epoch:449/1000\n",
      "Loss on train= 0.01273847185075283\n",
      "Loss on test= 0.013627033680677414\n",
      "acc for Lsat= 0.06713745693365733 \n",
      "acc for Psat= 0.08411751770310932 \n",
      "acc for optim= 0.1317419610503647\n",
      "Epoch:450/1000\n",
      "Loss on train= 0.012253303080797195\n",
      "Loss on test= 0.013164456933736801\n",
      "acc for Lsat= 0.07614617562956282 \n",
      "acc for Psat= 0.0935403522517946 \n",
      "acc for optim= 0.12840793408039541\n",
      "Epoch:451/1000\n",
      "Loss on train= 0.011945522390305996\n",
      "Loss on test= 0.012503000907599926\n",
      "acc for Lsat= 0.07375635918643739 \n",
      "acc for Psat= 0.09361446946859359 \n",
      "acc for optim= 0.1304150464841061\n",
      "Epoch:452/1000\n",
      "Loss on train= 0.01171242818236351\n",
      "Loss on test= 0.01259133592247963\n",
      "acc for Lsat= 0.0636225072046121 \n",
      "acc for Psat= 0.09122452636559804 \n",
      "acc for optim= 0.1292542165093538\n",
      "Epoch:453/1000\n",
      "Loss on train= 0.011991116218268871\n",
      "Loss on test= 0.01373563427478075\n",
      "acc for Lsat= 0.0769140193859736 \n",
      "acc for Psat= 0.09480464607477188 \n",
      "acc for optim= 0.12933252776662507\n",
      "Epoch:454/1000\n",
      "Loss on train= 0.011829054914414883\n",
      "Loss on test= 0.012974596582353115\n",
      "acc for Lsat= 0.06313314330246714 \n",
      "acc for Psat= 0.09039788411723243 \n",
      "acc for optim= 0.13044705121881434\n",
      "Epoch:455/1000\n",
      "Loss on train= 0.011806819587945938\n",
      "Loss on test= 0.013283465057611465\n",
      "acc for Lsat= 0.06742759611871509 \n",
      "acc for Psat= 0.08596882944305737 \n",
      "acc for optim= 0.12993897681848873\n",
      "Epoch:456/1000\n",
      "Loss on train= 0.01173804048448801\n",
      "Loss on test= 0.014160901308059692\n",
      "acc for Lsat= 0.06492151965697607 \n",
      "acc for Psat= 0.08588142395019531 \n",
      "acc for optim= 0.1308807702217665\n",
      "Epoch:457/1000\n",
      "Loss on train= 0.011584961786866188\n",
      "Loss on test= 0.013456592336297035\n",
      "acc for Lsat= 0.06724567065636317 \n",
      "acc for Psat= 0.09108821733130351 \n",
      "acc for optim= 0.13049374127553565\n",
      "Epoch:458/1000\n",
      "Loss on train= 0.012117371894419193\n",
      "Loss on test= 0.015240364708006382\n",
      "acc for Lsat= 0.08404950383636688 \n",
      "acc for Psat= 0.11136010355419584 \n",
      "acc for optim= 0.13558778866297672\n",
      "Epoch:459/1000\n",
      "Loss on train= 0.012217875570058823\n",
      "Loss on test= 0.013507474213838577\n",
      "acc for Lsat= 0.07310124387343725 \n",
      "acc for Psat= 0.08964331580532922 \n",
      "acc for optim= 0.1309654173751672\n",
      "Epoch:460/1000\n",
      "Loss on train= 0.01207639928907156\n",
      "Loss on test= 0.013182250782847404\n",
      "acc for Lsat= 0.07148747477266525 \n",
      "acc for Psat= 0.10970781544844307 \n",
      "acc for optim= 0.12960447227168415\n",
      "Epoch:461/1000\n",
      "Loss on train= 0.012575890868902206\n",
      "Loss on test= 0.013266174122691154\n",
      "acc for Lsat= 0.06764236547880703 \n",
      "acc for Psat= 0.08751338620980581 \n",
      "acc for optim= 0.13227513648776545\n",
      "Epoch:462/1000\n",
      "Loss on train= 0.012073936872184277\n",
      "Loss on test= 0.013204967603087425\n",
      "acc for Lsat= 0.07752687666151258 \n",
      "acc for Psat= 0.09296114676528505 \n",
      "acc for optim= 0.12899284455795876\n",
      "Epoch:463/1000\n",
      "Loss on train= 0.011624814011156559\n",
      "Loss on test= 0.013262039981782436\n",
      "acc for Lsat= 0.0644907815588845 \n",
      "acc for Psat= 0.08865506897370021 \n",
      "acc for optim= 0.1298964676887004\n",
      "Epoch:464/1000\n",
      "Loss on train= 0.011941850185394287\n",
      "Loss on test= 0.012901713140308857\n",
      "acc for Lsat= 0.06677357289526198 \n",
      "acc for Psat= 0.08963319568170443 \n",
      "acc for optim= 0.13034072487304607\n",
      "Epoch:465/1000\n",
      "Loss on train= 0.011937729083001614\n",
      "Loss on test= 0.013431686908006668\n",
      "acc for Lsat= 0.07228288915422226 \n",
      "acc for Psat= 0.09491196895639104 \n",
      "acc for optim= 0.1316677170702153\n",
      "Epoch:466/1000\n",
      "Loss on train= 0.011664182879030704\n",
      "Loss on test= 0.013343986123800278\n",
      "acc for Lsat= 0.07129701607757144 \n",
      "acc for Psat= 0.09166299137804243 \n",
      "acc for optim= 0.1298756638334857\n",
      "Epoch:467/1000\n",
      "Loss on train= 0.012040319852530956\n",
      "Loss on test= 0.012789377942681313\n",
      "acc for Lsat= 0.06712729516956542 \n",
      "acc for Psat= 0.08653999302122327 \n",
      "acc for optim= 0.13052108312646546\n",
      "Epoch:468/1000\n",
      "Loss on train= 0.011749603785574436\n",
      "Loss on test= 0.012513857334852219\n",
      "acc for Lsat= 0.06931887633270688 \n",
      "acc for Psat= 0.08523815443946256 \n",
      "acc for optim= 0.12988049681815833\n",
      "Epoch:469/1000\n",
      "Loss on train= 0.012388077564537525\n",
      "Loss on test= 0.013740588910877705\n",
      "acc for Lsat= 0.0713528909617 \n",
      "acc for Psat= 0.09414276704192162 \n",
      "acc for optim= 0.1315811802322666\n",
      "Epoch:470/1000\n",
      "Loss on train= 0.011835896410048008\n",
      "Loss on test= 0.013262424618005753\n",
      "acc for Lsat= 0.07163871808184517 \n",
      "acc for Psat= 0.10205166505442725 \n",
      "acc for optim= 0.13014148461321992\n",
      "Epoch:471/1000\n",
      "Loss on train= 0.011830529198050499\n",
      "Loss on test= 0.012681219726800919\n",
      "acc for Lsat= 0.0692832536995411 \n",
      "acc for Psat= 0.09465400824944178 \n",
      "acc for optim= 0.13065080518523853\n",
      "Epoch:472/1000\n",
      "Loss on train= 0.011825677938759327\n",
      "Loss on test= 0.013071838766336441\n",
      "acc for Lsat= 0.07269892493883769 \n",
      "acc for Psat= 0.08426140203244156 \n",
      "acc for optim= 0.13090616456336446\n",
      "Epoch:473/1000\n",
      "Loss on train= 0.011705324985086918\n",
      "Loss on test= 0.01241478230804205\n",
      "acc for Lsat= 0.06303069591522217 \n",
      "acc for Psat= 0.088887341072162 \n",
      "acc for optim= 0.13082334724151426\n",
      "Epoch:474/1000\n",
      "Loss on train= 0.011612633243203163\n",
      "Loss on test= 0.013419914990663528\n",
      "acc for Lsat= 0.06546582099464206 \n",
      "acc for Psat= 0.08599582446946037 \n",
      "acc for optim= 0.12937793934510813\n",
      "Epoch:475/1000\n",
      "Loss on train= 0.011451724916696548\n",
      "Loss on test= 0.013923780992627144\n",
      "acc for Lsat= 0.06913917147450976 \n",
      "acc for Psat= 0.0974396068188879 \n",
      "acc for optim= 0.13021758858942326\n",
      "Epoch:476/1000\n",
      "Loss on train= 0.011327069252729416\n",
      "Loss on test= 0.013337506912648678\n",
      "acc for Lsat= 0.07478810532225504 \n",
      "acc for Psat= 0.09356092951363988 \n",
      "acc for optim= 0.13279797732830048\n",
      "Epoch:477/1000\n",
      "Loss on train= 0.011874654330313206\n",
      "Loss on test= 0.01410604827105999\n",
      "acc for Lsat= 0.0684911129375299 \n",
      "acc for Psat= 0.08675323596431149 \n",
      "acc for optim= 0.1305937006448706\n",
      "Epoch:478/1000\n",
      "Loss on train= 0.0117638586089015\n",
      "Loss on test= 0.012725291773676872\n",
      "acc for Lsat= 0.06976895125375854 \n",
      "acc for Psat= 0.09012317996886039 \n",
      "acc for optim= 0.13239997459782493\n",
      "Epoch:479/1000\n",
      "Loss on train= 0.011913210153579712\n",
      "Loss on test= 0.013090673834085464\n",
      "acc for Lsat= 0.07052888207965428 \n",
      "acc for Psat= 0.08231220642725627 \n",
      "acc for optim= 0.13219928433083825\n",
      "Epoch:480/1000\n",
      "Loss on train= 0.011959251016378403\n",
      "Loss on test= 0.01276929397135973\n",
      "acc for Lsat= 0.06949256410201392 \n",
      "acc for Psat= 0.09535024555193056 \n",
      "acc for optim= 0.1286044942525526\n",
      "Epoch:481/1000\n",
      "Loss on train= 0.012032192200422287\n",
      "Loss on test= 0.01369286235421896\n",
      "acc for Lsat= 0.07177530618177519 \n",
      "acc for Psat= 0.09066641115479998 \n",
      "acc for optim= 0.1298436890459723\n",
      "Epoch:482/1000\n",
      "Loss on train= 0.011333055794239044\n",
      "Loss on test= 0.013981910422444344\n",
      "acc for Lsat= 0.06767019869552719 \n",
      "acc for Psat= 0.09101802967488765 \n",
      "acc for optim= 0.1296346308870448\n",
      "Epoch:483/1000\n",
      "Loss on train= 0.011490225791931152\n",
      "Loss on test= 0.013338527642190456\n",
      "acc for Lsat= 0.07122316426701016 \n",
      "acc for Psat= 0.08667644328541224 \n",
      "acc for optim= 0.13167011001043852\n",
      "Epoch:484/1000\n",
      "Loss on train= 0.012294062413275242\n",
      "Loss on test= 0.013620167970657349\n",
      "acc for Lsat= 0.06364213592476314 \n",
      "acc for Psat= 0.0856139991018507 \n",
      "acc for optim= 0.1307788575585518\n",
      "Epoch:485/1000\n",
      "Loss on train= 0.011514782905578613\n",
      "Loss on test= 0.013691650703549385\n",
      "acc for Lsat= 0.07419928676552243 \n",
      "acc for Psat= 0.0943855445418093 \n",
      "acc for optim= 0.130901662984656\n",
      "Epoch:486/1000\n",
      "Loss on train= 0.011652711778879166\n",
      "Loss on test= 0.013448519632220268\n",
      "acc for Lsat= 0.06990531699524985 \n",
      "acc for Psat= 0.08848138923446339 \n",
      "acc for optim= 0.1301741379416651\n",
      "Epoch:487/1000\n",
      "Loss on train= 0.011866237968206406\n",
      "Loss on test= 0.01267838105559349\n",
      "acc for Lsat= 0.07460983064439562 \n",
      "acc for Psat= 0.09337119625674355 \n",
      "acc for optim= 0.13083413797948096\n",
      "Epoch:488/1000\n",
      "Loss on train= 0.011522021144628525\n",
      "Loss on test= 0.013439241796731949\n",
      "acc for Lsat= 0.06622444672716989 \n",
      "acc for Psat= 0.09574034553435114 \n",
      "acc for optim= 0.13193395404766\n",
      "Epoch:489/1000\n",
      "Loss on train= 0.011330261826515198\n",
      "Loss on test= 0.014321979135274887\n",
      "acc for Lsat= 0.07934953818718592 \n",
      "acc for Psat= 0.1141169665588273 \n",
      "acc for optim= 0.12848284846792618\n",
      "Epoch:490/1000\n",
      "Loss on train= 0.011455055326223373\n",
      "Loss on test= 0.013658232055604458\n",
      "acc for Lsat= 0.0821000200178888 \n",
      "acc for Psat= 0.10160681042406294 \n",
      "acc for optim= 0.12950947280559275\n",
      "Epoch:491/1000\n",
      "Loss on train= 0.011892667971551418\n",
      "Loss on test= 0.013965217396616936\n",
      "acc for Lsat= 0.07270640631516774 \n",
      "acc for Psat= 0.09613970186975268 \n",
      "acc for optim= 0.1294171556002564\n",
      "Epoch:492/1000\n",
      "Loss on train= 0.01157845463603735\n",
      "Loss on test= 0.013078211806714535\n",
      "acc for Lsat= 0.07425977306233511 \n",
      "acc for Psat= 0.0966560354663266 \n",
      "acc for optim= 0.12955850549042222\n",
      "Epoch:493/1000\n",
      "Loss on train= 0.011615320108830929\n",
      "Loss on test= 0.013920108787715435\n",
      "acc for Lsat= 0.07799653824832704 \n",
      "acc for Psat= 0.0976089502374331 \n",
      "acc for optim= 0.1335460595993532\n",
      "Epoch:494/1000\n",
      "Loss on train= 0.012017392553389072\n",
      "Loss on test= 0.013641797006130219\n",
      "acc for Lsat= 0.07102681845426559 \n",
      "acc for Psat= 0.09420552402734755 \n",
      "acc for optim= 0.12822278281673788\n",
      "Epoch:495/1000\n",
      "Loss on train= 0.011144880205392838\n",
      "Loss on test= 0.013158895075321198\n",
      "acc for Lsat= 0.07198268332415156 \n",
      "acc for Psat= 0.09478247770004802 \n",
      "acc for optim= 0.1291218986113866\n",
      "Epoch:496/1000\n",
      "Loss on train= 0.01193435862660408\n",
      "Loss on test= 0.012835098430514336\n",
      "acc for Lsat= 0.0707438189122412 \n",
      "acc for Psat= 0.10432771527104906 \n",
      "acc for optim= 0.12740069600856968\n",
      "Epoch:497/1000\n",
      "Loss on train= 0.011547321453690529\n",
      "Loss on test= 0.012361097149550915\n",
      "acc for Lsat= 0.06664621101485357 \n",
      "acc for Psat= 0.09867049223846859 \n",
      "acc for optim= 0.12967043269632592\n",
      "Epoch:498/1000\n",
      "Loss on train= 0.011890499852597713\n",
      "Loss on test= 0.01247934065759182\n",
      "acc for Lsat= 0.06680904544062086 \n",
      "acc for Psat= 0.08937029052111838 \n",
      "acc for optim= 0.1301522442036205\n",
      "Epoch:499/1000\n",
      "Loss on train= 0.01119740679860115\n",
      "Loss on test= 0.013555859215557575\n",
      "acc for Lsat= 0.08041492071416644 \n",
      "acc for Psat= 0.1031465427743064 \n",
      "acc for optim= 0.1296450475230813\n",
      "Epoch:500/1000\n",
      "Loss on train= 0.011429566890001297\n",
      "Loss on test= 0.012360555119812489\n",
      "acc for Lsat= 0.06127621622549165 \n",
      "acc for Psat= 0.09048670041892264 \n",
      "acc for optim= 0.1294412832707167\n",
      "Epoch:501/1000\n",
      "Loss on train= 0.011401096358895302\n",
      "Loss on test= 0.013838721439242363\n",
      "acc for Lsat= 0.07532557762331432 \n",
      "acc for Psat= 0.09123603933387332 \n",
      "acc for optim= 0.12838109588353996\n",
      "Epoch:502/1000\n",
      "Loss on train= 0.011593355797231197\n",
      "Loss on test= 0.013278339989483356\n",
      "acc for Lsat= 0.07456706232494778 \n",
      "acc for Psat= 0.09457754732833969 \n",
      "acc for optim= 0.13096473312212362\n",
      "Epoch:503/1000\n",
      "Loss on train= 0.011400127783417702\n",
      "Loss on test= 0.012239795178174973\n",
      "acc for Lsat= 0.07212441547049417 \n",
      "acc for Psat= 0.08700199738765758 \n",
      "acc for optim= 0.13255913824670848\n",
      "Epoch:504/1000\n",
      "Loss on train= 0.011574635282158852\n",
      "Loss on test= 0.014063581824302673\n",
      "acc for Lsat= 0.07121403300099903 \n",
      "acc for Psat= 0.08566942248079512 \n",
      "acc for optim= 0.13198632672429084\n",
      "Epoch:505/1000\n",
      "Loss on train= 0.01136105414479971\n",
      "Loss on test= 0.013474982231855392\n",
      "acc for Lsat= 0.0696176626616054 \n",
      "acc for Psat= 0.09102465063333512 \n",
      "acc for optim= 0.12809674238993063\n",
      "Epoch:506/1000\n",
      "Loss on train= 0.011264235712587833\n",
      "Loss on test= 0.01238795556128025\n",
      "acc for Lsat= 0.07474454161193636 \n",
      "acc for Psat= 0.08473699854479896 \n",
      "acc for optim= 0.12951091318908664\n",
      "Epoch:507/1000\n",
      "Loss on train= 0.011420398019254208\n",
      "Loss on test= 0.012793815694749355\n",
      "acc for Lsat= 0.06603323908315765 \n",
      "acc for Psat= 0.08344256621268059 \n",
      "acc for optim= 0.13361264214747479\n",
      "Epoch:508/1000\n",
      "Loss on train= 0.011239136569201946\n",
      "Loss on test= 0.01276363804936409\n",
      "acc for Lsat= 0.06614540500773324 \n",
      "acc for Psat= 0.083727417099807 \n",
      "acc for optim= 0.13032525512907242\n",
      "Epoch:509/1000\n",
      "Loss on train= 0.010725772939622402\n",
      "Loss on test= 0.012441675178706646\n",
      "acc for Lsat= 0.06635238213671578 \n",
      "acc for Psat= 0.08302925841675864 \n",
      "acc for optim= 0.1304469272701277\n",
      "Epoch:510/1000\n",
      "Loss on train= 0.011597717180848122\n",
      "Loss on test= 0.012560557574033737\n",
      "acc for Lsat= 0.07058901919258967 \n",
      "acc for Psat= 0.090240169233746 \n",
      "acc for optim= 0.12942578394949023\n",
      "Epoch:511/1000\n",
      "Loss on train= 0.011442890390753746\n",
      "Loss on test= 0.012219834141433239\n",
      "acc for Lsat= 0.0669443580839369 \n",
      "acc for Psat= 0.088403083384037 \n",
      "acc for optim= 0.12912806009666786\n",
      "Epoch:512/1000\n",
      "Loss on train= 0.01109777856618166\n",
      "Loss on test= 0.012594982050359249\n",
      "acc for Lsat= 0.06486035601960288 \n",
      "acc for Psat= 0.08703337320023113 \n",
      "acc for optim= 0.1278347257628209\n",
      "Epoch:513/1000\n",
      "Loss on train= 0.0116191441193223\n",
      "Loss on test= 0.01355187501758337\n",
      "acc for Lsat= 0.08628021619386142 \n",
      "acc for Psat= 0.08600993777314823 \n",
      "acc for optim= 0.13142382721934054\n",
      "Epoch:514/1000\n",
      "Loss on train= 0.011397979222238064\n",
      "Loss on test= 0.012857611291110516\n",
      "acc for Lsat= 0.06732701675759421 \n",
      "acc for Psat= 0.0936794951558113 \n",
      "acc for optim= 0.12940787790964048\n",
      "Epoch:515/1000\n",
      "Loss on train= 0.011685574427247047\n",
      "Loss on test= 0.013309390284121037\n",
      "acc for Lsat= 0.07884507642851937 \n",
      "acc for Psat= 0.10005866487820944 \n",
      "acc for optim= 0.13139586943305204\n",
      "Epoch:516/1000\n",
      "Loss on train= 0.011700475588440895\n",
      "Loss on test= 0.013446260243654251\n",
      "acc for Lsat= 0.06744437954492039 \n",
      "acc for Psat= 0.09022226673033501 \n",
      "acc for optim= 0.13099140135778323\n",
      "Epoch:517/1000\n",
      "Loss on train= 0.011164910160005093\n",
      "Loss on test= 0.013325618579983711\n",
      "acc for Lsat= 0.07061511932147874 \n",
      "acc for Psat= 0.0840569286607206 \n",
      "acc for optim= 0.13035752827094663\n",
      "Epoch:518/1000\n",
      "Loss on train= 0.011362556368112564\n",
      "Loss on test= 0.012617184780538082\n",
      "acc for Lsat= 0.06985263393984902 \n",
      "acc for Psat= 0.08253873665299682 \n",
      "acc for optim= 0.1288650572610398\n",
      "Epoch:519/1000\n",
      "Loss on train= 0.011331741698086262\n",
      "Loss on test= 0.012372228316962719\n",
      "acc for Lsat= 0.06896746340725157 \n",
      "acc for Psat= 0.09552509172095193 \n",
      "acc for optim= 0.1314718379742569\n",
      "Epoch:520/1000\n",
      "Loss on train= 0.011429519392549992\n",
      "Loss on test= 0.013059965334832668\n",
      "acc for Lsat= 0.06454986524250773 \n",
      "acc for Psat= 0.08528941017058161 \n",
      "acc for optim= 0.12829010147187442\n",
      "Epoch:521/1000\n",
      "Loss on train= 0.01098565198481083\n",
      "Loss on test= 0.014582212083041668\n",
      "acc for Lsat= 0.07203552573919295 \n",
      "acc for Psat= 0.1035583103696505 \n",
      "acc for optim= 0.1316118700429797\n",
      "Epoch:522/1000\n",
      "Loss on train= 0.011119484901428223\n",
      "Loss on test= 0.012839422561228275\n",
      "acc for Lsat= 0.07157677693499459 \n",
      "acc for Psat= 0.09171669317616357 \n",
      "acc for optim= 0.13170310113475553\n",
      "Epoch:523/1000\n",
      "Loss on train= 0.011201925575733185\n",
      "Loss on test= 0.013770516961812973\n",
      "acc for Lsat= 0.07088848948478699 \n",
      "acc for Psat= 0.09907604042026733 \n",
      "acc for optim= 0.1299860167110132\n",
      "Epoch:524/1000\n",
      "Loss on train= 0.011185306124389172\n",
      "Loss on test= 0.013301433064043522\n",
      "acc for Lsat= 0.06543406347433726 \n",
      "acc for Psat= 0.09268142382303873 \n",
      "acc for optim= 0.13021592144957844\n",
      "Epoch:525/1000\n",
      "Loss on train= 0.01113832090049982\n",
      "Loss on test= 0.012483562342822552\n",
      "acc for Lsat= 0.07378795875443352 \n",
      "acc for Psat= 0.09084719849957362 \n",
      "acc for optim= 0.13308919799617594\n",
      "Epoch:526/1000\n",
      "Loss on train= 0.011045324616134167\n",
      "Loss on test= 0.013829058967530727\n",
      "acc for Lsat= 0.07002238829930624 \n",
      "acc for Psat= 0.08618156545692018 \n",
      "acc for optim= 0.13066657322148484\n",
      "Epoch:527/1000\n",
      "Loss on train= 0.011607090011239052\n",
      "Loss on test= 0.013064844533801079\n",
      "acc for Lsat= 0.06735264625814225 \n",
      "acc for Psat= 0.08582805453075303 \n",
      "acc for optim= 0.1315732071797053\n",
      "Epoch:528/1000\n",
      "Loss on train= 0.011078194715082645\n",
      "Loss on test= 0.013648442924022675\n",
      "acc for Lsat= 0.07058844284878836 \n",
      "acc for Psat= 0.09254852185646695 \n",
      "acc for optim= 0.13111947702450885\n",
      "Epoch:529/1000\n",
      "Loss on train= 0.011484503746032715\n",
      "Loss on test= 0.012399576604366302\n",
      "acc for Lsat= 0.067910279168023 \n",
      "acc for Psat= 0.09143706179327435 \n",
      "acc for optim= 0.12784711178650873\n",
      "Epoch:530/1000\n",
      "Loss on train= 0.011315601877868176\n",
      "Loss on test= 0.012942373752593994\n",
      "acc for Lsat= 0.06528522753053241 \n",
      "acc for Psat= 0.08452175474829143 \n",
      "acc for optim= 0.12950561734744243\n",
      "Epoch:531/1000\n",
      "Loss on train= 0.011377519927918911\n",
      "Loss on test= 0.013236661441624165\n",
      "acc for Lsat= 0.06194410680068864 \n",
      "acc for Psat= 0.08425793564981886 \n",
      "acc for optim= 0.12902266472164126\n",
      "Epoch:532/1000\n",
      "Loss on train= 0.01131152268499136\n",
      "Loss on test= 0.013713864609599113\n",
      "acc for Lsat= 0.06332024219963286 \n",
      "acc for Psat= 0.0819337449140019 \n",
      "acc for optim= 0.13151596794939704\n",
      "Epoch:533/1000\n",
      "Loss on train= 0.011409400962293148\n",
      "Loss on test= 0.013016324490308762\n",
      "acc for Lsat= 0.06258185514145427 \n",
      "acc for Psat= 0.08435512847370571 \n",
      "acc for optim= 0.13035777000089485\n",
      "Epoch:534/1000\n",
      "Loss on train= 0.011054586619138718\n",
      "Loss on test= 0.013454470783472061\n",
      "acc for Lsat= 0.06810083786646524 \n",
      "acc for Psat= 0.08909466076228353 \n",
      "acc for optim= 0.13028395906504658\n",
      "Epoch:535/1000\n",
      "Loss on train= 0.01139040570706129\n",
      "Loss on test= 0.013132084161043167\n",
      "acc for Lsat= 0.07084991981585821 \n",
      "acc for Psat= 0.0881641680167781 \n",
      "acc for optim= 0.12899075983118058\n",
      "Epoch:536/1000\n",
      "Loss on train= 0.011291809380054474\n",
      "Loss on test= 0.013490772806107998\n",
      "acc for Lsat= 0.06764162513944838 \n",
      "acc for Psat= 0.08936160529653231 \n",
      "acc for optim= 0.13094534240663053\n",
      "Epoch:537/1000\n",
      "Loss on train= 0.010450501926243305\n",
      "Loss on test= 0.013425738550722599\n",
      "acc for Lsat= 0.06403655194573932 \n",
      "acc for Psat= 0.08859563817580542 \n",
      "acc for optim= 0.13035913422289822\n",
      "Epoch:538/1000\n",
      "Loss on train= 0.011182853952050209\n",
      "Loss on test= 0.014395715668797493\n",
      "acc for Lsat= 0.07323183616002402 \n",
      "acc for Psat= 0.09678978770971298 \n",
      "acc for optim= 0.12907894552788804\n",
      "Epoch:539/1000\n",
      "Loss on train= 0.010808759368956089\n",
      "Loss on test= 0.013157431036233902\n",
      "acc for Lsat= 0.06267881542444229 \n",
      "acc for Psat= 0.08877555570668645 \n",
      "acc for optim= 0.12992047524700565\n",
      "Epoch:540/1000\n",
      "Loss on train= 0.01068355143070221\n",
      "Loss on test= 0.012738925404846668\n",
      "acc for Lsat= 0.0663167796201176 \n",
      "acc for Psat= 0.09335137489769195 \n",
      "acc for optim= 0.12898271295966374\n",
      "Epoch:541/1000\n",
      "Loss on train= 0.010926672257483006\n",
      "Loss on test= 0.013452316634356976\n",
      "acc for Lsat= 0.06649611973100239 \n",
      "acc for Psat= 0.08557105991575453 \n",
      "acc for optim= 0.12903884115318456\n",
      "Epoch:542/1000\n",
      "Loss on train= 0.011005882173776627\n",
      "Loss on test= 0.01317419484257698\n",
      "acc for Lsat= 0.07939849131637149 \n",
      "acc for Psat= 0.09249276862376266 \n",
      "acc for optim= 0.13283135518431663\n",
      "Epoch:543/1000\n",
      "Loss on train= 0.011541263200342655\n",
      "Loss on test= 0.012573259882628918\n",
      "acc for Lsat= 0.0712188192539745 \n",
      "acc for Psat= 0.08761482586463293 \n",
      "acc for optim= 0.13150986275739138\n",
      "Epoch:544/1000\n",
      "Loss on train= 0.01098533533513546\n",
      "Loss on test= 0.013423091731965542\n",
      "acc for Lsat= 0.07324321269989013 \n",
      "acc for Psat= 0.08571399640705851 \n",
      "acc for optim= 0.13334618312203222\n",
      "Epoch:545/1000\n",
      "Loss on train= 0.01104542426764965\n",
      "Loss on test= 0.012803196907043457\n",
      "acc for Lsat= 0.07064796686172486 \n",
      "acc for Psat= 0.09374859912527932 \n",
      "acc for optim= 0.13003617655485866\n",
      "Epoch:546/1000\n",
      "Loss on train= 0.01115293800830841\n",
      "Loss on test= 0.013418191112577915\n",
      "acc for Lsat= 0.07064320461617575 \n",
      "acc for Psat= 0.09628288737601703 \n",
      "acc for optim= 0.13142713726394706\n",
      "Epoch:547/1000\n",
      "Loss on train= 0.010814637877047062\n",
      "Loss on test= 0.013288930989801884\n",
      "acc for Lsat= 0.07368387613031599 \n",
      "acc for Psat= 0.09501264467835428 \n",
      "acc for optim= 0.13067457030216853\n",
      "Epoch:548/1000\n",
      "Loss on train= 0.010771320201456547\n",
      "Loss on test= 0.012933937832713127\n",
      "acc for Lsat= 0.06635953552193113 \n",
      "acc for Psat= 0.0903306397298972 \n",
      "acc for optim= 0.1293845853561329\n",
      "Epoch:549/1000\n",
      "Loss on train= 0.011330129578709602\n",
      "Loss on test= 0.013409489765763283\n",
      "acc for Lsat= 0.07020671905742752 \n",
      "acc for Psat= 0.09235278682576285 \n",
      "acc for optim= 0.13130774262050784\n",
      "Epoch:550/1000\n",
      "Loss on train= 0.011103671975433826\n",
      "Loss on test= 0.013652604073286057\n",
      "acc for Lsat= 0.0720419380399916 \n",
      "acc for Psat= 0.08882320514983601 \n",
      "acc for optim= 0.13005997284005089\n",
      "Epoch:551/1000\n",
      "Loss on train= 0.010699800215661526\n",
      "Loss on test= 0.013083417899906635\n",
      "acc for Lsat= 0.07429331276151868 \n",
      "acc for Psat= 0.08686292270819346 \n",
      "acc for optim= 0.13189436280065112\n",
      "Epoch:552/1000\n",
      "Loss on train= 0.011058053933084011\n",
      "Loss on test= 0.013121701776981354\n",
      "acc for Lsat= 0.06383004006412293 \n",
      "acc for Psat= 0.08910535325606665 \n",
      "acc for optim= 0.13012757369627556\n",
      "Epoch:553/1000\n",
      "Loss on train= 0.011016001924872398\n",
      "Loss on test= 0.012990291230380535\n",
      "acc for Lsat= 0.068005284998152 \n",
      "acc for Psat= 0.09170371914903323 \n",
      "acc for optim= 0.13136542468435233\n",
      "Epoch:554/1000\n",
      "Loss on train= 0.01067922543734312\n",
      "Loss on test= 0.013353589922189713\n",
      "acc for Lsat= 0.07636588745647006 \n",
      "acc for Psat= 0.10664671311775842 \n",
      "acc for optim= 0.13017971635692652\n",
      "Epoch:555/1000\n",
      "Loss on train= 0.010558852925896645\n",
      "Loss on test= 0.013098198920488358\n",
      "acc for Lsat= 0.07116244170400832 \n",
      "acc for Psat= 0.08689058787292904 \n",
      "acc for optim= 0.1302264804641406\n",
      "Epoch:556/1000\n",
      "Loss on train= 0.010549945756793022\n",
      "Loss on test= 0.014059338718652725\n",
      "acc for Lsat= 0.06469122750891582 \n",
      "acc for Psat= 0.0944557726383209 \n",
      "acc for optim= 0.130343071402361\n",
      "Epoch:557/1000\n",
      "Loss on train= 0.011232995428144932\n",
      "Loss on test= 0.013193456456065178\n",
      "acc for Lsat= 0.0675281474987666 \n",
      "acc for Psat= 0.09146899291210704 \n",
      "acc for optim= 0.13123286031186585\n",
      "Epoch:558/1000\n",
      "Loss on train= 0.010703036561608315\n",
      "Loss on test= 0.013058505952358246\n",
      "acc for Lsat= 0.063932287444671 \n",
      "acc for Psat= 0.09522108518415026 \n",
      "acc for optim= 0.12964112241235046\n",
      "Epoch:559/1000\n",
      "Loss on train= 0.010892149992287159\n",
      "Loss on test= 0.012941109947860241\n",
      "acc for Lsat= 0.06257270011636945 \n",
      "acc for Psat= 0.09244920859734217 \n",
      "acc for optim= 0.13298180115719635\n",
      "Epoch:560/1000\n",
      "Loss on train= 0.010656396858394146\n",
      "Loss on test= 0.013414640910923481\n",
      "acc for Lsat= 0.0609448991715908 \n",
      "acc for Psat= 0.08849613244334857 \n",
      "acc for optim= 0.13322019891606435\n",
      "Epoch:561/1000\n",
      "Loss on train= 0.01102479174733162\n",
      "Loss on test= 0.013821926899254322\n",
      "acc for Lsat= 0.06488243689139683 \n",
      "acc for Psat= 0.08505787087811363 \n",
      "acc for optim= 0.12974341035717066\n",
      "Epoch:562/1000\n",
      "Loss on train= 0.011012866161763668\n",
      "Loss on test= 0.01315154880285263\n",
      "acc for Lsat= 0.06637067794799806 \n",
      "acc for Psat= 0.08503995512922605 \n",
      "acc for optim= 0.1317480206075642\n",
      "Epoch:563/1000\n",
      "Loss on train= 0.010606173425912857\n",
      "Loss on test= 0.013853645883500576\n",
      "acc for Lsat= 0.0684840225511127 \n",
      "acc for Psat= 0.08551940926247173 \n",
      "acc for optim= 0.13210061515371005\n",
      "Epoch:564/1000\n",
      "Loss on train= 0.010761711746454239\n",
      "Loss on test= 0.014067383483052254\n",
      "acc for Lsat= 0.07023192809687721 \n",
      "acc for Psat= 0.08339414408999599 \n",
      "acc for optim= 0.13310859178503354\n",
      "Epoch:565/1000\n",
      "Loss on train= 0.01100130658596754\n",
      "Loss on test= 0.014132719486951828\n",
      "acc for Lsat= 0.06595011759135459 \n",
      "acc for Psat= 0.08465318762593797 \n",
      "acc for optim= 0.1326857394228379\n",
      "Epoch:566/1000\n",
      "Loss on train= 0.011096647940576077\n",
      "Loss on test= 0.013423019088804722\n",
      "acc for Lsat= 0.06836863847242461 \n",
      "acc for Psat= 0.09674571338627073 \n",
      "acc for optim= 0.1321407348331478\n",
      "Epoch:567/1000\n",
      "Loss on train= 0.01066572591662407\n",
      "Loss on test= 0.013471705839037895\n",
      "acc for Lsat= 0.06754306728641193 \n",
      "acc for Psat= 0.08976931869983672 \n",
      "acc for optim= 0.13107227798965237\n",
      "Epoch:568/1000\n",
      "Loss on train= 0.01070142351090908\n",
      "Loss on test= 0.013106586411595345\n",
      "acc for Lsat= 0.0709766278664271 \n",
      "acc for Psat= 0.09227838582462734 \n",
      "acc for optim= 0.13056968907929128\n",
      "Epoch:569/1000\n",
      "Loss on train= 0.010554692707955837\n",
      "Loss on test= 0.013386539183557034\n",
      "acc for Lsat= 0.07167752848731146 \n",
      "acc for Psat= 0.09472460341122414 \n",
      "acc for optim= 0.13415158515175182\n",
      "Epoch:570/1000\n",
      "Loss on train= 0.011058995500206947\n",
      "Loss on test= 0.013293267227709293\n",
      "acc for Lsat= 0.06315904201732742 \n",
      "acc for Psat= 0.08219162383013301 \n",
      "acc for optim= 0.13243426101075279\n",
      "Epoch:571/1000\n",
      "Loss on train= 0.010902170091867447\n",
      "Loss on test= 0.013880014419555664\n",
      "acc for Lsat= 0.06403533203734292 \n",
      "acc for Psat= 0.08120548581290576 \n",
      "acc for optim= 0.13098641923732227\n",
      "Epoch:572/1000\n",
      "Loss on train= 0.010491663590073586\n",
      "Loss on test= 0.013344528153538704\n",
      "acc for Lsat= 0.07021402832534578 \n",
      "acc for Psat= 0.0855997702313794 \n",
      "acc for optim= 0.13034241319530537\n",
      "Epoch:573/1000\n",
      "Loss on train= 0.011236852966248989\n",
      "Loss on test= 0.013281906954944134\n",
      "acc for Lsat= 0.06592623367905617 \n",
      "acc for Psat= 0.0798986384438144 \n",
      "acc for optim= 0.13228683455122844\n",
      "Epoch:574/1000\n",
      "Loss on train= 0.010230735875666142\n",
      "Loss on test= 0.01331314817070961\n",
      "acc for Lsat= 0.06334372286995253 \n",
      "acc for Psat= 0.08508630353543493 \n",
      "acc for optim= 0.12955425706588558\n",
      "Epoch:575/1000\n",
      "Loss on train= 0.010555238462984562\n",
      "Loss on test= 0.012797296978533268\n",
      "acc for Lsat= 0.06560405592123668 \n",
      "acc for Psat= 0.08753698335753546 \n",
      "acc for optim= 0.12945423780216112\n",
      "Epoch:576/1000\n",
      "Loss on train= 0.010762235149741173\n",
      "Loss on test= 0.013638336211442947\n",
      "acc for Lsat= 0.06459037214517593 \n",
      "acc for Psat= 0.08212718918091721 \n",
      "acc for optim= 0.13306739955312674\n",
      "Epoch:577/1000\n",
      "Loss on train= 0.010373280383646488\n",
      "Loss on test= 0.012925195507705212\n",
      "acc for Lsat= 0.06868371748261981 \n",
      "acc for Psat= 0.08957083250085512 \n",
      "acc for optim= 0.13298424229853684\n",
      "Epoch:578/1000\n",
      "Loss on train= 0.010478150099515915\n",
      "Loss on test= 0.013827580958604813\n",
      "acc for Lsat= 0.064426313009527 \n",
      "acc for Psat= 0.08497931779258781 \n",
      "acc for optim= 0.1302957957817448\n",
      "Epoch:579/1000\n",
      "Loss on train= 0.010630584321916103\n",
      "Loss on test= 0.013766195625066757\n",
      "acc for Lsat= 0.06488870912128025 \n",
      "acc for Psat= 0.08359507690701219 \n",
      "acc for optim= 0.13158192162712415\n",
      "Epoch:580/1000\n",
      "Loss on train= 0.010390515439212322\n",
      "Loss on test= 0.012649105861783028\n",
      "acc for Lsat= 0.06476015647252402 \n",
      "acc for Psat= 0.0836825860457288 \n",
      "acc for optim= 0.13268926226430466\n",
      "Epoch:581/1000\n",
      "Loss on train= 0.010813700966536999\n",
      "Loss on test= 0.013473402708768845\n",
      "acc for Lsat= 0.0656981239716212 \n",
      "acc for Psat= 0.08647289615538385 \n",
      "acc for optim= 0.1332730820402503\n",
      "Epoch:582/1000\n",
      "Loss on train= 0.01061344426125288\n",
      "Loss on test= 0.012837070971727371\n",
      "acc for Lsat= 0.06206127587291929 \n",
      "acc for Psat= 0.08246539375848239 \n",
      "acc for optim= 0.1295693090806405\n",
      "Epoch:583/1000\n",
      "Loss on train= 0.01058235950767994\n",
      "Loss on test= 0.014053543098270893\n",
      "acc for Lsat= 0.07202018010947439 \n",
      "acc for Psat= 0.08566002063453197 \n",
      "acc for optim= 0.1331310940699445\n",
      "Epoch:584/1000\n",
      "Loss on train= 0.010509217157959938\n",
      "Loss on test= 0.013123299926519394\n",
      "acc for Lsat= 0.06181980296969414 \n",
      "acc for Psat= 0.08494406636390422 \n",
      "acc for optim= 0.12987484404196342\n",
      "Epoch:585/1000\n",
      "Loss on train= 0.010451049543917179\n",
      "Loss on test= 0.013055386021733284\n",
      "acc for Lsat= 0.06679871562454437 \n",
      "acc for Psat= 0.0885710620217853 \n",
      "acc for optim= 0.13254425504969225\n",
      "Epoch:586/1000\n",
      "Loss on train= 0.010367176495492458\n",
      "Loss on test= 0.013511493802070618\n",
      "acc for Lsat= 0.0666461479332712 \n",
      "acc for Psat= 0.08501974108318486 \n",
      "acc for optim= 0.13114904260469806\n",
      "Epoch:587/1000\n",
      "Loss on train= 0.01059847790747881\n",
      "Loss on test= 0.014024040661752224\n",
      "acc for Lsat= 0.06407021565569772 \n",
      "acc for Psat= 0.08673158693644736 \n",
      "acc for optim= 0.13262882853547733\n",
      "Epoch:588/1000\n",
      "Loss on train= 0.01078353077173233\n",
      "Loss on test= 0.014044631272554398\n",
      "acc for Lsat= 0.06328745103544658 \n",
      "acc for Psat= 0.08088166260470947 \n",
      "acc for optim= 0.13649212395151455\n",
      "Epoch:589/1000\n",
      "Loss on train= 0.010572293773293495\n",
      "Loss on test= 0.012739378958940506\n",
      "acc for Lsat= 0.06211353027158314 \n",
      "acc for Psat= 0.0836762464708752 \n",
      "acc for optim= 0.13224085577660138\n",
      "Epoch:590/1000\n",
      "Loss on train= 0.010401294566690922\n",
      "Loss on test= 0.014060386456549168\n",
      "acc for Lsat= 0.06961106492413414 \n",
      "acc for Psat= 0.09107285994622442 \n",
      "acc for optim= 0.13322616281608746\n",
      "Epoch:591/1000\n",
      "Loss on train= 0.010688603855669498\n",
      "Loss on test= 0.013876380398869514\n",
      "acc for Lsat= 0.06902013685968188 \n",
      "acc for Psat= 0.09133126677738296 \n",
      "acc for optim= 0.13122991029587058\n",
      "Epoch:592/1000\n",
      "Loss on train= 0.010694872587919235\n",
      "Loss on test= 0.013154111802577972\n",
      "acc for Lsat= 0.06561740305688646 \n",
      "acc for Psat= 0.09071950962146125 \n",
      "acc for optim= 0.13336934012671314\n",
      "Epoch:593/1000\n",
      "Loss on train= 0.010050089098513126\n",
      "Loss on test= 0.012773552909493446\n",
      "acc for Lsat= 0.0719972711470392 \n",
      "acc for Psat= 0.08624896837605371 \n",
      "acc for optim= 0.13159157058431042\n",
      "Epoch:594/1000\n",
      "Loss on train= 0.01063911896198988\n",
      "Loss on test= 0.012702081352472305\n",
      "acc for Lsat= 0.07405189052224159 \n",
      "acc for Psat= 0.0889743192328347 \n",
      "acc for optim= 0.1307121629102363\n",
      "Epoch:595/1000\n",
      "Loss on train= 0.010326323099434376\n",
      "Loss on test= 0.013677556067705154\n",
      "acc for Lsat= 0.06464956353108088 \n",
      "acc for Psat= 0.08504669856694008 \n",
      "acc for optim= 0.13271543540888364\n",
      "Epoch:596/1000\n",
      "Loss on train= 0.010519224219024181\n",
      "Loss on test= 0.01353477593511343\n",
      "acc for Lsat= 0.06724372390243742 \n",
      "acc for Psat= 0.08827789343065685 \n",
      "acc for optim= 0.13232033993634912\n",
      "Epoch:597/1000\n",
      "Loss on train= 0.010700374841690063\n",
      "Loss on test= 0.01368503924459219\n",
      "acc for Lsat= 0.05996116937862502 \n",
      "acc for Psat= 0.08312598475151592 \n",
      "acc for optim= 0.1313024967494938\n",
      "Epoch:598/1000\n",
      "Loss on train= 0.009990904480218887\n",
      "Loss on test= 0.013171537779271603\n",
      "acc for Lsat= 0.06712006156643231 \n",
      "acc for Psat= 0.08781640670365759 \n",
      "acc for optim= 0.12994390266637007\n",
      "Epoch:599/1000\n",
      "Loss on train= 0.010314752347767353\n",
      "Loss on test= 0.01316418219357729\n",
      "acc for Lsat= 0.07345483443803258 \n",
      "acc for Psat= 0.08540749264260133 \n",
      "acc for optim= 0.1329007705466615\n",
      "Epoch:600/1000\n",
      "Loss on train= 0.0104518448933959\n",
      "Loss on test= 0.013088404200971127\n",
      "acc for Lsat= 0.07289786752727297 \n",
      "acc for Psat= 0.08780428080095186 \n",
      "acc for optim= 0.13169660746223394\n",
      "Epoch:601/1000\n",
      "Loss on train= 0.010332953184843063\n",
      "Loss on test= 0.012025767005980015\n",
      "acc for Lsat= 0.06294484494460954 \n",
      "acc for Psat= 0.08589631571537919 \n",
      "acc for optim= 0.13183315890944666\n",
      "Epoch:602/1000\n",
      "Loss on train= 0.010054461658000946\n",
      "Loss on test= 0.012950597330927849\n",
      "acc for Lsat= 0.06639002818200324 \n",
      "acc for Psat= 0.08802226450708178 \n",
      "acc for optim= 0.1333460955984063\n",
      "Epoch:603/1000\n",
      "Loss on train= 0.010294073261320591\n",
      "Loss on test= 0.012971440330147743\n",
      "acc for Lsat= 0.06692458689212798 \n",
      "acc for Psat= 0.08733201610545319 \n",
      "acc for optim= 0.13080835073358482\n",
      "Epoch:604/1000\n",
      "Loss on train= 0.010265510529279709\n",
      "Loss on test= 0.013464579358696938\n",
      "acc for Lsat= 0.07156577143404219 \n",
      "acc for Psat= 0.10433884685238201 \n",
      "acc for optim= 0.13092488778962028\n",
      "Epoch:605/1000\n",
      "Loss on train= 0.010639280080795288\n",
      "Loss on test= 0.013348400592803955\n",
      "acc for Lsat= 0.06158725395798682 \n",
      "acc for Psat= 0.0818044314160943 \n",
      "acc for optim= 0.1351140344308482\n",
      "Epoch:606/1000\n",
      "Loss on train= 0.01078469306230545\n",
      "Loss on test= 0.01305537112057209\n",
      "acc for Lsat= 0.06364565797977978 \n",
      "acc for Psat= 0.09111290565795367 \n",
      "acc for optim= 0.1325633626845148\n",
      "Epoch:607/1000\n",
      "Loss on train= 0.010173381306231022\n",
      "Loss on test= 0.013935810886323452\n",
      "acc for Lsat= 0.05850564680165715 \n",
      "acc for Psat= 0.08934405023852984 \n",
      "acc for optim= 0.13075704638742736\n",
      "Epoch:608/1000\n",
      "Loss on train= 0.010152218863368034\n",
      "Loss on test= 0.013043069280683994\n",
      "acc for Lsat= 0.06406728612879913 \n",
      "acc for Psat= 0.0850004885552658 \n",
      "acc for optim= 0.1305678493446774\n",
      "Epoch:609/1000\n",
      "Loss on train= 0.01011213380843401\n",
      "Loss on test= 0.013734396547079086\n",
      "acc for Lsat= 0.06521790383590591 \n",
      "acc for Psat= 0.08233669981774358 \n",
      "acc for optim= 0.13276579392453036\n",
      "Epoch:610/1000\n",
      "Loss on train= 0.010681509040296078\n",
      "Loss on test= 0.012522674165666103\n",
      "acc for Lsat= 0.07166394657558865 \n",
      "acc for Psat= 0.0952321917646461 \n",
      "acc for optim= 0.13082338211437064\n",
      "Epoch:611/1000\n",
      "Loss on train= 0.010514767840504646\n",
      "Loss on test= 0.013301168568432331\n",
      "acc for Lsat= 0.06101335063576699 \n",
      "acc for Psat= 0.08301173655523193 \n",
      "acc for optim= 0.131653925196992\n",
      "Epoch:612/1000\n",
      "Loss on train= 0.010297337546944618\n",
      "Loss on test= 0.013413455337285995\n",
      "acc for Lsat= 0.07519147478871875 \n",
      "acc for Psat= 0.09198863531152407 \n",
      "acc for optim= 0.13191486472884814\n",
      "Epoch:613/1000\n",
      "Loss on train= 0.010289888828992844\n",
      "Loss on test= 0.014391995035111904\n",
      "acc for Lsat= 0.06278364782532056 \n",
      "acc for Psat= 0.08478764817118646 \n",
      "acc for optim= 0.13419318596522012\n",
      "Epoch:614/1000\n",
      "Loss on train= 0.010217797942459583\n",
      "Loss on test= 0.013225440867245197\n",
      "acc for Lsat= 0.06333980278836356 \n",
      "acc for Psat= 0.0833869887308942 \n",
      "acc for optim= 0.13571537039760087\n",
      "Epoch:615/1000\n",
      "Loss on train= 0.010552928782999516\n",
      "Loss on test= 0.013974481262266636\n",
      "acc for Lsat= 0.0620059072971344 \n",
      "acc for Psat= 0.08332666506369907 \n",
      "acc for optim= 0.13315682253903816\n",
      "Epoch:616/1000\n",
      "Loss on train= 0.009908398613333702\n",
      "Loss on test= 0.014025403186678886\n",
      "acc for Lsat= 0.07134466485844719 \n",
      "acc for Psat= 0.08578339334991243 \n",
      "acc for optim= 0.1333911731011338\n",
      "Epoch:617/1000\n",
      "Loss on train= 0.01025212649255991\n",
      "Loss on test= 0.01294038537889719\n",
      "acc for Lsat= 0.058922275155782695 \n",
      "acc for Psat= 0.08125185531874497 \n",
      "acc for optim= 0.13200387896762955\n",
      "Epoch:618/1000\n",
      "Loss on train= 0.010470694862306118\n",
      "Loss on test= 0.012884820811450481\n",
      "acc for Lsat= 0.06179739841156536 \n",
      "acc for Psat= 0.0827982328625189 \n",
      "acc for optim= 0.13327332759896915\n",
      "Epoch:619/1000\n",
      "Loss on train= 0.009960481896996498\n",
      "Loss on test= 0.013114755973219872\n",
      "acc for Lsat= 0.06269650864932272 \n",
      "acc for Psat= 0.08064914979558023 \n",
      "acc for optim= 0.13365209847688675\n",
      "Epoch:620/1000\n",
      "Loss on train= 0.009866971522569656\n",
      "Loss on test= 0.013636435382068157\n",
      "acc for Lsat= 0.07039492204785346 \n",
      "acc for Psat= 0.08449156197408833 \n",
      "acc for optim= 0.13321028674642246\n",
      "Epoch:621/1000\n",
      "Loss on train= 0.010593428276479244\n",
      "Loss on test= 0.012476606294512749\n",
      "acc for Lsat= 0.0618196582628621 \n",
      "acc for Psat= 0.0823839082900021 \n",
      "acc for optim= 0.13026466077814497\n",
      "Epoch:622/1000\n",
      "Loss on train= 0.010420308448374271\n",
      "Loss on test= 0.01349399983882904\n",
      "acc for Lsat= 0.06204192050629192 \n",
      "acc for Psat= 0.08804344667328728 \n",
      "acc for optim= 0.13299901344709927\n",
      "Epoch:623/1000\n",
      "Loss on train= 0.010080560110509396\n",
      "Loss on test= 0.013145054690539837\n",
      "acc for Lsat= 0.06734600919816229 \n",
      "acc for Psat= 0.08798960567348535 \n",
      "acc for optim= 0.13024513806319896\n",
      "Epoch:624/1000\n",
      "Loss on train= 0.010119262151420116\n",
      "Loss on test= 0.013587354682385921\n",
      "acc for Lsat= 0.06537241165836653 \n",
      "acc for Psat= 0.08952326993975374 \n",
      "acc for optim= 0.13148909645775955\n",
      "Epoch:625/1000\n",
      "Loss on train= 0.010181927122175694\n",
      "Loss on test= 0.012832563370466232\n",
      "acc for Lsat= 0.06070345342159272 \n",
      "acc for Psat= 0.08104385071330601 \n",
      "acc for optim= 0.13280818164348604\n",
      "Epoch:626/1000\n",
      "Loss on train= 0.010382491163909435\n",
      "Loss on test= 0.013117042370140553\n",
      "acc for Lsat= 0.06681532512108485 \n",
      "acc for Psat= 0.08357012456076013 \n",
      "acc for optim= 0.13080927733745842\n",
      "Epoch:627/1000\n",
      "Loss on train= 0.010243267752230167\n",
      "Loss on test= 0.012949235737323761\n",
      "acc for Lsat= 0.06633184949556986 \n",
      "acc for Psat= 0.08745771969358124 \n",
      "acc for optim= 0.13176126360065407\n",
      "Epoch:628/1000\n",
      "Loss on train= 0.010275527834892273\n",
      "Loss on test= 0.012731162831187248\n",
      "acc for Lsat= 0.06621715583735042 \n",
      "acc for Psat= 0.0809739501370738 \n",
      "acc for optim= 0.1304519347432587\n",
      "Epoch:629/1000\n",
      "Loss on train= 0.010489758104085922\n",
      "Loss on test= 0.013636928051710129\n",
      "acc for Lsat= 0.06640690565109254 \n",
      "acc for Psat= 0.09133334424760606 \n",
      "acc for optim= 0.12956986067195733\n",
      "Epoch:630/1000\n",
      "Loss on train= 0.010654336772859097\n",
      "Loss on test= 0.013842843472957611\n",
      "acc for Lsat= 0.06425398083196746 \n",
      "acc for Psat= 0.08980201093686951 \n",
      "acc for optim= 0.1295121807605028\n",
      "Epoch:631/1000\n",
      "Loss on train= 0.00987005140632391\n",
      "Loss on test= 0.013123216107487679\n",
      "acc for Lsat= 0.06460609518819385 \n",
      "acc for Psat= 0.08704932125078307 \n",
      "acc for optim= 0.12980063663174712\n",
      "Epoch:632/1000\n",
      "Loss on train= 0.010738157667219639\n",
      "Loss on test= 0.014182101003825665\n",
      "acc for Lsat= 0.06738473317689365 \n",
      "acc for Psat= 0.08662186720305017 \n",
      "acc for optim= 0.13249725751164887\n",
      "Epoch:633/1000\n",
      "Loss on train= 0.01007727812975645\n",
      "Loss on test= 0.01392986997961998\n",
      "acc for Lsat= 0.06010582066244549 \n",
      "acc for Psat= 0.08318730857637192 \n",
      "acc for optim= 0.13046385053959156\n",
      "Epoch:634/1000\n",
      "Loss on train= 0.010296078398823738\n",
      "Loss on test= 0.013950634747743607\n",
      "acc for Lsat= 0.06854723186956513 \n",
      "acc for Psat= 0.08812712704141935 \n",
      "acc for optim= 0.1293931079407533\n",
      "Epoch:635/1000\n",
      "Loss on train= 0.01015465334057808\n",
      "Loss on test= 0.01454161573201418\n",
      "acc for Lsat= 0.06180828470322822 \n",
      "acc for Psat= 0.08218797747459676 \n",
      "acc for optim= 0.13035945751600794\n",
      "Epoch:636/1000\n",
      "Loss on train= 0.010081766173243523\n",
      "Loss on test= 0.014506523497402668\n",
      "acc for Lsat= 0.059884623934825264 \n",
      "acc for Psat= 0.08848907959957918 \n",
      "acc for optim= 0.1308477174076769\n",
      "Epoch:637/1000\n",
      "Loss on train= 0.010031629353761673\n",
      "Loss on test= 0.01354663074016571\n",
      "acc for Lsat= 0.06696538751324016 \n",
      "acc for Psat= 0.08474376354780462 \n",
      "acc for optim= 0.13297307133260702\n",
      "Epoch:638/1000\n",
      "Loss on train= 0.010281559079885483\n",
      "Loss on test= 0.01421623956412077\n",
      "acc for Lsat= 0.07708520872725382 \n",
      "acc for Psat= 0.09075246395336257 \n",
      "acc for optim= 0.13101176160077257\n",
      "Epoch:639/1000\n",
      "Loss on train= 0.010091456584632397\n",
      "Loss on test= 0.013682650402188301\n",
      "acc for Lsat= 0.06582862635453542 \n",
      "acc for Psat= 0.0826159329464038 \n",
      "acc for optim= 0.13233846293555362\n",
      "Epoch:640/1000\n",
      "Loss on train= 0.009964960627257824\n",
      "Loss on test= 0.014170121401548386\n",
      "acc for Lsat= 0.07163180179066128 \n",
      "acc for Psat= 0.08455243027872508 \n",
      "acc for optim= 0.13349023945629596\n",
      "Epoch:641/1000\n",
      "Loss on train= 0.009973959065973759\n",
      "Loss on test= 0.013315260410308838\n",
      "acc for Lsat= 0.06203618028925525 \n",
      "acc for Psat= 0.081545811249978 \n",
      "acc for optim= 0.13130483134753174\n",
      "Epoch:642/1000\n",
      "Loss on train= 0.010073918849229813\n",
      "Loss on test= 0.012724674306809902\n",
      "acc for Lsat= 0.06549278646707535 \n",
      "acc for Psat= 0.08441978130075665 \n",
      "acc for optim= 0.13063023220747708\n",
      "Epoch:643/1000\n",
      "Loss on train= 0.009904972277581692\n",
      "Loss on test= 0.012874528765678406\n",
      "acc for Lsat= 0.06816644834147559 \n",
      "acc for Psat= 0.0830388172633118 \n",
      "acc for optim= 0.13141472115077907\n",
      "Epoch:644/1000\n",
      "Loss on train= 0.010044588707387447\n",
      "Loss on test= 0.012867824174463749\n",
      "acc for Lsat= 0.06705073085096147 \n",
      "acc for Psat= 0.08700215386019813 \n",
      "acc for optim= 0.1299578119276298\n",
      "Epoch:645/1000\n",
      "Loss on train= 0.010276571847498417\n",
      "Loss on test= 0.013693324290215969\n",
      "acc for Lsat= 0.06251760365234481 \n",
      "acc for Psat= 0.08343643806874752 \n",
      "acc for optim= 0.13155411117606694\n",
      "Epoch:646/1000\n",
      "Loss on train= 0.009654580615460873\n",
      "Loss on test= 0.013801323249936104\n",
      "acc for Lsat= 0.0670321630520953 \n",
      "acc for Psat= 0.0824370799275736 \n",
      "acc for optim= 0.1322962103618516\n",
      "Epoch:647/1000\n",
      "Loss on train= 0.01028579380363226\n",
      "Loss on test= 0.0133609464392066\n",
      "acc for Lsat= 0.06596230930752225 \n",
      "acc for Psat= 0.10142065866125954 \n",
      "acc for optim= 0.13176314607262612\n",
      "Epoch:648/1000\n",
      "Loss on train= 0.009893455542623997\n",
      "Loss on test= 0.014144036918878555\n",
      "acc for Lsat= 0.0608900197678142 \n",
      "acc for Psat= 0.08350527377592193 \n",
      "acc for optim= 0.13255957836906118\n",
      "Epoch:649/1000\n",
      "Loss on train= 0.010031559504568577\n",
      "Loss on test= 0.012681219726800919\n",
      "acc for Lsat= 0.06588767419258754 \n",
      "acc for Psat= 0.09747239674131075 \n",
      "acc for optim= 0.13218405635820496\n",
      "Epoch:650/1000\n",
      "Loss on train= 0.010445553809404373\n",
      "Loss on test= 0.012325827963650227\n",
      "acc for Lsat= 0.06261547770765093 \n",
      "acc for Psat= 0.09078543831904727 \n",
      "acc for optim= 0.13050597202446726\n",
      "Epoch:651/1000\n",
      "Loss on train= 0.009916942566633224\n",
      "Loss on test= 0.014099609106779099\n",
      "acc for Lsat= 0.059326990114318 \n",
      "acc for Psat= 0.08392808354563182 \n",
      "acc for optim= 0.13296820533772313\n",
      "Epoch:652/1000\n",
      "Loss on train= 0.010368681512773037\n",
      "Loss on test= 0.012729737907648087\n",
      "acc for Lsat= 0.06050745199124019 \n",
      "acc for Psat= 0.08187119720710648 \n",
      "acc for optim= 0.13177840246094596\n",
      "Epoch:653/1000\n",
      "Loss on train= 0.009934650734066963\n",
      "Loss on test= 0.013368344865739346\n",
      "acc for Lsat= 0.061600421948565375 \n",
      "acc for Psat= 0.08388165053394105 \n",
      "acc for optim= 0.13061123774904343\n",
      "Epoch:654/1000\n",
      "Loss on train= 0.010174575261771679\n",
      "Loss on test= 0.013559449464082718\n",
      "acc for Lsat= 0.060294887423515325 \n",
      "acc for Psat= 0.08079717751178477 \n",
      "acc for optim= 0.1323119494236178\n",
      "Epoch:655/1000\n",
      "Loss on train= 0.009690954349935055\n",
      "Loss on test= 0.01400582492351532\n",
      "acc for Lsat= 0.05879021891289287 \n",
      "acc for Psat= 0.0823532719165087 \n",
      "acc for optim= 0.1357547966556417\n",
      "Epoch:656/1000\n",
      "Loss on train= 0.01013717520982027\n",
      "Loss on test= 0.013145733624696732\n",
      "acc for Lsat= 0.06050321004456944 \n",
      "acc for Psat= 0.07925037297730646 \n",
      "acc for optim= 0.13129169599463542\n",
      "Epoch:657/1000\n",
      "Loss on train= 0.009774569422006607\n",
      "Loss on test= 0.01396500039845705\n",
      "acc for Lsat= 0.06580421345101463 \n",
      "acc for Psat= 0.09032105886273915 \n",
      "acc for optim= 0.1329667462449935\n",
      "Epoch:658/1000\n",
      "Loss on train= 0.009843584150075912\n",
      "Loss on test= 0.012995574623346329\n",
      "acc for Lsat= 0.07313340389066272 \n",
      "acc for Psat= 0.0879345488217142 \n",
      "acc for optim= 0.13203235535571972\n",
      "Epoch:659/1000\n",
      "Loss on train= 0.009738232009112835\n",
      "Loss on test= 0.013924641534686089\n",
      "acc for Lsat= 0.06384115897946889 \n",
      "acc for Psat= 0.08063039427830114 \n",
      "acc for optim= 0.13089088449875513\n",
      "Epoch:660/1000\n",
      "Loss on train= 0.009690855629742146\n",
      "Loss on test= 0.013092512264847755\n",
      "acc for Lsat= 0.06323926655782593 \n",
      "acc for Psat= 0.08367437252567875 \n",
      "acc for optim= 0.13414132069382406\n",
      "Epoch:661/1000\n",
      "Loss on train= 0.010182148776948452\n",
      "Loss on test= 0.013520430773496628\n",
      "acc for Lsat= 0.06537742101483875 \n",
      "acc for Psat= 0.08555847762359511 \n",
      "acc for optim= 0.13080266610615782\n",
      "Epoch:662/1000\n",
      "Loss on train= 0.010029704309999943\n",
      "Loss on test= 0.013644549064338207\n",
      "acc for Lsat= 0.07254042128721874 \n",
      "acc for Psat= 0.08992338023251957 \n",
      "acc for optim= 0.1306151416282066\n",
      "Epoch:663/1000\n",
      "Loss on train= 0.009963953867554665\n",
      "Loss on test= 0.01315611507743597\n",
      "acc for Lsat= 0.06519713782601887 \n",
      "acc for Psat= 0.08600494282113182 \n",
      "acc for optim= 0.13245933910624846\n",
      "Epoch:664/1000\n",
      "Loss on train= 0.01007548626512289\n",
      "Loss on test= 0.012981990352272987\n",
      "acc for Lsat= 0.0654267720050282 \n",
      "acc for Psat= 0.08411058456533486 \n",
      "acc for optim= 0.12997929776708286\n",
      "Epoch:665/1000\n",
      "Loss on train= 0.009516804479062557\n",
      "Loss on test= 0.014201574958860874\n",
      "acc for Lsat= 0.06019438107808431 \n",
      "acc for Psat= 0.08125743392027086 \n",
      "acc for optim= 0.13191945424510373\n",
      "Epoch:666/1000\n",
      "Loss on train= 0.009999987669289112\n",
      "Loss on test= 0.012876924127340317\n",
      "acc for Lsat= 0.06294214609596466 \n",
      "acc for Psat= 0.07890477854719696 \n",
      "acc for optim= 0.13224908953739536\n",
      "Epoch:667/1000\n",
      "Loss on train= 0.009702716954052448\n",
      "Loss on test= 0.012644191272556782\n",
      "acc for Lsat= 0.060387146969636284 \n",
      "acc for Psat= 0.0825161125510931 \n",
      "acc for optim= 0.1310970353997416\n",
      "Epoch:668/1000\n",
      "Loss on train= 0.009781106375157833\n",
      "Loss on test= 0.012765811756253242\n",
      "acc for Lsat= 0.06869108776251474 \n",
      "acc for Psat= 0.09190206800897917 \n",
      "acc for optim= 0.13019256885680888\n",
      "Epoch:669/1000\n",
      "Loss on train= 0.010193470865488052\n",
      "Loss on test= 0.013489793986082077\n",
      "acc for Lsat= 0.06384134764472643 \n",
      "acc for Psat= 0.08538326755579974 \n",
      "acc for optim= 0.1320076356538468\n",
      "Epoch:670/1000\n",
      "Loss on train= 0.010054991580545902\n",
      "Loss on test= 0.01302186306566\n",
      "acc for Lsat= 0.06355577665898535 \n",
      "acc for Psat= 0.08762213016549746 \n",
      "acc for optim= 0.13050008695572612\n",
      "Epoch:671/1000\n",
      "Loss on train= 0.009672146290540695\n",
      "Loss on test= 0.01359039917588234\n",
      "acc for Lsat= 0.06214014664292335 \n",
      "acc for Psat= 0.08305757260984845 \n",
      "acc for optim= 0.13167061710523234\n",
      "Epoch:672/1000\n",
      "Loss on train= 0.010035787709057331\n",
      "Loss on test= 0.013066280633211136\n",
      "acc for Lsat= 0.0605946206384235 \n",
      "acc for Psat= 0.08276502283083069 \n",
      "acc for optim= 0.13082563069959482\n",
      "Epoch:673/1000\n",
      "Loss on train= 0.009614855982363224\n",
      "Loss on test= 0.013709519058465958\n",
      "acc for Lsat= 0.06281878782643212 \n",
      "acc for Psat= 0.08169393108950722 \n",
      "acc for optim= 0.13230477964712514\n",
      "Epoch:674/1000\n",
      "Loss on train= 0.00974405650049448\n",
      "Loss on test= 0.013728468678891659\n",
      "acc for Lsat= 0.06415839393933614 \n",
      "acc for Psat= 0.08320170852045217 \n",
      "acc for optim= 0.1332698137809833\n",
      "Epoch:675/1000\n",
      "Loss on train= 0.009689493104815483\n",
      "Loss on test= 0.01319937314838171\n",
      "acc for Lsat= 0.06848153745134672 \n",
      "acc for Psat= 0.08142576151423982 \n",
      "acc for optim= 0.1335743825882673\n",
      "Epoch:676/1000\n",
      "Loss on train= 0.009784666821360588\n",
      "Loss on test= 0.01309140957891941\n",
      "acc for Lsat= 0.059486074745655065 \n",
      "acc for Psat= 0.08117824728704161 \n",
      "acc for optim= 0.13463928641544448\n",
      "Epoch:677/1000\n",
      "Loss on train= 0.009800971485674381\n",
      "Loss on test= 0.012491710484027863\n",
      "acc for Lsat= 0.06262471791770724 \n",
      "acc for Psat= 0.0853676648603545 \n",
      "acc for optim= 0.1322056251681513\n",
      "Epoch:678/1000\n",
      "Loss on train= 0.00999143160879612\n",
      "Loss on test= 0.013322069309651852\n",
      "acc for Lsat= 0.0773046044839753 \n",
      "acc for Psat= 0.09547561258077622 \n",
      "acc for optim= 0.1314499942378865\n",
      "Epoch:679/1000\n",
      "Loss on train= 0.00984427984803915\n",
      "Loss on test= 0.012784728780388832\n",
      "acc for Lsat= 0.06167667988273833 \n",
      "acc for Psat= 0.08350221224957044 \n",
      "acc for optim= 0.13294939086255098\n",
      "Epoch:680/1000\n",
      "Loss on train= 0.009724036790430546\n",
      "Loss on test= 0.014926948584616184\n",
      "acc for Lsat= 0.06211806833744049 \n",
      "acc for Psat= 0.08148741442710161 \n",
      "acc for optim= 0.13392479978501795\n",
      "Epoch:681/1000\n",
      "Loss on train= 0.009745965711772442\n",
      "Loss on test= 0.013220300897955894\n",
      "acc for Lsat= 0.06783218880494435 \n",
      "acc for Psat= 0.0882615638275941 \n",
      "acc for optim= 0.1333988250957595\n",
      "Epoch:682/1000\n",
      "Loss on train= 0.009498164057731628\n",
      "Loss on test= 0.013471696525812149\n",
      "acc for Lsat= 0.06633716904454762 \n",
      "acc for Psat= 0.08551636578308214 \n",
      "acc for optim= 0.13307526769737402\n",
      "Epoch:683/1000\n",
      "Loss on train= 0.009405315853655338\n",
      "Loss on test= 0.013587754219770432\n",
      "acc for Lsat= 0.06165418277184168 \n",
      "acc for Psat= 0.09128710826237997 \n",
      "acc for optim= 0.13105318922963405\n",
      "Epoch:684/1000\n",
      "Loss on train= 0.009763142094016075\n",
      "Loss on test= 0.014129875227808952\n",
      "acc for Lsat= 0.06362450942397117 \n",
      "acc for Psat= 0.08902108131183518 \n",
      "acc for optim= 0.13077676432828106\n",
      "Epoch:685/1000\n",
      "Loss on train= 0.010126465000212193\n",
      "Loss on test= 0.014460934326052666\n",
      "acc for Lsat= 0.06922981705930498 \n",
      "acc for Psat= 0.08786177295777532 \n",
      "acc for optim= 0.13218523205982316\n",
      "Epoch:686/1000\n",
      "Loss on train= 0.009658130817115307\n",
      "Loss on test= 0.0130233783274889\n",
      "acc for Lsat= 0.06585913623372713 \n",
      "acc for Psat= 0.08700666121310659 \n",
      "acc for optim= 0.13204711419012813\n",
      "Epoch:687/1000\n",
      "Loss on train= 0.009572761133313179\n",
      "Loss on test= 0.013638956472277641\n",
      "acc for Lsat= 0.061744657655557 \n",
      "acc for Psat= 0.09532550937599607 \n",
      "acc for optim= 0.13195974955128298\n",
      "Epoch:688/1000\n",
      "Loss on train= 0.009547852911055088\n",
      "Loss on test= 0.014248743653297424\n",
      "acc for Lsat= 0.06164177225695717 \n",
      "acc for Psat= 0.08476880043745043 \n",
      "acc for optim= 0.13247277134408553\n",
      "Epoch:689/1000\n",
      "Loss on train= 0.009812659583985806\n",
      "Loss on test= 0.014583941549062729\n",
      "acc for Lsat= 0.06649632851282755 \n",
      "acc for Psat= 0.0866710669464535 \n",
      "acc for optim= 0.132530020053188\n",
      "Epoch:690/1000\n",
      "Loss on train= 0.00969965010881424\n",
      "Loss on test= 0.013453307561576366\n",
      "acc for Lsat= 0.07077494884530705 \n",
      "acc for Psat= 0.08947694715526369 \n",
      "acc for optim= 0.13236381130086053\n",
      "Epoch:691/1000\n",
      "Loss on train= 0.009939976967871189\n",
      "Loss on test= 0.012608565390110016\n",
      "acc for Lsat= 0.06433272245857449 \n",
      "acc for Psat= 0.08796260936392677 \n",
      "acc for optim= 0.13183640850087006\n",
      "Epoch:692/1000\n",
      "Loss on train= 0.00958801805973053\n",
      "Loss on test= 0.013677889481186867\n",
      "acc for Lsat= 0.06001597137914764 \n",
      "acc for Psat= 0.0827371370047331 \n",
      "acc for optim= 0.1320792580644289\n",
      "Epoch:693/1000\n",
      "Loss on train= 0.009577871300280094\n",
      "Loss on test= 0.013386857695877552\n",
      "acc for Lsat= 0.06001408199469248 \n",
      "acc for Psat= 0.07761729853227733 \n",
      "acc for optim= 0.13347184548361435\n",
      "Epoch:694/1000\n",
      "Loss on train= 0.009667027741670609\n",
      "Loss on test= 0.013736487366259098\n",
      "acc for Lsat= 0.07119096716245016 \n",
      "acc for Psat= 0.09396302244729465 \n",
      "acc for optim= 0.1317457855058213\n",
      "Epoch:695/1000\n",
      "Loss on train= 0.009741686284542084\n",
      "Loss on test= 0.013903121463954449\n",
      "acc for Lsat= 0.06490351855754853 \n",
      "acc for Psat= 0.08664226258794468 \n",
      "acc for optim= 0.1333188864505953\n",
      "Epoch:696/1000\n",
      "Loss on train= 0.009681720286607742\n",
      "Loss on test= 0.013323824852705002\n",
      "acc for Lsat= 0.06201864207784334 \n",
      "acc for Psat= 0.09310950073930953 \n",
      "acc for optim= 0.12981096014587415\n",
      "Epoch:697/1000\n",
      "Loss on train= 0.009990818798542023\n",
      "Loss on test= 0.013436839915812016\n",
      "acc for Lsat= 0.0662603830297788 \n",
      "acc for Psat= 0.09304773559172948 \n",
      "acc for optim= 0.13006466247348325\n",
      "Epoch:698/1000\n",
      "Loss on train= 0.009607529267668724\n",
      "Loss on test= 0.013610942289233208\n",
      "acc for Lsat= 0.06687605033318203 \n",
      "acc for Psat= 0.08282017140752738 \n",
      "acc for optim= 0.13218643582529493\n",
      "Epoch:699/1000\n",
      "Loss on train= 0.009723306633532047\n",
      "Loss on test= 0.013094902038574219\n",
      "acc for Lsat= 0.060829875452650915 \n",
      "acc for Psat= 0.08672121490041412 \n",
      "acc for optim= 0.13148780767288473\n",
      "Epoch:700/1000\n",
      "Loss on train= 0.009704038500785828\n",
      "Loss on test= 0.013300638645887375\n",
      "acc for Lsat= 0.062409362859196134 \n",
      "acc for Psat= 0.08008477778898344 \n",
      "acc for optim= 0.13328033544951015\n",
      "Epoch:701/1000\n",
      "Loss on train= 0.00971903558820486\n",
      "Loss on test= 0.013448144309222698\n",
      "acc for Lsat= 0.05979766969879469 \n",
      "acc for Psat= 0.08415078413155345 \n",
      "acc for optim= 0.13299004286527633\n",
      "Epoch:702/1000\n",
      "Loss on train= 0.009589692577719688\n",
      "Loss on test= 0.013852805830538273\n",
      "acc for Lsat= 0.06154126119282512 \n",
      "acc for Psat= 0.08408918757405547 \n",
      "acc for optim= 0.13259732932266263\n",
      "Epoch:703/1000\n",
      "Loss on train= 0.009493386372923851\n",
      "Loss on test= 0.013737928122282028\n",
      "acc for Lsat= 0.06446536266141467 \n",
      "acc for Psat= 0.08537060419718423 \n",
      "acc for optim= 0.1324440939144956\n",
      "Epoch:704/1000\n",
      "Loss on train= 0.009739100933074951\n",
      "Loss on test= 0.012721716426312923\n",
      "acc for Lsat= 0.05813658833503724 \n",
      "acc for Psat= 0.08471349912385147 \n",
      "acc for optim= 0.13218454809652436\n",
      "Epoch:705/1000\n",
      "Loss on train= 0.009654521010816097\n",
      "Loss on test= 0.012865681201219559\n",
      "acc for Lsat= 0.06138717871573237 \n",
      "acc for Psat= 0.08178355627589756 \n",
      "acc for optim= 0.13121116316566866\n",
      "Epoch:706/1000\n",
      "Loss on train= 0.009758267551660538\n",
      "Loss on test= 0.013633672147989273\n",
      "acc for Lsat= 0.05813508207599324 \n",
      "acc for Psat= 0.08608927085167832 \n",
      "acc for optim= 0.1322285010996792\n",
      "Epoch:707/1000\n",
      "Loss on train= 0.009863696061074734\n",
      "Loss on test= 0.013708272017538548\n",
      "acc for Lsat= 0.05867876642280155 \n",
      "acc for Psat= 0.08397358105414443 \n",
      "acc for optim= 0.13199820303254658\n",
      "Epoch:708/1000\n",
      "Loss on train= 0.009576844982802868\n",
      "Loss on test= 0.013823289424180984\n",
      "acc for Lsat= 0.06018947197331323 \n",
      "acc for Psat= 0.08251078247817027 \n",
      "acc for optim= 0.13294972351027862\n",
      "Epoch:709/1000\n",
      "Loss on train= 0.009694302454590797\n",
      "Loss on test= 0.012974312528967857\n",
      "acc for Lsat= 0.061046160757541656 \n",
      "acc for Psat= 0.08373821870320373 \n",
      "acc for optim= 0.13223385682536498\n",
      "Epoch:710/1000\n",
      "Loss on train= 0.009711132384836674\n",
      "Loss on test= 0.014188438653945923\n",
      "acc for Lsat= 0.05972856001721489 \n",
      "acc for Psat= 0.07998408240576585 \n",
      "acc for optim= 0.13322128454844157\n",
      "Epoch:711/1000\n",
      "Loss on train= 0.009467783384025097\n",
      "Loss on test= 0.01436135359108448\n",
      "acc for Lsat= 0.058693592995405205 \n",
      "acc for Psat= 0.08357483603888087 \n",
      "acc for optim= 0.1341402439193593\n",
      "Epoch:712/1000\n",
      "Loss on train= 0.009836940094828606\n",
      "Loss on test= 0.01382654719054699\n",
      "acc for Lsat= 0.0612349443965488 \n",
      "acc for Psat= 0.08390779197216035 \n",
      "acc for optim= 0.13121561544636884\n",
      "Epoch:713/1000\n",
      "Loss on train= 0.0096027422696352\n",
      "Loss on test= 0.014129001647233963\n",
      "acc for Lsat= 0.06904241806930965 \n",
      "acc for Psat= 0.09426770417226686 \n",
      "acc for optim= 0.13127788859936926\n",
      "Epoch:714/1000\n",
      "Loss on train= 0.009533324278891087\n",
      "Loss on test= 0.013192152604460716\n",
      "acc for Lsat= 0.06024246861537298 \n",
      "acc for Psat= 0.08938909868399302 \n",
      "acc for optim= 0.13159997649490832\n",
      "Epoch:715/1000\n",
      "Loss on train= 0.009686792269349098\n",
      "Loss on test= 0.014314353466033936\n",
      "acc for Lsat= 0.0655911753523267 \n",
      "acc for Psat= 0.07845472156898016 \n",
      "acc for optim= 0.13254919172161156\n",
      "Epoch:716/1000\n",
      "Loss on train= 0.009920735843479633\n",
      "Loss on test= 0.01398854423314333\n",
      "acc for Lsat= 0.06356732915414291 \n",
      "acc for Psat= 0.08182974461879994 \n",
      "acc for optim= 0.13239200851983493\n",
      "Epoch:717/1000\n",
      "Loss on train= 0.00948918517678976\n",
      "Loss on test= 0.012506401166319847\n",
      "acc for Lsat= 0.058634198622571104 \n",
      "acc for Psat= 0.08454600564307636 \n",
      "acc for optim= 0.1309569635325008\n",
      "Epoch:718/1000\n",
      "Loss on train= 0.009286881424486637\n",
      "Loss on test= 0.013338001444935799\n",
      "acc for Lsat= 0.057152241302861104 \n",
      "acc for Psat= 0.08273442391720083 \n",
      "acc for optim= 0.1322348679933283\n",
      "Epoch:719/1000\n",
      "Loss on train= 0.009438294917345047\n",
      "Loss on test= 0.013897372409701347\n",
      "acc for Lsat= 0.06418588997589217 \n",
      "acc for Psat= 0.08407644240392578 \n",
      "acc for optim= 0.13191443040139147\n",
      "Epoch:720/1000\n",
      "Loss on train= 0.009530625306069851\n",
      "Loss on test= 0.013444032520055771\n",
      "acc for Lsat= 0.06025905799534586 \n",
      "acc for Psat= 0.08676422345969412 \n",
      "acc for optim= 0.13017436957193743\n",
      "Epoch:721/1000\n",
      "Loss on train= 0.009744794107973576\n",
      "Loss on test= 0.01394559908658266\n",
      "acc for Lsat= 0.06194961311088668 \n",
      "acc for Psat= 0.0854843521076772 \n",
      "acc for optim= 0.13093888178053825\n",
      "Epoch:722/1000\n",
      "Loss on train= 0.00953087117522955\n",
      "Loss on test= 0.013011178933084011\n",
      "acc for Lsat= 0.06605041780405574 \n",
      "acc for Psat= 0.0835701027678119 \n",
      "acc for optim= 0.13222820473213992\n",
      "Epoch:723/1000\n",
      "Loss on train= 0.009517437778413296\n",
      "Loss on test= 0.012923398055136204\n",
      "acc for Lsat= 0.06669581731160483 \n",
      "acc for Psat= 0.0895686995651987 \n",
      "acc for optim= 0.13203472503357463\n",
      "Epoch:724/1000\n",
      "Loss on train= 0.009603362530469894\n",
      "Loss on test= 0.013775897212326527\n",
      "acc for Lsat= 0.06576004169053504 \n",
      "acc for Psat= 0.08326662215921615 \n",
      "acc for optim= 0.13264859074519741\n",
      "Epoch:725/1000\n",
      "Loss on train= 0.009197861887514591\n",
      "Loss on test= 0.014241067692637444\n",
      "acc for Lsat= 0.06377006777458721 \n",
      "acc for Psat= 0.08469466211067306 \n",
      "acc for optim= 0.1336943592462275\n",
      "Epoch:726/1000\n",
      "Loss on train= 0.009717622771859169\n",
      "Loss on test= 0.013475910760462284\n",
      "acc for Lsat= 0.0659466286500295 \n",
      "acc for Psat= 0.08993921644157833 \n",
      "acc for optim= 0.1296109980282684\n",
      "Epoch:727/1000\n",
      "Loss on train= 0.009593365713953972\n",
      "Loss on test= 0.0131627656519413\n",
      "acc for Lsat= 0.06129813823435041 \n",
      "acc for Psat= 0.08439282741811541 \n",
      "acc for optim= 0.12976055230117509\n",
      "Epoch:728/1000\n",
      "Loss on train= 0.009497929364442825\n",
      "Loss on test= 0.013505801558494568\n",
      "acc for Lsat= 0.061960613189472075 \n",
      "acc for Psat= 0.08032758653733052 \n",
      "acc for optim= 0.1308402093955212\n",
      "Epoch:729/1000\n",
      "Loss on train= 0.009407764300704002\n",
      "Loss on test= 0.015435229986906052\n",
      "acc for Lsat= 0.07462349798944261 \n",
      "acc for Psat= 0.09152280216415726 \n",
      "acc for optim= 0.1310992986895144\n",
      "Epoch:730/1000\n",
      "Loss on train= 0.009632658213376999\n",
      "Loss on test= 0.013505788519978523\n",
      "acc for Lsat= 0.06504757586452696 \n",
      "acc for Psat= 0.08191309298078218 \n",
      "acc for optim= 0.13054466363456516\n",
      "Epoch:731/1000\n",
      "Loss on train= 0.009562312625348568\n",
      "Loss on test= 0.01406422071158886\n",
      "acc for Lsat= 0.06116093811061648 \n",
      "acc for Psat= 0.08466342671049965 \n",
      "acc for optim= 0.13243097240726154\n",
      "Epoch:732/1000\n",
      "Loss on train= 0.009473349899053574\n",
      "Loss on test= 0.014316337183117867\n",
      "acc for Lsat= 0.05817130456368128 \n",
      "acc for Psat= 0.08087401642567581 \n",
      "acc for optim= 0.13064534761425517\n",
      "Epoch:733/1000\n",
      "Loss on train= 0.009504544548690319\n",
      "Loss on test= 0.01314220204949379\n",
      "acc for Lsat= 0.062393688576089006 \n",
      "acc for Psat= 0.08468007817864417 \n",
      "acc for optim= 0.13032162133604291\n",
      "Epoch:734/1000\n",
      "Loss on train= 0.009186716750264168\n",
      "Loss on test= 0.013788934797048569\n",
      "acc for Lsat= 0.06516422803203265 \n",
      "acc for Psat= 0.08441431915594472 \n",
      "acc for optim= 0.13089376495530208\n",
      "Epoch:735/1000\n",
      "Loss on train= 0.009326529689133167\n",
      "Loss on test= 0.014112689532339573\n",
      "acc for Lsat= 0.06362607015503778 \n",
      "acc for Psat= 0.09345438240302933 \n",
      "acc for optim= 0.13053071000095873\n",
      "Epoch:736/1000\n",
      "Loss on train= 0.009200066328048706\n",
      "Loss on test= 0.013226580806076527\n",
      "acc for Lsat= 0.0605194530553288 \n",
      "acc for Psat= 0.09245090699858133 \n",
      "acc for optim= 0.1301781783087386\n",
      "Epoch:737/1000\n",
      "Loss on train= 0.009229885414242744\n",
      "Loss on test= 0.013627909123897552\n",
      "acc for Lsat= 0.0625792642434438 \n",
      "acc for Psat= 0.09165431244505777 \n",
      "acc for optim= 0.13028417515257995\n",
      "Epoch:738/1000\n",
      "Loss on train= 0.009446695446968079\n",
      "Loss on test= 0.012899871915578842\n",
      "acc for Lsat= 0.0663929045200348 \n",
      "acc for Psat= 0.10333035232292281 \n",
      "acc for optim= 0.1309879035171535\n",
      "Epoch:739/1000\n",
      "Loss on train= 0.009408600628376007\n",
      "Loss on test= 0.013847357593476772\n",
      "acc for Lsat= 0.060724142028225785 \n",
      "acc for Psat= 0.09820524561736317 \n",
      "acc for optim= 0.1332164332542258\n",
      "Epoch:740/1000\n",
      "Loss on train= 0.009537174366414547\n",
      "Loss on test= 0.012893480248749256\n",
      "acc for Lsat= 0.06293135434389113 \n",
      "acc for Psat= 0.08629883395300972 \n",
      "acc for optim= 0.13014422374674015\n",
      "Epoch:741/1000\n",
      "Loss on train= 0.009372202679514885\n",
      "Loss on test= 0.014118656516075134\n",
      "acc for Lsat= 0.061887608137395654 \n",
      "acc for Psat= 0.0841887723757989 \n",
      "acc for optim= 0.13211941739751232\n",
      "Epoch:742/1000\n",
      "Loss on train= 0.009463346563279629\n",
      "Loss on test= 0.013528186827898026\n",
      "acc for Lsat= 0.06509946775105264 \n",
      "acc for Psat= 0.08499150507979923 \n",
      "acc for optim= 0.12986717503517864\n",
      "Epoch:743/1000\n",
      "Loss on train= 0.00929250754415989\n",
      "Loss on test= 0.013733398169279099\n",
      "acc for Lsat= 0.06298480571972 \n",
      "acc for Psat= 0.095749348650376 \n",
      "acc for optim= 0.13099590875208378\n",
      "Epoch:744/1000\n",
      "Loss on train= 0.009612343274056911\n",
      "Loss on test= 0.01325524877756834\n",
      "acc for Lsat= 0.0605931781232357 \n",
      "acc for Psat= 0.0840945590287447 \n",
      "acc for optim= 0.13169086776259872\n",
      "Epoch:745/1000\n",
      "Loss on train= 0.009181736968457699\n",
      "Loss on test= 0.013396537862718105\n",
      "acc for Lsat= 0.05847263356877698 \n",
      "acc for Psat= 0.08092494039899772 \n",
      "acc for optim= 0.1320724956277344\n",
      "Epoch:746/1000\n",
      "Loss on train= 0.009335257112979889\n",
      "Loss on test= 0.01300132554024458\n",
      "acc for Lsat= 0.06270182538363668 \n",
      "acc for Psat= 0.08666068654921319 \n",
      "acc for optim= 0.1318765013996098\n",
      "Epoch:747/1000\n",
      "Loss on train= 0.00939424242824316\n",
      "Loss on test= 0.01430643629282713\n",
      "acc for Lsat= 0.06763055937157737 \n",
      "acc for Psat= 0.08592123372687235 \n",
      "acc for optim= 0.13192883429841865\n",
      "Epoch:748/1000\n",
      "Loss on train= 0.009576037526130676\n",
      "Loss on test= 0.013468343764543533\n",
      "acc for Lsat= 0.06495581641793252 \n",
      "acc for Psat= 0.089142657071352 \n",
      "acc for optim= 0.13295576506190832\n",
      "Epoch:749/1000\n",
      "Loss on train= 0.009386598132550716\n",
      "Loss on test= 0.014168709516525269\n",
      "acc for Lsat= 0.059866559091541496 \n",
      "acc for Psat= 0.08465746330718199 \n",
      "acc for optim= 0.1335391821546687\n",
      "Epoch:750/1000\n",
      "Loss on train= 0.009254657663404942\n",
      "Loss on test= 0.013377692550420761\n",
      "acc for Lsat= 0.06797011279397541 \n",
      "acc for Psat= 0.08939381970299613 \n",
      "acc for optim= 0.13166858046833013\n",
      "Epoch:751/1000\n",
      "Loss on train= 0.009646197780966759\n",
      "Loss on test= 0.01360479835420847\n",
      "acc for Lsat= 0.058457799835337526 \n",
      "acc for Psat= 0.0816076592852672 \n",
      "acc for optim= 0.13298366638935277\n",
      "Epoch:752/1000\n",
      "Loss on train= 0.009371601976454258\n",
      "Loss on test= 0.012606941163539886\n",
      "acc for Lsat= 0.058391190651390285 \n",
      "acc for Psat= 0.08310066734751065 \n",
      "acc for optim= 0.13261023805373245\n",
      "Epoch:753/1000\n",
      "Loss on train= 0.009195489808917046\n",
      "Loss on test= 0.013426695019006729\n",
      "acc for Lsat= 0.07214417457580566 \n",
      "acc for Psat= 0.0925586544805103 \n",
      "acc for optim= 0.13252346016880537\n",
      "Epoch:754/1000\n",
      "Loss on train= 0.009065228514373302\n",
      "Loss on test= 0.014349420554935932\n",
      "acc for Lsat= 0.058148486001624 \n",
      "acc for Psat= 0.08298616657654447 \n",
      "acc for optim= 0.13359269561866915\n",
      "Epoch:755/1000\n",
      "Loss on train= 0.009523526765406132\n",
      "Loss on test= 0.012708554975688457\n",
      "acc for Lsat= 0.06099503123097949 \n",
      "acc for Psat= 0.08504438077410063 \n",
      "acc for optim= 0.13126179294453727\n",
      "Epoch:756/1000\n",
      "Loss on train= 0.00891949888318777\n",
      "Loss on test= 0.015510398894548416\n",
      "acc for Lsat= 0.07231637818945778 \n",
      "acc for Psat= 0.109187233613597 \n",
      "acc for optim= 0.1305865434722768\n",
      "Epoch:757/1000\n",
      "Loss on train= 0.009466949850320816\n",
      "Loss on test= 0.01419992744922638\n",
      "acc for Lsat= 0.06230347835355335 \n",
      "acc for Psat= 0.0832543028311597 \n",
      "acc for optim= 0.13125292300764058\n",
      "Epoch:758/1000\n",
      "Loss on train= 0.009443379007279873\n",
      "Loss on test= 0.01328764483332634\n",
      "acc for Lsat= 0.05977301299571991 \n",
      "acc for Psat= 0.08517879471182824 \n",
      "acc for optim= 0.13296280668841468\n",
      "Epoch:759/1000\n",
      "Loss on train= 0.009034354239702225\n",
      "Loss on test= 0.01374789047986269\n",
      "acc for Lsat= 0.063941397683488 \n",
      "acc for Psat= 0.08206210980812709 \n",
      "acc for optim= 0.13241896461695432\n",
      "Epoch:760/1000\n",
      "Loss on train= 0.009200838394463062\n",
      "Loss on test= 0.013330436311662197\n",
      "acc for Lsat= 0.059146784742673236 \n",
      "acc for Psat= 0.07945772779898512 \n",
      "acc for optim= 0.13333188647197353\n",
      "Epoch:761/1000\n",
      "Loss on train= 0.009341005235910416\n",
      "Loss on test= 0.013710392639040947\n",
      "acc for Lsat= 0.06571149842606651 \n",
      "acc for Psat= 0.08486739330821567 \n",
      "acc for optim= 0.1332054218484296\n",
      "Epoch:762/1000\n",
      "Loss on train= 0.009639338590204716\n",
      "Loss on test= 0.013802144676446915\n",
      "acc for Lsat= 0.06588049944904115 \n",
      "acc for Psat= 0.0806068822948469 \n",
      "acc for optim= 0.13334968512256942\n",
      "Epoch:763/1000\n",
      "Loss on train= 0.00954268965870142\n",
      "Loss on test= 0.013277285732328892\n",
      "acc for Lsat= 0.05906207151710988 \n",
      "acc for Psat= 0.0809042895057549 \n",
      "acc for optim= 0.1322920845200618\n",
      "Epoch:764/1000\n",
      "Loss on train= 0.009569312445819378\n",
      "Loss on test= 0.014417729340493679\n",
      "acc for Lsat= 0.06176315512922075 \n",
      "acc for Psat= 0.08400558556119601 \n",
      "acc for optim= 0.13241644118809043\n",
      "Epoch:765/1000\n",
      "Loss on train= 0.009511389769613743\n",
      "Loss on test= 0.013502150774002075\n",
      "acc for Lsat= 0.059384985475076583 \n",
      "acc for Psat= 0.08487440111736456 \n",
      "acc for optim= 0.13360425333182013\n",
      "Epoch:766/1000\n",
      "Loss on train= 0.009012226946651936\n",
      "Loss on test= 0.014337650500237942\n",
      "acc for Lsat= 0.06748543332020443 \n",
      "acc for Psat= 0.09259006612830692 \n",
      "acc for optim= 0.13325745463371277\n",
      "Epoch:767/1000\n",
      "Loss on train= 0.009396794252097607\n",
      "Loss on test= 0.011888636276125908\n",
      "acc for Lsat= 0.062136662668652004 \n",
      "acc for Psat= 0.08382736812863084 \n",
      "acc for optim= 0.1309449582050244\n",
      "Epoch:768/1000\n",
      "Loss on train= 0.009224025532603264\n",
      "Loss on test= 0.01385653205215931\n",
      "acc for Lsat= 0.06840364287296932 \n",
      "acc for Psat= 0.09267275954286258 \n",
      "acc for optim= 0.13084294568333366\n",
      "Epoch:769/1000\n",
      "Loss on train= 0.009304988197982311\n",
      "Loss on test= 0.013731947168707848\n",
      "acc for Lsat= 0.0679670713841915 \n",
      "acc for Psat= 0.08865366578102113 \n",
      "acc for optim= 0.13189985822472308\n",
      "Epoch:770/1000\n",
      "Loss on train= 0.009386677294969559\n",
      "Loss on test= 0.01305717509239912\n",
      "acc for Lsat= 0.06013384420010778 \n",
      "acc for Psat= 0.08719005526767838 \n",
      "acc for optim= 0.13166351308011348\n",
      "Epoch:771/1000\n",
      "Loss on train= 0.00918089970946312\n",
      "Loss on test= 0.013602633960545063\n",
      "acc for Lsat= 0.0656673519147767 \n",
      "acc for Psat= 0.08409461805389987 \n",
      "acc for optim= 0.12946290067500538\n",
      "Epoch:772/1000\n",
      "Loss on train= 0.00934238638728857\n",
      "Loss on test= 0.014068497344851494\n",
      "acc for Lsat= 0.0612715049750275 \n",
      "acc for Psat= 0.08216345997320282 \n",
      "acc for optim= 0.1308948187571433\n",
      "Epoch:773/1000\n",
      "Loss on train= 0.009440306574106216\n",
      "Loss on test= 0.01416309829801321\n",
      "acc for Lsat= 0.059807039135032235 \n",
      "acc for Psat= 0.08305107623131738 \n",
      "acc for optim= 0.1333106288893355\n",
      "Epoch:774/1000\n",
      "Loss on train= 0.009512077085673809\n",
      "Loss on test= 0.013966696336865425\n",
      "acc for Lsat= 0.06502381389339765 \n",
      "acc for Psat= 0.08983612623479632 \n",
      "acc for optim= 0.1329359757403533\n",
      "Epoch:775/1000\n",
      "Loss on train= 0.009273110888898373\n",
      "Loss on test= 0.013191319070756435\n",
      "acc for Lsat= 0.06498037320044306 \n",
      "acc for Psat= 0.0808907038014796 \n",
      "acc for optim= 0.13143339682784347\n",
      "Epoch:776/1000\n",
      "Loss on train= 0.009363419376313686\n",
      "Loss on test= 0.013090655207633972\n",
      "acc for Lsat= 0.06842776661117872 \n",
      "acc for Psat= 0.08572567477822304 \n",
      "acc for optim= 0.1318394153482384\n",
      "Epoch:777/1000\n",
      "Loss on train= 0.009593461640179157\n",
      "Loss on test= 0.014260535128414631\n",
      "acc for Lsat= 0.06890040768517387 \n",
      "acc for Psat= 0.09000512543651792 \n",
      "acc for optim= 0.13134171497076752\n",
      "Epoch:778/1000\n",
      "Loss on train= 0.0091548478230834\n",
      "Loss on test= 0.013302862644195557\n",
      "acc for Lsat= 0.06480193071895174 \n",
      "acc for Psat= 0.08952646983994378 \n",
      "acc for optim= 0.13159744656748243\n",
      "Epoch:779/1000\n",
      "Loss on train= 0.00972474180161953\n",
      "Loss on test= 0.014791171066462994\n",
      "acc for Lsat= 0.06437736799319586 \n",
      "acc for Psat= 0.08745719500713878 \n",
      "acc for optim= 0.13265133417314953\n",
      "Epoch:780/1000\n",
      "Loss on train= 0.009604747407138348\n",
      "Loss on test= 0.014245344325900078\n",
      "acc for Lsat= 0.07183183398511675 \n",
      "acc for Psat= 0.08451096051269108 \n",
      "acc for optim= 0.1316338393009371\n",
      "Epoch:781/1000\n",
      "Loss on train= 0.00934663973748684\n",
      "Loss on test= 0.012725736945867538\n",
      "acc for Lsat= 0.07184344538384013 \n",
      "acc for Psat= 0.08489468569556873 \n",
      "acc for optim= 0.13124745513002078\n",
      "Epoch:782/1000\n",
      "Loss on train= 0.00908228661864996\n",
      "Loss on test= 0.013282936066389084\n",
      "acc for Lsat= 0.06286616739299562 \n",
      "acc for Psat= 0.08393523929019771 \n",
      "acc for optim= 0.1320523217320442\n",
      "Epoch:783/1000\n",
      "Loss on train= 0.009130515158176422\n",
      "Loss on test= 0.013417404145002365\n",
      "acc for Lsat= 0.059224715911679805 \n",
      "acc for Psat= 0.08544544283714559 \n",
      "acc for optim= 0.1311914310687118\n",
      "Epoch:784/1000\n",
      "Loss on train= 0.009250938892364502\n",
      "Loss on test= 0.013228798285126686\n",
      "acc for Lsat= 0.06452034380700852 \n",
      "acc for Psat= 0.08531000307864614 \n",
      "acc for optim= 0.13180638692445223\n",
      "Epoch:785/1000\n",
      "Loss on train= 0.009109284728765488\n",
      "Loss on test= 0.014159104786813259\n",
      "acc for Lsat= 0.06138729535871082 \n",
      "acc for Psat= 0.09405703859196768 \n",
      "acc for optim= 0.13019414631028972\n",
      "Epoch:786/1000\n",
      "Loss on train= 0.009586838074028492\n",
      "Loss on test= 0.013470700941979885\n",
      "acc for Lsat= 0.06407059066825443 \n",
      "acc for Psat= 0.08426625078750982 \n",
      "acc for optim= 0.1313894542761975\n",
      "Epoch:787/1000\n",
      "Loss on train= 0.008568216115236282\n",
      "Loss on test= 0.012893576174974442\n",
      "acc for Lsat= 0.0672994034157859 \n",
      "acc for Psat= 0.0932584816382991 \n",
      "acc for optim= 0.13057095528476767\n",
      "Epoch:788/1000\n",
      "Loss on train= 0.009299598634243011\n",
      "Loss on test= 0.013368665240705013\n",
      "acc for Lsat= 0.06021034005615447 \n",
      "acc for Psat= 0.08037784901551073 \n",
      "acc for optim= 0.13207356846994822\n",
      "Epoch:789/1000\n",
      "Loss on train= 0.009157045744359493\n",
      "Loss on test= 0.014559690840542316\n",
      "acc for Lsat= 0.06168031079901589 \n",
      "acc for Psat= 0.08321777426948149 \n",
      "acc for optim= 0.1318952733029922\n",
      "Epoch:790/1000\n",
      "Loss on train= 0.009375554509460926\n",
      "Loss on test= 0.013727962039411068\n",
      "acc for Lsat= 0.05968997172183461 \n",
      "acc for Psat= 0.08189325622386404 \n",
      "acc for optim= 0.13297285528646577\n",
      "Epoch:791/1000\n",
      "Loss on train= 0.00931108370423317\n",
      "Loss on test= 0.013925377279520035\n",
      "acc for Lsat= 0.05804020530647702 \n",
      "acc for Psat= 0.07940298647412823 \n",
      "acc for optim= 0.13047552675836616\n",
      "Epoch:792/1000\n",
      "Loss on train= 0.009290707297623158\n",
      "Loss on test= 0.012711589224636555\n",
      "acc for Lsat= 0.06404702398512098 \n",
      "acc for Psat= 0.08348189223971633 \n",
      "acc for optim= 0.13004237725916834\n",
      "Epoch:793/1000\n",
      "Loss on train= 0.008988600224256516\n",
      "Loss on test= 0.012898946180939674\n",
      "acc for Lsat= 0.060429309805234276 \n",
      "acc for Psat= 0.0884667085690631 \n",
      "acc for optim= 0.1301372298763858\n",
      "Epoch:794/1000\n",
      "Loss on train= 0.009373403154313564\n",
      "Loss on test= 0.014138000085949898\n",
      "acc for Lsat= 0.062317340407106604 \n",
      "acc for Psat= 0.08161487602111367 \n",
      "acc for optim= 0.1314914303107394\n",
      "Epoch:795/1000\n",
      "Loss on train= 0.009073562920093536\n",
      "Loss on test= 0.013605710119009018\n",
      "acc for Lsat= 0.05843232969443002 \n",
      "acc for Psat= 0.08364110808405611 \n",
      "acc for optim= 0.12995299980458286\n",
      "Epoch:796/1000\n",
      "Loss on train= 0.009278986603021622\n",
      "Loss on test= 0.01325855404138565\n",
      "acc for Lsat= 0.06080563966598776 \n",
      "acc for Psat= 0.08161689641161099 \n",
      "acc for optim= 0.13150161711706054\n",
      "Epoch:797/1000\n",
      "Loss on train= 0.009459656663239002\n",
      "Loss on test= 0.013513942249119282\n",
      "acc for Lsat= 0.058204738547404614 \n",
      "acc for Psat= 0.07917449525040057 \n",
      "acc for optim= 0.13137780415515105\n",
      "Epoch:798/1000\n",
      "Loss on train= 0.009265893138945103\n",
      "Loss on test= 0.013659754768013954\n",
      "acc for Lsat= 0.0577991766234239 \n",
      "acc for Psat= 0.0814938018816368 \n",
      "acc for optim= 0.13051420632335875\n",
      "Epoch:799/1000\n",
      "Loss on train= 0.00913145300000906\n",
      "Loss on test= 0.013996528461575508\n",
      "acc for Lsat= 0.062359491570128336 \n",
      "acc for Psat= 0.08288310335742104 \n",
      "acc for optim= 0.13094320313798058\n",
      "Epoch:800/1000\n",
      "Loss on train= 0.009408514946699142\n",
      "Loss on test= 0.012748442590236664\n",
      "acc for Lsat= 0.060355791946252194 \n",
      "acc for Psat= 0.08511181978715793 \n",
      "acc for optim= 0.13113182340231205\n",
      "Epoch:801/1000\n",
      "Loss on train= 0.00888287927955389\n",
      "Loss on test= 0.013699715957045555\n",
      "acc for Lsat= 0.05819763806131151 \n",
      "acc for Psat= 0.07931852104763189 \n",
      "acc for optim= 0.131018584056033\n",
      "Epoch:802/1000\n",
      "Loss on train= 0.00898033007979393\n",
      "Loss on test= 0.01397789642214775\n",
      "acc for Lsat= 0.06968581361903087 \n",
      "acc for Psat= 0.08866115303503143 \n",
      "acc for optim= 0.13117075591451596\n",
      "Epoch:803/1000\n",
      "Loss on train= 0.009256177581846714\n",
      "Loss on test= 0.013636324554681778\n",
      "acc for Lsat= 0.06109080596102608 \n",
      "acc for Psat= 0.08008230090296517 \n",
      "acc for optim= 0.1311892886956533\n",
      "Epoch:804/1000\n",
      "Loss on train= 0.009012497030198574\n",
      "Loss on test= 0.014294602908194065\n",
      "acc for Lsat= 0.059100321349170475 \n",
      "acc for Psat= 0.07870007018662163 \n",
      "acc for optim= 0.13231459024051825\n",
      "Epoch:805/1000\n",
      "Loss on train= 0.00907370075583458\n",
      "Loss on test= 0.013551154173910618\n",
      "acc for Lsat= 0.06870512084828483 \n",
      "acc for Psat= 0.09081831864184804 \n",
      "acc for optim= 0.13075359921074575\n",
      "Epoch:806/1000\n",
      "Loss on train= 0.0092219989746809\n",
      "Loss on test= 0.012476164847612381\n",
      "acc for Lsat= 0.059447395553191514 \n",
      "acc for Psat= 0.08375415661268765 \n",
      "acc for optim= 0.13165636414455045\n",
      "Epoch:807/1000\n",
      "Loss on train= 0.008964892476797104\n",
      "Loss on test= 0.012878001667559147\n",
      "acc for Lsat= 0.06226656817727619 \n",
      "acc for Psat= 0.08227854602866704 \n",
      "acc for optim= 0.13087434799720843\n",
      "Epoch:808/1000\n",
      "Loss on train= 0.009358038194477558\n",
      "Loss on test= 0.012651372700929642\n",
      "acc for Lsat= 0.06022129845288065 \n",
      "acc for Psat= 0.08134329223798381 \n",
      "acc for optim= 0.13066650471753544\n",
      "Epoch:809/1000\n",
      "Loss on train= 0.009231292642652988\n",
      "Loss on test= 0.012679517269134521\n",
      "acc for Lsat= 0.05799034610390663 \n",
      "acc for Psat= 0.08206718847569493 \n",
      "acc for optim= 0.12985600048883092\n",
      "Epoch:810/1000\n",
      "Loss on train= 0.0091026546433568\n",
      "Loss on test= 0.012633445672690868\n",
      "acc for Lsat= 0.05981528225044409 \n",
      "acc for Psat= 0.08531590880205235 \n",
      "acc for optim= 0.13010274124228294\n",
      "Epoch:811/1000\n",
      "Loss on train= 0.009045355021953583\n",
      "Loss on test= 0.013409670442342758\n",
      "acc for Lsat= 0.05811558705237177 \n",
      "acc for Psat= 0.08068697671923372 \n",
      "acc for optim= 0.13328648304773702\n",
      "Epoch:812/1000\n",
      "Loss on train= 0.008961400017142296\n",
      "Loss on test= 0.013666211627423763\n",
      "acc for Lsat= 0.0589149437016911 \n",
      "acc for Psat= 0.08012772590542833 \n",
      "acc for optim= 0.1311162043155895\n",
      "Epoch:813/1000\n",
      "Loss on train= 0.009182658046483994\n",
      "Loss on test= 0.013733897358179092\n",
      "acc for Lsat= 0.060492453724145884 \n",
      "acc for Psat= 0.08478819313976499 \n",
      "acc for optim= 0.1304622948169708\n",
      "Epoch:814/1000\n",
      "Loss on train= 0.009241900406777859\n",
      "Loss on test= 0.01510875765234232\n",
      "acc for Lsat= 0.06033880338072777 \n",
      "acc for Psat= 0.08128705192357304 \n",
      "acc for optim= 0.1316385947374834\n",
      "Epoch:815/1000\n",
      "Loss on train= 0.009184093214571476\n",
      "Loss on test= 0.014169505797326565\n",
      "acc for Lsat= 0.06390399974253443 \n",
      "acc for Psat= 0.0867532196558184 \n",
      "acc for optim= 0.13164078812632296\n",
      "Epoch:816/1000\n",
      "Loss on train= 0.009269645437598228\n",
      "Loss on test= 0.01425740122795105\n",
      "acc for Lsat= 0.06917835225661596 \n",
      "acc for Psat= 0.10321419686079025 \n",
      "acc for optim= 0.1304895660115613\n",
      "Epoch:817/1000\n",
      "Loss on train= 0.008853264153003693\n",
      "Loss on test= 0.01243467815220356\n",
      "acc for Lsat= 0.06709699556231498 \n",
      "acc for Psat= 0.08519969801935884 \n",
      "acc for optim= 0.13132985942065717\n",
      "Epoch:818/1000\n",
      "Loss on train= 0.009142116643488407\n",
      "Loss on test= 0.012727361172437668\n",
      "acc for Lsat= 0.05665759547717042 \n",
      "acc for Psat= 0.08089350033551454 \n",
      "acc for optim= 0.13317686141365104\n",
      "Epoch:819/1000\n",
      "Loss on train= 0.00905918050557375\n",
      "Loss on test= 0.012542953714728355\n",
      "acc for Lsat= 0.059437471793757544 \n",
      "acc for Psat= 0.08051999947056174 \n",
      "acc for optim= 0.13320221776763597\n",
      "Epoch:820/1000\n",
      "Loss on train= 0.008698110468685627\n",
      "Loss on test= 0.013777440413832664\n",
      "acc for Lsat= 0.06164453667071131 \n",
      "acc for Psat= 0.08677873975700802 \n",
      "acc for optim= 0.1341411710613304\n",
      "Epoch:821/1000\n",
      "Loss on train= 0.008974387310445309\n",
      "Loss on test= 0.014460076577961445\n",
      "acc for Lsat= 0.05790263621343507 \n",
      "acc for Psat= 0.08820487513310378 \n",
      "acc for optim= 0.13384047477609584\n",
      "Epoch:822/1000\n",
      "Loss on train= 0.009066788479685783\n",
      "Loss on test= 0.01326928660273552\n",
      "acc for Lsat= 0.06761903605527347 \n",
      "acc for Psat= 0.09576656503809823 \n",
      "acc for optim= 0.1330838614453872\n",
      "Epoch:823/1000\n",
      "Loss on train= 0.008806093595921993\n",
      "Loss on test= 0.013276136480271816\n",
      "acc for Lsat= 0.06039206948545244 \n",
      "acc for Psat= 0.08471361228989231 \n",
      "acc for optim= 0.13224766666276588\n",
      "Epoch:824/1000\n",
      "Loss on train= 0.009147158823907375\n",
      "Loss on test= 0.01339625008404255\n",
      "acc for Lsat= 0.056794544019632874 \n",
      "acc for Psat= 0.07990778734286626 \n",
      "acc for optim= 0.1316599112211002\n",
      "Epoch:825/1000\n",
      "Loss on train= 0.00884638074785471\n",
      "Loss on test= 0.013827841728925705\n",
      "acc for Lsat= 0.06485934886667465 \n",
      "acc for Psat= 0.08751625994013416 \n",
      "acc for optim= 0.13022251828677125\n",
      "Epoch:826/1000\n",
      "Loss on train= 0.008878341875970364\n",
      "Loss on test= 0.013980756513774395\n",
      "acc for Lsat= 0.060651742418607074 \n",
      "acc for Psat= 0.0886824091275533 \n",
      "acc for optim= 0.13048008151559365\n",
      "Epoch:827/1000\n",
      "Loss on train= 0.008751443587243557\n",
      "Loss on test= 0.013603721745312214\n",
      "acc for Lsat= 0.0596314314338896 \n",
      "acc for Psat= 0.08329252099825277 \n",
      "acc for optim= 0.13002887078457406\n",
      "Epoch:828/1000\n",
      "Loss on train= 0.009115105494856834\n",
      "Loss on test= 0.014161518774926662\n",
      "acc for Lsat= 0.05875314056045479 \n",
      "acc for Psat= 0.07807637457218435 \n",
      "acc for optim= 0.13042300678789612\n",
      "Epoch:829/1000\n",
      "Loss on train= 0.009394045919179916\n",
      "Loss on test= 0.014152654446661472\n",
      "acc for Lsat= 0.0640226610004902 \n",
      "acc for Psat= 0.07900301445366091 \n",
      "acc for optim= 0.13118228279054164\n",
      "Epoch:830/1000\n",
      "Loss on train= 0.008644632063806057\n",
      "Loss on test= 0.013020619750022888\n",
      "acc for Lsat= 0.06346679230531056 \n",
      "acc for Psat= 0.08108714189794329 \n",
      "acc for optim= 0.13308141086664463\n",
      "Epoch:831/1000\n",
      "Loss on train= 0.008823792450129986\n",
      "Loss on test= 0.014378923922777176\n",
      "acc for Lsat= 0.05671496064298683 \n",
      "acc for Psat= 0.08403108165495926 \n",
      "acc for optim= 0.13166549143691858\n",
      "Epoch:832/1000\n",
      "Loss on train= 0.009097234345972538\n",
      "Loss on test= 0.013502929359674454\n",
      "acc for Lsat= 0.05801926925778389 \n",
      "acc for Psat= 0.07820500847366123 \n",
      "acc for optim= 0.13129501682188777\n",
      "Epoch:833/1000\n",
      "Loss on train= 0.008603516034781933\n",
      "Loss on test= 0.014161801896989346\n",
      "acc for Lsat= 0.06390493454204665 \n",
      "acc for Psat= 0.08868822865188122 \n",
      "acc for optim= 0.13122107539739872\n",
      "Epoch:834/1000\n",
      "Loss on train= 0.009478720836341381\n",
      "Loss on test= 0.01392977312207222\n",
      "acc for Lsat= 0.05849055010411475 \n",
      "acc for Psat= 0.08033530716008196 \n",
      "acc for optim= 0.13188314357151587\n",
      "Epoch:835/1000\n",
      "Loss on train= 0.008933080360293388\n",
      "Loss on test= 0.013720445334911346\n",
      "acc for Lsat= 0.05586356371641158 \n",
      "acc for Psat= 0.0789653479018145 \n",
      "acc for optim= 0.13201177629331745\n",
      "Epoch:836/1000\n",
      "Loss on train= 0.008864980190992355\n",
      "Loss on test= 0.014238263480365276\n",
      "acc for Lsat= 0.06080627234445676 \n",
      "acc for Psat= 0.08155406154692174 \n",
      "acc for optim= 0.13153758115238612\n",
      "Epoch:837/1000\n",
      "Loss on train= 0.009261867962777615\n",
      "Loss on test= 0.013796641491353512\n",
      "acc for Lsat= 0.061395850529273356 \n",
      "acc for Psat= 0.08512401266230478 \n",
      "acc for optim= 0.1309536465547151\n",
      "Epoch:838/1000\n",
      "Loss on train= 0.009020939469337463\n",
      "Loss on test= 0.013089069165289402\n",
      "acc for Lsat= 0.060564284440543924 \n",
      "acc for Psat= 0.08304605935182835 \n",
      "acc for optim= 0.13238849724746415\n",
      "Epoch:839/1000\n",
      "Loss on train= 0.009063614532351494\n",
      "Loss on test= 0.012976507656276226\n",
      "acc for Lsat= 0.06155818833245171 \n",
      "acc for Psat= 0.08409499712288379 \n",
      "acc for optim= 0.1304122223622269\n",
      "Epoch:840/1000\n",
      "Loss on train= 0.009054245427250862\n",
      "Loss on test= 0.013896831311285496\n",
      "acc for Lsat= 0.06975613964928522 \n",
      "acc for Psat= 0.08521621872981391 \n",
      "acc for optim= 0.13127194852050808\n",
      "Epoch:841/1000\n",
      "Loss on train= 0.008939302526414394\n",
      "Loss on test= 0.012965311296284199\n",
      "acc for Lsat= 0.06372641780310206 \n",
      "acc for Psat= 0.08483646590676573 \n",
      "acc for optim= 0.13133837470991747\n",
      "Epoch:842/1000\n",
      "Loss on train= 0.008728846907615662\n",
      "Loss on test= 0.01396272610872984\n",
      "acc for Lsat= 0.06106608096096252 \n",
      "acc for Psat= 0.08602346285349793 \n",
      "acc for optim= 0.13054226198130184\n",
      "Epoch:843/1000\n",
      "Loss on train= 0.008920407854020596\n",
      "Loss on test= 0.013927972875535488\n",
      "acc for Lsat= 0.060510182877381635 \n",
      "acc for Psat= 0.08038935201863447 \n",
      "acc for optim= 0.13176113821359145\n",
      "Epoch:844/1000\n",
      "Loss on train= 0.008946928195655346\n",
      "Loss on test= 0.013322362676262856\n",
      "acc for Lsat= 0.05903504747483465 \n",
      "acc for Psat= 0.08008423418117067 \n",
      "acc for optim= 0.1322972948766417\n",
      "Epoch:845/1000\n",
      "Loss on train= 0.009191778488457203\n",
      "Loss on test= 0.01412054430693388\n",
      "acc for Lsat= 0.05785214830603865 \n",
      "acc for Psat= 0.08177665163659387 \n",
      "acc for optim= 0.13252870839916997\n",
      "Epoch:846/1000\n",
      "Loss on train= 0.008957917802035809\n",
      "Loss on test= 0.013820086605846882\n",
      "acc for Lsat= 0.060177566856145864 \n",
      "acc for Psat= 0.08147579652981624 \n",
      "acc for optim= 0.13246601303625438\n",
      "Epoch:847/1000\n",
      "Loss on train= 0.009002875536680222\n",
      "Loss on test= 0.013321620412170887\n",
      "acc for Lsat= 0.05958530546890365 \n",
      "acc for Psat= 0.08169853142980073 \n",
      "acc for optim= 0.13390766671962206\n",
      "Epoch:848/1000\n",
      "Loss on train= 0.009217353537678719\n",
      "Loss on test= 0.013408821076154709\n",
      "acc for Lsat= 0.07128044962882996 \n",
      "acc for Psat= 0.09333687391546036 \n",
      "acc for optim= 0.13162134391152194\n",
      "Epoch:849/1000\n",
      "Loss on train= 0.008665177971124649\n",
      "Loss on test= 0.012813563458621502\n",
      "acc for Lsat= 0.0659044850203726 \n",
      "acc for Psat= 0.08149772526489364 \n",
      "acc for optim= 0.1323186768632796\n",
      "Epoch:850/1000\n",
      "Loss on train= 0.009112266823649406\n",
      "Loss on test= 0.013691908679902554\n",
      "acc for Lsat= 0.05998240063587826 \n",
      "acc for Psat= 0.07975074915836254 \n",
      "acc for optim= 0.13249544869694443\n",
      "Epoch:851/1000\n",
      "Loss on train= 0.009231493808329105\n",
      "Loss on test= 0.014367648400366306\n",
      "acc for Lsat= 0.05789258682893382 \n",
      "acc for Psat= 0.08080831964810688 \n",
      "acc for optim= 0.13203572928905485\n",
      "Epoch:852/1000\n",
      "Loss on train= 0.009091942571103573\n",
      "Loss on test= 0.013945456594228745\n",
      "acc for Lsat= 0.05773641785813702 \n",
      "acc for Psat= 0.08197675177620518 \n",
      "acc for optim= 0.13279650784615016\n",
      "Epoch:853/1000\n",
      "Loss on train= 0.008932702243328094\n",
      "Loss on test= 0.01377576682716608\n",
      "acc for Lsat= 0.06252130692203839 \n",
      "acc for Psat= 0.08097849964267678 \n",
      "acc for optim= 0.13250085500379405\n",
      "Epoch:854/1000\n",
      "Loss on train= 0.008729063905775547\n",
      "Loss on test= 0.013230640441179276\n",
      "acc for Lsat= 0.06380156510406071 \n",
      "acc for Psat= 0.08755358796980645 \n",
      "acc for optim= 0.13184041518511047\n",
      "Epoch:855/1000\n",
      "Loss on train= 0.009009834378957748\n",
      "Loss on test= 0.014632103964686394\n",
      "acc for Lsat= 0.06463001635339526 \n",
      "acc for Psat= 0.08179643255554968 \n",
      "acc for optim= 0.1313897333832251\n",
      "Epoch:856/1000\n",
      "Loss on train= 0.008784092031419277\n",
      "Loss on test= 0.012567542493343353\n",
      "acc for Lsat= 0.06049987235003048 \n",
      "acc for Psat= 0.08421548902988435 \n",
      "acc for optim= 0.13308397908177638\n",
      "Epoch:857/1000\n",
      "Loss on train= 0.008951648138463497\n",
      "Loss on test= 0.01370304636657238\n",
      "acc for Lsat= 0.05572077905138333 \n",
      "acc for Psat= 0.07947898416055572 \n",
      "acc for optim= 0.13445984961258042\n",
      "Epoch:858/1000\n",
      "Loss on train= 0.009136127308011055\n",
      "Loss on test= 0.013713701628148556\n",
      "acc for Lsat= 0.05996164240770869 \n",
      "acc for Psat= 0.08497300652994051 \n",
      "acc for optim= 0.13377817095153866\n",
      "Epoch:859/1000\n",
      "Loss on train= 0.009224675595760345\n",
      "Loss on test= 0.013811876066029072\n",
      "acc for Lsat= 0.06130650473965539 \n",
      "acc for Psat= 0.08173700575199391 \n",
      "acc for optim= 0.13105254107051426\n",
      "Epoch:860/1000\n",
      "Loss on train= 0.008856463246047497\n",
      "Loss on test= 0.013929375447332859\n",
      "acc for Lsat= 0.05929650606380568 \n",
      "acc for Psat= 0.08155876124898592 \n",
      "acc for optim= 0.13100660331547262\n",
      "Epoch:861/1000\n",
      "Loss on train= 0.009280138649046421\n",
      "Loss on test= 0.014097316190600395\n",
      "acc for Lsat= 0.058415649955471355 \n",
      "acc for Psat= 0.08345215552383 \n",
      "acc for optim= 0.13084416021075515\n",
      "Epoch:862/1000\n",
      "Loss on train= 0.008818726986646652\n",
      "Loss on test= 0.012994891032576561\n",
      "acc for Lsat= 0.06279532338182131 \n",
      "acc for Psat= 0.08815177285836803 \n",
      "acc for optim= 0.13018463987650145\n",
      "Epoch:863/1000\n",
      "Loss on train= 0.008954983204603195\n",
      "Loss on test= 0.013515545055270195\n",
      "acc for Lsat= 0.06582001952661408 \n",
      "acc for Psat= 0.08422519266605377 \n",
      "acc for optim= 0.1298934547437562\n",
      "Epoch:864/1000\n",
      "Loss on train= 0.00905490480363369\n",
      "Loss on test= 0.013827760703861713\n",
      "acc for Lsat= 0.06808125277360282 \n",
      "acc for Psat= 0.09196656685736443 \n",
      "acc for optim= 0.12977009560498926\n",
      "Epoch:865/1000\n",
      "Loss on train= 0.009190422482788563\n",
      "Loss on test= 0.013275204226374626\n",
      "acc for Lsat= 0.06490925790535079 \n",
      "acc for Psat= 0.0899919290509489 \n",
      "acc for optim= 0.131583101178209\n",
      "Epoch:866/1000\n",
      "Loss on train= 0.008765088394284248\n",
      "Loss on test= 0.013978123664855957\n",
      "acc for Lsat= 0.05923574475778474 \n",
      "acc for Psat= 0.08193494077357981 \n",
      "acc for optim= 0.1327695349852244\n",
      "Epoch:867/1000\n",
      "Loss on train= 0.008511923253536224\n",
      "Loss on test= 0.014128272421658039\n",
      "acc for Lsat= 0.07010574572616153 \n",
      "acc for Psat= 0.08985865968796941 \n",
      "acc for optim= 0.13173837045000658\n",
      "Epoch:868/1000\n",
      "Loss on train= 0.009232969023287296\n",
      "Loss on test= 0.01323867216706276\n",
      "acc for Lsat= 0.058343448241551715 \n",
      "acc for Psat= 0.07959137927326893 \n",
      "acc for optim= 0.13231129919489223\n",
      "Epoch:869/1000\n",
      "Loss on train= 0.008924060501158237\n",
      "Loss on test= 0.014109675772488117\n",
      "acc for Lsat= 0.0620298875702752 \n",
      "acc for Psat= 0.08013143380069071 \n",
      "acc for optim= 0.1327403179059426\n",
      "Epoch:870/1000\n",
      "Loss on train= 0.008872149512171745\n",
      "Loss on test= 0.013928317464888096\n",
      "acc for Lsat= 0.061720817370547194 \n",
      "acc for Psat= 0.083160578004188 \n",
      "acc for optim= 0.13262192507584888\n",
      "Epoch:871/1000\n",
      "Loss on train= 0.009082944132387638\n",
      "Loss on test= 0.013547953218221664\n",
      "acc for Lsat= 0.06070120988620652 \n",
      "acc for Psat= 0.09171068237887489 \n",
      "acc for optim= 0.13072450734261012\n",
      "Epoch:872/1000\n",
      "Loss on train= 0.008676795288920403\n",
      "Loss on test= 0.013934810645878315\n",
      "acc for Lsat= 0.05936417223678695 \n",
      "acc for Psat= 0.08396842529376346 \n",
      "acc for optim= 0.13174872381819616\n",
      "Epoch:873/1000\n",
      "Loss on train= 0.008775655180215836\n",
      "Loss on test= 0.013415176421403885\n",
      "acc for Lsat= 0.0637174023522271 \n",
      "acc for Psat= 0.09311244313915572 \n",
      "acc for optim= 0.13164922998597223\n",
      "Epoch:874/1000\n",
      "Loss on train= 0.008836104534566402\n",
      "Loss on test= 0.013678189367055893\n",
      "acc for Lsat= 0.05842706014712652 \n",
      "acc for Psat= 0.0853786175035768 \n",
      "acc for optim= 0.13019979196704098\n",
      "Epoch:875/1000\n",
      "Loss on train= 0.008803621865808964\n",
      "Loss on test= 0.01330726221203804\n",
      "acc for Lsat= 0.06299349235163795 \n",
      "acc for Psat= 0.08390024213327302 \n",
      "acc for optim= 0.13012536573741168\n",
      "Epoch:876/1000\n",
      "Loss on train= 0.008583872579038143\n",
      "Loss on test= 0.013656694442033768\n",
      "acc for Lsat= 0.06021484384934106 \n",
      "acc for Psat= 0.07994841434475447 \n",
      "acc for optim= 0.1322291911062267\n",
      "Epoch:877/1000\n",
      "Loss on train= 0.008849608711898327\n",
      "Loss on test= 0.013946635648608208\n",
      "acc for Lsat= 0.05915091600683001 \n",
      "acc for Psat= 0.08004595074388716 \n",
      "acc for optim= 0.1315640406476127\n",
      "Epoch:878/1000\n",
      "Loss on train= 0.008790387772023678\n",
      "Loss on test= 0.013733815401792526\n",
      "acc for Lsat= 0.05778159904811117 \n",
      "acc for Psat= 0.08088291312257448 \n",
      "acc for optim= 0.13150432167781723\n",
      "Epoch:879/1000\n",
      "Loss on train= 0.008723940700292587\n",
      "Loss on test= 0.013289154507219791\n",
      "acc for Lsat= 0.059699279897742805 \n",
      "acc for Psat= 0.0814196311765247 \n",
      "acc for optim= 0.13066629241738054\n",
      "Epoch:880/1000\n",
      "Loss on train= 0.008796491660177708\n",
      "Loss on test= 0.014050926081836224\n",
      "acc for Lsat= 0.059401368432574805 \n",
      "acc for Psat= 0.08281421371632153 \n",
      "acc for optim= 0.12974131714759599\n",
      "Epoch:881/1000\n",
      "Loss on train= 0.008796746842563152\n",
      "Loss on test= 0.013682891614735126\n",
      "acc for Lsat= 0.05909594421585402 \n",
      "acc for Psat= 0.08243085162507163 \n",
      "acc for optim= 0.13084652797422477\n",
      "Epoch:882/1000\n",
      "Loss on train= 0.008989877998828888\n",
      "Loss on test= 0.014429796487092972\n",
      "acc for Lsat= 0.06100442533691724 \n",
      "acc for Psat= 0.08106501876480049 \n",
      "acc for optim= 0.13099323404538962\n",
      "Epoch:883/1000\n",
      "Loss on train= 0.009081288240849972\n",
      "Loss on test= 0.013658412732183933\n",
      "acc for Lsat= 0.06441450723343425 \n",
      "acc for Psat= 0.08265661034319136 \n",
      "acc for optim= 0.13325411081314087\n",
      "Epoch:884/1000\n",
      "Loss on train= 0.009006504900753498\n",
      "Loss on test= 0.014334448613226414\n",
      "acc for Lsat= 0.062445122251907986 \n",
      "acc for Psat= 0.08210393962346846 \n",
      "acc for optim= 0.13183691178758938\n",
      "Epoch:885/1000\n",
      "Loss on train= 0.008845075033605099\n",
      "Loss on test= 0.012649057433009148\n",
      "acc for Lsat= 0.06256558183166716 \n",
      "acc for Psat= 0.09088613150848283 \n",
      "acc for optim= 0.13249614663008188\n",
      "Epoch:886/1000\n",
      "Loss on train= 0.008395183831453323\n",
      "Loss on test= 0.01379189919680357\n",
      "acc for Lsat= 0.05853106470571624 \n",
      "acc for Psat= 0.0852529962443643 \n",
      "acc for optim= 0.13326381171743074\n",
      "Epoch:887/1000\n",
      "Loss on train= 0.008727183565497398\n",
      "Loss on test= 0.01345757208764553\n",
      "acc for Lsat= 0.061703573622637314 \n",
      "acc for Psat= 0.08228789634627498 \n",
      "acc for optim= 0.13381003273857964\n",
      "Epoch:888/1000\n",
      "Loss on train= 0.008411567658185959\n",
      "Loss on test= 0.01294685248285532\n",
      "acc for Lsat= 0.058833643380138606 \n",
      "acc for Psat= 0.08079050361282297 \n",
      "acc for optim= 0.13376992758777406\n",
      "Epoch:889/1000\n",
      "Loss on train= 0.00869511067867279\n",
      "Loss on test= 0.01352120004594326\n",
      "acc for Lsat= 0.0663465773065885 \n",
      "acc for Psat= 0.08791620408495267 \n",
      "acc for optim= 0.13179384453429116\n",
      "Epoch:890/1000\n",
      "Loss on train= 0.008820817805826664\n",
      "Loss on test= 0.013603240251541138\n",
      "acc for Lsat= 0.06753198156754175 \n",
      "acc for Psat= 0.08266906879014439 \n",
      "acc for optim= 0.13197612106386158\n",
      "Epoch:891/1000\n",
      "Loss on train= 0.008990253321826458\n",
      "Loss on test= 0.014041881076991558\n",
      "acc for Lsat= 0.05958127710554335 \n",
      "acc for Psat= 0.08506520051095219 \n",
      "acc for optim= 0.13189191135267417\n",
      "Epoch:892/1000\n",
      "Loss on train= 0.008806455880403519\n",
      "Loss on test= 0.01437932439148426\n",
      "acc for Lsat= 0.05881942386428515 \n",
      "acc for Psat= 0.0849015822013219 \n",
      "acc for optim= 0.13074043364160592\n",
      "Epoch:893/1000\n",
      "Loss on train= 0.009023488499224186\n",
      "Loss on test= 0.013401845470070839\n",
      "acc for Lsat= 0.05634444008270899 \n",
      "acc for Psat= 0.0817897727506028 \n",
      "acc for optim= 0.1315392742347386\n",
      "Epoch:894/1000\n",
      "Loss on train= 0.008676346391439438\n",
      "Loss on test= 0.014072757214307785\n",
      "acc for Lsat= 0.06385077958305677 \n",
      "acc for Psat= 0.08872558375199636 \n",
      "acc for optim= 0.1307603826953305\n",
      "Epoch:895/1000\n",
      "Loss on train= 0.008352861739695072\n",
      "Loss on test= 0.013702764175832272\n",
      "acc for Lsat= 0.06238623236616452 \n",
      "acc for Psat= 0.08102545986572901 \n",
      "acc for optim= 0.1303795643357767\n",
      "Epoch:896/1000\n",
      "Loss on train= 0.008740485645830631\n",
      "Loss on test= 0.013694141991436481\n",
      "acc for Lsat= 0.057375766005780954 \n",
      "acc for Psat= 0.08043006128735013 \n",
      "acc for optim= 0.1326772994051377\n",
      "Epoch:897/1000\n",
      "Loss on train= 0.008539322763681412\n",
      "Loss on test= 0.013149689882993698\n",
      "acc for Lsat= 0.06060639594992 \n",
      "acc for Psat= 0.08433915546370878 \n",
      "acc for optim= 0.13214649396638076\n",
      "Epoch:898/1000\n",
      "Loss on train= 0.008703459054231644\n",
      "Loss on test= 0.015213001519441605\n",
      "acc for Lsat= 0.05891175592939059 \n",
      "acc for Psat= 0.08125545517024066 \n",
      "acc for optim= 0.13314371444284914\n",
      "Epoch:899/1000\n",
      "Loss on train= 0.008622495457530022\n",
      "Loss on test= 0.01360378973186016\n",
      "acc for Lsat= 0.06319496623343893 \n",
      "acc for Psat= 0.08513327232665487 \n",
      "acc for optim= 0.13260924936168722\n",
      "Epoch:900/1000\n",
      "Loss on train= 0.0085951779037714\n",
      "Loss on test= 0.014031107537448406\n",
      "acc for Lsat= 0.0602854781680637 \n",
      "acc for Psat= 0.0891708496544096 \n",
      "acc for optim= 0.1324229158875015\n",
      "Epoch:901/1000\n",
      "Loss on train= 0.009159838780760765\n",
      "Loss on test= 0.013918942771852016\n",
      "acc for Lsat= 0.0621040334718095 \n",
      "acc for Psat= 0.07908408354139991 \n",
      "acc for optim= 0.1322607125259108\n",
      "Epoch:902/1000\n",
      "Loss on train= 0.009135627187788486\n",
      "Loss on test= 0.014508945867419243\n",
      "acc for Lsat= 0.05799745917320252 \n",
      "acc for Psat= 0.08472228261331718 \n",
      "acc for optim= 0.13225075068573156\n",
      "Epoch:903/1000\n",
      "Loss on train= 0.008680697530508041\n",
      "Loss on test= 0.014200177043676376\n",
      "acc for Lsat= 0.05827055118150182 \n",
      "acc for Psat= 0.07952317275727788 \n",
      "acc for optim= 0.13263806659314364\n",
      "Epoch:904/1000\n",
      "Loss on train= 0.008653836324810982\n",
      "Loss on test= 0.01464885100722313\n",
      "acc for Lsat= 0.06104956873589093 \n",
      "acc for Psat= 0.08511452099515332 \n",
      "acc for optim= 0.13207418976558583\n",
      "Epoch:905/1000\n",
      "Loss on train= 0.008928378112614155\n",
      "Loss on test= 0.013508345931768417\n",
      "acc for Lsat= 0.05861175540420744 \n",
      "acc for Psat= 0.08052929620155029 \n",
      "acc for optim= 0.13185539605716864\n",
      "Epoch:906/1000\n",
      "Loss on train= 0.008350997231900692\n",
      "Loss on test= 0.013382834382355213\n",
      "acc for Lsat= 0.060264223400089474 \n",
      "acc for Psat= 0.08249966001345052 \n",
      "acc for optim= 0.1313275143918064\n",
      "Epoch:907/1000\n",
      "Loss on train= 0.008730222471058369\n",
      "Loss on test= 0.013561299070715904\n",
      "acc for Lsat= 0.06728415803776848 \n",
      "acc for Psat= 0.09017449691891671 \n",
      "acc for optim= 0.13215579034553634\n",
      "Epoch:908/1000\n",
      "Loss on train= 0.008337031118571758\n",
      "Loss on test= 0.013908679597079754\n",
      "acc for Lsat= 0.058818423913584814 \n",
      "acc for Psat= 0.08148021265450452 \n",
      "acc for optim= 0.13177472760693895\n",
      "Epoch:909/1000\n",
      "Loss on train= 0.00875293742865324\n",
      "Loss on test= 0.013899758458137512\n",
      "acc for Lsat= 0.05878497904373539 \n",
      "acc for Psat= 0.08129943605098461 \n",
      "acc for optim= 0.13162494757109214\n",
      "Epoch:910/1000\n",
      "Loss on train= 0.0090178856626153\n",
      "Loss on test= 0.013764134608209133\n",
      "acc for Lsat= 0.057683712657954946 \n",
      "acc for Psat= 0.08544649216863845 \n",
      "acc for optim= 0.13167988148828347\n",
      "Epoch:911/1000\n",
      "Loss on train= 0.008860536850988865\n",
      "Loss on test= 0.01298337709158659\n",
      "acc for Lsat= 0.05884361134635078 \n",
      "acc for Psat= 0.08064029071893958 \n",
      "acc for optim= 0.1329786920713054\n",
      "Epoch:912/1000\n",
      "Loss on train= 0.00841843243688345\n",
      "Loss on test= 0.012969134375452995\n",
      "acc for Lsat= 0.056303843524720926 \n",
      "acc for Psat= 0.08054428208205436 \n",
      "acc for optim= 0.13230671307279004\n",
      "Epoch:913/1000\n",
      "Loss on train= 0.008580966852605343\n",
      "Loss on test= 0.014272447675466537\n",
      "acc for Lsat= 0.058088054259618126 \n",
      "acc for Psat= 0.08253232486959962 \n",
      "acc for optim= 0.1314982834375567\n",
      "Epoch:914/1000\n",
      "Loss on train= 0.009028197266161442\n",
      "Loss on test= 0.014274786226451397\n",
      "acc for Lsat= 0.058398728569348655 \n",
      "acc for Psat= 0.08134145682884585 \n",
      "acc for optim= 0.13138204879230925\n",
      "Epoch:915/1000\n",
      "Loss on train= 0.00875174906104803\n",
      "Loss on test= 0.013380145654082298\n",
      "acc for Lsat= 0.057605708266297975 \n",
      "acc for Psat= 0.08149331153059998 \n",
      "acc for optim= 0.13290487639605997\n",
      "Epoch:916/1000\n",
      "Loss on train= 0.008612209931015968\n",
      "Loss on test= 0.013621672987937927\n",
      "acc for Lsat= 0.05721478205588129 \n",
      "acc for Psat= 0.07933046683772571 \n",
      "acc for optim= 0.13234477817184395\n",
      "Epoch:917/1000\n",
      "Loss on train= 0.008575157262384892\n",
      "Loss on test= 0.01342760305851698\n",
      "acc for Lsat= 0.05867175898618168 \n",
      "acc for Psat= 0.08023977612869608 \n",
      "acc for optim= 0.13184815895640187\n",
      "Epoch:918/1000\n",
      "Loss on train= 0.008402642793953419\n",
      "Loss on test= 0.01508068572729826\n",
      "acc for Lsat= 0.06350799045628971 \n",
      "acc for Psat= 0.09094114402929943 \n",
      "acc for optim= 0.13091205070830053\n",
      "Epoch:919/1000\n",
      "Loss on train= 0.008237004280090332\n",
      "Loss on test= 0.013869703747332096\n",
      "acc for Lsat= 0.059147663000557155 \n",
      "acc for Psat= 0.07999089595137371 \n",
      "acc for optim= 0.1326364497136739\n",
      "Epoch:920/1000\n",
      "Loss on train= 0.008701259270310402\n",
      "Loss on test= 0.013400410301983356\n",
      "acc for Lsat= 0.06002984278731876 \n",
      "acc for Psat= 0.08894796901279026 \n",
      "acc for optim= 0.1319784584144751\n",
      "Epoch:921/1000\n",
      "Loss on train= 0.00848629605025053\n",
      "Loss on test= 0.014665908180177212\n",
      "acc for Lsat= 0.0573378938767645 \n",
      "acc for Psat= 0.08507289936145147 \n",
      "acc for optim= 0.13132122531533238\n",
      "Epoch:922/1000\n",
      "Loss on train= 0.008831171318888664\n",
      "Loss on test= 0.013692256063222885\n",
      "acc for Lsat= 0.058038233717282624 \n",
      "acc for Psat= 0.07905222032115693 \n",
      "acc for optim= 0.13172546846585143\n",
      "Epoch:923/1000\n",
      "Loss on train= 0.008542004972696304\n",
      "Loss on test= 0.013861719518899918\n",
      "acc for Lsat= 0.059551762789487844 \n",
      "acc for Psat= 0.08391418187982508 \n",
      "acc for optim= 0.13101791070981156\n",
      "Epoch:924/1000\n",
      "Loss on train= 0.008520450443029404\n",
      "Loss on test= 0.013199839740991592\n",
      "acc for Lsat= 0.06206759106781748 \n",
      "acc for Psat= 0.0863478178779284 \n",
      "acc for optim= 0.13082176807026066\n",
      "Epoch:925/1000\n",
      "Loss on train= 0.008462060242891312\n",
      "Loss on test= 0.013125821016728878\n",
      "acc for Lsat= 0.059225561138656416 \n",
      "acc for Psat= 0.08709297759665384 \n",
      "acc for optim= 0.1309932960611251\n",
      "Epoch:926/1000\n",
      "Loss on train= 0.00856651272624731\n",
      "Loss on test= 0.013084124773740768\n",
      "acc for Lsat= 0.061442471957869005 \n",
      "acc for Psat= 0.0867827861259381 \n",
      "acc for optim= 0.13138272696071202\n",
      "Epoch:927/1000\n",
      "Loss on train= 0.008566472679376602\n",
      "Loss on test= 0.013114283792674541\n",
      "acc for Lsat= 0.06115765016939905 \n",
      "acc for Psat= 0.08254003028074902 \n",
      "acc for optim= 0.1307146639459663\n",
      "Epoch:928/1000\n",
      "Loss on train= 0.008749471977353096\n",
      "Loss on test= 0.015504362061619759\n",
      "acc for Lsat= 0.06357497895757358 \n",
      "acc for Psat= 0.08123907035009728 \n",
      "acc for optim= 0.13204737417399884\n",
      "Epoch:929/1000\n",
      "Loss on train= 0.008755221962928772\n",
      "Loss on test= 0.014263282530009747\n",
      "acc for Lsat= 0.05808890445364846 \n",
      "acc for Psat= 0.08008461878117588 \n",
      "acc for optim= 0.13054608549508784\n",
      "Epoch:930/1000\n",
      "Loss on train= 0.008507857099175453\n",
      "Loss on test= 0.013426980003714561\n",
      "acc for Lsat= 0.05969413245717684 \n",
      "acc for Psat= 0.08058879425128301 \n",
      "acc for optim= 0.13059060219675303\n",
      "Epoch:931/1000\n",
      "Loss on train= 0.008876717649400234\n",
      "Loss on test= 0.013522226363420486\n",
      "acc for Lsat= 0.058198757221301384 \n",
      "acc for Psat= 0.07968792606972988 \n",
      "acc for optim= 0.13235183080865276\n",
      "Epoch:932/1000\n",
      "Loss on train= 0.008375236764550209\n",
      "Loss on test= 0.013880731537938118\n",
      "acc for Lsat= 0.06226995210680697 \n",
      "acc for Psat= 0.0823208828870621 \n",
      "acc for optim= 0.13158233805249134\n",
      "Epoch:933/1000\n",
      "Loss on train= 0.008500594645738602\n",
      "Loss on test= 0.014760117046535015\n",
      "acc for Lsat= 0.06249233012398084 \n",
      "acc for Psat= 0.08588345903489324 \n",
      "acc for optim= 0.1305826412099931\n",
      "Epoch:934/1000\n",
      "Loss on train= 0.00851029995828867\n",
      "Loss on test= 0.014520734548568726\n",
      "acc for Lsat= 0.06175787440604633 \n",
      "acc for Psat= 0.09124637908405728 \n",
      "acc for optim= 0.13242275623811614\n",
      "Epoch:935/1000\n",
      "Loss on train= 0.008209115825593472\n",
      "Loss on test= 0.013432708568871021\n",
      "acc for Lsat= 0.06064692354864544 \n",
      "acc for Psat= 0.0828909772551722 \n",
      "acc for optim= 0.13219157084822655\n",
      "Epoch:936/1000\n",
      "Loss on train= 0.008998196572065353\n",
      "Loss on test= 0.01311990525573492\n",
      "acc for Lsat= 0.05917092619670761 \n",
      "acc for Psat= 0.08276943073918422 \n",
      "acc for optim= 0.13121923456589382\n",
      "Epoch:937/1000\n",
      "Loss on train= 0.008615284226834774\n",
      "Loss on test= 0.012529376894235611\n",
      "acc for Lsat= 0.057634629102216825 \n",
      "acc for Psat= 0.08330417548616727 \n",
      "acc for optim= 0.13120032242602772\n",
      "Epoch:938/1000\n",
      "Loss on train= 0.00858883187174797\n",
      "Loss on test= 0.012675132602453232\n",
      "acc for Lsat= 0.05763436357180277 \n",
      "acc for Psat= 0.07971627227962017 \n",
      "acc for optim= 0.131386191646258\n",
      "Epoch:939/1000\n",
      "Loss on train= 0.008852366358041763\n",
      "Loss on test= 0.012820267118513584\n",
      "acc for Lsat= 0.059068947037061066 \n",
      "acc for Psat= 0.08105661848353016 \n",
      "acc for optim= 0.13154880503813426\n",
      "Epoch:940/1000\n",
      "Loss on train= 0.00816826056689024\n",
      "Loss on test= 0.01318699773401022\n",
      "acc for Lsat= 0.056331617716285934 \n",
      "acc for Psat= 0.084374614391062 \n",
      "acc for optim= 0.13139892278446091\n",
      "Epoch:941/1000\n",
      "Loss on train= 0.008312465623021126\n",
      "Loss on test= 0.01343440916389227\n",
      "acc for Lsat= 0.05866140474875768 \n",
      "acc for Psat= 0.08369599564207925 \n",
      "acc for optim= 0.1310132415137357\n",
      "Epoch:942/1000\n",
      "Loss on train= 0.00893682986497879\n",
      "Loss on test= 0.014164273627102375\n",
      "acc for Lsat= 0.05748436513046424 \n",
      "acc for Psat= 0.08819368829329807 \n",
      "acc for optim= 0.13065822216578657\n",
      "Epoch:943/1000\n",
      "Loss on train= 0.009165238589048386\n",
      "Loss on test= 0.014020875096321106\n",
      "acc for Lsat= 0.06435104086995125 \n",
      "acc for Psat= 0.08495822105970649 \n",
      "acc for optim= 0.1319073479829563\n",
      "Epoch:944/1000\n",
      "Loss on train= 0.008360202424228191\n",
      "Loss on test= 0.013980350457131863\n",
      "acc for Lsat= 0.05632843859493733 \n",
      "acc for Psat= 0.08064565509557725 \n",
      "acc for optim= 0.130856117606163\n",
      "Epoch:945/1000\n",
      "Loss on train= 0.008767817169427872\n",
      "Loss on test= 0.01356712356209755\n",
      "acc for Lsat= 0.057237522138489615 \n",
      "acc for Psat= 0.07884036236339145 \n",
      "acc for optim= 0.13202661813961136\n",
      "Epoch:946/1000\n",
      "Loss on train= 0.00860570278018713\n",
      "Loss on test= 0.013701016083359718\n",
      "acc for Lsat= 0.058526585085524455 \n",
      "acc for Psat= 0.08064367019881806 \n",
      "acc for optim= 0.13126821558301652\n",
      "Epoch:947/1000\n",
      "Loss on train= 0.008677731268107891\n",
      "Loss on test= 0.01335926167666912\n",
      "acc for Lsat= 0.0559746626764536 \n",
      "acc for Psat= 0.07857199614065595 \n",
      "acc for optim= 0.1316775730293658\n",
      "Epoch:948/1000\n",
      "Loss on train= 0.008372674696147442\n",
      "Loss on test= 0.013859670609235764\n",
      "acc for Lsat= 0.05780984734495482 \n",
      "acc for Psat= 0.08252090803451007 \n",
      "acc for optim= 0.1301794739957485\n",
      "Epoch:949/1000\n",
      "Loss on train= 0.00851172674447298\n",
      "Loss on test= 0.012779517099261284\n",
      "acc for Lsat= 0.05615280332664649 \n",
      "acc for Psat= 0.08147835832917028 \n",
      "acc for optim= 0.12963994762135878\n",
      "Epoch:950/1000\n",
      "Loss on train= 0.008887242525815964\n",
      "Loss on test= 0.01356323342770338\n",
      "acc for Lsat= 0.06271379830108749 \n",
      "acc for Psat= 0.08751504448139005 \n",
      "acc for optim= 0.12913747097675998\n",
      "Epoch:951/1000\n",
      "Loss on train= 0.008741351775825024\n",
      "Loss on test= 0.013775440864264965\n",
      "acc for Lsat= 0.05678931950694984 \n",
      "acc for Psat= 0.07959495585204827 \n",
      "acc for optim= 0.13047346122976805\n",
      "Epoch:952/1000\n",
      "Loss on train= 0.008587057702243328\n",
      "Loss on test= 0.013725711032748222\n",
      "acc for Lsat= 0.06461886970533265 \n",
      "acc for Psat= 0.08577131239904297 \n",
      "acc for optim= 0.1307743089273572\n",
      "Epoch:953/1000\n",
      "Loss on train= 0.008832384832203388\n",
      "Loss on test= 0.013139921240508556\n",
      "acc for Lsat= 0.06115623000595305 \n",
      "acc for Psat= 0.08447467991047435 \n",
      "acc for optim= 0.13122344617214468\n",
      "Epoch:954/1000\n",
      "Loss on train= 0.008662051521241665\n",
      "Loss on test= 0.01361573301255703\n",
      "acc for Lsat= 0.062433365649647174 \n",
      "acc for Psat= 0.08505992053283586 \n",
      "acc for optim= 0.13046099719487958\n",
      "Epoch:955/1000\n",
      "Loss on train= 0.008248238824307919\n",
      "Loss on test= 0.012875880114734173\n",
      "acc for Lsat= 0.05851657531327671 \n",
      "acc for Psat= 0.08265217348105378 \n",
      "acc for optim= 0.1307873958721757\n",
      "Epoch:956/1000\n",
      "Loss on train= 0.008823469281196594\n",
      "Loss on test= 0.013059776276350021\n",
      "acc for Lsat= 0.05804785009887484 \n",
      "acc for Psat= 0.0852462092621459 \n",
      "acc for optim= 0.1312089492049482\n",
      "Epoch:957/1000\n",
      "Loss on train= 0.008603430353105068\n",
      "Loss on test= 0.013139437884092331\n",
      "acc for Lsat= 0.06210447500149409 \n",
      "acc for Psat= 0.08601539706190427 \n",
      "acc for optim= 0.12980648778999845\n",
      "Epoch:958/1000\n",
      "Loss on train= 0.008605943992733955\n",
      "Loss on test= 0.01403083372861147\n",
      "acc for Lsat= 0.06080602564745479 \n",
      "acc for Psat= 0.08353396041525735 \n",
      "acc for optim= 0.12946639442040275\n",
      "Epoch:959/1000\n",
      "Loss on train= 0.00828490499407053\n",
      "Loss on test= 0.014369012787938118\n",
      "acc for Lsat= 0.05744777412878142 \n",
      "acc for Psat= 0.07877624850306246 \n",
      "acc for optim= 0.13130914103239774\n",
      "Epoch:960/1000\n",
      "Loss on train= 0.008080687373876572\n",
      "Loss on test= 0.013993940316140652\n",
      "acc for Lsat= 0.058025690168142324 \n",
      "acc for Psat= 0.08252871833327742 \n",
      "acc for optim= 0.13113610057367214\n",
      "Epoch:961/1000\n",
      "Loss on train= 0.008524278178811073\n",
      "Loss on test= 0.013472486287355423\n",
      "acc for Lsat= 0.063393554257022 \n",
      "acc for Psat= 0.08473245547049574 \n",
      "acc for optim= 0.13114727834860485\n",
      "Epoch:962/1000\n",
      "Loss on train= 0.008284317329525948\n",
      "Loss on test= 0.013368040323257446\n",
      "acc for Lsat= 0.057907311618328086 \n",
      "acc for Psat= 0.08200901101032892 \n",
      "acc for optim= 0.13093045049657423\n",
      "Epoch:963/1000\n",
      "Loss on train= 0.00842464342713356\n",
      "Loss on test= 0.012988848611712456\n",
      "acc for Lsat= 0.05973820446266068 \n",
      "acc for Psat= 0.0804644306914674 \n",
      "acc for optim= 0.1314237381021182\n",
      "Epoch:964/1000\n",
      "Loss on train= 0.008469165302813053\n",
      "Loss on test= 0.013136543333530426\n",
      "acc for Lsat= 0.06169381414850553 \n",
      "acc for Psat= 0.08872295700841477 \n",
      "acc for optim= 0.1311158118562566\n",
      "Epoch:965/1000\n",
      "Loss on train= 0.008567679673433304\n",
      "Loss on test= 0.013802164234220982\n",
      "acc for Lsat= 0.0635933578842216 \n",
      "acc for Psat= 0.08277756435175737 \n",
      "acc for optim= 0.13135264892545012\n",
      "Epoch:966/1000\n",
      "Loss on train= 0.008514218963682652\n",
      "Loss on test= 0.013239271938800812\n",
      "acc for Lsat= 0.06157860515846145 \n",
      "acc for Psat= 0.0817628763202164 \n",
      "acc for optim= 0.13052537196005384\n",
      "Epoch:967/1000\n",
      "Loss on train= 0.008437205106019974\n",
      "Loss on test= 0.013486621901392937\n",
      "acc for Lsat= 0.058663164575894676 \n",
      "acc for Psat= 0.0811890263731281 \n",
      "acc for optim= 0.13071119961225328\n",
      "Epoch:968/1000\n",
      "Loss on train= 0.008322486653923988\n",
      "Loss on test= 0.014685023576021194\n",
      "acc for Lsat= 0.058493521105911994 \n",
      "acc for Psat= 0.07932432367362911 \n",
      "acc for optim= 0.13147827063997586\n",
      "Epoch:969/1000\n",
      "Loss on train= 0.0087139206007123\n",
      "Loss on test= 0.013111944310367107\n",
      "acc for Lsat= 0.05811603342493375 \n",
      "acc for Psat= 0.08186094212449259 \n",
      "acc for optim= 0.13137617173294228\n",
      "Epoch:970/1000\n",
      "Loss on train= 0.008448075503110886\n",
      "Loss on test= 0.012762036174535751\n",
      "acc for Lsat= 0.06289005618956353 \n",
      "acc for Psat= 0.08375883334212833 \n",
      "acc for optim= 0.13046170608657928\n",
      "Epoch:971/1000\n",
      "Loss on train= 0.008460160344839096\n",
      "Loss on test= 0.01326789055019617\n",
      "acc for Lsat= 0.061280893368853465 \n",
      "acc for Psat= 0.08235857830279403 \n",
      "acc for optim= 0.13192292017241317\n",
      "Epoch:972/1000\n",
      "Loss on train= 0.008629768155515194\n",
      "Loss on test= 0.013740848749876022\n",
      "acc for Lsat= 0.06038347896602419 \n",
      "acc for Psat= 0.08905211343533463 \n",
      "acc for optim= 0.13110303576621743\n",
      "Epoch:973/1000\n",
      "Loss on train= 0.008450300432741642\n",
      "Loss on test= 0.013846066780388355\n",
      "acc for Lsat= 0.055944439437654286 \n",
      "acc for Psat= 0.07864138384660085 \n",
      "acc for optim= 0.131958398843805\n",
      "Epoch:974/1000\n",
      "Loss on train= 0.008378124795854092\n",
      "Loss on test= 0.014246286824345589\n",
      "acc for Lsat= 0.05959369010395474 \n",
      "acc for Psat= 0.08043947695857949 \n",
      "acc for optim= 0.13179247886356382\n",
      "Epoch:975/1000\n",
      "Loss on train= 0.008752789348363876\n",
      "Loss on test= 0.013201181776821613\n",
      "acc for Lsat= 0.060174854265318975 \n",
      "acc for Psat= 0.0861425528095828 \n",
      "acc for optim= 0.131353837541408\n",
      "Epoch:976/1000\n",
      "Loss on train= 0.008579451590776443\n",
      "Loss on test= 0.014188546687364578\n",
      "acc for Lsat= 0.07298133737511106 \n",
      "acc for Psat= 0.09309560250904825 \n",
      "acc for optim= 0.13018658194277022\n",
      "Epoch:977/1000\n",
      "Loss on train= 0.008606751449406147\n",
      "Loss on test= 0.013534209690988064\n",
      "acc for Lsat= 0.06759434954987631 \n",
      "acc for Psat= 0.0874805926448769 \n",
      "acc for optim= 0.13223886485728953\n",
      "Epoch:978/1000\n",
      "Loss on train= 0.008574710227549076\n",
      "Loss on test= 0.013659510761499405\n",
      "acc for Lsat= 0.05936156283650134 \n",
      "acc for Psat= 0.08450543151961433 \n",
      "acc for optim= 0.1317056875882877\n",
      "Epoch:979/1000\n",
      "Loss on train= 0.008446169085800648\n",
      "Loss on test= 0.01345373596996069\n",
      "acc for Lsat= 0.06276656778322326 \n",
      "acc for Psat= 0.08840726373924149 \n",
      "acc for optim= 0.13160359079225198\n",
      "Epoch:980/1000\n",
      "Loss on train= 0.008233710192143917\n",
      "Loss on test= 0.013664569705724716\n",
      "acc for Lsat= 0.05858481857511734 \n",
      "acc for Psat= 0.08144841500454476 \n",
      "acc for optim= 0.13202674210899407\n",
      "Epoch:981/1000\n",
      "Loss on train= 0.008190181106328964\n",
      "Loss on test= 0.014044569805264473\n",
      "acc for Lsat= 0.06355315984951125 \n",
      "acc for Psat= 0.09115634014209112 \n",
      "acc for optim= 0.13051947828175295\n",
      "Epoch:982/1000\n",
      "Loss on train= 0.008423756808042526\n",
      "Loss on test= 0.013550829142332077\n",
      "acc for Lsat= 0.058302723285224706 \n",
      "acc for Psat= 0.08336835950613022 \n",
      "acc for optim= 0.13153681507748036\n",
      "Epoch:983/1000\n",
      "Loss on train= 0.008546266704797745\n",
      "Loss on test= 0.013669256120920181\n",
      "acc for Lsat= 0.06161142355865902 \n",
      "acc for Psat= 0.0845517658525043 \n",
      "acc for optim= 0.13206181263344155\n",
      "Epoch:984/1000\n",
      "Loss on train= 0.00826012622565031\n",
      "Loss on test= 0.013337483629584312\n",
      "acc for Lsat= 0.06254910851518312 \n",
      "acc for Psat= 0.08683406834801038 \n",
      "acc for optim= 0.13299491026749216\n",
      "Epoch:985/1000\n",
      "Loss on train= 0.008366862311959267\n",
      "Loss on test= 0.013541514985263348\n",
      "acc for Lsat= 0.07895513806078171 \n",
      "acc for Psat= 0.09570883992645476 \n",
      "acc for optim= 0.13193708004222973\n",
      "Epoch:986/1000\n",
      "Loss on train= 0.008516996167600155\n",
      "Loss on test= 0.013015604577958584\n",
      "acc for Lsat= 0.06104473513033654 \n",
      "acc for Psat= 0.08843772552079623 \n",
      "acc for optim= 0.1314778031160434\n",
      "Epoch:987/1000\n",
      "Loss on train= 0.008349955081939697\n",
      "Loss on test= 0.013218546286225319\n",
      "acc for Lsat= 0.0586106088426378 \n",
      "acc for Psat= 0.0808481990877125 \n",
      "acc for optim= 0.13095327066192922\n",
      "Epoch:988/1000\n",
      "Loss on train= 0.008208720944821835\n",
      "Loss on test= 0.014257518574595451\n",
      "acc for Lsat= 0.05733850018845664 \n",
      "acc for Psat= 0.0833595923251576 \n",
      "acc for optim= 0.13227005696131122\n",
      "Epoch:989/1000\n",
      "Loss on train= 0.008136660791933537\n",
      "Loss on test= 0.013853941112756729\n",
      "acc for Lsat= 0.057597887184884816 \n",
      "acc for Psat= 0.08219944677419132 \n",
      "acc for optim= 0.13300274055865074\n",
      "Epoch:990/1000\n",
      "Loss on train= 0.008308745920658112\n",
      "Loss on test= 0.013449311256408691\n",
      "acc for Lsat= 0.05884237587451935 \n",
      "acc for Psat= 0.08238585009757016 \n",
      "acc for optim= 0.13181981028368076\n",
      "Epoch:991/1000\n",
      "Loss on train= 0.008516392670571804\n",
      "Loss on test= 0.014436108991503716\n",
      "acc for Lsat= 0.06587704428368145 \n",
      "acc for Psat= 0.09086531284782622 \n",
      "acc for optim= 0.1308552985234807\n",
      "Epoch:992/1000\n",
      "Loss on train= 0.008543923497200012\n",
      "Loss on test= 0.013859111815690994\n",
      "acc for Lsat= 0.06046992250614696 \n",
      "acc for Psat= 0.09040192183521059 \n",
      "acc for optim= 0.13097253812270035\n",
      "Epoch:993/1000\n",
      "Loss on train= 0.008836613036692142\n",
      "Loss on test= 0.013349923305213451\n",
      "acc for Lsat= 0.05791653034587701 \n",
      "acc for Psat= 0.08371169610569874 \n",
      "acc for optim= 0.13248723542524707\n",
      "Epoch:994/1000\n",
      "Loss on train= 0.00846768170595169\n",
      "Loss on test= 0.013347875326871872\n",
      "acc for Lsat= 0.06475584929188091 \n",
      "acc for Psat= 0.08905466579728656 \n",
      "acc for optim= 0.13343593618936012\n",
      "Epoch:995/1000\n",
      "Loss on train= 0.008413567207753658\n",
      "Loss on test= 0.014199528843164444\n",
      "acc for Lsat= 0.06016613509919909 \n",
      "acc for Psat= 0.08678855262696741 \n",
      "acc for optim= 0.1320395577078064\n",
      "Epoch:996/1000\n",
      "Loss on train= 0.008485519327223301\n",
      "Loss on test= 0.013470913283526897\n",
      "acc for Lsat= 0.05862799170944425 \n",
      "acc for Psat= 0.08471958748996258 \n",
      "acc for optim= 0.13264653008017277\n",
      "Epoch:997/1000\n",
      "Loss on train= 0.008157542906701565\n",
      "Loss on test= 0.013667718507349491\n",
      "acc for Lsat= 0.059724393404192395 \n",
      "acc for Psat= 0.08494095268348854 \n",
      "acc for optim= 0.13340500866373375\n",
      "Epoch:998/1000\n",
      "Loss on train= 0.008407745510339737\n",
      "Loss on test= 0.013505459763109684\n",
      "acc for Lsat= 0.060013325264056526 \n",
      "acc for Psat= 0.08241146583524014 \n",
      "acc for optim= 0.13247562646865846\n",
      "Epoch:999/1000\n",
      "Loss on train= 0.008580449037253857\n",
      "Loss on test= 0.013523423112928867\n",
      "acc for Lsat= 0.06384843612710635 \n",
      "acc for Psat= 0.08337629892759854 \n",
      "acc for optim= 0.13238164600398802\n",
      "Epoch:1000/1000\n",
      "Loss on train= 0.008572574704885483\n",
      "Loss on test= 0.012726015411317348\n",
      "acc for Lsat= 0.05985242567128606 \n",
      "acc for Psat= 0.08177292686369685 \n",
      "acc for optim= 0.13226272621088558\n",
      "Fold 4\n",
      "Epoch:1/1000\n",
      "Loss on train= 0.19450682401657104\n",
      "Loss on test= 0.0922236442565918\n",
      "acc for Lsat= 0.42535404430495366 \n",
      "acc for Psat= 0.5263858788543279 \n",
      "acc for optim= 0.19520164459115932\n",
      "Epoch:2/1000\n",
      "Loss on train= 0.08224993199110031\n",
      "Loss on test= 0.07430589944124222\n",
      "acc for Lsat= 0.2781535400284661 \n",
      "acc for Psat= 0.4830210242006514 \n",
      "acc for optim= 0.16610276136133403\n",
      "Epoch:3/1000\n",
      "Loss on train= 0.06342638283967972\n",
      "Loss on test= 0.06205233559012413\n",
      "acc for Lsat= 0.2869304345713722 \n",
      "acc for Psat= 0.41664957801500957 \n",
      "acc for optim= 0.16420593079593448\n",
      "Epoch:4/1000\n",
      "Loss on train= 0.058859262615442276\n",
      "Loss on test= 0.0712149441242218\n",
      "acc for Lsat= 0.3423703796333737 \n",
      "acc for Psat= 0.3851743292477396 \n",
      "acc for optim= 0.18712901522715886\n",
      "Epoch:5/1000\n",
      "Loss on train= 0.057752467691898346\n",
      "Loss on test= 0.05849174037575722\n",
      "acc for Lsat= 0.28110216591093273 \n",
      "acc for Psat= 0.4103206833203634 \n",
      "acc for optim= 0.16445271923310228\n",
      "Epoch:6/1000\n",
      "Loss on train= 0.051496781408786774\n",
      "Loss on test= 0.051507093012332916\n",
      "acc for Lsat= 0.27666569352149967 \n",
      "acc for Psat= 0.39189263184865314 \n",
      "acc for optim= 0.16637662599484124\n",
      "Epoch:7/1000\n",
      "Loss on train= 0.05251561105251312\n",
      "Loss on test= 0.05268259346485138\n",
      "acc for Lsat= 0.30946748041444355 \n",
      "acc for Psat= 0.3527709586752786 \n",
      "acc for optim= 0.16713211586078006\n",
      "Epoch:8/1000\n",
      "Loss on train= 0.051145780831575394\n",
      "Loss on test= 0.051289670169353485\n",
      "acc for Lsat= 0.2558750099605984 \n",
      "acc for Psat= 0.4304581522941589 \n",
      "acc for optim= 0.18521681179602942\n",
      "Epoch:9/1000\n",
      "Loss on train= 0.05190503969788551\n",
      "Loss on test= 0.05048299953341484\n",
      "acc for Lsat= 0.2590395828088125 \n",
      "acc for Psat= 0.3716723567909665 \n",
      "acc for optim= 0.16876362181372115\n",
      "Epoch:10/1000\n",
      "Loss on train= 0.047634437680244446\n",
      "Loss on test= 0.0508207231760025\n",
      "acc for Lsat= 0.2615471873018477 \n",
      "acc for Psat= 0.3827333794699775 \n",
      "acc for optim= 0.16281498281492127\n",
      "Epoch:11/1000\n",
      "Loss on train= 0.04675477370619774\n",
      "Loss on test= 0.046936243772506714\n",
      "acc for Lsat= 0.25290262699127203 \n",
      "acc for Psat= 0.3558318386475245 \n",
      "acc for optim= 0.16389475241303444\n",
      "Epoch:12/1000\n",
      "Loss on train= 0.046220775693655014\n",
      "Loss on test= 0.044631585478782654\n",
      "acc for Lsat= 0.2603536638948653 \n",
      "acc for Psat= 0.3509406536817551 \n",
      "acc for optim= 0.16508746784594322\n",
      "Epoch:13/1000\n",
      "Loss on train= 0.04452582076191902\n",
      "Loss on test= 0.04359794035553932\n",
      "acc for Lsat= 0.25151365300019585 \n",
      "acc for Psat= 0.342776044872072 \n",
      "acc for optim= 0.1709722583906518\n",
      "Epoch:14/1000\n",
      "Loss on train= 0.044628433883190155\n",
      "Loss on test= 0.042265888303518295\n",
      "acc for Lsat= 0.2635746591620975 \n",
      "acc for Psat= 0.3245792064401838 \n",
      "acc for optim= 0.16199348982837466\n",
      "Epoch:15/1000\n",
      "Loss on train= 0.04349745810031891\n",
      "Loss on test= 0.05236417055130005\n",
      "acc for Lsat= 0.2717680586708917 \n",
      "acc for Psat= 0.45992078317536234 \n",
      "acc for optim= 0.1662689300047027\n",
      "Epoch:16/1000\n",
      "Loss on train= 0.04528282210230827\n",
      "Loss on test= 0.04545364901423454\n",
      "acc for Lsat= 0.2587125658988953 \n",
      "acc for Psat= 0.30994610554642144 \n",
      "acc for optim= 0.16168336955209572\n",
      "Epoch:17/1000\n",
      "Loss on train= 0.04274868965148926\n",
      "Loss on test= 0.041565343737602234\n",
      "acc for Lsat= 0.2470873862504959 \n",
      "acc for Psat= 0.30206861562199067 \n",
      "acc for optim= 0.16605098065402768\n",
      "Epoch:18/1000\n",
      "Loss on train= 0.04354856535792351\n",
      "Loss on test= 0.04136524721980095\n",
      "acc for Lsat= 0.27222665879461494 \n",
      "acc for Psat= 0.3011751413345337 \n",
      "acc for optim= 0.17085235109552743\n",
      "Epoch:19/1000\n",
      "Loss on train= 0.04493771493434906\n",
      "Loss on test= 0.0519992858171463\n",
      "acc for Lsat= 0.3811119086212582 \n",
      "acc for Psat= 0.3182821322232485 \n",
      "acc for optim= 0.1655491630236308\n",
      "Epoch:20/1000\n",
      "Loss on train= 0.04374495521187782\n",
      "Loss on test= 0.04107082262635231\n",
      "acc for Lsat= 0.2570750219954385 \n",
      "acc for Psat= 0.30540968312157524 \n",
      "acc for optim= 0.16425281895531546\n",
      "Epoch:21/1000\n",
      "Loss on train= 0.04169555380940437\n",
      "Loss on test= 0.03938670456409454\n",
      "acc for Lsat= 0.2570889784230126 \n",
      "acc for Psat= 0.32251653042104506 \n",
      "acc for optim= 0.16244110688567162\n",
      "Epoch:22/1000\n",
      "Loss on train= 0.04020679369568825\n",
      "Loss on test= 0.044536832720041275\n",
      "acc for Lsat= 0.2647126664717992 \n",
      "acc for Psat= 0.3188542574644089 \n",
      "acc for optim= 0.16198180475168755\n",
      "Epoch:23/1000\n",
      "Loss on train= 0.04082229733467102\n",
      "Loss on test= 0.043304648250341415\n",
      "acc for Lsat= 0.2528452727529738 \n",
      "acc for Psat= 0.31012920571698077 \n",
      "acc for optim= 0.16422835952705805\n",
      "Epoch:24/1000\n",
      "Loss on train= 0.04052680730819702\n",
      "Loss on test= 0.04012057185173035\n",
      "acc for Lsat= 0.260212988489204 \n",
      "acc for Psat= 0.2982309952378273 \n",
      "acc for optim= 0.16370181068778042\n",
      "Epoch:25/1000\n",
      "Loss on train= 0.038748931139707565\n",
      "Loss on test= 0.042040128260850906\n",
      "acc for Lsat= 0.28574034902784556 \n",
      "acc for Psat= 0.29031910465823274 \n",
      "acc for optim= 0.1693631625423829\n",
      "Epoch:26/1000\n",
      "Loss on train= 0.038934119045734406\n",
      "Loss on test= 0.04009678214788437\n",
      "acc for Lsat= 0.2507599807447857 \n",
      "acc for Psat= 0.34179068538877705 \n",
      "acc for optim= 0.1618744481768873\n",
      "Epoch:27/1000\n",
      "Loss on train= 0.041668884456157684\n",
      "Loss on test= 0.04498209059238434\n",
      "acc for Lsat= 0.3204666602114836 \n",
      "acc for Psat= 0.2821507917510139 \n",
      "acc for optim= 0.16252649070488082\n",
      "Epoch:28/1000\n",
      "Loss on train= 0.03926178440451622\n",
      "Loss on test= 0.037270788103342056\n",
      "acc for Lsat= 0.24466152720981174 \n",
      "acc for Psat= 0.3012860831287172 \n",
      "acc for optim= 0.16587837479180756\n",
      "Epoch:29/1000\n",
      "Loss on train= 0.037231188267469406\n",
      "Loss on test= 0.03975522890686989\n",
      "acc for Lsat= 0.2696099687781599 \n",
      "acc for Psat= 0.26425864092177825 \n",
      "acc for optim= 0.16089272184504402\n",
      "Epoch:30/1000\n",
      "Loss on train= 0.036436084657907486\n",
      "Loss on test= 0.0392044298350811\n",
      "acc for Lsat= 0.2438478006256951 \n",
      "acc for Psat= 0.2958229293425878 \n",
      "acc for optim= 0.1620669743253125\n",
      "Epoch:31/1000\n",
      "Loss on train= 0.03792865574359894\n",
      "Loss on test= 0.03918539732694626\n",
      "acc for Lsat= 0.26611827048990455 \n",
      "acc for Psat= 0.2673904942141639 \n",
      "acc for optim= 0.1659570508533054\n",
      "Epoch:32/1000\n",
      "Loss on train= 0.03772222250699997\n",
      "Loss on test= 0.03791226074099541\n",
      "acc for Lsat= 0.25699583772155976 \n",
      "acc for Psat= 0.2673706885841158 \n",
      "acc for optim= 0.17546165866984262\n",
      "Epoch:33/1000\n",
      "Loss on train= 0.03639201074838638\n",
      "Loss on test= 0.03662183880805969\n",
      "acc for Lsat= 0.24392316192388538 \n",
      "acc for Psat= 0.27731115669012074 \n",
      "acc for optim= 0.16511473962002332\n",
      "Epoch:34/1000\n",
      "Loss on train= 0.03603966534137726\n",
      "Loss on test= 0.03877183422446251\n",
      "acc for Lsat= 0.26695020861095853 \n",
      "acc for Psat= 0.26987314075231555 \n",
      "acc for optim= 0.16480458296007577\n",
      "Epoch:35/1000\n",
      "Loss on train= 0.035717908293008804\n",
      "Loss on test= 0.03793899714946747\n",
      "acc for Lsat= 0.2511212292644713 \n",
      "acc for Psat= 0.325135123067432 \n",
      "acc for optim= 0.16650799504584735\n",
      "Epoch:36/1000\n",
      "Loss on train= 0.03615255653858185\n",
      "Loss on test= 0.03806019201874733\n",
      "acc for Lsat= 0.26466794378227665 \n",
      "acc for Psat= 0.2998455521133211 \n",
      "acc for optim= 0.16296177787913216\n",
      "Epoch:37/1000\n",
      "Loss on train= 0.035432472825050354\n",
      "Loss on test= 0.03560816869139671\n",
      "acc for Lsat= 0.2603112594948875 \n",
      "acc for Psat= 0.28017788430054974 \n",
      "acc for optim= 0.1787242218852043\n",
      "Epoch:38/1000\n",
      "Loss on train= 0.0364103764295578\n",
      "Loss on test= 0.03364818915724754\n",
      "acc for Lsat= 0.24422519074545967 \n",
      "acc for Psat= 0.26913525760173795 \n",
      "acc for optim= 0.15945410008231803\n",
      "Epoch:39/1000\n",
      "Loss on train= 0.03294488787651062\n",
      "Loss on test= 0.03748207539319992\n",
      "acc for Lsat= 0.2471245969335238 \n",
      "acc for Psat= 0.28370241754584835 \n",
      "acc for optim= 0.1683337535709143\n",
      "Epoch:40/1000\n",
      "Loss on train= 0.034477416425943375\n",
      "Loss on test= 0.036909621208906174\n",
      "acc for Lsat= 0.2595856226152844 \n",
      "acc for Psat= 0.24750083635250725 \n",
      "acc for optim= 0.160347134537167\n",
      "Epoch:41/1000\n",
      "Loss on train= 0.03448585048317909\n",
      "Loss on test= 0.037675414234399796\n",
      "acc for Lsat= 0.24808472759193842 \n",
      "acc for Psat= 0.3295729802714454 \n",
      "acc for optim= 0.15941928231881722\n",
      "Epoch:42/1000\n",
      "Loss on train= 0.03487711772322655\n",
      "Loss on test= 0.033280327916145325\n",
      "acc for Lsat= 0.24694093333350287 \n",
      "acc for Psat= 0.24163855496380066 \n",
      "acc for optim= 0.16378220634327997\n",
      "Epoch:43/1000\n",
      "Loss on train= 0.03370286896824837\n",
      "Loss on test= 0.0355718769133091\n",
      "acc for Lsat= 0.25835980061027736 \n",
      "acc for Psat= 0.2375467465983497 \n",
      "acc for optim= 0.16375337812221716\n",
      "Epoch:44/1000\n",
      "Loss on train= 0.03264376148581505\n",
      "Loss on test= 0.03362826630473137\n",
      "acc for Lsat= 0.25404605799251134 \n",
      "acc for Psat= 0.24306850168440075 \n",
      "acc for optim= 0.1580230606926812\n",
      "Epoch:45/1000\n",
      "Loss on train= 0.03333979472517967\n",
      "Loss on test= 0.03446914255619049\n",
      "acc for Lsat= 0.2862504983941714 \n",
      "acc for Psat= 0.2367294298277961 \n",
      "acc for optim= 0.1632013853225443\n",
      "Epoch:46/1000\n",
      "Loss on train= 0.032537791877985\n",
      "Loss on test= 0.03436297923326492\n",
      "acc for Lsat= 0.2429739303059048 \n",
      "acc for Psat= 0.26031069788667893 \n",
      "acc for optim= 0.1604055318567488\n",
      "Epoch:47/1000\n",
      "Loss on train= 0.03300953656435013\n",
      "Loss on test= 0.034135591238737106\n",
      "acc for Lsat= 0.23275219632519611 \n",
      "acc for Psat= 0.30248935951126943 \n",
      "acc for optim= 0.159152706588308\n",
      "Epoch:48/1000\n",
      "Loss on train= 0.033427659422159195\n",
      "Loss on test= 0.033023685216903687\n",
      "acc for Lsat= 0.23313747131162219 \n",
      "acc for Psat= 0.23255277987983491 \n",
      "acc for optim= 0.1603105159269439\n",
      "Epoch:49/1000\n",
      "Loss on train= 0.03202402964234352\n",
      "Loss on test= 0.03988642990589142\n",
      "acc for Lsat= 0.2521165172259013 \n",
      "acc for Psat= 0.32755288150575423 \n",
      "acc for optim= 0.16262462437152864\n",
      "Epoch:50/1000\n",
      "Loss on train= 0.033486779779195786\n",
      "Loss on test= 0.03286514803767204\n",
      "acc for Lsat= 0.254105778866344 \n",
      "acc for Psat= 0.24182202882236903 \n",
      "acc for optim= 0.15902285430994295\n",
      "Epoch:51/1000\n",
      "Loss on train= 0.03250080719590187\n",
      "Loss on test= 0.03470255807042122\n",
      "acc for Lsat= 0.24624653806289037 \n",
      "acc for Psat= 0.24341123269663914 \n",
      "acc for optim= 0.16106076505449085\n",
      "Epoch:52/1000\n",
      "Loss on train= 0.03288250416517258\n",
      "Loss on test= 0.03344079479575157\n",
      "acc for Lsat= 0.22605447785721886 \n",
      "acc for Psat= 0.23102757069799637 \n",
      "acc for optim= 0.1621844874487983\n",
      "Epoch:53/1000\n",
      "Loss on train= 0.03065475821495056\n",
      "Loss on test= 0.031842030584812164\n",
      "acc for Lsat= 0.22692322499222226 \n",
      "acc for Psat= 0.25407220853699575 \n",
      "acc for optim= 0.1592799017826716\n",
      "Epoch:54/1000\n",
      "Loss on train= 0.030401499941945076\n",
      "Loss on test= 0.03385104984045029\n",
      "acc for Lsat= 0.237245327896542 \n",
      "acc for Psat= 0.23836946090062464 \n",
      "acc for optim= 0.16166728453503718\n",
      "Epoch:55/1000\n",
      "Loss on train= 0.031209181994199753\n",
      "Loss on test= 0.03719828277826309\n",
      "acc for Lsat= 0.23942454655965173 \n",
      "acc for Psat= 0.3748665650685628 \n",
      "acc for optim= 0.16262136757787726\n",
      "Epoch:56/1000\n",
      "Loss on train= 0.03257346525788307\n",
      "Loss on test= 0.04035691171884537\n",
      "acc for Lsat= 0.27405311597718135 \n",
      "acc for Psat= 0.26632713129123053 \n",
      "acc for optim= 0.161669962025351\n",
      "Epoch:57/1000\n",
      "Loss on train= 0.03378156200051308\n",
      "Loss on test= 0.03245358169078827\n",
      "acc for Lsat= 0.2432737797498703 \n",
      "acc for Psat= 0.24967022803094652 \n",
      "acc for optim= 0.1577946852064795\n",
      "Epoch:58/1000\n",
      "Loss on train= 0.030177714303135872\n",
      "Loss on test= 0.031407229602336884\n",
      "acc for Lsat= 0.23156280964612957 \n",
      "acc for Psat= 0.23626528547869782 \n",
      "acc for optim= 0.15929697478810942\n",
      "Epoch:59/1000\n",
      "Loss on train= 0.03087158128619194\n",
      "Loss on test= 0.03268539905548096\n",
      "acc for Lsat= 0.21491378446420029 \n",
      "acc for Psat= 0.21795331570837234 \n",
      "acc for optim= 0.16608922945128549\n",
      "Epoch:60/1000\n",
      "Loss on train= 0.02957441285252571\n",
      "Loss on test= 0.03135901317000389\n",
      "acc for Lsat= 0.23035500115818447 \n",
      "acc for Psat= 0.2582575658957163 \n",
      "acc for optim= 0.1570239166418711\n",
      "Epoch:61/1000\n",
      "Loss on train= 0.02918161079287529\n",
      "Loss on test= 0.031889840960502625\n",
      "acc for Lsat= 0.2701861320270433 \n",
      "acc for Psat= 0.2165284760296345 \n",
      "acc for optim= 0.15859343848294682\n",
      "Epoch:62/1000\n",
      "Loss on train= 0.029110556468367577\n",
      "Loss on test= 0.03061184287071228\n",
      "acc for Lsat= 0.21857169833448198 \n",
      "acc for Psat= 0.22498473856184217 \n",
      "acc for optim= 0.15762641641000907\n",
      "Epoch:63/1000\n",
      "Loss on train= 0.029582412913441658\n",
      "Loss on test= 0.030750112608075142\n",
      "acc for Lsat= 0.20972850537962384 \n",
      "acc for Psat= 0.21480278372764583 \n",
      "acc for optim= 0.15872912901039754\n",
      "Epoch:64/1000\n",
      "Loss on train= 0.030916552990674973\n",
      "Loss on test= 0.0332125648856163\n",
      "acc for Lsat= 0.24026681201325525 \n",
      "acc for Psat= 0.23491068217489453 \n",
      "acc for optim= 0.15676654809051088\n",
      "Epoch:65/1000\n",
      "Loss on train= 0.029157297685742378\n",
      "Loss on test= 0.028773298487067223\n",
      "acc for Lsat= 0.19867558810446 \n",
      "acc for Psat= 0.20607253139217693 \n",
      "acc for optim= 0.16192620123426119\n",
      "Epoch:66/1000\n",
      "Loss on train= 0.02817670814692974\n",
      "Loss on test= 0.029781464487314224\n",
      "acc for Lsat= 0.21011454065640767 \n",
      "acc for Psat= 0.21983511414792806 \n",
      "acc for optim= 0.15937078181240294\n",
      "Epoch:67/1000\n",
      "Loss on train= 0.028797995299100876\n",
      "Loss on test= 0.029115796089172363\n",
      "acc for Lsat= 0.17476772169272103 \n",
      "acc for Psat= 0.1955243933945894 \n",
      "acc for optim= 0.1620025775498814\n",
      "Epoch:68/1000\n",
      "Loss on train= 0.028211655095219612\n",
      "Loss on test= 0.026967549696564674\n",
      "acc for Lsat= 0.1738791522052553 \n",
      "acc for Psat= 0.20946093847354252 \n",
      "acc for optim= 0.16613177255623868\n",
      "Epoch:69/1000\n",
      "Loss on train= 0.027014292776584625\n",
      "Loss on test= 0.028883477672934532\n",
      "acc for Lsat= 0.174217792848746 \n",
      "acc for Psat= 0.1912524468368954 \n",
      "acc for optim= 0.15880700523654617\n",
      "Epoch:70/1000\n",
      "Loss on train= 0.02606118470430374\n",
      "Loss on test= 0.029224319383502007\n",
      "acc for Lsat= 0.1908697641558117 \n",
      "acc for Psat= 0.21825084785620372 \n",
      "acc for optim= 0.16119945637053912\n",
      "Epoch:71/1000\n",
      "Loss on train= 0.02670770324766636\n",
      "Loss on test= 0.028071582317352295\n",
      "acc for Lsat= 0.1603120373355018 \n",
      "acc for Psat= 0.18528397439254654 \n",
      "acc for optim= 0.1581025497780906\n",
      "Epoch:72/1000\n",
      "Loss on train= 0.02555791288614273\n",
      "Loss on test= 0.02739259973168373\n",
      "acc for Lsat= 0.1681943818926811 \n",
      "acc for Psat= 0.23622254331906634 \n",
      "acc for optim= 0.16084827615155117\n",
      "Epoch:73/1000\n",
      "Loss on train= 0.025440942496061325\n",
      "Loss on test= 0.027285346761345863\n",
      "acc for Lsat= 0.1526119331518809 \n",
      "acc for Psat= 0.2014102982150184 \n",
      "acc for optim= 0.15931836234198676\n",
      "Epoch:74/1000\n",
      "Loss on train= 0.025322945788502693\n",
      "Loss on test= 0.026996992528438568\n",
      "acc for Lsat= 0.16664242082171968 \n",
      "acc for Psat= 0.190897986623976 \n",
      "acc for optim= 0.15937935966584418\n",
      "Epoch:75/1000\n",
      "Loss on train= 0.026423178613185883\n",
      "Loss on test= 0.028667330741882324\n",
      "acc for Lsat= 0.15140006012386747 \n",
      "acc for Psat= 0.20197163422902423 \n",
      "acc for optim= 0.15759111477269064\n",
      "Epoch:76/1000\n",
      "Loss on train= 0.025818057358264923\n",
      "Loss on test= 0.02703227661550045\n",
      "acc for Lsat= 0.1561198517680168 \n",
      "acc for Psat= 0.1992845290237003 \n",
      "acc for optim= 0.16776491006215416\n",
      "Epoch:77/1000\n",
      "Loss on train= 0.025273161008954048\n",
      "Loss on test= 0.03085298277437687\n",
      "acc for Lsat= 0.16525180041790008 \n",
      "acc for Psat= 0.23426303929752773 \n",
      "acc for optim= 0.16109268135494656\n",
      "Epoch:78/1000\n",
      "Loss on train= 0.026321519166231155\n",
      "Loss on test= 0.02606300264596939\n",
      "acc for Lsat= 0.1647784626318349 \n",
      "acc for Psat= 0.17538928389549258 \n",
      "acc for optim= 0.1635790992114279\n",
      "Epoch:79/1000\n",
      "Loss on train= 0.02483024075627327\n",
      "Loss on test= 0.028523899614810944\n",
      "acc for Lsat= 0.16798753705289632 \n",
      "acc for Psat= 0.1871559702687794 \n",
      "acc for optim= 0.15565146294732887\n",
      "Epoch:80/1000\n",
      "Loss on train= 0.025052212178707123\n",
      "Loss on test= 0.027904119342565536\n",
      "acc for Lsat= 0.1458573165867064 \n",
      "acc for Psat= 0.17245085537433624 \n",
      "acc for optim= 0.15771324750449922\n",
      "Epoch:81/1000\n",
      "Loss on train= 0.023907875642180443\n",
      "Loss on test= 0.024588633328676224\n",
      "acc for Lsat= 0.1356588794125451 \n",
      "acc for Psat= 0.18624288936456043 \n",
      "acc for optim= 0.15986645734972427\n",
      "Epoch:82/1000\n",
      "Loss on train= 0.02475905232131481\n",
      "Loss on test= 0.025686128064990044\n",
      "acc for Lsat= 0.13709417548444533 \n",
      "acc for Psat= 0.19425900081793468 \n",
      "acc for optim= 0.16072877653770978\n",
      "Epoch:83/1000\n",
      "Loss on train= 0.024321042001247406\n",
      "Loss on test= 0.02458968013525009\n",
      "acc for Lsat= 0.1581298049953249 \n",
      "acc for Psat= 0.18668754994869233 \n",
      "acc for optim= 0.15765983313322068\n",
      "Epoch:84/1000\n",
      "Loss on train= 0.02408008649945259\n",
      "Loss on test= 0.026610324159264565\n",
      "acc for Lsat= 0.14853087034490373 \n",
      "acc for Psat= 0.1901961372958289 \n",
      "acc for optim= 0.15688914987776015\n",
      "Epoch:85/1000\n",
      "Loss on train= 0.024278420954942703\n",
      "Loss on test= 0.025913573801517487\n",
      "acc for Lsat= 0.14029390861590704 \n",
      "acc for Psat= 0.17406248533063462 \n",
      "acc for optim= 0.15974196303221913\n",
      "Epoch:86/1000\n",
      "Loss on train= 0.024419816210865974\n",
      "Loss on test= 0.026805423200130463\n",
      "acc for Lsat= 0.1408248073524899 \n",
      "acc for Psat= 0.2034613821241591 \n",
      "acc for optim= 0.15890486124489045\n",
      "Epoch:87/1000\n",
      "Loss on train= 0.02363518439233303\n",
      "Loss on test= 0.025049304589629173\n",
      "acc for Lsat= 0.1249557359351052 \n",
      "acc for Psat= 0.1659370960460769 \n",
      "acc for optim= 0.15922459686795873\n",
      "Epoch:88/1000\n",
      "Loss on train= 0.023848015815019608\n",
      "Loss on test= 0.02436527982354164\n",
      "acc for Lsat= 0.15637319518460166 \n",
      "acc for Psat= 0.17071886228190525 \n",
      "acc for optim= 0.15710521853632398\n",
      "Epoch:89/1000\n",
      "Loss on train= 0.024028705433011055\n",
      "Loss on test= 0.024474872276186943\n",
      "acc for Lsat= 0.13775993883609772 \n",
      "acc for Psat= 0.16825128032101525 \n",
      "acc for optim= 0.1581302319963773\n",
      "Epoch:90/1000\n",
      "Loss on train= 0.02369195967912674\n",
      "Loss on test= 0.024310700595378876\n",
      "acc for Lsat= 0.14844490488370257 \n",
      "acc for Psat= 0.1678203586075041 \n",
      "acc for optim= 0.15936152786016466\n",
      "Epoch:91/1000\n",
      "Loss on train= 0.02285488322377205\n",
      "Loss on test= 0.02593623846769333\n",
      "acc for Lsat= 0.1438895218902164 \n",
      "acc for Psat= 0.17887946797741783 \n",
      "acc for optim= 0.15585956383082603\n",
      "Epoch:92/1000\n",
      "Loss on train= 0.023374713957309723\n",
      "Loss on test= 0.027356673032045364\n",
      "acc for Lsat= 0.15694961249828338 \n",
      "acc for Psat= 0.1765790863169564 \n",
      "acc for optim= 0.15488697273863686\n",
      "Epoch:93/1000\n",
      "Loss on train= 0.02376164123415947\n",
      "Loss on test= 0.023785676807165146\n",
      "acc for Lsat= 0.1181521318025059 \n",
      "acc for Psat= 0.18307250572575462 \n",
      "acc for optim= 0.15612338648902044\n",
      "Epoch:94/1000\n",
      "Loss on train= 0.022251451388001442\n",
      "Loss on test= 0.02479315735399723\n",
      "acc for Lsat= 0.13180837399429743 \n",
      "acc for Psat= 0.17221741643216876 \n",
      "acc for optim= 0.1594605026973618\n",
      "Epoch:95/1000\n",
      "Loss on train= 0.022100670263171196\n",
      "Loss on test= 0.022393541410565376\n",
      "acc for Lsat= 0.10778690013620588 \n",
      "acc for Psat= 0.15187557140986124 \n",
      "acc for optim= 0.15404522630075612\n",
      "Epoch:96/1000\n",
      "Loss on train= 0.021939802914857864\n",
      "Loss on test= 0.022957123816013336\n",
      "acc for Lsat= 0.11028142588006125 \n",
      "acc for Psat= 0.1527704328298569 \n",
      "acc for optim= 0.15335832188526788\n",
      "Epoch:97/1000\n",
      "Loss on train= 0.022402029484510422\n",
      "Loss on test= 0.02301032841205597\n",
      "acc for Lsat= 0.12622291809982722 \n",
      "acc for Psat= 0.18577299316724144 \n",
      "acc for optim= 0.15672232806682584\n",
      "Epoch:98/1000\n",
      "Loss on train= 0.02181442826986313\n",
      "Loss on test= 0.025473657995462418\n",
      "acc for Lsat= 0.12115972671243878 \n",
      "acc for Psat= 0.16211760971281264 \n",
      "acc for optim= 0.1605092566874292\n",
      "Epoch:99/1000\n",
      "Loss on train= 0.022142980247735977\n",
      "Loss on test= 0.023078035563230515\n",
      "acc for Lsat= 0.11572380993101332 \n",
      "acc for Psat= 0.15918615957101184 \n",
      "acc for optim= 0.1596609327528212\n",
      "Epoch:100/1000\n",
      "Loss on train= 0.021881865337491035\n",
      "Loss on test= 0.025723012164235115\n",
      "acc for Lsat= 0.12536527978049383 \n",
      "acc for Psat= 0.1863689661026001 \n",
      "acc for optim= 0.15912842063440225\n",
      "Epoch:101/1000\n",
      "Loss on train= 0.022119980305433273\n",
      "Loss on test= 0.022406578063964844\n",
      "acc for Lsat= 0.11182143919997747 \n",
      "acc for Psat= 0.14981583654880526 \n",
      "acc for optim= 0.1615331118305524\n",
      "Epoch:102/1000\n",
      "Loss on train= 0.021057697013020515\n",
      "Loss on test= 0.02326815389096737\n",
      "acc for Lsat= 0.13195275498761072 \n",
      "acc for Psat= 0.17450552615854475 \n",
      "acc for optim= 0.1565939398275481\n",
      "Epoch:103/1000\n",
      "Loss on train= 0.021570472046732903\n",
      "Loss on test= 0.024183334782719612\n",
      "acc for Lsat= 0.11418019069565667 \n",
      "acc for Psat= 0.15158647894859315 \n",
      "acc for optim= 0.15322214414676028\n",
      "Epoch:104/1000\n",
      "Loss on train= 0.021649640053510666\n",
      "Loss on test= 0.02298547700047493\n",
      "acc for Lsat= 0.12619769705666437 \n",
      "acc for Psat= 0.1601109200053745 \n",
      "acc for optim= 0.15942892937196623\n",
      "Epoch:105/1000\n",
      "Loss on train= 0.02146776206791401\n",
      "Loss on test= 0.022966133430600166\n",
      "acc for Lsat= 0.11440649264388617 \n",
      "acc for Psat= 0.1567447238498264 \n",
      "acc for optim= 0.1555182073679235\n",
      "Epoch:106/1000\n",
      "Loss on train= 0.020828353241086006\n",
      "Loss on test= 0.025163425132632256\n",
      "acc for Lsat= 0.11722860137621563 \n",
      "acc for Psat= 0.17931027279959785 \n",
      "acc for optim= 0.1576320635775725\n",
      "Epoch:107/1000\n",
      "Loss on train= 0.020730026066303253\n",
      "Loss on test= 0.022563956677913666\n",
      "acc for Lsat= 0.11934208538797167 \n",
      "acc for Psat= 0.1570177088181178 \n",
      "acc for optim= 0.15716268287764656\n",
      "Epoch:108/1000\n",
      "Loss on train= 0.02095142751932144\n",
      "Loss on test= 0.02426965907216072\n",
      "acc for Lsat= 0.14406851298279233 \n",
      "acc for Psat= 0.1757754296064377 \n",
      "acc for optim= 0.15844022615088357\n",
      "Epoch:109/1000\n",
      "Loss on train= 0.020786352455615997\n",
      "Loss on test= 0.022799711674451828\n",
      "acc for Lsat= 0.11008574068546295 \n",
      "acc for Psat= 0.16300342049863603 \n",
      "acc for optim= 0.15269918226533466\n",
      "Epoch:110/1000\n",
      "Loss on train= 0.020871760323643684\n",
      "Loss on test= 0.02305399440228939\n",
      "acc for Lsat= 0.11041308773888481 \n",
      "acc for Psat= 0.15649978154235417 \n",
      "acc for optim= 0.15863691485590406\n",
      "Epoch:111/1000\n",
      "Loss on train= 0.02147662825882435\n",
      "Loss on test= 0.024778354912996292\n",
      "acc for Lsat= 0.15709736181630027 \n",
      "acc for Psat= 0.18115116953849794 \n",
      "acc for optim= 0.15424100450343553\n",
      "Epoch:112/1000\n",
      "Loss on train= 0.021170005202293396\n",
      "Loss on test= 0.021678583696484566\n",
      "acc for Lsat= 0.116966450214386 \n",
      "acc for Psat= 0.14747596051957873 \n",
      "acc for optim= 0.15563820335600112\n",
      "Epoch:113/1000\n",
      "Loss on train= 0.019407233223319054\n",
      "Loss on test= 0.021800553426146507\n",
      "acc for Lsat= 0.11722357140647041 \n",
      "acc for Psat= 0.14670493569638993 \n",
      "acc for optim= 0.15151304097639193\n",
      "Epoch:114/1000\n",
      "Loss on train= 0.020079340785741806\n",
      "Loss on test= 0.022423382848501205\n",
      "acc for Lsat= 0.11262056214941873 \n",
      "acc for Psat= 0.1526898996697532 \n",
      "acc for optim= 0.15431856761376067\n",
      "Epoch:115/1000\n",
      "Loss on train= 0.02014269307255745\n",
      "Loss on test= 0.023137934505939484\n",
      "acc for Lsat= 0.10339269008901382 \n",
      "acc for Psat= 0.1389960441324446 \n",
      "acc for optim= 0.1554893727103869\n",
      "Epoch:116/1000\n",
      "Loss on train= 0.020581616088747978\n",
      "Loss on test= 0.022626124322414398\n",
      "acc for Lsat= 0.11263880878686905 \n",
      "acc for Psat= 0.1472994784514109 \n",
      "acc for optim= 0.15322427708241673\n",
      "Epoch:117/1000\n",
      "Loss on train= 0.02062557265162468\n",
      "Loss on test= 0.02160174772143364\n",
      "acc for Lsat= 0.1015217352244589 \n",
      "acc for Psat= 0.1540224403142929 \n",
      "acc for optim= 0.15451126967867215\n",
      "Epoch:118/1000\n",
      "Loss on train= 0.019688434898853302\n",
      "Loss on test= 0.02393237315118313\n",
      "acc for Lsat= 0.11807334356837801 \n",
      "acc for Psat= 0.1636083308193419 \n",
      "acc for optim= 0.1535119372109572\n",
      "Epoch:119/1000\n",
      "Loss on train= 0.02049405314028263\n",
      "Loss on test= 0.022617744281888008\n",
      "acc for Lsat= 0.13758670488993324 \n",
      "acc for Psat= 0.1750429249472088 \n",
      "acc for optim= 0.15898049821456275\n",
      "Epoch:120/1000\n",
      "Loss on train= 0.020271729677915573\n",
      "Loss on test= 0.022562824189662933\n",
      "acc for Lsat= 0.13324945602152083 \n",
      "acc for Psat= 0.1529627038372888 \n",
      "acc for optim= 0.15650584904683967\n",
      "Epoch:121/1000\n",
      "Loss on train= 0.019321994855999947\n",
      "Loss on test= 0.02162928134202957\n",
      "acc for Lsat= 0.10560828083091313 \n",
      "acc for Psat= 0.16382975810103945 \n",
      "acc for optim= 0.15396789304084246\n",
      "Epoch:122/1000\n",
      "Loss on train= 0.020493002608418465\n",
      "Loss on test= 0.022799493744969368\n",
      "acc for Lsat= 0.11819914744959936 \n",
      "acc for Psat= 0.16458985805511472 \n",
      "acc for optim= 0.1536281153559685\n",
      "Epoch:123/1000\n",
      "Loss on train= 0.020059578120708466\n",
      "Loss on test= 0.021855924278497696\n",
      "acc for Lsat= 0.10803497483332952 \n",
      "acc for Psat= 0.14269071767727534 \n",
      "acc for optim= 0.15426205893357595\n",
      "Epoch:124/1000\n",
      "Loss on train= 0.020014166831970215\n",
      "Loss on test= 0.02265903726220131\n",
      "acc for Lsat= 0.11313045488463509 \n",
      "acc for Psat= 0.1553671790493859 \n",
      "acc for optim= 0.15598152577877045\n",
      "Epoch:125/1000\n",
      "Loss on train= 0.01971699856221676\n",
      "Loss on test= 0.021190457046031952\n",
      "acc for Lsat= 0.10555814173486497 \n",
      "acc for Psat= 0.13615706198745303 \n",
      "acc for optim= 0.1554288739959399\n",
      "Epoch:126/1000\n",
      "Loss on train= 0.01974712312221527\n",
      "Loss on test= 0.022948380559682846\n",
      "acc for Lsat= 0.10565556155310735 \n",
      "acc for Psat= 0.16040121449364556 \n",
      "acc for optim= 0.1533499642378754\n",
      "Epoch:127/1000\n",
      "Loss on train= 0.0199617650359869\n",
      "Loss on test= 0.022386008873581886\n",
      "acc for Lsat= 0.13291884660720824 \n",
      "acc for Psat= 0.18421184155676099 \n",
      "acc for optim= 0.15025740928120085\n",
      "Epoch:128/1000\n",
      "Loss on train= 0.018990756943821907\n",
      "Loss on test= 0.021353410556912422\n",
      "acc for Lsat= 0.0988919178644816 \n",
      "acc for Psat= 0.12944830639494792 \n",
      "acc for optim= 0.15105177404152023\n",
      "Epoch:129/1000\n",
      "Loss on train= 0.019919417798519135\n",
      "Loss on test= 0.02172083966434002\n",
      "acc for Lsat= 0.09914956655767228 \n",
      "acc for Psat= 0.13633847071064842 \n",
      "acc for optim= 0.15208821760283572\n",
      "Epoch:130/1000\n",
      "Loss on train= 0.01922406256198883\n",
      "Loss on test= 0.021776437759399414\n",
      "acc for Lsat= 0.09873155852158864 \n",
      "acc for Psat= 0.1398919148577584 \n",
      "acc for optim= 0.15165024507376884\n",
      "Epoch:131/1000\n",
      "Loss on train= 0.019889123737812042\n",
      "Loss on test= 0.022390834987163544\n",
      "acc for Lsat= 0.13295943008528815 \n",
      "acc for Psat= 0.16788269016477797 \n",
      "acc for optim= 0.16001324885421328\n",
      "Epoch:132/1000\n",
      "Loss on train= 0.01930876448750496\n",
      "Loss on test= 0.021257588639855385\n",
      "acc for Lsat= 0.09667042394479115 \n",
      "acc for Psat= 0.1412519014543957 \n",
      "acc for optim= 0.15290389267934695\n",
      "Epoch:133/1000\n",
      "Loss on train= 0.019461529329419136\n",
      "Loss on test= 0.02093007229268551\n",
      "acc for Lsat= 0.11891123917367723 \n",
      "acc for Psat= 0.15814141564899023 \n",
      "acc for optim= 0.1515782356262207\n",
      "Epoch:134/1000\n",
      "Loss on train= 0.019320057705044746\n",
      "Loss on test= 0.02064312994480133\n",
      "acc for Lsat= 0.10492737856176165 \n",
      "acc for Psat= 0.1498234212398529 \n",
      "acc for optim= 0.15750417427884209\n",
      "Epoch:135/1000\n",
      "Loss on train= 0.018915854394435883\n",
      "Loss on test= 0.02105291187763214\n",
      "acc for Lsat= 0.10038048608435524 \n",
      "acc for Psat= 0.14256806737846803 \n",
      "acc for optim= 0.1526220742199156\n",
      "Epoch:136/1000\n",
      "Loss on train= 0.018853377550840378\n",
      "Loss on test= 0.02069445326924324\n",
      "acc for Lsat= 0.09668230877982245 \n",
      "acc for Psat= 0.13732938021421429 \n",
      "acc for optim= 0.15457661863830355\n",
      "Epoch:137/1000\n",
      "Loss on train= 0.018893737345933914\n",
      "Loss on test= 0.020989062264561653\n",
      "acc for Lsat= 0.10263449930482442 \n",
      "acc for Psat= 0.13099444889360004 \n",
      "acc for optim= 0.15003399145272045\n",
      "Epoch:138/1000\n",
      "Loss on train= 0.018933536484837532\n",
      "Loss on test= 0.02140108123421669\n",
      "acc for Lsat= 0.10164054416947894 \n",
      "acc for Psat= 0.13454000287585788 \n",
      "acc for optim= 0.1548986608783404\n",
      "Epoch:139/1000\n",
      "Loss on train= 0.01871749386191368\n",
      "Loss on test= 0.019571544602513313\n",
      "acc for Lsat= 0.11058892375893063 \n",
      "acc for Psat= 0.16124576926231382 \n",
      "acc for optim= 0.15283294361498623\n",
      "Epoch:140/1000\n",
      "Loss on train= 0.018465248867869377\n",
      "Loss on test= 0.020416932180523872\n",
      "acc for Lsat= 0.09227921730942196 \n",
      "acc for Psat= 0.12801008290714688 \n",
      "acc for optim= 0.1508890769961807\n",
      "Epoch:141/1000\n",
      "Loss on train= 0.01894443854689598\n",
      "Loss on test= 0.019733302295207977\n",
      "acc for Lsat= 0.10183933973312378 \n",
      "acc for Psat= 0.14620339771111807 \n",
      "acc for optim= 0.14926604636841348\n",
      "Epoch:142/1000\n",
      "Loss on train= 0.018680565059185028\n",
      "Loss on test= 0.019502365961670876\n",
      "acc for Lsat= 0.11834774000777139 \n",
      "acc for Psat= 0.13863533867730035 \n",
      "acc for optim= 0.15360086303618217\n",
      "Epoch:143/1000\n",
      "Loss on train= 0.01846734993159771\n",
      "Loss on test= 0.022009659558534622\n",
      "acc for Lsat= 0.10978809297084809 \n",
      "acc for Psat= 0.15352191262774995 \n",
      "acc for optim= 0.15118779904312557\n",
      "Epoch:144/1000\n",
      "Loss on train= 0.018654411658644676\n",
      "Loss on test= 0.019248973578214645\n",
      "acc for Lsat= 0.1010795886317889 \n",
      "acc for Psat= 0.1452676753203074 \n",
      "acc for optim= 0.15566106662154197\n",
      "Epoch:145/1000\n",
      "Loss on train= 0.01845521107316017\n",
      "Loss on test= 0.020207012072205544\n",
      "acc for Lsat= 0.10436277472310596 \n",
      "acc for Psat= 0.1572991367843416 \n",
      "acc for optim= 0.1506691657006741\n",
      "Epoch:146/1000\n",
      "Loss on train= 0.01856396161019802\n",
      "Loss on test= 0.01989121176302433\n",
      "acc for Lsat= 0.11189494447575675 \n",
      "acc for Psat= 0.1404539260599348 \n",
      "acc for optim= 0.1514669967194398\n",
      "Epoch:147/1000\n",
      "Loss on train= 0.018244635313749313\n",
      "Loss on test= 0.019391870126128197\n",
      "acc for Lsat= 0.09070720358027354 \n",
      "acc for Psat= 0.1276614220605956 \n",
      "acc for optim= 0.1511517618265417\n",
      "Epoch:148/1000\n",
      "Loss on train= 0.01876227743923664\n",
      "Loss on test= 0.018991008400917053\n",
      "acc for Lsat= 0.0992503719197379 \n",
      "acc for Psat= 0.14628604716724816 \n",
      "acc for optim= 0.15082587889499133\n",
      "Epoch:149/1000\n",
      "Loss on train= 0.01842554844915867\n",
      "Loss on test= 0.02066796086728573\n",
      "acc for Lsat= 0.10006544474098418 \n",
      "acc for Psat= 0.12285024374723433 \n",
      "acc for optim= 0.15172067590885693\n",
      "Epoch:150/1000\n",
      "Loss on train= 0.018438786268234253\n",
      "Loss on test= 0.01899484544992447\n",
      "acc for Lsat= 0.1005177229642868 \n",
      "acc for Psat= 0.12812745802932315 \n",
      "acc for optim= 0.15173940757910412\n",
      "Epoch:151/1000\n",
      "Loss on train= 0.017909711226820946\n",
      "Loss on test= 0.022563809528946877\n",
      "acc for Lsat= 0.11145129005114235 \n",
      "acc for Psat= 0.15102317863040501 \n",
      "acc for optim= 0.15209198478195407\n",
      "Epoch:152/1000\n",
      "Loss on train= 0.018364718183875084\n",
      "Loss on test= 0.019634729251265526\n",
      "acc for Lsat= 0.10391659157143698 \n",
      "acc for Psat= 0.12625387112299602 \n",
      "acc for optim= 0.15059387104378805\n",
      "Epoch:153/1000\n",
      "Loss on train= 0.017902981489896774\n",
      "Loss on test= 0.020168976858258247\n",
      "acc for Lsat= 0.0948146073354615 \n",
      "acc for Psat= 0.139974754386478 \n",
      "acc for optim= 0.1493857683406936\n",
      "Epoch:154/1000\n",
      "Loss on train= 0.017221156507730484\n",
      "Loss on test= 0.020896514877676964\n",
      "acc for Lsat= 0.10107355730401148 \n",
      "acc for Psat= 0.1483749571773741 \n",
      "acc for optim= 0.14777554232213236\n",
      "Epoch:155/1000\n",
      "Loss on train= 0.01764225959777832\n",
      "Loss on test= 0.020120391622185707\n",
      "acc for Lsat= 0.09709997011555566 \n",
      "acc for Psat= 0.13561491303973727 \n",
      "acc for optim= 0.15123081679145498\n",
      "Epoch:156/1000\n",
      "Loss on train= 0.018046261742711067\n",
      "Loss on test= 0.01899232156574726\n",
      "acc for Lsat= 0.08824008587333892 \n",
      "acc for Psat= 0.1346696840392219 \n",
      "acc for optim= 0.1521784390012423\n",
      "Epoch:157/1000\n",
      "Loss on train= 0.017729736864566803\n",
      "Loss on test= 0.01982281170785427\n",
      "acc for Lsat= 0.09240289595392015 \n",
      "acc for Psat= 0.1328541490766737 \n",
      "acc for optim= 0.1503727520505587\n",
      "Epoch:158/1000\n",
      "Loss on train= 0.017927562817931175\n",
      "Loss on test= 0.019267544150352478\n",
      "acc for Lsat= 0.10125971949762766 \n",
      "acc for Psat= 0.14298060072792898 \n",
      "acc for optim= 0.14917620155546396\n",
      "Epoch:159/1000\n",
      "Loss on train= 0.017923081293702126\n",
      "Loss on test= 0.019310029223561287\n",
      "acc for Lsat= 0.09236440873808333 \n",
      "acc for Psat= 0.12840260532167222 \n",
      "acc for optim= 0.15006457178129087\n",
      "Epoch:160/1000\n",
      "Loss on train= 0.01773575134575367\n",
      "Loss on test= 0.02000167779624462\n",
      "acc for Lsat= 0.09872084276543726 \n",
      "acc for Psat= 0.16767524778842927 \n",
      "acc for optim= 0.1475301403966215\n",
      "Epoch:161/1000\n",
      "Loss on train= 0.017290621995925903\n",
      "Loss on test= 0.021031538024544716\n",
      "acc for Lsat= 0.09388689961698321 \n",
      "acc for Psat= 0.14583098391691843 \n",
      "acc for optim= 0.14675373716486825\n",
      "Epoch:162/1000\n",
      "Loss on train= 0.01742253452539444\n",
      "Loss on test= 0.01942186802625656\n",
      "acc for Lsat= 0.09553546077675287 \n",
      "acc for Psat= 0.13180231319533453 \n",
      "acc for optim= 0.1495978591342767\n",
      "Epoch:163/1000\n",
      "Loss on train= 0.017351092770695686\n",
      "Loss on test= 0.019737815484404564\n",
      "acc for Lsat= 0.10323139561547173 \n",
      "acc for Psat= 0.14817101193798912 \n",
      "acc for optim= 0.14651781477861936\n",
      "Epoch:164/1000\n",
      "Loss on train= 0.016996441408991814\n",
      "Loss on test= 0.01891494356095791\n",
      "acc for Lsat= 0.08752900660037993 \n",
      "acc for Psat= 0.1262405488226149 \n",
      "acc for optim= 0.14774404615163803\n",
      "Epoch:165/1000\n",
      "Loss on train= 0.017530204728245735\n",
      "Loss on test= 0.022530846297740936\n",
      "acc for Lsat= 0.10124618808428447 \n",
      "acc for Psat= 0.14206728471650018 \n",
      "acc for optim= 0.14668919121225674\n",
      "Epoch:166/1000\n",
      "Loss on train= 0.017351699993014336\n",
      "Loss on test= 0.01941632479429245\n",
      "acc for Lsat= 0.09128774454196294 \n",
      "acc for Psat= 0.12670514119995965 \n",
      "acc for optim= 0.14848715373211435\n",
      "Epoch:167/1000\n",
      "Loss on train= 0.017818886786699295\n",
      "Loss on test= 0.019155260175466537\n",
      "acc for Lsat= 0.1045559975836012 \n",
      "acc for Psat= 0.17873965468671588 \n",
      "acc for optim= 0.1470901105552912\n",
      "Epoch:168/1000\n",
      "Loss on train= 0.018007641658186913\n",
      "Loss on test= 0.02035943791270256\n",
      "acc for Lsat= 0.09348117841614617 \n",
      "acc for Psat= 0.1239765038092931 \n",
      "acc for optim= 0.15220048319962287\n",
      "Epoch:169/1000\n",
      "Loss on train= 0.017280492931604385\n",
      "Loss on test= 0.019161641597747803\n",
      "acc for Lsat= 0.10537917349073621 \n",
      "acc for Psat= 0.12512429952621462 \n",
      "acc for optim= 0.14790556273526617\n",
      "Epoch:170/1000\n",
      "Loss on train= 0.017609210684895515\n",
      "Loss on test= 0.02019204944372177\n",
      "acc for Lsat= 0.09575825896528029 \n",
      "acc for Psat= 0.13374728229310776 \n",
      "acc for optim= 0.15369203868839476\n",
      "Epoch:171/1000\n",
      "Loss on train= 0.017532141879200935\n",
      "Loss on test= 0.020884787663817406\n",
      "acc for Lsat= 0.11331055296791925 \n",
      "acc for Psat= 0.13863972955279882 \n",
      "acc for optim= 0.147033875518375\n",
      "Epoch:172/1000\n",
      "Loss on train= 0.01705261878669262\n",
      "Loss on test= 0.019513588398694992\n",
      "acc for Lsat= 0.0889326885342598 \n",
      "acc for Psat= 0.12763292060958017 \n",
      "acc for optim= 0.15332462987345125\n",
      "Epoch:173/1000\n",
      "Loss on train= 0.017820918932557106\n",
      "Loss on test= 0.018737681210041046\n",
      "acc for Lsat= 0.08881396171119477 \n",
      "acc for Psat= 0.12666225814157064 \n",
      "acc for optim= 0.15428584242860477\n",
      "Epoch:174/1000\n",
      "Loss on train= 0.016972683370113373\n",
      "Loss on test= 0.019117238000035286\n",
      "acc for Lsat= 0.09826638632350497 \n",
      "acc for Psat= 0.1414941367175844 \n",
      "acc for optim= 0.15015262009369004\n",
      "Epoch:175/1000\n",
      "Loss on train= 0.016794128343462944\n",
      "Loss on test= 0.018238846212625504\n",
      "acc for Lsat= 0.08864011002911462 \n",
      "acc for Psat= 0.13734614335828355 \n",
      "acc for optim= 0.1514754858281877\n",
      "Epoch:176/1000\n",
      "Loss on train= 0.017564035952091217\n",
      "Loss on test= 0.018753819167613983\n",
      "acc for Lsat= 0.09505594505204097 \n",
      "acc for Psat= 0.14109527568022412 \n",
      "acc for optim= 0.15075951897435722\n",
      "Epoch:177/1000\n",
      "Loss on train= 0.017596473917365074\n",
      "Loss on test= 0.020118115469813347\n",
      "acc for Lsat= 0.09896717551681732 \n",
      "acc for Psat= 0.1264931450287501 \n",
      "acc for optim= 0.14953828449878429\n",
      "Epoch:178/1000\n",
      "Loss on train= 0.017540080472826958\n",
      "Loss on test= 0.019846927374601364\n",
      "acc for Lsat= 0.10282754368252224 \n",
      "acc for Psat= 0.12988932331403097 \n",
      "acc for optim= 0.14752065497967934\n",
      "Epoch:179/1000\n",
      "Loss on train= 0.016507189720869064\n",
      "Loss on test= 0.019752001389861107\n",
      "acc for Lsat= 0.11388014720545875 \n",
      "acc for Psat= 0.1276810006962882 \n",
      "acc for optim= 0.14762223437428476\n",
      "Epoch:180/1000\n",
      "Loss on train= 0.017304779961705208\n",
      "Loss on test= 0.01916581019759178\n",
      "acc for Lsat= 0.10281516131427554 \n",
      "acc for Psat= 0.14671673576037092 \n",
      "acc for optim= 0.15219125126798944\n",
      "Epoch:181/1000\n",
      "Loss on train= 0.017038391903042793\n",
      "Loss on test= 0.019228635355830193\n",
      "acc for Lsat= 0.1007434003882938 \n",
      "acc for Psat= 0.12492672254641851 \n",
      "acc for optim= 0.149615714367893\n",
      "Epoch:182/1000\n",
      "Loss on train= 0.01682664267718792\n",
      "Loss on test= 0.018551694229245186\n",
      "acc for Lsat= 0.09475095785326428 \n",
      "acc for Psat= 0.12206015934546789 \n",
      "acc for optim= 0.15362982066969078\n",
      "Epoch:183/1000\n",
      "Loss on train= 0.017210451886057854\n",
      "Loss on test= 0.019853500649333\n",
      "acc for Lsat= 0.12153110338581932 \n",
      "acc for Psat= 0.1375900877846612 \n",
      "acc for optim= 0.1491913841002517\n",
      "Epoch:184/1000\n",
      "Loss on train= 0.017307091504335403\n",
      "Loss on test= 0.01819794438779354\n",
      "acc for Lsat= 0.09269190877676009 \n",
      "acc for Psat= 0.11422850340604783 \n",
      "acc for optim= 0.14812912975127498\n",
      "Epoch:185/1000\n",
      "Loss on train= 0.01693776436150074\n",
      "Loss on test= 0.01856924407184124\n",
      "acc for Lsat= 0.0954013685385386 \n",
      "acc for Psat= 0.12537476387288832 \n",
      "acc for optim= 0.15030754738383822\n",
      "Epoch:186/1000\n",
      "Loss on train= 0.016643747687339783\n",
      "Loss on test= 0.02054416574537754\n",
      "acc for Lsat= 0.10200026316775217 \n",
      "acc for Psat= 0.1470636291636361 \n",
      "acc for optim= 0.1464383511079682\n",
      "Epoch:187/1000\n",
      "Loss on train= 0.01627870835363865\n",
      "Loss on test= 0.020261896774172783\n",
      "acc for Lsat= 0.11044920086860656 \n",
      "acc for Psat= 0.13682150575849744 \n",
      "acc for optim= 0.14754432348741425\n",
      "Epoch:188/1000\n",
      "Loss on train= 0.01699553243815899\n",
      "Loss on test= 0.01950790360569954\n",
      "acc for Lsat= 0.0923107460141182 \n",
      "acc for Psat= 0.13824993438190883 \n",
      "acc for optim= 0.1502896798153718\n",
      "Epoch:189/1000\n",
      "Loss on train= 0.016687635332345963\n",
      "Loss on test= 0.02012612484395504\n",
      "acc for Lsat= 0.12917014658451081 \n",
      "acc for Psat= 0.1515532495246993 \n",
      "acc for optim= 0.14963284403913552\n",
      "Epoch:190/1000\n",
      "Loss on train= 0.01722116582095623\n",
      "Loss on test= 0.019295409321784973\n",
      "acc for Lsat= 0.0876470587319798 \n",
      "acc for Psat= 0.12213792105515799 \n",
      "acc for optim= 0.150065062319239\n",
      "Epoch:191/1000\n",
      "Loss on train= 0.016233766451478004\n",
      "Loss on test= 0.019955718889832497\n",
      "acc for Lsat= 0.1008895066049364 \n",
      "acc for Psat= 0.13258686860402424 \n",
      "acc for optim= 0.14606480072769853\n",
      "Epoch:192/1000\n",
      "Loss on train= 0.016704760491847992\n",
      "Loss on test= 0.017066320404410362\n",
      "acc for Lsat= 0.08251082615719901 \n",
      "acc for Psat= 0.12108339601092867 \n",
      "acc for optim= 0.14664018580483065\n",
      "Epoch:193/1000\n",
      "Loss on train= 0.016523657366633415\n",
      "Loss on test= 0.01940676011145115\n",
      "acc for Lsat= 0.10267570780383216 \n",
      "acc for Psat= 0.13648138344287875 \n",
      "acc for optim= 0.1481310067077478\n",
      "Epoch:194/1000\n",
      "Loss on train= 0.016320617869496346\n",
      "Loss on test= 0.019139785319566727\n",
      "acc for Lsat= 0.09105730371342764 \n",
      "acc for Psat= 0.13619701630539366 \n",
      "acc for optim= 0.14596818118459645\n",
      "Epoch:195/1000\n",
      "Loss on train= 0.016103994101285934\n",
      "Loss on test= 0.019035447388887405\n",
      "acc for Lsat= 0.09221495307154125 \n",
      "acc for Psat= 0.1285316566626231 \n",
      "acc for optim= 0.14883906183143453\n",
      "Epoch:196/1000\n",
      "Loss on train= 0.01644919626414776\n",
      "Loss on test= 0.0193440280854702\n",
      "acc for Lsat= 0.0899522594279713 \n",
      "acc for Psat= 0.125674939652284 \n",
      "acc for optim= 0.14681047221852675\n",
      "Epoch:197/1000\n",
      "Loss on train= 0.016624214127659798\n",
      "Loss on test= 0.01825261302292347\n",
      "acc for Lsat= 0.08602352307902444 \n",
      "acc for Psat= 0.12557192676597173 \n",
      "acc for optim= 0.14560710096524823\n",
      "Epoch:198/1000\n",
      "Loss on train= 0.016092179343104362\n",
      "Loss on test= 0.019191714003682137\n",
      "acc for Lsat= 0.0889384999871254 \n",
      "acc for Psat= 0.1195961836311552 \n",
      "acc for optim= 0.14665017546051076\n",
      "Epoch:199/1000\n",
      "Loss on train= 0.016027314588427544\n",
      "Loss on test= 0.018967261537909508\n",
      "acc for Lsat= 0.086518831551075 \n",
      "acc for Psat= 0.1273336089319653 \n",
      "acc for optim= 0.1489678533540832\n",
      "Epoch:200/1000\n",
      "Loss on train= 0.016068046912550926\n",
      "Loss on test= 0.018540849909186363\n",
      "acc for Lsat= 0.09165644215212927 \n",
      "acc for Psat= 0.13443771070904204 \n",
      "acc for optim= 0.15404407713148333\n",
      "Epoch:201/1000\n",
      "Loss on train= 0.016457783058285713\n",
      "Loss on test= 0.018105709925293922\n",
      "acc for Lsat= 0.10736681090460883 \n",
      "acc for Psat= 0.12997631000147927 \n",
      "acc for optim= 0.14853769461106922\n",
      "Epoch:202/1000\n",
      "Loss on train= 0.0167243629693985\n",
      "Loss on test= 0.019061705097556114\n",
      "acc for Lsat= 0.10339701672395069 \n",
      "acc for Psat= 0.15288755231433443 \n",
      "acc for optim= 0.14729565216435325\n",
      "Epoch:203/1000\n",
      "Loss on train= 0.01570294052362442\n",
      "Loss on test= 0.01874597556889057\n",
      "acc for Lsat= 0.0865410680572192 \n",
      "acc for Psat= 0.12613126966688368 \n",
      "acc for optim= 0.14763916774342456\n",
      "Epoch:204/1000\n",
      "Loss on train= 0.016022540628910065\n",
      "Loss on test= 0.019133906811475754\n",
      "acc for Lsat= 0.10543476674291821 \n",
      "acc for Psat= 0.15176914831002553 \n",
      "acc for optim= 0.14637436523205702\n",
      "Epoch:205/1000\n",
      "Loss on train= 0.016090407967567444\n",
      "Loss on test= 0.018006278201937675\n",
      "acc for Lsat= 0.09298568848106598 \n",
      "acc for Psat= 0.13104946811993917 \n",
      "acc for optim= 0.1448846721607778\n",
      "Epoch:206/1000\n",
      "Loss on train= 0.015667274594306946\n",
      "Loss on test= 0.017608417198061943\n",
      "acc for Lsat= 0.10425779786374832 \n",
      "acc for Psat= 0.12686363922225105 \n",
      "acc for optim= 0.14807595428493292\n",
      "Epoch:207/1000\n",
      "Loss on train= 0.01623222418129444\n",
      "Loss on test= 0.018668685108423233\n",
      "acc for Lsat= 0.08376346478859582 \n",
      "acc for Psat= 0.14437279833687677 \n",
      "acc for optim= 0.1496993629468812\n",
      "Epoch:208/1000\n",
      "Loss on train= 0.01645081490278244\n",
      "Loss on test= 0.018073726445436478\n",
      "acc for Lsat= 0.08328776078091726 \n",
      "acc for Psat= 0.11112856600019667 \n",
      "acc for optim= 0.14901213008496497\n",
      "Epoch:209/1000\n",
      "Loss on train= 0.016052348539233208\n",
      "Loss on test= 0.019964193925261497\n",
      "acc for Lsat= 0.10954091648260753 \n",
      "acc for Psat= 0.13212223649024962 \n",
      "acc for optim= 0.1481961751977603\n",
      "Epoch:210/1000\n",
      "Loss on train= 0.015873374417424202\n",
      "Loss on test= 0.016912780702114105\n",
      "acc for Lsat= 0.08644814358817206 \n",
      "acc for Psat= 0.12116326259242162 \n",
      "acc for optim= 0.15071106735203\n",
      "Epoch:211/1000\n",
      "Loss on train= 0.016229920089244843\n",
      "Loss on test= 0.018577244132757187\n",
      "acc for Lsat= 0.08419985969861349 \n",
      "acc for Psat= 0.12516126367780897 \n",
      "acc for optim= 0.14562036179833945\n",
      "Epoch:212/1000\n",
      "Loss on train= 0.015875888988375664\n",
      "Loss on test= 0.018379265442490578\n",
      "acc for Lsat= 0.08423717535204356 \n",
      "acc for Psat= 0.12127253413200378 \n",
      "acc for optim= 0.1456822311298715\n",
      "Epoch:213/1000\n",
      "Loss on train= 0.01651778072118759\n",
      "Loss on test= 0.018883585929870605\n",
      "acc for Lsat= 0.12088418652613958 \n",
      "acc for Psat= 0.12625544832812416 \n",
      "acc for optim= 0.14674247105916344\n",
      "Epoch:214/1000\n",
      "Loss on train= 0.01605120860040188\n",
      "Loss on test= 0.019325444474816322\n",
      "acc for Lsat= 0.12027538451883528 \n",
      "acc for Psat= 0.1246759365002314 \n",
      "acc for optim= 0.14528568680915568\n",
      "Epoch:215/1000\n",
      "Loss on train= 0.015516639687120914\n",
      "Loss on test= 0.01791289448738098\n",
      "acc for Lsat= 0.08409622990422778 \n",
      "acc for Psat= 0.11275979942745631 \n",
      "acc for optim= 0.148536899437507\n",
      "Epoch:216/1000\n",
      "Loss on train= 0.015993107110261917\n",
      "Loss on test= 0.01832437328994274\n",
      "acc for Lsat= 0.08633461511797375 \n",
      "acc for Psat= 0.11210856437683105 \n",
      "acc for optim= 0.14428914528754025\n",
      "Epoch:217/1000\n",
      "Loss on train= 0.015622174367308617\n",
      "Loss on test= 0.017127223312854767\n",
      "acc for Lsat= 0.09127253111865787 \n",
      "acc for Psat= 0.11920009089840783 \n",
      "acc for optim= 0.1454842261556122\n",
      "Epoch:218/1000\n",
      "Loss on train= 0.015323742292821407\n",
      "Loss on test= 0.017215047031641006\n",
      "acc for Lsat= 0.08567091590828366 \n",
      "acc for Psat= 0.11742888374461069 \n",
      "acc for optim= 0.14726998731493948\n",
      "Epoch:219/1000\n",
      "Loss on train= 0.01604371704161167\n",
      "Loss on test= 0.020955249667167664\n",
      "acc for Lsat= 0.12915812234083812 \n",
      "acc for Psat= 0.21517513261901006 \n",
      "acc for optim= 0.14501196237074007\n",
      "Epoch:220/1000\n",
      "Loss on train= 0.015935717150568962\n",
      "Loss on test= 0.01918407529592514\n",
      "acc for Lsat= 0.08345721099111769 \n",
      "acc for Psat= 0.12298227945963541 \n",
      "acc for optim= 0.14681457203373108\n",
      "Epoch:221/1000\n",
      "Loss on train= 0.01564902812242508\n",
      "Loss on test= 0.01859714835882187\n",
      "acc for Lsat= 0.08688753710852731 \n",
      "acc for Psat= 0.14557422233952416 \n",
      "acc for optim= 0.1499077027870549\n",
      "Epoch:222/1000\n",
      "Loss on train= 0.015843989327549934\n",
      "Loss on test= 0.01837932877242565\n",
      "acc for Lsat= 0.08638788047764037 \n",
      "acc for Psat= 0.11682223710748886 \n",
      "acc for optim= 0.14532710417277284\n",
      "Epoch:223/1000\n",
      "Loss on train= 0.015750057995319366\n",
      "Loss on test= 0.017864180728793144\n",
      "acc for Lsat= 0.08355595469474793 \n",
      "acc for Psat= 0.11783901784155103 \n",
      "acc for optim= 0.14487495273351675\n",
      "Epoch:224/1000\n",
      "Loss on train= 0.015880579128861427\n",
      "Loss on test= 0.018770886585116386\n",
      "acc for Lsat= 0.10571731229623159 \n",
      "acc for Psat= 0.1487383226553599 \n",
      "acc for optim= 0.15305403023958208\n",
      "Epoch:225/1000\n",
      "Loss on train= 0.01615203358232975\n",
      "Loss on test= 0.018235057592391968\n",
      "acc for Lsat= 0.09043267203701869 \n",
      "acc for Psat= 0.12434059944417743 \n",
      "acc for optim= 0.14493890909685028\n",
      "Epoch:226/1000\n",
      "Loss on train= 0.015838421881198883\n",
      "Loss on test= 0.01731162518262863\n",
      "acc for Lsat= 0.09859876301553515 \n",
      "acc for Psat= 0.16165208220481875 \n",
      "acc for optim= 0.1450279137326611\n",
      "Epoch:227/1000\n",
      "Loss on train= 0.015635525807738304\n",
      "Loss on test= 0.01887880451977253\n",
      "acc for Lsat= 0.08856788178284963 \n",
      "acc for Psat= 0.11664835876888698 \n",
      "acc for optim= 0.14832332722014852\n",
      "Epoch:228/1000\n",
      "Loss on train= 0.01539347693324089\n",
      "Loss on test= 0.017029961571097374\n",
      "acc for Lsat= 0.08204893718163173 \n",
      "acc for Psat= 0.11305631928973729 \n",
      "acc for optim= 0.14794461311151585\n",
      "Epoch:229/1000\n",
      "Loss on train= 0.015121382661163807\n",
      "Loss on test= 0.017518723383545876\n",
      "acc for Lsat= 0.09186578740676245 \n",
      "acc for Psat= 0.12070530321862963 \n",
      "acc for optim= 0.1437356401234865\n",
      "Epoch:230/1000\n",
      "Loss on train= 0.015269046649336815\n",
      "Loss on test= 0.019192848354578018\n",
      "acc for Lsat= 0.0899322180284394 \n",
      "acc for Psat= 0.12236976789103614 \n",
      "acc for optim= 0.14559558282295862\n",
      "Epoch:231/1000\n",
      "Loss on train= 0.015507015399634838\n",
      "Loss on test= 0.01838921383023262\n",
      "acc for Lsat= 0.08089326653215621 \n",
      "acc for Psat= 0.14383454256587555 \n",
      "acc for optim= 0.14512995166911022\n",
      "Epoch:232/1000\n",
      "Loss on train= 0.0151304816827178\n",
      "Loss on test= 0.018949778750538826\n",
      "acc for Lsat= 0.08417936000559066 \n",
      "acc for Psat= 0.1096034632788764 \n",
      "acc for optim= 0.14602211631006667\n",
      "Epoch:233/1000\n",
      "Loss on train= 0.015552481636404991\n",
      "Loss on test= 0.01843232288956642\n",
      "acc for Lsat= 0.07666375637054441 \n",
      "acc for Psat= 0.12594196233484478 \n",
      "acc for optim= 0.14764601331618096\n",
      "Epoch:234/1000\n",
      "Loss on train= 0.015492158941924572\n",
      "Loss on test= 0.01767161302268505\n",
      "acc for Lsat= 0.089458067715168 \n",
      "acc for Psat= 0.12716446419556937 \n",
      "acc for optim= 0.147824932096733\n",
      "Epoch:235/1000\n",
      "Loss on train= 0.015258348546922207\n",
      "Loss on test= 0.01872953586280346\n",
      "acc for Lsat= 0.10137872298558553 \n",
      "acc for Psat= 0.1147542115714815 \n",
      "acc for optim= 0.14410627860989839\n",
      "Epoch:236/1000\n",
      "Loss on train= 0.016028011217713356\n",
      "Loss on test= 0.01711254194378853\n",
      "acc for Lsat= 0.08731366511848236 \n",
      "acc for Psat= 0.13749889300929174 \n",
      "acc for optim= 0.1496157984352774\n",
      "Epoch:237/1000\n",
      "Loss on train= 0.015506446361541748\n",
      "Loss on test= 0.017726780846714973\n",
      "acc for Lsat= 0.08978097058004803 \n",
      "acc for Psat= 0.11752462320857576 \n",
      "acc for optim= 0.14636100948684747\n",
      "Epoch:238/1000\n",
      "Loss on train= 0.015097066760063171\n",
      "Loss on test= 0.019518176093697548\n",
      "acc for Lsat= 0.08470732404126063 \n",
      "acc for Psat= 0.1236932737959756 \n",
      "acc for optim= 0.148669128285514\n",
      "Epoch:239/1000\n",
      "Loss on train= 0.015247645787894726\n",
      "Loss on test= 0.01831796206533909\n",
      "acc for Lsat= 0.0835217297077179 \n",
      "acc for Psat= 0.13061148160033756 \n",
      "acc for optim= 0.14356047097179625\n",
      "Epoch:240/1000\n",
      "Loss on train= 0.015479614958167076\n",
      "Loss on test= 0.017665984109044075\n",
      "acc for Lsat= 0.0892732505997022 \n",
      "acc for Psat= 0.12297017210059696 \n",
      "acc for optim= 0.14487857284645236\n",
      "Epoch:241/1000\n",
      "Loss on train= 0.015314947813749313\n",
      "Loss on test= 0.01700587011873722\n",
      "acc for Lsat= 0.09092383484045664 \n",
      "acc for Psat= 0.117291279302703 \n",
      "acc for optim= 0.1500448138349586\n",
      "Epoch:242/1000\n",
      "Loss on train= 0.015240335837006569\n",
      "Loss on test= 0.01767423190176487\n",
      "acc for Lsat= 0.09677390075392195 \n",
      "acc for Psat= 0.1400580108165741 \n",
      "acc for optim= 0.1460636578500271\n",
      "Epoch:243/1000\n",
      "Loss on train= 0.01532194297760725\n",
      "Loss on test= 0.0186937153339386\n",
      "acc for Lsat= 0.1009196110897594 \n",
      "acc for Psat= 0.11837083134386273 \n",
      "acc for optim= 0.14885474207500615\n",
      "Epoch:244/1000\n",
      "Loss on train= 0.015417304821312428\n",
      "Loss on test= 0.017555899918079376\n",
      "acc for Lsat= 0.08807187957896127 \n",
      "acc for Psat= 0.11707165671719445 \n",
      "acc for optim= 0.14716767051981555\n",
      "Epoch:245/1000\n",
      "Loss on train= 0.015070643275976181\n",
      "Loss on test= 0.01790396310389042\n",
      "acc for Lsat= 0.0769369951552815 \n",
      "acc for Psat= 0.11425886154174804 \n",
      "acc for optim= 0.1461292893936237\n",
      "Epoch:246/1000\n",
      "Loss on train= 0.014563539065420628\n",
      "Loss on test= 0.01911829598248005\n",
      "acc for Lsat= 0.08925974981652367 \n",
      "acc for Psat= 0.1339999149243037 \n",
      "acc for optim= 0.14484775427521931\n",
      "Epoch:247/1000\n",
      "Loss on train= 0.014917063526809216\n",
      "Loss on test= 0.016863085329532623\n",
      "acc for Lsat= 0.08665011641052034 \n",
      "acc for Psat= 0.1160667167769538 \n",
      "acc for optim= 0.14766656797793176\n",
      "Epoch:248/1000\n",
      "Loss on train= 0.014986245892941952\n",
      "Loss on test= 0.01900801993906498\n",
      "acc for Lsat= 0.08886455148458482 \n",
      "acc for Psat= 0.1301779359579086 \n",
      "acc for optim= 0.14459697869088914\n",
      "Epoch:249/1000\n",
      "Loss on train= 0.01506989635527134\n",
      "Loss on test= 0.018910253420472145\n",
      "acc for Lsat= 0.10073545442687143 \n",
      "acc for Psat= 0.15765272743172112 \n",
      "acc for optim= 0.14625870125989118\n",
      "Epoch:250/1000\n",
      "Loss on train= 0.015267730690538883\n",
      "Loss on test= 0.01805218681693077\n",
      "acc for Lsat= 0.07773381471633911 \n",
      "acc for Psat= 0.1143545130888621 \n",
      "acc for optim= 0.14253969366351762\n",
      "Epoch:251/1000\n",
      "Loss on train= 0.015360611490905285\n",
      "Loss on test= 0.018515944480895996\n",
      "acc for Lsat= 0.09483203573359383 \n",
      "acc for Psat= 0.14595612817340425 \n",
      "acc for optim= 0.1439250847324729\n",
      "Epoch:252/1000\n",
      "Loss on train= 0.014882155694067478\n",
      "Loss on test= 0.018211660906672478\n",
      "acc for Lsat= 0.09753517044915094 \n",
      "acc for Psat= 0.1276611010233561 \n",
      "acc for optim= 0.1472881394128005\n",
      "Epoch:253/1000\n",
      "Loss on train= 0.015290559269487858\n",
      "Loss on test= 0.01738674007356167\n",
      "acc for Lsat= 0.08355637507306204 \n",
      "acc for Psat= 0.11020400921503702 \n",
      "acc for optim= 0.1458784945309162\n",
      "Epoch:254/1000\n",
      "Loss on train= 0.015101825818419456\n",
      "Loss on test= 0.017703481018543243\n",
      "acc for Lsat= 0.08474676592482461 \n",
      "acc for Psat= 0.1111986753013399 \n",
      "acc for optim= 0.14452280203501386\n",
      "Epoch:255/1000\n",
      "Loss on train= 0.014992931857705116\n",
      "Loss on test= 0.01707036793231964\n",
      "acc for Lsat= 0.07722970263825521 \n",
      "acc for Psat= 0.10991748107804193 \n",
      "acc for optim= 0.1479925974375672\n",
      "Epoch:256/1000\n",
      "Loss on train= 0.014671274460852146\n",
      "Loss on test= 0.01762034371495247\n",
      "acc for Lsat= 0.09982221672932307 \n",
      "acc for Psat= 0.12190584705935582 \n",
      "acc for optim= 0.14702857641710174\n",
      "Epoch:257/1000\n",
      "Loss on train= 0.014863618649542332\n",
      "Loss on test= 0.01786743476986885\n",
      "acc for Lsat= 0.07871869405110675 \n",
      "acc for Psat= 0.11280627532137766 \n",
      "acc for optim= 0.14807224952512318\n",
      "Epoch:258/1000\n",
      "Loss on train= 0.014928176999092102\n",
      "Loss on test= 0.019479773938655853\n",
      "acc for Lsat= 0.09160974886682298 \n",
      "acc for Psat= 0.14748543070422282 \n",
      "acc for optim= 0.14440286589993373\n",
      "Epoch:259/1000\n",
      "Loss on train= 0.015075068920850754\n",
      "Loss on test= 0.017119169235229492\n",
      "acc for Lsat= 0.08351140833563274 \n",
      "acc for Psat= 0.10613325751490062 \n",
      "acc for optim= 0.1447958690838681\n",
      "Epoch:260/1000\n",
      "Loss on train= 0.014583304524421692\n",
      "Loss on test= 0.01739691197872162\n",
      "acc for Lsat= 0.0833867953883277 \n",
      "acc for Psat= 0.11694967250029246 \n",
      "acc for optim= 0.14743452237712013\n",
      "Epoch:261/1000\n",
      "Loss on train= 0.014362913556396961\n",
      "Loss on test= 0.018020877614617348\n",
      "acc for Lsat= 0.09736117786831328 \n",
      "acc for Psat= 0.1315887302160263 \n",
      "acc for optim= 0.14416747432616023\n",
      "Epoch:262/1000\n",
      "Loss on train= 0.014610275626182556\n",
      "Loss on test= 0.0185585655272007\n",
      "acc for Lsat= 0.08730415950218837 \n",
      "acc for Psat= 0.11294680734475454 \n",
      "acc for optim= 0.14782528147722285\n",
      "Epoch:263/1000\n",
      "Loss on train= 0.014574816450476646\n",
      "Loss on test= 0.017303667962551117\n",
      "acc for Lsat= 0.07766527947452333 \n",
      "acc for Psat= 0.11094471447997623 \n",
      "acc for optim= 0.14726818427443503\n",
      "Epoch:264/1000\n",
      "Loss on train= 0.014097798615694046\n",
      "Loss on test= 0.017529575154185295\n",
      "acc for Lsat= 0.07522602892584271 \n",
      "acc for Psat= 0.1235021067990197 \n",
      "acc for optim= 0.14314376511093643\n",
      "Epoch:265/1000\n",
      "Loss on train= 0.014518135227262974\n",
      "Loss on test= 0.01794915832579136\n",
      "acc for Lsat= 0.08273219797346326 \n",
      "acc for Psat= 0.12675099935796527 \n",
      "acc for optim= 0.14746313210990694\n",
      "Epoch:266/1000\n",
      "Loss on train= 0.014228645712137222\n",
      "Loss on test= 0.018277527764439583\n",
      "acc for Lsat= 0.08523125251134238 \n",
      "acc for Psat= 0.11420587235026887 \n",
      "acc for optim= 0.14479706014196075\n",
      "Epoch:267/1000\n",
      "Loss on train= 0.014138799160718918\n",
      "Loss on test= 0.017618073150515556\n",
      "acc for Lsat= 0.10770380033387078 \n",
      "acc for Psat= 0.1290047781334983 \n",
      "acc for optim= 0.1490559450454182\n",
      "Epoch:268/1000\n",
      "Loss on train= 0.014715522527694702\n",
      "Loss on test= 0.017179250717163086\n",
      "acc for Lsat= 0.08521852294603985 \n",
      "acc for Psat= 0.11262340355250572 \n",
      "acc for optim= 0.1442114550413357\n",
      "Epoch:269/1000\n",
      "Loss on train= 0.015061411075294018\n",
      "Loss on test= 0.017878644168376923\n",
      "acc for Lsat= 0.112965284453498 \n",
      "acc for Psat= 0.12020259333981409 \n",
      "acc for optim= 0.1444677228315009\n",
      "Epoch:270/1000\n",
      "Loss on train= 0.015072659589350224\n",
      "Loss on test= 0.017014870420098305\n",
      "acc for Lsat= 0.08469845487011803 \n",
      "acc for Psat= 0.11345548596647051 \n",
      "acc for optim= 0.14651195104751324\n",
      "Epoch:271/1000\n",
      "Loss on train= 0.014317044988274574\n",
      "Loss on test= 0.016971131786704063\n",
      "acc for Lsat= 0.08190287318494584 \n",
      "acc for Psat= 0.11177000436517925 \n",
      "acc for optim= 0.1462010153983202\n",
      "Epoch:272/1000\n",
      "Loss on train= 0.014089093543589115\n",
      "Loss on test= 0.016912462189793587\n",
      "acc for Lsat= 0.09318637649218244 \n",
      "acc for Psat= 0.12781180805630157 \n",
      "acc for optim= 0.14402547807743152\n",
      "Epoch:273/1000\n",
      "Loss on train= 0.014126353897154331\n",
      "Loss on test= 0.019232721999287605\n",
      "acc for Lsat= 0.1103456222348743 \n",
      "acc for Psat= 0.1313153717252943 \n",
      "acc for optim= 0.1430127982257141\n",
      "Epoch:274/1000\n",
      "Loss on train= 0.013907602056860924\n",
      "Loss on test= 0.017080774530768394\n",
      "acc for Lsat= 0.08377313680118986 \n",
      "acc for Psat= 0.1386170188585917 \n",
      "acc for optim= 0.14631107221874923\n",
      "Epoch:275/1000\n",
      "Loss on train= 0.014406141825020313\n",
      "Loss on test= 0.016622470691800117\n",
      "acc for Lsat= 0.08826769408252505 \n",
      "acc for Psat= 0.10799166858196257 \n",
      "acc for optim= 0.1469664073652691\n",
      "Epoch:276/1000\n",
      "Loss on train= 0.014107287861406803\n",
      "Loss on test= 0.017081866040825844\n",
      "acc for Lsat= 0.08571316020356284 \n",
      "acc for Psat= 0.1242473433415095 \n",
      "acc for optim= 0.14508738837515314\n",
      "Epoch:277/1000\n",
      "Loss on train= 0.014284652657806873\n",
      "Loss on test= 0.016718503087759018\n",
      "acc for Lsat= 0.07885241640938652 \n",
      "acc for Psat= 0.11171822879049514 \n",
      "acc for optim= 0.1481481841040982\n",
      "Epoch:278/1000\n",
      "Loss on train= 0.01445474848151207\n",
      "Loss on test= 0.018085075542330742\n",
      "acc for Lsat= 0.08298516737090217 \n",
      "acc for Psat= 0.10846899913416969 \n",
      "acc for optim= 0.14582989505595634\n",
      "Epoch:279/1000\n",
      "Loss on train= 0.01420071441680193\n",
      "Loss on test= 0.016830187290906906\n",
      "acc for Lsat= 0.08242385503318575 \n",
      "acc for Psat= 0.11810136967235141 \n",
      "acc for optim= 0.1466418834196197\n",
      "Epoch:280/1000\n",
      "Loss on train= 0.014138973318040371\n",
      "Loss on test= 0.019110605120658875\n",
      "acc for Lsat= 0.0894823185271687 \n",
      "acc for Psat= 0.11878332992394765 \n",
      "acc for optim= 0.14744899372259773\n",
      "Epoch:281/1000\n",
      "Loss on train= 0.014436411671340466\n",
      "Loss on test= 0.01717228628695011\n",
      "acc for Lsat= 0.07308158973852794 \n",
      "acc for Psat= 0.11590526070859698 \n",
      "acc for optim= 0.1482412858141793\n",
      "Epoch:282/1000\n",
      "Loss on train= 0.013719309121370316\n",
      "Loss on test= 0.017159314826130867\n",
      "acc for Lsat= 0.08941428081856834 \n",
      "acc for Psat= 0.13494739996062385 \n",
      "acc for optim= 0.1429168928362843\n",
      "Epoch:283/1000\n",
      "Loss on train= 0.01404696423560381\n",
      "Loss on test= 0.016842825338244438\n",
      "acc for Lsat= 0.08443699114852482 \n",
      "acc for Psat= 0.12035702235168882 \n",
      "acc for optim= 0.14856264169017475\n",
      "Epoch:284/1000\n",
      "Loss on train= 0.014205044135451317\n",
      "Loss on test= 0.016863323748111725\n",
      "acc for Lsat= 0.07615327636400859 \n",
      "acc for Psat= 0.11492392486996122 \n",
      "acc for optim= 0.14566111115531785\n",
      "Epoch:285/1000\n",
      "Loss on train= 0.014545327052474022\n",
      "Loss on test= 0.017071489244699478\n",
      "acc for Lsat= 0.08832061141729354 \n",
      "acc for Psat= 0.11783301432927448 \n",
      "acc for optim= 0.1435292868150605\n",
      "Epoch:286/1000\n",
      "Loss on train= 0.014155714772641659\n",
      "Loss on test= 0.017234966158866882\n",
      "acc for Lsat= 0.07526154269774755 \n",
      "acc for Psat= 0.11706951724158395 \n",
      "acc for optim= 0.14697870843940308\n",
      "Epoch:287/1000\n",
      "Loss on train= 0.013974905014038086\n",
      "Loss on test= 0.018185444176197052\n",
      "acc for Lsat= 0.09163244581884808 \n",
      "acc for Psat= 0.11690661907196044 \n",
      "acc for optim= 0.14245183741052944\n",
      "Epoch:288/1000\n",
      "Loss on train= 0.014582071453332901\n",
      "Loss on test= 0.016620062291622162\n",
      "acc for Lsat= 0.07885800235801273 \n",
      "acc for Psat= 0.10945777793725332 \n",
      "acc for optim= 0.1457246218290594\n",
      "Epoch:289/1000\n",
      "Loss on train= 0.0139039047062397\n",
      "Loss on test= 0.019045613706111908\n",
      "acc for Lsat= 0.08591084116035039 \n",
      "acc for Psat= 0.12997935381200582 \n",
      "acc for optim= 0.1451645567185349\n",
      "Epoch:290/1000\n",
      "Loss on train= 0.014229861088097095\n",
      "Loss on test= 0.01812553033232689\n",
      "acc for Lsat= 0.09683628843890296 \n",
      "acc for Psat= 0.14011278947194417 \n",
      "acc for optim= 0.14636016550163428\n",
      "Epoch:291/1000\n",
      "Loss on train= 0.01388795766979456\n",
      "Loss on test= 0.01741713657975197\n",
      "acc for Lsat= 0.0855333321624332 \n",
      "acc for Psat= 0.12850897974438139 \n",
      "acc for optim= 0.1458722504062785\n",
      "Epoch:292/1000\n",
      "Loss on train= 0.013774869032204151\n",
      "Loss on test= 0.017970573157072067\n",
      "acc for Lsat= 0.07896210700273515 \n",
      "acc for Psat= 0.10635544856389362 \n",
      "acc for optim= 0.14516965837942228\n",
      "Epoch:293/1000\n",
      "Loss on train= 0.014118700288236141\n",
      "Loss on test= 0.017767352983355522\n",
      "acc for Lsat= 0.08940600487920973 \n",
      "acc for Psat= 0.10855379866229162 \n",
      "acc for optim= 0.14511215877201822\n",
      "Epoch:294/1000\n",
      "Loss on train= 0.013846615329384804\n",
      "Loss on test= 0.017064841464161873\n",
      "acc for Lsat= 0.08580814533763462 \n",
      "acc for Psat= 0.11584409707122376 \n",
      "acc for optim= 0.14531625045670404\n",
      "Epoch:295/1000\n",
      "Loss on train= 0.013947004452347755\n",
      "Loss on test= 0.016775231808423996\n",
      "acc for Lsat= 0.08828085230456457 \n",
      "acc for Psat= 0.12173774705992806 \n",
      "acc for optim= 0.15322708868318133\n",
      "Epoch:296/1000\n",
      "Loss on train= 0.014744146727025509\n",
      "Loss on test= 0.017575984820723534\n",
      "acc for Lsat= 0.08498140457603666 \n",
      "acc for Psat= 0.10703515261411668 \n",
      "acc for optim= 0.14322766210469934\n",
      "Epoch:297/1000\n",
      "Loss on train= 0.01393111702054739\n",
      "Loss on test= 0.017325999215245247\n",
      "acc for Lsat= 0.08074007911814582 \n",
      "acc for Psat= 0.12771229214138455 \n",
      "acc for optim= 0.14365469382868873\n",
      "Epoch:298/1000\n",
      "Loss on train= 0.013874449767172337\n",
      "Loss on test= 0.017421428114175797\n",
      "acc for Lsat= 0.08484549158149295 \n",
      "acc for Psat= 0.12077151768737367 \n",
      "acc for optim= 0.14626245233747695\n",
      "Epoch:299/1000\n",
      "Loss on train= 0.013819501735270023\n",
      "Loss on test= 0.016426442191004753\n",
      "acc for Lsat= 0.08709122720691893 \n",
      "acc for Psat= 0.11123103896776836 \n",
      "acc for optim= 0.1455762433095111\n",
      "Epoch:300/1000\n",
      "Loss on train= 0.014582505449652672\n",
      "Loss on test= 0.016885431483387947\n",
      "acc for Lsat= 0.09179564631647535 \n",
      "acc for Psat= 0.10967313084337445 \n",
      "acc for optim= 0.1434113824947013\n",
      "Epoch:301/1000\n",
      "Loss on train= 0.014029139652848244\n",
      "Loss on test= 0.01588408835232258\n",
      "acc for Lsat= 0.08000389734903972 \n",
      "acc for Psat= 0.0997081200281779 \n",
      "acc for optim= 0.14436816775964362\n",
      "Epoch:302/1000\n",
      "Loss on train= 0.013671460561454296\n",
      "Loss on test= 0.01660834811627865\n",
      "acc for Lsat= 0.09052924414475759 \n",
      "acc for Psat= 0.10583959288067288 \n",
      "acc for optim= 0.14416910658280055\n",
      "Epoch:303/1000\n",
      "Loss on train= 0.013761276379227638\n",
      "Loss on test= 0.019077729433774948\n",
      "acc for Lsat= 0.07422375587953461 \n",
      "acc for Psat= 0.10211040476957957 \n",
      "acc for optim= 0.14673671655149922\n",
      "Epoch:304/1000\n",
      "Loss on train= 0.013729587197303772\n",
      "Loss on test= 0.016965381801128387\n",
      "acc for Lsat= 0.08580884966585371 \n",
      "acc for Psat= 0.12239294217692483 \n",
      "acc for optim= 0.14715466548999148\n",
      "Epoch:305/1000\n",
      "Loss on train= 0.013585451059043407\n",
      "Loss on test= 0.016635486856102943\n",
      "acc for Lsat= 0.08089411507050197 \n",
      "acc for Psat= 0.1080981812543339 \n",
      "acc for optim= 0.14502852686370413\n",
      "Epoch:306/1000\n",
      "Loss on train= 0.013949469663202763\n",
      "Loss on test= 0.01610419899225235\n",
      "acc for Lsat= 0.07120094249645868 \n",
      "acc for Psat= 0.10465636551380159 \n",
      "acc for optim= 0.14770796474897196\n",
      "Epoch:307/1000\n",
      "Loss on train= 0.013816102407872677\n",
      "Loss on test= 0.016805510967969894\n",
      "acc for Lsat= 0.08460574878586663 \n",
      "acc for Psat= 0.10409403691689173 \n",
      "acc for optim= 0.14689409670730436\n",
      "Epoch:308/1000\n",
      "Loss on train= 0.01386286411434412\n",
      "Loss on test= 0.01641327328979969\n",
      "acc for Lsat= 0.07745956977208457 \n",
      "acc for Psat= 0.11480229761865404 \n",
      "acc for optim= 0.14401391662864219\n",
      "Epoch:309/1000\n",
      "Loss on train= 0.014029589481651783\n",
      "Loss on test= 0.01664445921778679\n",
      "acc for Lsat= 0.08674192908737394 \n",
      "acc for Psat= 0.10958740214506786 \n",
      "acc for optim= 0.14357391705529557\n",
      "Epoch:310/1000\n",
      "Loss on train= 0.014057758264243603\n",
      "Loss on test= 0.01668120175600052\n",
      "acc for Lsat= 0.08045369949605731 \n",
      "acc for Psat= 0.11202442513571843 \n",
      "acc for optim= 0.14802969853497214\n",
      "Epoch:311/1000\n",
      "Loss on train= 0.013917229138314724\n",
      "Loss on test= 0.017060963436961174\n",
      "acc for Lsat= 0.07985449400213031 \n",
      "acc for Psat= 0.11887599594063228 \n",
      "acc for optim= 0.1442196969770723\n",
      "Epoch:312/1000\n",
      "Loss on train= 0.013561341911554337\n",
      "Loss on test= 0.01641540415585041\n",
      "acc for Lsat= 0.08092277612951067 \n",
      "acc for Psat= 0.11505700912740496 \n",
      "acc for optim= 0.14298324233127968\n",
      "Epoch:313/1000\n",
      "Loss on train= 0.013550995849072933\n",
      "Loss on test= 0.017313547432422638\n",
      "acc for Lsat= 0.07443001800113254 \n",
      "acc for Psat= 0.12785546282927196 \n",
      "acc for optim= 0.14416324810849293\n",
      "Epoch:314/1000\n",
      "Loss on train= 0.0140510443598032\n",
      "Loss on test= 0.016474701464176178\n",
      "acc for Lsat= 0.07589471058713065 \n",
      "acc for Psat= 0.09473190704981485 \n",
      "acc for optim= 0.144821461616084\n",
      "Epoch:315/1000\n",
      "Loss on train= 0.01349522266536951\n",
      "Loss on test= 0.016912616789340973\n",
      "acc for Lsat= 0.07562684847248925 \n",
      "acc for Psat= 0.10087952084011502 \n",
      "acc for optim= 0.14223299405873857\n",
      "Epoch:316/1000\n",
      "Loss on train= 0.013469140976667404\n",
      "Loss on test= 0.01775919459760189\n",
      "acc for Lsat= 0.08280745330784056 \n",
      "acc for Psat= 0.10598603387673695 \n",
      "acc for optim= 0.1442686619444026\n",
      "Epoch:317/1000\n",
      "Loss on train= 0.01377636194229126\n",
      "Loss on test= 0.016428565606474876\n",
      "acc for Lsat= 0.07754272023836771 \n",
      "acc for Psat= 0.10395475245184368 \n",
      "acc for optim= 0.14821509545048078\n",
      "Epoch:318/1000\n",
      "Loss on train= 0.013581394217908382\n",
      "Loss on test= 0.017774753272533417\n",
      "acc for Lsat= 0.08337832142909368 \n",
      "acc for Psat= 0.11523291733529832 \n",
      "acc for optim= 0.14494888919095203\n",
      "Epoch:319/1000\n",
      "Loss on train= 0.01353989914059639\n",
      "Loss on test= 0.017241952940821648\n",
      "acc for Lsat= 0.08175195654233297 \n",
      "acc for Psat= 0.11147547099325389 \n",
      "acc for optim= 0.14140758050812618\n",
      "Epoch:320/1000\n",
      "Loss on train= 0.013561694882810116\n",
      "Loss on test= 0.016702212393283844\n",
      "acc for Lsat= 0.08427512231800291 \n",
      "acc for Psat= 0.11855565971798367 \n",
      "acc for optim= 0.1462549235257838\n",
      "Epoch:321/1000\n",
      "Loss on train= 0.013315463438630104\n",
      "Loss on test= 0.01789226196706295\n",
      "acc for Lsat= 0.10452749563588035 \n",
      "acc for Psat= 0.11353245675563814 \n",
      "acc for optim= 0.14816009038024477\n",
      "Epoch:322/1000\n",
      "Loss on train= 0.013714260421693325\n",
      "Loss on test= 0.016183776780962944\n",
      "acc for Lsat= 0.07275416884157393 \n",
      "acc for Psat= 0.10954607327779135 \n",
      "acc for optim= 0.14507630003823171\n",
      "Epoch:323/1000\n",
      "Loss on train= 0.01362618152052164\n",
      "Loss on test= 0.016417454928159714\n",
      "acc for Lsat= 0.0806902214884758 \n",
      "acc for Psat= 0.10869966016875374 \n",
      "acc for optim= 0.14647353432244725\n",
      "Epoch:324/1000\n",
      "Loss on train= 0.013438328169286251\n",
      "Loss on test= 0.01912251114845276\n",
      "acc for Lsat= 0.09292906059159173 \n",
      "acc for Psat= 0.13841192490524717 \n",
      "acc for optim= 0.14251538117726648\n",
      "Epoch:325/1000\n",
      "Loss on train= 0.01321212388575077\n",
      "Loss on test= 0.01748267561197281\n",
      "acc for Lsat= 0.08246379544337593 \n",
      "acc for Psat= 0.10659880174530877 \n",
      "acc for optim= 0.14622978485292856\n",
      "Epoch:326/1000\n",
      "Loss on train= 0.013452550396323204\n",
      "Loss on test= 0.015969395637512207\n",
      "acc for Lsat= 0.07580990178717507 \n",
      "acc for Psat= 0.10111548735035791 \n",
      "acc for optim= 0.14496717900037767\n",
      "Epoch:327/1000\n",
      "Loss on train= 0.013412592001259327\n",
      "Loss on test= 0.016797078773379326\n",
      "acc for Lsat= 0.08017003734906517 \n",
      "acc for Psat= 0.1154330227110121 \n",
      "acc for optim= 0.1437705613258812\n",
      "Epoch:328/1000\n",
      "Loss on train= 0.013542789034545422\n",
      "Loss on test= 0.01739654503762722\n",
      "acc for Lsat= 0.07192277163267134 \n",
      "acc for Psat= 0.10948422633939318 \n",
      "acc for optim= 0.14269631132483485\n",
      "Epoch:329/1000\n",
      "Loss on train= 0.013486851006746292\n",
      "Loss on test= 0.01799418032169342\n",
      "acc for Lsat= 0.07648618817329407 \n",
      "acc for Psat= 0.1043255047665702 \n",
      "acc for optim= 0.14368509803381233\n",
      "Epoch:330/1000\n",
      "Loss on train= 0.013397815637290478\n",
      "Loss on test= 0.016371536999940872\n",
      "acc for Lsat= 0.09628553357389237 \n",
      "acc for Psat= 0.12377564211686451 \n",
      "acc for optim= 0.14298373489744132\n",
      "Epoch:331/1000\n",
      "Loss on train= 0.013594653457403183\n",
      "Loss on test= 0.01777052879333496\n",
      "acc for Lsat= 0.09416717224650915 \n",
      "acc for Psat= 0.17047339876492817 \n",
      "acc for optim= 0.1408920879372292\n",
      "Epoch:332/1000\n",
      "Loss on train= 0.013404155150055885\n",
      "Loss on test= 0.016583196818828583\n",
      "acc for Lsat= 0.07599240177207525 \n",
      "acc for Psat= 0.10790998140970867 \n",
      "acc for optim= 0.14219085499644282\n",
      "Epoch:333/1000\n",
      "Loss on train= 0.013604473322629929\n",
      "Loss on test= 0.017432542517781258\n",
      "acc for Lsat= 0.08691056023041406 \n",
      "acc for Psat= 0.1194353650841448 \n",
      "acc for optim= 0.14376722416943977\n",
      "Epoch:334/1000\n",
      "Loss on train= 0.013636681251227856\n",
      "Loss on test= 0.016930600628256798\n",
      "acc for Lsat= 0.07473690542909833 \n",
      "acc for Psat= 0.10564058621724447 \n",
      "acc for optim= 0.1440845911287599\n",
      "Epoch:335/1000\n",
      "Loss on train= 0.013086307793855667\n",
      "Loss on test= 0.01612839102745056\n",
      "acc for Lsat= 0.07985175400972366 \n",
      "acc for Psat= 0.10831772320800356 \n",
      "acc for optim= 0.14128850284549926\n",
      "Epoch:336/1000\n",
      "Loss on train= 0.012751037254929543\n",
      "Loss on test= 0.016862310469150543\n",
      "acc for Lsat= 0.07238563117053773 \n",
      "acc for Psat= 0.09912275787856845 \n",
      "acc for optim= 0.1448490586545733\n",
      "Epoch:337/1000\n",
      "Loss on train= 0.013115662150084972\n",
      "Loss on test= 0.01781858503818512\n",
      "acc for Lsat= 0.0883719431029426 \n",
      "acc for Psat= 0.11118442217508953 \n",
      "acc for optim= 0.1412600400961108\n",
      "Epoch:338/1000\n",
      "Loss on train= 0.013495414517819881\n",
      "Loss on test= 0.016712838783860207\n",
      "acc for Lsat= 0.08770738161272472 \n",
      "acc for Psat= 0.11564290258619521 \n",
      "acc for optim= 0.1479143985857566\n",
      "Epoch:339/1000\n",
      "Loss on train= 0.013225529342889786\n",
      "Loss on test= 0.017543405294418335\n",
      "acc for Lsat= 0.08932920909590192 \n",
      "acc for Psat= 0.11731355422072941 \n",
      "acc for optim= 0.1425509051600885\n",
      "Epoch:340/1000\n",
      "Loss on train= 0.013522115536034107\n",
      "Loss on test= 0.0177294984459877\n",
      "acc for Lsat= 0.07729375014702479 \n",
      "acc for Psat= 0.10897030499246385 \n",
      "acc for optim= 0.14341261064012845\n",
      "Epoch:341/1000\n",
      "Loss on train= 0.013498780317604542\n",
      "Loss on test= 0.017292456701397896\n",
      "acc for Lsat= 0.09850384328100417 \n",
      "acc for Psat= 0.10822399672534729 \n",
      "acc for optim= 0.1479717495540778\n",
      "Epoch:342/1000\n",
      "Loss on train= 0.013586758635938168\n",
      "Loss on test= 0.016176637262105942\n",
      "acc for Lsat= 0.07981724656290479 \n",
      "acc for Psat= 0.1187068372964859 \n",
      "acc for optim= 0.14340652666158146\n",
      "Epoch:343/1000\n",
      "Loss on train= 0.01342419907450676\n",
      "Loss on test= 0.01822628825902939\n",
      "acc for Lsat= 0.0778246291809612 \n",
      "acc for Psat= 0.11063990659183925 \n",
      "acc for optim= 0.14118723918994266\n",
      "Epoch:344/1000\n",
      "Loss on train= 0.013246613554656506\n",
      "Loss on test= 0.017241492867469788\n",
      "acc for Lsat= 0.09317796064747703 \n",
      "acc for Psat= 0.12397552960448793 \n",
      "acc for optim= 0.14121197183719938\n",
      "Epoch:345/1000\n",
      "Loss on train= 0.01322160568088293\n",
      "Loss on test= 0.016944309696555138\n",
      "acc for Lsat= 0.08182211236821281 \n",
      "acc for Psat= 0.10700008008215164 \n",
      "acc for optim= 0.14187081690049833\n",
      "Epoch:346/1000\n",
      "Loss on train= 0.013308260589838028\n",
      "Loss on test= 0.016628935933113098\n",
      "acc for Lsat= 0.07414351685179604 \n",
      "acc for Psat= 0.10178637339009178 \n",
      "acc for optim= 0.14133528655187952\n",
      "Epoch:347/1000\n",
      "Loss on train= 0.01338280737400055\n",
      "Loss on test= 0.016245625913143158\n",
      "acc for Lsat= 0.0836160237590472 \n",
      "acc for Psat= 0.12045485377311706 \n",
      "acc for optim= 0.1459689252078533\n",
      "Epoch:348/1000\n",
      "Loss on train= 0.012653403915464878\n",
      "Loss on test= 0.01718650758266449\n",
      "acc for Lsat= 0.07291400581598281 \n",
      "acc for Psat= 0.10490849432018065 \n",
      "acc for optim= 0.1397736112690634\n",
      "Epoch:349/1000\n",
      "Loss on train= 0.013020822778344154\n",
      "Loss on test= 0.017358306795358658\n",
      "acc for Lsat= 0.0921241189042727 \n",
      "acc for Psat= 0.11309877071115707 \n",
      "acc for optim= 0.14561024208863577\n",
      "Epoch:350/1000\n",
      "Loss on train= 0.013135965913534164\n",
      "Loss on test= 0.017473503947257996\n",
      "acc for Lsat= 0.08883585002687241 \n",
      "acc for Psat= 0.11165946887599097 \n",
      "acc for optim= 0.14190583001408308\n",
      "Epoch:351/1000\n",
      "Loss on train= 0.012783903628587723\n",
      "Loss on test= 0.01661110855638981\n",
      "acc for Lsat= 0.0815590191218588 \n",
      "acc for Psat= 0.10013553980324003 \n",
      "acc for optim= 0.14533728609482444\n",
      "Epoch:352/1000\n",
      "Loss on train= 0.012877640314400196\n",
      "Loss on test= 0.01545487716794014\n",
      "acc for Lsat= 0.08051314685079787 \n",
      "acc for Psat= 0.10020717581113178 \n",
      "acc for optim= 0.13992111349685324\n",
      "Epoch:353/1000\n",
      "Loss on train= 0.0130319157615304\n",
      "Loss on test= 0.016372941434383392\n",
      "acc for Lsat= 0.07570917507012685 \n",
      "acc for Psat= 0.10663599603705937 \n",
      "acc for optim= 0.1446363877505064\n",
      "Epoch:354/1000\n",
      "Loss on train= 0.012676769867539406\n",
      "Loss on test= 0.01776416227221489\n",
      "acc for Lsat= 0.08236356675624848 \n",
      "acc for Psat= 0.11874595185120901 \n",
      "acc for optim= 0.14335195053782732\n",
      "Epoch:355/1000\n",
      "Loss on train= 0.013046842068433762\n",
      "Loss on test= 0.016735346987843513\n",
      "acc for Lsat= 0.07941633297337428 \n",
      "acc for Psat= 0.10498459537823995 \n",
      "acc for optim= 0.14811783399846823\n",
      "Epoch:356/1000\n",
      "Loss on train= 0.013225257396697998\n",
      "Loss on test= 0.019345875829458237\n",
      "acc for Lsat= 0.10069640013906689 \n",
      "acc for Psat= 0.13168946769502426 \n",
      "acc for optim= 0.14475241199963623\n",
      "Epoch:357/1000\n",
      "Loss on train= 0.013285047374665737\n",
      "Loss on test= 0.01651362143456936\n",
      "acc for Lsat= 0.07095715237988365 \n",
      "acc for Psat= 0.09876432981755998 \n",
      "acc for optim= 0.1442504071113136\n",
      "Epoch:358/1000\n",
      "Loss on train= 0.013162383809685707\n",
      "Loss on test= 0.01749485544860363\n",
      "acc for Lsat= 0.07449011670218572 \n",
      "acc for Psat= 0.10165832208262549 \n",
      "acc for optim= 0.14020433152715361\n",
      "Epoch:359/1000\n",
      "Loss on train= 0.013580319471657276\n",
      "Loss on test= 0.016700783744454384\n",
      "acc for Lsat= 0.07333004855447346 \n",
      "acc for Psat= 0.09499662766853968 \n",
      "acc for optim= 0.1461093284603622\n",
      "Epoch:360/1000\n",
      "Loss on train= 0.01291794702410698\n",
      "Loss on test= 0.015212631784379482\n",
      "acc for Lsat= 0.08330572661426333 \n",
      "acc for Psat= 0.11717934012413027 \n",
      "acc for optim= 0.14261673332916366\n",
      "Epoch:361/1000\n",
      "Loss on train= 0.01296085212379694\n",
      "Loss on test= 0.016973065212368965\n",
      "acc for Lsat= 0.07709408352772394 \n",
      "acc for Psat= 0.12050032234854166 \n",
      "acc for optim= 0.14156182971265585\n",
      "Epoch:362/1000\n",
      "Loss on train= 0.013251758180558681\n",
      "Loss on test= 0.01718887872993946\n",
      "acc for Lsat= 0.0843080543809467 \n",
      "acc for Psat= 0.1228167861700058 \n",
      "acc for optim= 0.14792759352260165\n",
      "Epoch:363/1000\n",
      "Loss on train= 0.013484480790793896\n",
      "Loss on test= 0.016607923433184624\n",
      "acc for Lsat= 0.08483259992467032 \n",
      "acc for Psat= 0.11178438464800516 \n",
      "acc for optim= 0.14355870790055228\n",
      "Epoch:364/1000\n",
      "Loss on train= 0.012629305943846703\n",
      "Loss on test= 0.016455508768558502\n",
      "acc for Lsat= 0.08392486688163545 \n",
      "acc for Psat= 0.12336556216080982 \n",
      "acc for optim= 0.14381689209904935\n",
      "Epoch:365/1000\n",
      "Loss on train= 0.01300357561558485\n",
      "Loss on test= 0.016934417188167572\n",
      "acc for Lsat= 0.08457967473400962 \n",
      "acc for Psat= 0.11617256204287213 \n",
      "acc for optim= 0.1445640821423795\n",
      "Epoch:366/1000\n",
      "Loss on train= 0.012880995869636536\n",
      "Loss on test= 0.017126724123954773\n",
      "acc for Lsat= 0.08015942507319979 \n",
      "acc for Psat= 0.1035123575064871 \n",
      "acc for optim= 0.13945840727537873\n",
      "Epoch:367/1000\n",
      "Loss on train= 0.013101870194077492\n",
      "Loss on test= 0.01591167040169239\n",
      "acc for Lsat= 0.08143459028667874 \n",
      "acc for Psat= 0.10350095828374228 \n",
      "acc for optim= 0.14361067704028554\n",
      "Epoch:368/1000\n",
      "Loss on train= 0.012776214629411697\n",
      "Loss on test= 0.017757438123226166\n",
      "acc for Lsat= 0.08428071671062046 \n",
      "acc for Psat= 0.10115426513883802 \n",
      "acc for optim= 0.14313216991722585\n",
      "Epoch:369/1000\n",
      "Loss on train= 0.013075905852019787\n",
      "Loss on test= 0.016888653859496117\n",
      "acc for Lsat= 0.07548740026023652 \n",
      "acc for Psat= 0.09715538091129726 \n",
      "acc for optim= 0.14257826324966216\n",
      "Epoch:370/1000\n",
      "Loss on train= 0.013164222240447998\n",
      "Loss on test= 0.015810150653123856\n",
      "acc for Lsat= 0.07345468435022567 \n",
      "acc for Psat= 0.0963950483335389 \n",
      "acc for optim= 0.14224074172476928\n",
      "Epoch:371/1000\n",
      "Loss on train= 0.012673052959144115\n",
      "Loss on test= 0.015532432124018669\n",
      "acc for Lsat= 0.07690825445784463 \n",
      "acc for Psat= 0.11326597167385949 \n",
      "acc for optim= 0.14699234060115285\n",
      "Epoch:372/1000\n",
      "Loss on train= 0.01256509218364954\n",
      "Loss on test= 0.017687031999230385\n",
      "acc for Lsat= 0.08241211258702806 \n",
      "acc for Psat= 0.1317844831281238 \n",
      "acc for optim= 0.14239464996175638\n",
      "Epoch:373/1000\n",
      "Loss on train= 0.012891984544694424\n",
      "Loss on test= 0.015725089237093925\n",
      "acc for Lsat= 0.09064538098043866 \n",
      "acc for Psat= 0.11669386294153 \n",
      "acc for optim= 0.1441802302168475\n",
      "Epoch:374/1000\n",
      "Loss on train= 0.012711760587990284\n",
      "Loss on test= 0.01747279055416584\n",
      "acc for Lsat= 0.07594331966506113 \n",
      "acc for Psat= 0.09889082478152383 \n",
      "acc for optim= 0.1434278713332282\n",
      "Epoch:375/1000\n",
      "Loss on train= 0.012632723897695541\n",
      "Loss on test= 0.017512541264295578\n",
      "acc for Lsat= 0.09146812342935139 \n",
      "acc for Psat= 0.11062486088938185 \n",
      "acc for optim= 0.14541282736592823\n",
      "Epoch:376/1000\n",
      "Loss on train= 0.012409918941557407\n",
      "Loss on test= 0.017315290868282318\n",
      "acc for Lsat= 0.09186526702509984 \n",
      "acc for Psat= 0.11342221564716762 \n",
      "acc for optim= 0.1437111275891463\n",
      "Epoch:377/1000\n",
      "Loss on train= 0.013241729699075222\n",
      "Loss on test= 0.01621641404926777\n",
      "acc for Lsat= 0.07083867010143068 \n",
      "acc for Psat= 0.09907779196898142 \n",
      "acc for optim= 0.14357454946471587\n",
      "Epoch:378/1000\n",
      "Loss on train= 0.012458561919629574\n",
      "Loss on test= 0.01675490103662014\n",
      "acc for Lsat= 0.08577600833442475 \n",
      "acc for Psat= 0.11147053423855041 \n",
      "acc for optim= 0.1430559092511733\n",
      "Epoch:379/1000\n",
      "Loss on train= 0.013296451419591904\n",
      "Loss on test= 0.015986766666173935\n",
      "acc for Lsat= 0.08368593768941032 \n",
      "acc for Psat= 0.11736353072855207 \n",
      "acc for optim= 0.14324628930124972\n",
      "Epoch:380/1000\n",
      "Loss on train= 0.01278208289295435\n",
      "Loss on test= 0.018542326986789703\n",
      "acc for Lsat= 0.08060847338702944 \n",
      "acc for Psat= 0.12380774153603448 \n",
      "acc for optim= 0.14338853872484628\n",
      "Epoch:381/1000\n",
      "Loss on train= 0.012704268097877502\n",
      "Loss on test= 0.016012517735362053\n",
      "acc for Lsat= 0.06953380323118634 \n",
      "acc for Psat= 0.1088498208257887 \n",
      "acc for optim= 0.14391399121118917\n",
      "Epoch:382/1000\n",
      "Loss on train= 0.012418588623404503\n",
      "Loss on test= 0.017115525901317596\n",
      "acc for Lsat= 0.07656186570723851 \n",
      "acc for Psat= 0.10085441652271482 \n",
      "acc for optim= 0.14326139647099703\n",
      "Epoch:383/1000\n",
      "Loss on train= 0.01266560796648264\n",
      "Loss on test= 0.016376443207263947\n",
      "acc for Lsat= 0.07848679886923898 \n",
      "acc for Psat= 0.10827066202958423 \n",
      "acc for optim= 0.14421850144863127\n",
      "Epoch:384/1000\n",
      "Loss on train= 0.01264477614313364\n",
      "Loss on test= 0.016512174159288406\n",
      "acc for Lsat= 0.08097939888636271 \n",
      "acc for Psat= 0.09811127218935224 \n",
      "acc for optim= 0.14351846737166246\n",
      "Epoch:385/1000\n",
      "Loss on train= 0.01239412184804678\n",
      "Loss on test= 0.016142580658197403\n",
      "acc for Lsat= 0.08333594782484902 \n",
      "acc for Psat= 0.11190557016266717 \n",
      "acc for optim= 0.1403570179620551\n",
      "Epoch:386/1000\n",
      "Loss on train= 0.012529021129012108\n",
      "Loss on test= 0.01665831357240677\n",
      "acc for Lsat= 0.07655552907122506 \n",
      "acc for Psat= 0.10108628057771259 \n",
      "acc for optim= 0.14027947955247433\n",
      "Epoch:387/1000\n",
      "Loss on train= 0.01248637493699789\n",
      "Loss on test= 0.015792017802596092\n",
      "acc for Lsat= 0.1079239050547282 \n",
      "acc for Psat= 0.12078022989961834 \n",
      "acc for optim= 0.14249063504652845\n",
      "Epoch:388/1000\n",
      "Loss on train= 0.012750614434480667\n",
      "Loss on test= 0.01750028133392334\n",
      "acc for Lsat= 0.0800155692630344 \n",
      "acc for Psat= 0.13917697767416637 \n",
      "acc for optim= 0.14258358420597184\n",
      "Epoch:389/1000\n",
      "Loss on train= 0.012727183289825916\n",
      "Loss on test= 0.016187578439712524\n",
      "acc for Lsat= 0.08109814375638963 \n",
      "acc for Psat= 0.1105478839741813 \n",
      "acc for optim= 0.14216942224237653\n",
      "Epoch:390/1000\n",
      "Loss on train= 0.012575569562613964\n",
      "Loss on test= 0.016445405781269073\n",
      "acc for Lsat= 0.07814231316248575 \n",
      "acc for Psat= 0.09717091255717808 \n",
      "acc for optim= 0.142387006059289\n",
      "Epoch:391/1000\n",
      "Loss on train= 0.012983870692551136\n",
      "Loss on test= 0.015815235674381256\n",
      "acc for Lsat= 0.07881392339865366 \n",
      "acc for Psat= 0.10009948015213013 \n",
      "acc for optim= 0.1404897546188699\n",
      "Epoch:392/1000\n",
      "Loss on train= 0.012468590401113033\n",
      "Loss on test= 0.01616179756820202\n",
      "acc for Lsat= 0.07067835628986359 \n",
      "acc for Psat= 0.11418602334128486 \n",
      "acc for optim= 0.1435351450824075\n",
      "Epoch:393/1000\n",
      "Loss on train= 0.012814952991902828\n",
      "Loss on test= 0.017003275454044342\n",
      "acc for Lsat= 0.07538072781430352 \n",
      "acc for Psat= 0.09728119654787912 \n",
      "acc for optim= 0.1419373388091723\n",
      "Epoch:394/1000\n",
      "Loss on train= 0.01268340740352869\n",
      "Loss on test= 0.017476007342338562\n",
      "acc for Lsat= 0.07964523169729446 \n",
      "acc for Psat= 0.10732527905040318 \n",
      "acc for optim= 0.14305607788264751\n",
      "Epoch:395/1000\n",
      "Loss on train= 0.012387237511575222\n",
      "Loss on test= 0.0159591231495142\n",
      "acc for Lsat= 0.07447557830148273 \n",
      "acc for Psat= 0.0982398377524482 \n",
      "acc for optim= 0.13933211813370386\n",
      "Epoch:396/1000\n",
      "Loss on train= 0.012093397788703442\n",
      "Loss on test= 0.016411982476711273\n",
      "acc for Lsat= 0.08701374232769012 \n",
      "acc for Psat= 0.10444351997640396 \n",
      "acc for optim= 0.1396120373159647\n",
      "Epoch:397/1000\n",
      "Loss on train= 0.012338208965957165\n",
      "Loss on test= 0.01689479686319828\n",
      "acc for Lsat= 0.06934368428256778 \n",
      "acc for Psat= 0.10335669451289706 \n",
      "acc for optim= 0.14250856886307398\n",
      "Epoch:398/1000\n",
      "Loss on train= 0.012040365487337112\n",
      "Loss on test= 0.017138643190264702\n",
      "acc for Lsat= 0.07809813171625138 \n",
      "acc for Psat= 0.11297312312655977 \n",
      "acc for optim= 0.14184005074203015\n",
      "Epoch:399/1000\n",
      "Loss on train= 0.012271377258002758\n",
      "Loss on test= 0.015372698195278645\n",
      "acc for Lsat= 0.08114045328564114 \n",
      "acc for Psat= 0.10337048868338268 \n",
      "acc for optim= 0.1431646974136432\n",
      "Epoch:400/1000\n",
      "Loss on train= 0.012698340229690075\n",
      "Loss on test= 0.016214072704315186\n",
      "acc for Lsat= 0.08369774139589733 \n",
      "acc for Psat= 0.09831329385439554 \n",
      "acc for optim= 0.14272109319766363\n",
      "Epoch:401/1000\n",
      "Loss on train= 0.012498014606535435\n",
      "Loss on test= 0.0179224144667387\n",
      "acc for Lsat= 0.0782640775044759 \n",
      "acc for Psat= 0.11628498567475212 \n",
      "acc for optim= 0.145626516888539\n",
      "Epoch:402/1000\n",
      "Loss on train= 0.012240582145750523\n",
      "Loss on test= 0.016684364527463913\n",
      "acc for Lsat= 0.08239479677544699 \n",
      "acc for Psat= 0.11068375806013743 \n",
      "acc for optim= 0.14157744579845008\n",
      "Epoch:403/1000\n",
      "Loss on train= 0.012396413832902908\n",
      "Loss on test= 0.01695074699819088\n",
      "acc for Lsat= 0.07441948056221008 \n",
      "acc for Psat= 0.10479938222302332 \n",
      "acc for optim= 0.1455144070502784\n",
      "Epoch:404/1000\n",
      "Loss on train= 0.01259526051580906\n",
      "Loss on test= 0.018452294170856476\n",
      "acc for Lsat= 0.09020983792013591 \n",
      "acc for Psat= 0.11686947643756868 \n",
      "acc for optim= 0.1444634359122978\n",
      "Epoch:405/1000\n",
      "Loss on train= 0.012537461705505848\n",
      "Loss on test= 0.01663963869214058\n",
      "acc for Lsat= 0.08908021367258495 \n",
      "acc for Psat= 0.11714721818765003 \n",
      "acc for optim= 0.14220618916054564\n",
      "Epoch:406/1000\n",
      "Loss on train= 0.012155863456428051\n",
      "Loss on test= 0.016731925308704376\n",
      "acc for Lsat= 0.07905625734064314 \n",
      "acc for Psat= 0.10644813279310862 \n",
      "acc for optim= 0.14061512362418901\n",
      "Epoch:407/1000\n",
      "Loss on train= 0.012700050137937069\n",
      "Loss on test= 0.01804514415562153\n",
      "acc for Lsat= 0.07926751010947758 \n",
      "acc for Psat= 0.11465919415156045 \n",
      "acc for optim= 0.14487179757820237\n",
      "Epoch:408/1000\n",
      "Loss on train= 0.01221670862287283\n",
      "Loss on test= 0.016955168917775154\n",
      "acc for Lsat= 0.08236809406015608 \n",
      "acc for Psat= 0.12184499038590325 \n",
      "acc for optim= 0.1436467382022076\n",
      "Epoch:409/1000\n",
      "Loss on train= 0.012523776851594448\n",
      "Loss on test= 0.016503887251019478\n",
      "acc for Lsat= 0.08002530816528532 \n",
      "acc for Psat= 0.10894968509674072 \n",
      "acc for optim= 0.14104036294544736\n",
      "Epoch:410/1000\n",
      "Loss on train= 0.01195585634559393\n",
      "Loss on test= 0.016698071733117104\n",
      "acc for Lsat= 0.08216141661008199 \n",
      "acc for Psat= 0.10377626319726309 \n",
      "acc for optim= 0.1416054961995946\n",
      "Epoch:411/1000\n",
      "Loss on train= 0.012489039450883865\n",
      "Loss on test= 0.015383279882371426\n",
      "acc for Lsat= 0.0704062036342091 \n",
      "acc for Psat= 0.10461297432581582 \n",
      "acc for optim= 0.13903409235386388\n",
      "Epoch:412/1000\n",
      "Loss on train= 0.01239012274891138\n",
      "Loss on test= 0.01622629165649414\n",
      "acc for Lsat= 0.07222597979836994 \n",
      "acc for Psat= 0.09591416998042002 \n",
      "acc for optim= 0.14258440194858446\n",
      "Epoch:413/1000\n",
      "Loss on train= 0.012754237279295921\n",
      "Loss on test= 0.015935219824314117\n",
      "acc for Lsat= 0.07620162963867189 \n",
      "acc for Psat= 0.11989137596554228 \n",
      "acc for optim= 0.1391024220217433\n",
      "Epoch:414/1000\n",
      "Loss on train= 0.012941855937242508\n",
      "Loss on test= 0.017888352274894714\n",
      "acc for Lsat= 0.09152251465453043 \n",
      "acc for Psat= 0.11681266393926408 \n",
      "acc for optim= 0.14240626777625748\n",
      "Epoch:415/1000\n",
      "Loss on train= 0.012745601125061512\n",
      "Loss on test= 0.015834469348192215\n",
      "acc for Lsat= 0.07490697552760442 \n",
      "acc for Psat= 0.10416516760985056 \n",
      "acc for optim= 0.139522979201542\n",
      "Epoch:416/1000\n",
      "Loss on train= 0.012359619140625\n",
      "Loss on test= 0.01566990092396736\n",
      "acc for Lsat= 0.07113507472806507 \n",
      "acc for Psat= 0.11537359588676027 \n",
      "acc for optim= 0.1420162073440022\n",
      "Epoch:417/1000\n",
      "Loss on train= 0.012272754684090614\n",
      "Loss on test= 0.01612776331603527\n",
      "acc for Lsat= 0.08879603412416247 \n",
      "acc for Psat= 0.10199928796953626 \n",
      "acc for optim= 0.14427159039510618\n",
      "Epoch:418/1000\n",
      "Loss on train= 0.012479349039494991\n",
      "Loss on test= 0.017520977184176445\n",
      "acc for Lsat= 0.08604722619056701 \n",
      "acc for Psat= 0.10841489699151781 \n",
      "acc for optim= 0.14256267452405558\n",
      "Epoch:419/1000\n",
      "Loss on train= 0.011975789442658424\n",
      "Loss on test= 0.01644054800271988\n",
      "acc for Lsat= 0.06996211244000329 \n",
      "acc for Psat= 0.10358107487360636 \n",
      "acc for optim= 0.14264720463090472\n",
      "Epoch:420/1000\n",
      "Loss on train= 0.012074004858732224\n",
      "Loss on test= 0.01761550083756447\n",
      "acc for Lsat= 0.09757364392280579 \n",
      "acc for Psat= 0.1092494303981463 \n",
      "acc for optim= 0.14117259217633144\n",
      "Epoch:421/1000\n",
      "Loss on train= 0.012213218957185745\n",
      "Loss on test= 0.016306566074490547\n",
      "acc for Lsat= 0.0745017084810469 \n",
      "acc for Psat= 0.09626789109574424 \n",
      "acc for optim= 0.14139155260183747\n",
      "Epoch:422/1000\n",
      "Loss on train= 0.01202978752553463\n",
      "Loss on test= 0.016812670975923538\n",
      "acc for Lsat= 0.08384376383490032 \n",
      "acc for Psat= 0.10760913888613383 \n",
      "acc for optim= 0.1393168310220871\n",
      "Epoch:423/1000\n",
      "Loss on train= 0.011964824050664902\n",
      "Loss on test= 0.01682150363922119\n",
      "acc for Lsat= 0.08127332992023892 \n",
      "acc for Psat= 0.10865557624234094 \n",
      "acc for optim= 0.13954006909496253\n",
      "Epoch:424/1000\n",
      "Loss on train= 0.01190202496945858\n",
      "Loss on test= 0.017176181077957153\n",
      "acc for Lsat= 0.08279946777555677 \n",
      "acc for Psat= 0.13196211556593576 \n",
      "acc for optim= 0.13826180746157965\n",
      "Epoch:425/1000\n",
      "Loss on train= 0.012597220949828625\n",
      "Loss on test= 0.01818120665848255\n",
      "acc for Lsat= 0.10130690038204193 \n",
      "acc for Psat= 0.13901869853337606 \n",
      "acc for optim= 0.14112398322257735\n",
      "Epoch:426/1000\n",
      "Loss on train= 0.011876866221427917\n",
      "Loss on test= 0.0161761324852705\n",
      "acc for Lsat= 0.08083427101373672 \n",
      "acc for Psat= 0.10516649666759703 \n",
      "acc for optim= 0.1387907610171371\n",
      "Epoch:427/1000\n",
      "Loss on train= 0.01176687516272068\n",
      "Loss on test= 0.01624184474349022\n",
      "acc for Lsat= 0.06926752676566442 \n",
      "acc for Psat= 0.1090074340502421 \n",
      "acc for optim= 0.13827672501405083\n",
      "Epoch:428/1000\n",
      "Loss on train= 0.011769171804189682\n",
      "Loss on test= 0.01607813686132431\n",
      "acc for Lsat= 0.07796534597873689 \n",
      "acc for Psat= 0.10299745963679421 \n",
      "acc for optim= 0.1395369551351501\n",
      "Epoch:429/1000\n",
      "Loss on train= 0.012167713604867458\n",
      "Loss on test= 0.016773590818047523\n",
      "acc for Lsat= 0.08706030663516785 \n",
      "acc for Psat= 0.10142780939737955 \n",
      "acc for optim= 0.13909326146046316\n",
      "Epoch:430/1000\n",
      "Loss on train= 0.01205391250550747\n",
      "Loss on test= 0.015807615593075752\n",
      "acc for Lsat= 0.07521762433979245 \n",
      "acc for Psat= 0.10445945825841693 \n",
      "acc for optim= 0.13709501621779052\n",
      "Epoch:431/1000\n",
      "Loss on train= 0.01217066589742899\n",
      "Loss on test= 0.017021384090185165\n",
      "acc for Lsat= 0.07584453192022111 \n",
      "acc for Psat= 0.10134232739607493 \n",
      "acc for optim= 0.13953124616915982\n",
      "Epoch:432/1000\n",
      "Loss on train= 0.012402499094605446\n",
      "Loss on test= 0.017173562198877335\n",
      "acc for Lsat= 0.08470272537734774 \n",
      "acc for Psat= 0.11436961160765752 \n",
      "acc for optim= 0.13882365329191088\n",
      "Epoch:433/1000\n",
      "Loss on train= 0.012167826294898987\n",
      "Loss on test= 0.016723327338695526\n",
      "acc for Lsat= 0.07227763219012157 \n",
      "acc for Psat= 0.10534709642330806 \n",
      "acc for optim= 0.1421401224409541\n",
      "Epoch:434/1000\n",
      "Loss on train= 0.01248357817530632\n",
      "Loss on test= 0.016807110980153084\n",
      "acc for Lsat= 0.07677301632033454 \n",
      "acc for Psat= 0.10886376963721382 \n",
      "acc for optim= 0.13923720130696896\n",
      "Epoch:435/1000\n",
      "Loss on train= 0.011854121461510658\n",
      "Loss on test= 0.01782241277396679\n",
      "acc for Lsat= 0.08786134786076015 \n",
      "acc for Psat= 0.12349935505125258 \n",
      "acc for optim= 0.13909359069592836\n",
      "Epoch:436/1000\n",
      "Loss on train= 0.011655441485345364\n",
      "Loss on test= 0.016007060185074806\n",
      "acc for Lsat= 0.06947467972834906 \n",
      "acc for Psat= 0.10353062483999463 \n",
      "acc for optim= 0.13976962872677376\n",
      "Epoch:437/1000\n",
      "Loss on train= 0.012000453658401966\n",
      "Loss on test= 0.016965480521321297\n",
      "acc for Lsat= 0.07002904564142227 \n",
      "acc for Psat= 0.10815648602114784 \n",
      "acc for optim= 0.1421991616487503\n",
      "Epoch:438/1000\n",
      "Loss on train= 0.011687499471008778\n",
      "Loss on test= 0.016298891976475716\n",
      "acc for Lsat= 0.08383070114586087 \n",
      "acc for Psat= 0.11949769953886667 \n",
      "acc for optim= 0.1409324758996566\n",
      "Epoch:439/1000\n",
      "Loss on train= 0.01214540284126997\n",
      "Loss on test= 0.016746561974287033\n",
      "acc for Lsat= 0.08127743932935927 \n",
      "acc for Psat= 0.1155339217848248 \n",
      "acc for optim= 0.14190666836996874\n",
      "Epoch:440/1000\n",
      "Loss on train= 0.012081023305654526\n",
      "Loss on test= 0.017933383584022522\n",
      "acc for Lsat= 0.08247693065139983 \n",
      "acc for Psat= 0.12342949575848047 \n",
      "acc for optim= 0.1405547056347132\n",
      "Epoch:441/1000\n",
      "Loss on train= 0.012092518620193005\n",
      "Loss on test= 0.01633451133966446\n",
      "acc for Lsat= 0.08167044123013814 \n",
      "acc for Psat= 0.1162650552060869 \n",
      "acc for optim= 0.13984202771551077\n",
      "Epoch:442/1000\n",
      "Loss on train= 0.012113865464925766\n",
      "Loss on test= 0.017037689685821533\n",
      "acc for Lsat= 0.10220750437842473 \n",
      "acc for Psat= 0.1224280118942261 \n",
      "acc for optim= 0.13982605187662153\n",
      "Epoch:443/1000\n",
      "Loss on train= 0.011989683844149113\n",
      "Loss on test= 0.016735324636101723\n",
      "acc for Lsat= 0.07562187131908206 \n",
      "acc for Psat= 0.11406961265537473 \n",
      "acc for optim= 0.13893687530524199\n",
      "Epoch:444/1000\n",
      "Loss on train= 0.01216594222933054\n",
      "Loss on test= 0.015956945717334747\n",
      "acc for Lsat= 0.08289995574288897 \n",
      "acc for Psat= 0.0989251062273979 \n",
      "acc for optim= 0.13734680695666207\n",
      "Epoch:445/1000\n",
      "Loss on train= 0.011804536916315556\n",
      "Loss on test= 0.01703599840402603\n",
      "acc for Lsat= 0.07820113218492933 \n",
      "acc for Psat= 0.09748614148961172 \n",
      "acc for optim= 0.1373442045930359\n",
      "Epoch:446/1000\n",
      "Loss on train= 0.011988564394414425\n",
      "Loss on test= 0.01621950790286064\n",
      "acc for Lsat= 0.071560437977314 \n",
      "acc for Psat= 0.09517442633708317 \n",
      "acc for optim= 0.13953698968721756\n",
      "Epoch:447/1000\n",
      "Loss on train= 0.011988011188805103\n",
      "Loss on test= 0.015881521627306938\n",
      "acc for Lsat= 0.08462420254945754 \n",
      "acc for Psat= 0.10913583371374343 \n",
      "acc for optim= 0.1410909443679783\n",
      "Epoch:448/1000\n",
      "Loss on train= 0.01239397469907999\n",
      "Loss on test= 0.016201015561819077\n",
      "acc for Lsat= 0.09254501826233336 \n",
      "acc for Psat= 0.1112899644507302 \n",
      "acc for optim= 0.1387359085803231\n",
      "Epoch:449/1000\n",
      "Loss on train= 0.012517783790826797\n",
      "Loss on test= 0.017185339704155922\n",
      "acc for Lsat= 0.08048125621345308 \n",
      "acc for Psat= 0.10382782419522602 \n",
      "acc for optim= 0.14043167140334845\n",
      "Epoch:450/1000\n",
      "Loss on train= 0.011739416979253292\n",
      "Loss on test= 0.01757415384054184\n",
      "acc for Lsat= 0.07697574479712381 \n",
      "acc for Psat= 0.11414521038532256 \n",
      "acc for optim= 0.13876880738470287\n",
      "Epoch:451/1000\n",
      "Loss on train= 0.011859648860991001\n",
      "Loss on test= 0.015925442799925804\n",
      "acc for Lsat= 0.06727074053552415 \n",
      "acc for Psat= 0.09995939599143135 \n",
      "acc for optim= 0.1405992589270075\n",
      "Epoch:452/1000\n",
      "Loss on train= 0.011776908300817013\n",
      "Loss on test= 0.016325699165463448\n",
      "acc for Lsat= 0.073308541211817 \n",
      "acc for Psat= 0.09797119630707635 \n",
      "acc for optim= 0.13941253080055405\n",
      "Epoch:453/1000\n",
      "Loss on train= 0.011751759797334671\n",
      "Loss on test= 0.014945387840270996\n",
      "acc for Lsat= 0.07912453214327494 \n",
      "acc for Psat= 0.11033671895662946 \n",
      "acc for optim= 0.13969994239095185\n",
      "Epoch:454/1000\n",
      "Loss on train= 0.011874157004058361\n",
      "Loss on test= 0.0162068959325552\n",
      "acc for Lsat= 0.08945740179883108 \n",
      "acc for Psat= 0.10852042585611342 \n",
      "acc for optim= 0.13903361894190314\n",
      "Epoch:455/1000\n",
      "Loss on train= 0.011540439911186695\n",
      "Loss on test= 0.01604483462870121\n",
      "acc for Lsat= 0.08081797046793832 \n",
      "acc for Psat= 0.10141290475924808 \n",
      "acc for optim= 0.14120213567382758\n",
      "Epoch:456/1000\n",
      "Loss on train= 0.011619701981544495\n",
      "Loss on test= 0.017437361180782318\n",
      "acc for Lsat= 0.08030136691199409 \n",
      "acc for Psat= 0.10528771314356061 \n",
      "acc for optim= 0.13824176664153734\n",
      "Epoch:457/1000\n",
      "Loss on train= 0.011938869953155518\n",
      "Loss on test= 0.017050450667738914\n",
      "acc for Lsat= 0.08036494702100755 \n",
      "acc for Psat= 0.1262462337811788 \n",
      "acc for optim= 0.14425852447748183\n",
      "Epoch:458/1000\n",
      "Loss on train= 0.011883020401000977\n",
      "Loss on test= 0.015470745041966438\n",
      "acc for Lsat= 0.08645172119140623 \n",
      "acc for Psat= 0.11273093058003318 \n",
      "acc for optim= 0.139499155845907\n",
      "Epoch:459/1000\n",
      "Loss on train= 0.011652544140815735\n",
      "Loss on test= 0.017352037131786346\n",
      "acc for Lsat= 0.06405107859108182 \n",
      "acc for Psat= 0.09898148890998629 \n",
      "acc for optim= 0.13914119040386544\n",
      "Epoch:460/1000\n",
      "Loss on train= 0.011806722730398178\n",
      "Loss on test= 0.015730425715446472\n",
      "acc for Lsat= 0.07326388557751974 \n",
      "acc for Psat= 0.10410154925452338 \n",
      "acc for optim= 0.14336682160695394\n",
      "Epoch:461/1000\n",
      "Loss on train= 0.011622629128396511\n",
      "Loss on test= 0.015755755826830864\n",
      "acc for Lsat= 0.06846869505114027 \n",
      "acc for Psat= 0.1051729722155465 \n",
      "acc for optim= 0.1401758276546995\n",
      "Epoch:462/1000\n",
      "Loss on train= 0.0115054277703166\n",
      "Loss on test= 0.016039932146668434\n",
      "acc for Lsat= 0.07218108905686274 \n",
      "acc for Psat= 0.10993102325333488 \n",
      "acc for optim= 0.14062693002116347\n",
      "Epoch:463/1000\n",
      "Loss on train= 0.011805150657892227\n",
      "Loss on test= 0.016991209238767624\n",
      "acc for Lsat= 0.0727155582772361 \n",
      "acc for Psat= 0.10418753657076095 \n",
      "acc for optim= 0.14009654319120776\n",
      "Epoch:464/1000\n",
      "Loss on train= 0.011538859456777573\n",
      "Loss on test= 0.016313504427671432\n",
      "acc for Lsat= 0.07015568912029266 \n",
      "acc for Psat= 0.09350911163621478 \n",
      "acc for optim= 0.13875691416776842\n",
      "Epoch:465/1000\n",
      "Loss on train= 0.011740688234567642\n",
      "Loss on test= 0.016739478334784508\n",
      "acc for Lsat= 0.09601140618324282 \n",
      "acc for Psat= 0.11435009671582118 \n",
      "acc for optim= 0.1371791861977221\n",
      "Epoch:466/1000\n",
      "Loss on train= 0.011600672267377377\n",
      "Loss on test= 0.016140468418598175\n",
      "acc for Lsat= 0.07373468461963865 \n",
      "acc for Psat= 0.13574232591523064 \n",
      "acc for optim= 0.14130157246771788\n",
      "Epoch:467/1000\n",
      "Loss on train= 0.011800223961472511\n",
      "Loss on test= 0.016344117000699043\n",
      "acc for Lsat= 0.07215062628189724 \n",
      "acc for Psat= 0.10103775411844255 \n",
      "acc for optim= 0.13843769655666416\n",
      "Epoch:468/1000\n",
      "Loss on train= 0.011728831566870213\n",
      "Loss on test= 0.01614120975136757\n",
      "acc for Lsat= 0.07973716275559532 \n",
      "acc for Psat= 0.10160623242457707 \n",
      "acc for optim= 0.1388172858912084\n",
      "Epoch:469/1000\n",
      "Loss on train= 0.011816768907010555\n",
      "Loss on test= 0.01476447656750679\n",
      "acc for Lsat= 0.08334358086188635 \n",
      "acc for Psat= 0.10869576334953307 \n",
      "acc for optim= 0.13979861409299904\n",
      "Epoch:470/1000\n",
      "Loss on train= 0.011166321113705635\n",
      "Loss on test= 0.01706020161509514\n",
      "acc for Lsat= 0.07602591084109413 \n",
      "acc for Psat= 0.10327071266041862 \n",
      "acc for optim= 0.1387862419606083\n",
      "Epoch:471/1000\n",
      "Loss on train= 0.011984338983893394\n",
      "Loss on test= 0.016082368791103363\n",
      "acc for Lsat= 0.07121979710128573 \n",
      "acc for Psat= 0.10031532744566601 \n",
      "acc for optim= 0.14006474257136384\n",
      "Epoch:472/1000\n",
      "Loss on train= 0.011305311694741249\n",
      "Loss on test= 0.015811586752533913\n",
      "acc for Lsat= 0.07900801847378411 \n",
      "acc for Psat= 0.11160128331846662 \n",
      "acc for optim= 0.14358627920349437\n",
      "Epoch:473/1000\n",
      "Loss on train= 0.011399091221392155\n",
      "Loss on test= 0.01575808972120285\n",
      "acc for Lsat= 0.07894114785724218 \n",
      "acc for Psat= 0.11096535523732504 \n",
      "acc for optim= 0.13810210490806235\n",
      "Epoch:474/1000\n",
      "Loss on train= 0.011732610873878002\n",
      "Loss on test= 0.01685710810124874\n",
      "acc for Lsat= 0.07514787067969639 \n",
      "acc for Psat= 0.10566596686840059 \n",
      "acc for optim= 0.139923135853476\n",
      "Epoch:475/1000\n",
      "Loss on train= 0.011665594764053822\n",
      "Loss on test= 0.01655658334493637\n",
      "acc for Lsat= 0.07324239859978358 \n",
      "acc for Psat= 0.10066583322154149 \n",
      "acc for optim= 0.14146082345396283\n",
      "Epoch:476/1000\n",
      "Loss on train= 0.011488538235425949\n",
      "Loss on test= 0.016319116577506065\n",
      "acc for Lsat= 0.07598875065644581 \n",
      "acc for Psat= 0.10108714335494572 \n",
      "acc for optim= 0.14101383143828972\n",
      "Epoch:477/1000\n",
      "Loss on train= 0.011664167046546936\n",
      "Loss on test= 0.016004344448447227\n",
      "acc for Lsat= 0.08114158279365963 \n",
      "acc for Psat= 0.10238144927554661 \n",
      "acc for optim= 0.13733713041163154\n",
      "Epoch:478/1000\n",
      "Loss on train= 0.011445817537605762\n",
      "Loss on test= 0.01653464138507843\n",
      "acc for Lsat= 0.0777642814649476 \n",
      "acc for Psat= 0.10068685263395309 \n",
      "acc for optim= 0.14032315934180387\n",
      "Epoch:479/1000\n",
      "Loss on train= 0.01161324605345726\n",
      "Loss on test= 0.016411446034908295\n",
      "acc for Lsat= 0.07473421593507129 \n",
      "acc for Psat= 0.10387789805730181 \n",
      "acc for optim= 0.1414334746284617\n",
      "Epoch:480/1000\n",
      "Loss on train= 0.011447552591562271\n",
      "Loss on test= 0.017262859269976616\n",
      "acc for Lsat= 0.07696709450748232 \n",
      "acc for Psat= 0.1024163805776172 \n",
      "acc for optim= 0.13813673982189761\n",
      "Epoch:481/1000\n",
      "Loss on train= 0.011311809532344341\n",
      "Loss on test= 0.015523988753557205\n",
      "acc for Lsat= 0.08120566805203755 \n",
      "acc for Psat= 0.09779268950223924 \n",
      "acc for optim= 0.1405332246174415\n",
      "Epoch:482/1000\n",
      "Loss on train= 0.011673938482999802\n",
      "Loss on test= 0.017431333661079407\n",
      "acc for Lsat= 0.069016698996226 \n",
      "acc for Psat= 0.10698380172252654 \n",
      "acc for optim= 0.13920499011874196\n",
      "Epoch:483/1000\n",
      "Loss on train= 0.011147218756377697\n",
      "Loss on test= 0.01546521671116352\n",
      "acc for Lsat= 0.08164361367623012 \n",
      "acc for Psat= 0.10771785643365649 \n",
      "acc for optim= 0.139230206505292\n",
      "Epoch:484/1000\n",
      "Loss on train= 0.011318841949105263\n",
      "Loss on test= 0.017567243427038193\n",
      "acc for Lsat= 0.07832228541374206 \n",
      "acc for Psat= 0.10619734658135307 \n",
      "acc for optim= 0.14040622134279046\n",
      "Epoch:485/1000\n",
      "Loss on train= 0.011226527392864227\n",
      "Loss on test= 0.01715819165110588\n",
      "acc for Lsat= 0.08643246856000689 \n",
      "acc for Psat= 0.10502035220464073 \n",
      "acc for optim= 0.1435152663124932\n",
      "Epoch:486/1000\n",
      "Loss on train= 0.011093087494373322\n",
      "Loss on test= 0.01621771790087223\n",
      "acc for Lsat= 0.07442986336019303 \n",
      "acc for Psat= 0.1077069017622206 \n",
      "acc for optim= 0.1375478692456252\n",
      "Epoch:487/1000\n",
      "Loss on train= 0.01109189260751009\n",
      "Loss on test= 0.01646515168249607\n",
      "acc for Lsat= 0.07862197425630357 \n",
      "acc for Psat= 0.10854964123831852 \n",
      "acc for optim= 0.14200462566481697\n",
      "Epoch:488/1000\n",
      "Loss on train= 0.011334594339132309\n",
      "Loss on test= 0.01599193550646305\n",
      "acc for Lsat= 0.07762449151939815 \n",
      "acc for Psat= 0.09801215198304915 \n",
      "acc for optim= 0.14369138575469456\n",
      "Epoch:489/1000\n",
      "Loss on train= 0.011149952188134193\n",
      "Loss on test= 0.017213933169841766\n",
      "acc for Lsat= 0.0707853686478403 \n",
      "acc for Psat= 0.10507432619730632 \n",
      "acc for optim= 0.13763688930517268\n",
      "Epoch:490/1000\n",
      "Loss on train= 0.01167492289096117\n",
      "Loss on test= 0.015733787789940834\n",
      "acc for Lsat= 0.0893111346496476 \n",
      "acc for Psat= 0.12183624340428247 \n",
      "acc for optim= 0.1438530596594016\n",
      "Epoch:491/1000\n",
      "Loss on train= 0.011776665225625038\n",
      "Loss on test= 0.015485867857933044\n",
      "acc for Lsat= 0.07242709497610728 \n",
      "acc for Psat= 0.11197507845030891 \n",
      "acc for optim= 0.13902812250372437\n",
      "Epoch:492/1000\n",
      "Loss on train= 0.011098847724497318\n",
      "Loss on test= 0.01604982279241085\n",
      "acc for Lsat= 0.07434441016780005 \n",
      "acc for Psat= 0.10123095495833292 \n",
      "acc for optim= 0.13968334125561851\n",
      "Epoch:493/1000\n",
      "Loss on train= 0.01132163405418396\n",
      "Loss on test= 0.017369171604514122\n",
      "acc for Lsat= 0.07123492343558205 \n",
      "acc for Psat= 0.11010109815332624 \n",
      "acc for optim= 0.14261411271161503\n",
      "Epoch:494/1000\n",
      "Loss on train= 0.011196653358638287\n",
      "Loss on test= 0.016665097326040268\n",
      "acc for Lsat= 0.07851178182495966 \n",
      "acc for Psat= 0.10924768745899201 \n",
      "acc for optim= 0.13778568531076116\n",
      "Epoch:495/1000\n",
      "Loss on train= 0.011042444966733456\n",
      "Loss on test= 0.01687968522310257\n",
      "acc for Lsat= 0.0761869478556845 \n",
      "acc for Psat= 0.09664700908793343 \n",
      "acc for optim= 0.1399905948382285\n",
      "Epoch:496/1000\n",
      "Loss on train= 0.011370719410479069\n",
      "Loss on test= 0.01645604893565178\n",
      "acc for Lsat= 0.07745585921737883 \n",
      "acc for Psat= 0.117080005341106 \n",
      "acc for optim= 0.135778262010879\n",
      "Epoch:497/1000\n",
      "Loss on train= 0.011289285495877266\n",
      "Loss on test= 0.01661491021513939\n",
      "acc for Lsat= 0.07393902275297376 \n",
      "acc for Psat= 0.10731079942650262 \n",
      "acc for optim= 0.14006590313381617\n",
      "Epoch:498/1000\n",
      "Loss on train= 0.011110138148069382\n",
      "Loss on test= 0.01701502874493599\n",
      "acc for Lsat= 0.07309555659691493 \n",
      "acc for Psat= 0.11142286062240601 \n",
      "acc for optim= 0.13955291126751235\n",
      "Epoch:499/1000\n",
      "Loss on train= 0.01125406101346016\n",
      "Loss on test= 0.017415018752217293\n",
      "acc for Lsat= 0.07904548462894229 \n",
      "acc for Psat= 0.10507506529490153 \n",
      "acc for optim= 0.14113925426370566\n",
      "Epoch:500/1000\n",
      "Loss on train= 0.01121163833886385\n",
      "Loss on test= 0.016330618411302567\n",
      "acc for Lsat= 0.07283811900350783 \n",
      "acc for Psat= 0.09380467500951555 \n",
      "acc for optim= 0.13978241272270683\n",
      "Epoch:501/1000\n",
      "Loss on train= 0.011263538151979446\n",
      "Loss on test= 0.01721576415002346\n",
      "acc for Lsat= 0.07448583675755394 \n",
      "acc for Psat= 0.0996725598971049 \n",
      "acc for optim= 0.1391495848281516\n",
      "Epoch:502/1000\n",
      "Loss on train= 0.011267619207501411\n",
      "Loss on test= 0.0170767642557621\n",
      "acc for Lsat= 0.07972174601422416 \n",
      "acc for Psat= 0.10738626619180042 \n",
      "acc for optim= 0.14016963231066862\n",
      "Epoch:503/1000\n",
      "Loss on train= 0.011283650994300842\n",
      "Loss on test= 0.01688360795378685\n",
      "acc for Lsat= 0.07580869148174922 \n",
      "acc for Psat= 0.10278435250123343 \n",
      "acc for optim= 0.13962579307456813\n",
      "Epoch:504/1000\n",
      "Loss on train= 0.010862758383154869\n",
      "Loss on test= 0.016285382211208344\n",
      "acc for Lsat= 0.07602667775419024 \n",
      "acc for Psat= 0.10582708120346068 \n",
      "acc for optim= 0.1401990709412429\n",
      "Epoch:505/1000\n",
      "Loss on train= 0.011231893673539162\n",
      "Loss on test= 0.015497580170631409\n",
      "acc for Lsat= 0.0677777126431465 \n",
      "acc for Psat= 0.09743802100419999 \n",
      "acc for optim= 0.14085256809161767\n",
      "Epoch:506/1000\n",
      "Loss on train= 0.01123858243227005\n",
      "Loss on test= 0.016943471506237984\n",
      "acc for Lsat= 0.09022415296898949 \n",
      "acc for Psat= 0.09984740366538367 \n",
      "acc for optim= 0.14017774855924978\n",
      "Epoch:507/1000\n",
      "Loss on train= 0.011080064810812473\n",
      "Loss on test= 0.016596516594290733\n",
      "acc for Lsat= 0.07628703481621212 \n",
      "acc for Psat= 0.09677126970556048 \n",
      "acc for optim= 0.13845995341220665\n",
      "Epoch:508/1000\n",
      "Loss on train= 0.011312155053019524\n",
      "Loss on test= 0.016319995746016502\n",
      "acc for Lsat= 0.07553588963217205 \n",
      "acc for Psat= 0.09814584453900656 \n",
      "acc for optim= 0.14059601173632674\n",
      "Epoch:509/1000\n",
      "Loss on train= 0.011824307031929493\n",
      "Loss on test= 0.0168306864798069\n",
      "acc for Lsat= 0.07407700535323886 \n",
      "acc for Psat= 0.10707385192314783 \n",
      "acc for optim= 0.14083937588665218\n",
      "Epoch:510/1000\n",
      "Loss on train= 0.011476423591375351\n",
      "Loss on test= 0.015907522290945053\n",
      "acc for Lsat= 0.06688340273168353 \n",
      "acc for Psat= 0.10700302090909745 \n",
      "acc for optim= 0.13800726357019613\n",
      "Epoch:511/1000\n",
      "Loss on train= 0.011091425083577633\n",
      "Loss on test= 0.016615010797977448\n",
      "acc for Lsat= 0.07753597597281138 \n",
      "acc for Psat= 0.10556642942958408 \n",
      "acc for optim= 0.13909289017319681\n",
      "Epoch:512/1000\n",
      "Loss on train= 0.011622518301010132\n",
      "Loss on test= 0.015947941690683365\n",
      "acc for Lsat= 0.07466754217942556 \n",
      "acc for Psat= 0.09918523944086498 \n",
      "acc for optim= 0.1418752209179931\n",
      "Epoch:513/1000\n",
      "Loss on train= 0.01116300467401743\n",
      "Loss on test= 0.016373053193092346\n",
      "acc for Lsat= 0.0700447345773379 \n",
      "acc for Psat= 0.10226374997033014 \n",
      "acc for optim= 0.14034536555409433\n",
      "Epoch:514/1000\n",
      "Loss on train= 0.011016312055289745\n",
      "Loss on test= 0.015713786706328392\n",
      "acc for Lsat= 0.07129078325298097 \n",
      "acc for Psat= 0.09857232438193426 \n",
      "acc for optim= 0.13896620698894063\n",
      "Epoch:515/1000\n",
      "Loss on train= 0.011656575836241245\n",
      "Loss on test= 0.016387127339839935\n",
      "acc for Lsat= 0.07294713225629595 \n",
      "acc for Psat= 0.10305302408006456 \n",
      "acc for optim= 0.138336416044169\n",
      "Epoch:516/1000\n",
      "Loss on train= 0.011014814488589764\n",
      "Loss on test= 0.016981299966573715\n",
      "acc for Lsat= 0.07045197669002744 \n",
      "acc for Psat= 0.1168893492884106 \n",
      "acc for optim= 0.1381635345518589\n",
      "Epoch:517/1000\n",
      "Loss on train= 0.010999586433172226\n",
      "Loss on test= 0.016394197940826416\n",
      "acc for Lsat= 0.06830993410613802 \n",
      "acc for Psat= 0.09400074796544179 \n",
      "acc for optim= 0.14205938114060299\n",
      "Epoch:518/1000\n",
      "Loss on train= 0.011291549541056156\n",
      "Loss on test= 0.016401953995227814\n",
      "acc for Lsat= 0.07225847707854377 \n",
      "acc for Psat= 0.09570071200529734 \n",
      "acc for optim= 0.1415192346399029\n",
      "Epoch:519/1000\n",
      "Loss on train= 0.011085473001003265\n",
      "Loss on test= 0.01572825014591217\n",
      "acc for Lsat= 0.06923395130369397 \n",
      "acc for Psat= 0.1034210208389494 \n",
      "acc for optim= 0.13754097291578848\n",
      "Epoch:520/1000\n",
      "Loss on train= 0.01122517604380846\n",
      "Loss on test= 0.016163107007741928\n",
      "acc for Lsat= 0.06834378772311739 \n",
      "acc for Psat= 0.10614710019694433 \n",
      "acc for optim= 0.1386559393360383\n",
      "Epoch:521/1000\n",
      "Loss on train= 0.010828397236764431\n",
      "Loss on test= 0.016308315098285675\n",
      "acc for Lsat= 0.07321201248301401 \n",
      "acc for Psat= 0.09641635119915008 \n",
      "acc for optim= 0.13736779898818996\n",
      "Epoch:522/1000\n",
      "Loss on train= 0.01081998459994793\n",
      "Loss on test= 0.017769889906048775\n",
      "acc for Lsat= 0.08681678606404199 \n",
      "acc for Psat= 0.14338703254858653 \n",
      "acc for optim= 0.14115474739422398\n",
      "Epoch:523/1000\n",
      "Loss on train= 0.011427774094045162\n",
      "Loss on test= 0.015198826789855957\n",
      "acc for Lsat= 0.07952327364020878 \n",
      "acc for Psat= 0.10178828338781994 \n",
      "acc for optim= 0.13692601000269253\n",
      "Epoch:524/1000\n",
      "Loss on train= 0.011394690722227097\n",
      "Loss on test= 0.01636367477476597\n",
      "acc for Lsat= 0.0714365060130755 \n",
      "acc for Psat= 0.09957136081324683 \n",
      "acc for optim= 0.13946462517811195\n",
      "Epoch:525/1000\n",
      "Loss on train= 0.01100115105509758\n",
      "Loss on test= 0.01703798584640026\n",
      "acc for Lsat= 0.07938751396205689 \n",
      "acc for Psat= 0.09634936634037229 \n",
      "acc for optim= 0.13933679742945565\n",
      "Epoch:526/1000\n",
      "Loss on train= 0.011404351331293583\n",
      "Loss on test= 0.015709970146417618\n",
      "acc for Lsat= 0.07557237231069142 \n",
      "acc for Psat= 0.09483969542715284 \n",
      "acc for optim= 0.13778454240333907\n",
      "Epoch:527/1000\n",
      "Loss on train= 0.011030342429876328\n",
      "Loss on test= 0.016344616189599037\n",
      "acc for Lsat= 0.07312601473596361 \n",
      "acc for Psat= 0.09562060799863605 \n",
      "acc for optim= 0.1383363757696417\n",
      "Epoch:528/1000\n",
      "Loss on train= 0.010850069113075733\n",
      "Loss on test= 0.01692502386868\n",
      "acc for Lsat= 0.0741910566886266 \n",
      "acc for Psat= 0.10069846080409156 \n",
      "acc for optim= 0.13644069979588191\n",
      "Epoch:529/1000\n",
      "Loss on train= 0.011344551108777523\n",
      "Loss on test= 0.01617560163140297\n",
      "acc for Lsat= 0.07226098163260353 \n",
      "acc for Psat= 0.10532304313447742 \n",
      "acc for optim= 0.13805863651860917\n",
      "Epoch:530/1000\n",
      "Loss on train= 0.010921663604676723\n",
      "Loss on test= 0.01663491129875183\n",
      "acc for Lsat= 0.08260291963815689 \n",
      "acc for Psat= 0.11626247697406347 \n",
      "acc for optim= 0.13945651336560128\n",
      "Epoch:531/1000\n",
      "Loss on train= 0.01131027564406395\n",
      "Loss on test= 0.01614007167518139\n",
      "acc for Lsat= 0.07243356357018153 \n",
      "acc for Psat= 0.0997550290491846 \n",
      "acc for optim= 0.13744932984312375\n",
      "Epoch:532/1000\n",
      "Loss on train= 0.011284449137747288\n",
      "Loss on test= 0.016007505357265472\n",
      "acc for Lsat= 0.07264544616142908 \n",
      "acc for Psat= 0.10291237798002033 \n",
      "acc for optim= 0.13930489357767833\n",
      "Epoch:533/1000\n",
      "Loss on train= 0.01092031691223383\n",
      "Loss on test= 0.0174935981631279\n",
      "acc for Lsat= 0.09227861331568823 \n",
      "acc for Psat= 0.12833070423867968 \n",
      "acc for optim= 0.1438374753213591\n",
      "Epoch:534/1000\n",
      "Loss on train= 0.01107965037226677\n",
      "Loss on test= 0.016565930098295212\n",
      "acc for Lsat= 0.07807876798841688 \n",
      "acc for Psat= 0.11240443487962087 \n",
      "acc for optim= 0.1391225622759925\n",
      "Epoch:535/1000\n",
      "Loss on train= 0.011051268316805363\n",
      "Loss on test= 0.01621387153863907\n",
      "acc for Lsat= 0.07251136915551291 \n",
      "acc for Psat= 0.0996967381901211 \n",
      "acc for optim= 0.13897220459249285\n",
      "Epoch:536/1000\n",
      "Loss on train= 0.01082408707588911\n",
      "Loss on test= 0.015670981258153915\n",
      "acc for Lsat= 0.06849379125568601 \n",
      "acc for Psat= 0.10616605745421516 \n",
      "acc for optim= 0.1372613157145679\n",
      "Epoch:537/1000\n",
      "Loss on train= 0.010547361336648464\n",
      "Loss on test= 0.015419387258589268\n",
      "acc for Lsat= 0.07613956713014179 \n",
      "acc for Psat= 0.09490359011623596 \n",
      "acc for optim= 0.13926501722178522\n",
      "Epoch:538/1000\n",
      "Loss on train= 0.011028566397726536\n",
      "Loss on test= 0.01617119461297989\n",
      "acc for Lsat= 0.07990195105473201 \n",
      "acc for Psat= 0.10411613848474292 \n",
      "acc for optim= 0.13889857122881544\n",
      "Epoch:539/1000\n",
      "Loss on train= 0.010962107218801975\n",
      "Loss on test= 0.016653997823596\n",
      "acc for Lsat= 0.0738591859738032 \n",
      "acc for Psat= 0.1054881407154931 \n",
      "acc for optim= 0.1390889174688103\n",
      "Epoch:540/1000\n",
      "Loss on train= 0.01056716963648796\n",
      "Loss on test= 0.01601458340883255\n",
      "acc for Lsat= 0.08492476542790732 \n",
      "acc for Psat= 0.10959593686792585 \n",
      "acc for optim= 0.14091020507944957\n",
      "Epoch:541/1000\n",
      "Loss on train= 0.010702668689191341\n",
      "Loss on test= 0.01681315340101719\n",
      "acc for Lsat= 0.06896280662881003 \n",
      "acc for Psat= 0.10387508637375302 \n",
      "acc for optim= 0.14097073239584765\n",
      "Epoch:542/1000\n",
      "Loss on train= 0.011014438234269619\n",
      "Loss on test= 0.015320520848035812\n",
      "acc for Lsat= 0.0731149767835935 \n",
      "acc for Psat= 0.11390308803982209 \n",
      "acc for optim= 0.137292279344466\n",
      "Epoch:543/1000\n",
      "Loss on train= 0.010668814182281494\n",
      "Loss on test= 0.01607530564069748\n",
      "acc for Lsat= 0.07679477797614204 \n",
      "acc for Psat= 0.09881695508956907 \n",
      "acc for optim= 0.13725953385647802\n",
      "Epoch:544/1000\n",
      "Loss on train= 0.010772442445158958\n",
      "Loss on test= 0.016901377588510513\n",
      "acc for Lsat= 0.0789370275206036 \n",
      "acc for Psat= 0.11687762207455105 \n",
      "acc for optim= 0.13898598731805883\n",
      "Epoch:545/1000\n",
      "Loss on train= 0.010875512845814228\n",
      "Loss on test= 0.01572272926568985\n",
      "acc for Lsat= 0.07672055231200324 \n",
      "acc for Psat= 0.11236087779204053 \n",
      "acc for optim= 0.1376846994739026\n",
      "Epoch:546/1000\n",
      "Loss on train= 0.011471523903310299\n",
      "Loss on test= 0.016272345557808876\n",
      "acc for Lsat= 0.08362251884407465 \n",
      "acc for Psat= 0.11084328326914045 \n",
      "acc for optim= 0.1413919549021456\n",
      "Epoch:547/1000\n",
      "Loss on train= 0.010920924134552479\n",
      "Loss on test= 0.015833407640457153\n",
      "acc for Lsat= 0.07291339619292153 \n",
      "acc for Psat= 0.09567906128035651 \n",
      "acc for optim= 0.138459495951732\n",
      "Epoch:548/1000\n",
      "Loss on train= 0.010589404962956905\n",
      "Loss on test= 0.01694660261273384\n",
      "acc for Lsat= 0.07702625542879105 \n",
      "acc for Psat= 0.10938996142811246 \n",
      "acc for optim= 0.13942777437882292\n",
      "Epoch:549/1000\n",
      "Loss on train= 0.01105100754648447\n",
      "Loss on test= 0.016054240986704826\n",
      "acc for Lsat= 0.07671049800184038 \n",
      "acc for Psat= 0.09607121431165272 \n",
      "acc for optim= 0.13719677946323322\n",
      "Epoch:550/1000\n",
      "Loss on train= 0.010766313411295414\n",
      "Loss on test= 0.016284305602312088\n",
      "acc for Lsat= 0.06878170784976748 \n",
      "acc for Psat= 0.09481606731812159 \n",
      "acc for optim= 0.13843607370896885\n",
      "Epoch:551/1000\n",
      "Loss on train= 0.010528584010899067\n",
      "Loss on test= 0.016131766140460968\n",
      "acc for Lsat= 0.0708236899640825 \n",
      "acc for Psat= 0.09924482206503553 \n",
      "acc for optim= 0.1391111133620143\n",
      "Epoch:552/1000\n",
      "Loss on train= 0.010907371528446674\n",
      "Loss on test= 0.016876257956027985\n",
      "acc for Lsat= 0.07800748960839378 \n",
      "acc for Psat= 0.1000499129295349 \n",
      "acc for optim= 0.1364406510566672\n",
      "Epoch:553/1000\n",
      "Loss on train= 0.01062555331736803\n",
      "Loss on test= 0.01654961332678795\n",
      "acc for Lsat= 0.07187672853469848 \n",
      "acc for Psat= 0.09972527705960803 \n",
      "acc for optim= 0.13985896111569474\n",
      "Epoch:554/1000\n",
      "Loss on train= 0.010588728822767735\n",
      "Loss on test= 0.016150260344147682\n",
      "acc for Lsat= 0.07044986668560241 \n",
      "acc for Psat= 0.09805840584966868 \n",
      "acc for optim= 0.1361953297862783\n",
      "Epoch:555/1000\n",
      "Loss on train= 0.010686640627682209\n",
      "Loss on test= 0.015775814652442932\n",
      "acc for Lsat= 0.07158865067693922 \n",
      "acc for Psat= 0.10321287910143533 \n",
      "acc for optim= 0.1382571784572469\n",
      "Epoch:556/1000\n",
      "Loss on train= 0.010540262795984745\n",
      "Loss on test= 0.016211984679102898\n",
      "acc for Lsat= 0.06834586295816633 \n",
      "acc for Psat= 0.1035584976275762 \n",
      "acc for optim= 0.1391422182528509\n",
      "Epoch:557/1000\n",
      "Loss on train= 0.010126001201570034\n",
      "Loss on test= 0.017153821885585785\n",
      "acc for Lsat= 0.0806711670425203 \n",
      "acc for Psat= 0.1058496399058236 \n",
      "acc for optim= 0.1386400390105943\n",
      "Epoch:558/1000\n",
      "Loss on train= 0.010882075875997543\n",
      "Loss on test= 0.016073688864707947\n",
      "acc for Lsat= 0.08378914147615432 \n",
      "acc for Psat= 0.12473066449165346 \n",
      "acc for optim= 0.13778555390631986\n",
      "Epoch:559/1000\n",
      "Loss on train= 0.010845513083040714\n",
      "Loss on test= 0.016109947115182877\n",
      "acc for Lsat= 0.09281919780704709 \n",
      "acc for Psat= 0.10921236409081353 \n",
      "acc for optim= 0.13729358383247423\n",
      "Epoch:560/1000\n",
      "Loss on train= 0.011200761422514915\n",
      "Loss on test= 0.015855751931667328\n",
      "acc for Lsat= 0.06951350553168191 \n",
      "acc for Psat= 0.10880485971768698 \n",
      "acc for optim= 0.13625918135771317\n",
      "Epoch:561/1000\n",
      "Loss on train= 0.011038878932595253\n",
      "Loss on test= 0.01546338852494955\n",
      "acc for Lsat= 0.07017106264829637 \n",
      "acc for Psat= 0.10560483634471894 \n",
      "acc for optim= 0.1380661585264736\n",
      "Epoch:562/1000\n",
      "Loss on train= 0.010579758323729038\n",
      "Loss on test= 0.015719203278422356\n",
      "acc for Lsat= 0.07753648724820879 \n",
      "acc for Psat= 0.10100340048472084 \n",
      "acc for optim= 0.1373647882292668\n",
      "Epoch:563/1000\n",
      "Loss on train= 0.010214448906481266\n",
      "Loss on test= 0.015736496075987816\n",
      "acc for Lsat= 0.07379519508944615 \n",
      "acc for Psat= 0.10343450638982984 \n",
      "acc for optim= 0.1391376350592408\n",
      "Epoch:564/1000\n",
      "Loss on train= 0.010552944615483284\n",
      "Loss on test= 0.015592389740049839\n",
      "acc for Lsat= 0.06966712888744142 \n",
      "acc for Psat= 0.0947527034415139 \n",
      "acc for optim= 0.13702455384708528\n",
      "Epoch:565/1000\n",
      "Loss on train= 0.010903998278081417\n",
      "Loss on test= 0.016693852841854095\n",
      "acc for Lsat= 0.07444614089197583 \n",
      "acc for Psat= 0.10314840541945564 \n",
      "acc for optim= 0.13849354585011805\n",
      "Epoch:566/1000\n",
      "Loss on train= 0.010219651274383068\n",
      "Loss on test= 0.016509106382727623\n",
      "acc for Lsat= 0.0730337642961078 \n",
      "acc for Psat= 0.11214263869656459 \n",
      "acc for optim= 0.13765660166067797\n",
      "Epoch:567/1000\n",
      "Loss on train= 0.010710585862398148\n",
      "Loss on test= 0.015637675300240517\n",
      "acc for Lsat= 0.07523635360929701 \n",
      "acc for Psat= 0.09940507643752626 \n",
      "acc for optim= 0.14025203093058536\n",
      "Epoch:568/1000\n",
      "Loss on train= 0.011118477210402489\n",
      "Loss on test= 0.016941238194704056\n",
      "acc for Lsat= 0.06825211693843206 \n",
      "acc for Psat= 0.10087714758184224 \n",
      "acc for optim= 0.13853029863288005\n",
      "Epoch:569/1000\n",
      "Loss on train= 0.010824432596564293\n",
      "Loss on test= 0.015244249254465103\n",
      "acc for Lsat= 0.07149982319937813 \n",
      "acc for Psat= 0.09953922728697458 \n",
      "acc for optim= 0.1397832218143675\n",
      "Epoch:570/1000\n",
      "Loss on train= 0.010733219794929028\n",
      "Loss on test= 0.017242485657334328\n",
      "acc for Lsat= 0.07373548067278332 \n",
      "acc for Psat= 0.10047772659195792 \n",
      "acc for optim= 0.14055366284317442\n",
      "Epoch:571/1000\n",
      "Loss on train= 0.010867655277252197\n",
      "Loss on test= 0.015711944550275803\n",
      "acc for Lsat= 0.06918130864699683 \n",
      "acc for Psat= 0.1025816026661131 \n",
      "acc for optim= 0.13763118419382306\n",
      "Epoch:572/1000\n",
      "Loss on train= 0.010340506210923195\n",
      "Loss on test= 0.016003282740712166\n",
      "acc for Lsat= 0.07676489965783224 \n",
      "acc for Psat= 0.10620665550231935 \n",
      "acc for optim= 0.13876666310760707\n",
      "Epoch:573/1000\n",
      "Loss on train= 0.011093148030340672\n",
      "Loss on test= 0.01699342578649521\n",
      "acc for Lsat= 0.07655363198783664 \n",
      "acc for Psat= 0.11149117946624755 \n",
      "acc for optim= 0.14065020870443223\n",
      "Epoch:574/1000\n",
      "Loss on train= 0.01091695949435234\n",
      "Loss on test= 0.01684325560927391\n",
      "acc for Lsat= 0.07738487405909432 \n",
      "acc for Psat= 0.10882154835595026 \n",
      "acc for optim= 0.1363823891720838\n",
      "Epoch:575/1000\n",
      "Loss on train= 0.010347702540457249\n",
      "Loss on test= 0.01776188239455223\n",
      "acc for Lsat= 0.07274692323472765 \n",
      "acc for Psat= 0.10517597463395861 \n",
      "acc for optim= 0.14016652759164575\n",
      "Epoch:576/1000\n",
      "Loss on train= 0.010473825968801975\n",
      "Loss on test= 0.016274942085146904\n",
      "acc for Lsat= 0.07384301142560111 \n",
      "acc for Psat= 0.10243026581075455 \n",
      "acc for optim= 0.1411846681394511\n",
      "Epoch:577/1000\n",
      "Loss on train= 0.010670676827430725\n",
      "Loss on test= 0.017101846635341644\n",
      "acc for Lsat= 0.07351654867331187 \n",
      "acc for Psat= 0.0998129344648785 \n",
      "acc for optim= 0.14000375204616122\n",
      "Epoch:578/1000\n",
      "Loss on train= 0.010540212504565716\n",
      "Loss on test= 0.015585299581289291\n",
      "acc for Lsat= 0.08052642444769541 \n",
      "acc for Psat= 0.0972342633538776 \n",
      "acc for optim= 0.13779466079754965\n",
      "Epoch:579/1000\n",
      "Loss on train= 0.010656546801328659\n",
      "Loss on test= 0.016567081212997437\n",
      "acc for Lsat= 0.07638815989096959 \n",
      "acc for Psat= 0.10049587686856588 \n",
      "acc for optim= 0.1381356116022087\n",
      "Epoch:580/1000\n",
      "Loss on train= 0.010529189370572567\n",
      "Loss on test= 0.017186271026730537\n",
      "acc for Lsat= 0.07701714949475395 \n",
      "acc for Psat= 0.1039724048640993 \n",
      "acc for optim= 0.13921470998062027\n",
      "Epoch:581/1000\n",
      "Loss on train= 0.010314512997865677\n",
      "Loss on test= 0.01558352168649435\n",
      "acc for Lsat= 0.07108153336577945 \n",
      "acc for Psat= 0.10465377370516461 \n",
      "acc for optim= 0.13858177214860917\n",
      "Epoch:582/1000\n",
      "Loss on train= 0.010814143344759941\n",
      "Loss on test= 0.016180511564016342\n",
      "acc for Lsat= 0.07258494148651758 \n",
      "acc for Psat= 0.09848725994427998 \n",
      "acc for optim= 0.13728767880497295\n",
      "Epoch:583/1000\n",
      "Loss on train= 0.010739505290985107\n",
      "Loss on test= 0.01629997417330742\n",
      "acc for Lsat= 0.07458337661292817 \n",
      "acc for Psat= 0.1109798255893919 \n",
      "acc for optim= 0.13899304096897444\n",
      "Epoch:584/1000\n",
      "Loss on train= 0.01072205975651741\n",
      "Loss on test= 0.017162689939141273\n",
      "acc for Lsat= 0.07669346332550048 \n",
      "acc for Psat= 0.09729628347688252 \n",
      "acc for optim= 0.14015724990102976\n",
      "Epoch:585/1000\n",
      "Loss on train= 0.010669458657503128\n",
      "Loss on test= 0.01677003689110279\n",
      "acc for Lsat= 0.07274526241752836 \n",
      "acc for Psat= 0.10112868911690182 \n",
      "acc for optim= 0.13979413592153125\n",
      "Epoch:586/1000\n",
      "Loss on train= 0.010477214120328426\n",
      "Loss on test= 0.016655990853905678\n",
      "acc for Lsat= 0.0732581893603007 \n",
      "acc for Psat= 0.10747359295686085 \n",
      "acc for optim= 0.13898075347145397\n",
      "Epoch:587/1000\n",
      "Loss on train= 0.010570567101240158\n",
      "Loss on test= 0.016166875138878822\n",
      "acc for Lsat= 0.07670876863929961 \n",
      "acc for Psat= 0.09482575373517141 \n",
      "acc for optim= 0.1367983747066723\n",
      "Epoch:588/1000\n",
      "Loss on train= 0.010869530960917473\n",
      "Loss on test= 0.015744760632514954\n",
      "acc for Lsat= 0.06913203795750936 \n",
      "acc for Psat= 0.09766426583131155 \n",
      "acc for optim= 0.13983942848733727\n",
      "Epoch:589/1000\n",
      "Loss on train= 0.010495669208467007\n",
      "Loss on test= 0.015670038759708405\n",
      "acc for Lsat= 0.07267538176642524 \n",
      "acc for Psat= 0.1018021865023507 \n",
      "acc for optim= 0.13757643011502094\n",
      "Epoch:590/1000\n",
      "Loss on train= 0.010858094319701195\n",
      "Loss on test= 0.0166091937571764\n",
      "acc for Lsat= 0.06787481655677159 \n",
      "acc for Psat= 0.0980327621102333 \n",
      "acc for optim= 0.1368516284868949\n",
      "Epoch:591/1000\n",
      "Loss on train= 0.010490741580724716\n",
      "Loss on test= 0.015511184930801392\n",
      "acc for Lsat= 0.06919062899218666 \n",
      "acc for Psat= 0.1034252345561981 \n",
      "acc for optim= 0.14025249974155382\n",
      "Epoch:592/1000\n",
      "Loss on train= 0.010332610458135605\n",
      "Loss on test= 0.01652120240032673\n",
      "acc for Lsat= 0.07016403526067733 \n",
      "acc for Psat= 0.09689707424905566 \n",
      "acc for optim= 0.14028044261245262\n",
      "Epoch:593/1000\n",
      "Loss on train= 0.010470882058143616\n",
      "Loss on test= 0.016393465921282768\n",
      "acc for Lsat= 0.07620392309294806 \n",
      "acc for Psat= 0.09602297825945748 \n",
      "acc for optim= 0.13724355928392876\n",
      "Epoch:594/1000\n",
      "Loss on train= 0.010294281877577305\n",
      "Loss on test= 0.01545241940766573\n",
      "acc for Lsat= 0.07207856045828925 \n",
      "acc for Psat= 0.108155267768436 \n",
      "acc for optim= 0.13877773113361197\n",
      "Epoch:595/1000\n",
      "Loss on train= 0.010508379898965359\n",
      "Loss on test= 0.016791988164186478\n",
      "acc for Lsat= 0.07567295796341365 \n",
      "acc for Psat= 0.11240697933567893 \n",
      "acc for optim= 0.13660872295602325\n",
      "Epoch:596/1000\n",
      "Loss on train= 0.010010940954089165\n",
      "Loss on test= 0.016616398468613625\n",
      "acc for Lsat= 0.07077301690975825 \n",
      "acc for Psat= 0.10667480627695719 \n",
      "acc for optim= 0.138860843061573\n",
      "Epoch:597/1000\n",
      "Loss on train= 0.010283819399774075\n",
      "Loss on test= 0.016219722107052803\n",
      "acc for Lsat= 0.0748370326227612 \n",
      "acc for Psat= 0.12523683011531828 \n",
      "acc for optim= 0.1369482771312404\n",
      "Epoch:598/1000\n",
      "Loss on train= 0.010633794590830803\n",
      "Loss on test= 0.015474559739232063\n",
      "acc for Lsat= 0.07643755293554728 \n",
      "acc for Psat= 0.10791965888606177 \n",
      "acc for optim= 0.1388307810657554\n",
      "Epoch:599/1000\n",
      "Loss on train= 0.010839984752237797\n",
      "Loss on test= 0.016656838357448578\n",
      "acc for Lsat= 0.07282793273528416 \n",
      "acc for Psat= 0.0980024594399664 \n",
      "acc for optim= 0.13861609937416183\n",
      "Epoch:600/1000\n",
      "Loss on train= 0.010743963532149792\n",
      "Loss on test= 0.015286445617675781\n",
      "acc for Lsat= 0.0758573356601927 \n",
      "acc for Psat= 0.10215668380260468 \n",
      "acc for optim= 0.1375900740321312\n",
      "Epoch:601/1000\n",
      "Loss on train= 0.010489805601537228\n",
      "Loss on test= 0.015790874138474464\n",
      "acc for Lsat= 0.07460336950090195 \n",
      "acc for Psat= 0.097568109134833 \n",
      "acc for optim= 0.13885350185963846\n",
      "Epoch:602/1000\n",
      "Loss on train= 0.010694723576307297\n",
      "Loss on test= 0.01787080615758896\n",
      "acc for Lsat= 0.07402225004302132 \n",
      "acc for Psat= 0.11286780867311691 \n",
      "acc for optim= 0.13582406450425166\n",
      "Epoch:603/1000\n",
      "Loss on train= 0.010415137745440006\n",
      "Loss on test= 0.016015199944376945\n",
      "acc for Lsat= 0.07635420478052563 \n",
      "acc for Psat= 0.09960210687584348 \n",
      "acc for optim= 0.13773837313055992\n",
      "Epoch:604/1000\n",
      "Loss on train= 0.010366743430495262\n",
      "Loss on test= 0.016134055331349373\n",
      "acc for Lsat= 0.07661262750625611 \n",
      "acc for Psat= 0.10861644777986738 \n",
      "acc for optim= 0.13731528234978516\n",
      "Epoch:605/1000\n",
      "Loss on train= 0.0104287751019001\n",
      "Loss on test= 0.01568242721259594\n",
      "acc for Lsat= 0.07265293863084582 \n",
      "acc for Psat= 0.09714923004309337 \n",
      "acc for optim= 0.13945291503849958\n",
      "Epoch:606/1000\n",
      "Loss on train= 0.010188435204327106\n",
      "Loss on test= 0.018062535673379898\n",
      "acc for Lsat= 0.07512325677606796 \n",
      "acc for Psat= 0.10325039459599389 \n",
      "acc for optim= 0.13877078677631088\n",
      "Epoch:607/1000\n",
      "Loss on train= 0.010230607353150845\n",
      "Loss on test= 0.016612233594059944\n",
      "acc for Lsat= 0.07121461994118161 \n",
      "acc for Psat= 0.1077886276774936 \n",
      "acc for optim= 0.1389280447943343\n",
      "Epoch:608/1000\n",
      "Loss on train= 0.010118058882653713\n",
      "Loss on test= 0.01706373505294323\n",
      "acc for Lsat= 0.07187070316738552 \n",
      "acc for Psat= 0.11054696076446109 \n",
      "acc for optim= 0.14153200284474424\n",
      "Epoch:609/1000\n",
      "Loss on train= 0.010520017705857754\n",
      "Loss on test= 0.015693865716457367\n",
      "acc for Lsat= 0.07242775840891733 \n",
      "acc for Psat= 0.11127267281214397 \n",
      "acc for optim= 0.13989393814570378\n",
      "Epoch:610/1000\n",
      "Loss on train= 0.010662500746548176\n",
      "Loss on test= 0.017142049968242645\n",
      "acc for Lsat= 0.06840263224310345 \n",
      "acc for Psat= 0.0962610647082329 \n",
      "acc for optim= 0.13907482135627006\n",
      "Epoch:611/1000\n",
      "Loss on train= 0.010208412073552608\n",
      "Loss on test= 0.01733560673892498\n",
      "acc for Lsat= 0.0762931474381023 \n",
      "acc for Psat= 0.09934207730823093 \n",
      "acc for optim= 0.13966016167153908\n",
      "Epoch:612/1000\n",
      "Loss on train= 0.010318731889128685\n",
      "Loss on test= 0.01668696478009224\n",
      "acc for Lsat= 0.0672486616505517 \n",
      "acc for Psat= 0.1008545955022176 \n",
      "acc for optim= 0.13802299840996662\n",
      "Epoch:613/1000\n",
      "Loss on train= 0.01017435546964407\n",
      "Loss on test= 0.01577686332166195\n",
      "acc for Lsat= 0.07700581103563309 \n",
      "acc for Psat= 0.10613085329532625 \n",
      "acc for optim= 0.13995640912196705\n",
      "Epoch:614/1000\n",
      "Loss on train= 0.010507700964808464\n",
      "Loss on test= 0.016787301748991013\n",
      "acc for Lsat= 0.07518386724922393 \n",
      "acc for Psat= 0.1042721066210005 \n",
      "acc for optim= 0.13917169978117777\n",
      "Epoch:615/1000\n",
      "Loss on train= 0.010242193005979061\n",
      "Loss on test= 0.017127834260463715\n",
      "acc for Lsat= 0.07383440583944322 \n",
      "acc for Psat= 0.11047154565652212 \n",
      "acc for optim= 0.14016618000136483\n",
      "Epoch:616/1000\n",
      "Loss on train= 0.01061727199703455\n",
      "Loss on test= 0.016482004895806313\n",
      "acc for Lsat= 0.07747502277294795 \n",
      "acc for Psat= 0.09876770046022203 \n",
      "acc for optim= 0.13959406419243248\n",
      "Epoch:617/1000\n",
      "Loss on train= 0.010344400070607662\n",
      "Loss on test= 0.016914626583456993\n",
      "acc for Lsat= 0.07190238303608365 \n",
      "acc for Psat= 0.11294562319914497 \n",
      "acc for optim= 0.14420141830212543\n",
      "Epoch:618/1000\n",
      "Loss on train= 0.01056734286248684\n",
      "Loss on test= 0.017496490851044655\n",
      "acc for Lsat= 0.0797338757250044 \n",
      "acc for Psat= 0.1174493822786543 \n",
      "acc for optim= 0.13874086679683786\n",
      "Epoch:619/1000\n",
      "Loss on train= 0.01024172455072403\n",
      "Loss on test= 0.01522289589047432\n",
      "acc for Lsat= 0.07277346336179309 \n",
      "acc for Psat= 0.0963171876139111 \n",
      "acc for optim= 0.13924194615748192\n",
      "Epoch:620/1000\n",
      "Loss on train= 0.010514932684600353\n",
      "Loss on test= 0.016337888315320015\n",
      "acc for Lsat= 0.0728273426493009 \n",
      "acc for Psat= 0.10133590400218963 \n",
      "acc for optim= 0.1378132290724251\n",
      "Epoch:621/1000\n",
      "Loss on train= 0.01007104106247425\n",
      "Loss on test= 0.016625642776489258\n",
      "acc for Lsat= 0.07664387789037493 \n",
      "acc for Psat= 0.11116169806983736 \n",
      "acc for optim= 0.138266198316382\n",
      "Epoch:622/1000\n",
      "Loss on train= 0.01034538820385933\n",
      "Loss on test= 0.017351269721984863\n",
      "acc for Lsat= 0.0803560647699568 \n",
      "acc for Psat= 0.1208190573586358 \n",
      "acc for optim= 0.14104039097825688\n",
      "Epoch:623/1000\n",
      "Loss on train= 0.009800211526453495\n",
      "Loss on test= 0.015814661979675293\n",
      "acc for Lsat= 0.07232835011349784 \n",
      "acc for Psat= 0.09364475293291939 \n",
      "acc for optim= 0.1385281921364367\n",
      "Epoch:624/1000\n",
      "Loss on train= 0.010483363643288612\n",
      "Loss on test= 0.016511226072907448\n",
      "acc for Lsat= 0.07059952368338902 \n",
      "acc for Psat= 0.10174118628104528 \n",
      "acc for optim= 0.13842492209643956\n",
      "Epoch:625/1000\n",
      "Loss on train= 0.010144706815481186\n",
      "Loss on test= 0.016242926940321922\n",
      "acc for Lsat= 0.07221057497792774 \n",
      "acc for Psat= 0.10595964458253648 \n",
      "acc for optim= 0.1387127170028786\n",
      "Epoch:626/1000\n",
      "Loss on train= 0.010164236649870872\n",
      "Loss on test= 0.01619417779147625\n",
      "acc for Lsat= 0.07780564328034718 \n",
      "acc for Psat= 0.09443312717808619 \n",
      "acc for optim= 0.1369159566031562\n",
      "Epoch:627/1000\n",
      "Loss on train= 0.0102316839620471\n",
      "Loss on test= 0.015206580050289631\n",
      "acc for Lsat= 0.07758292059103648 \n",
      "acc for Psat= 0.09321705583069059 \n",
      "acc for optim= 0.13688195245340468\n",
      "Epoch:628/1000\n",
      "Loss on train= 0.010219224728643894\n",
      "Loss on test= 0.016881443560123444\n",
      "acc for Lsat= 0.07692877319124009 \n",
      "acc for Psat= 0.09896515574720172 \n",
      "acc for optim= 0.138463106047776\n",
      "Epoch:629/1000\n",
      "Loss on train= 0.010141944512724876\n",
      "Loss on test= 0.014906429685652256\n",
      "acc for Lsat= 0.07354616224765778 \n",
      "acc for Psat= 0.09733886652522618 \n",
      "acc for optim= 0.14187774856885274\n",
      "Epoch:630/1000\n",
      "Loss on train= 0.009853930212557316\n",
      "Loss on test= 0.016728010028600693\n",
      "acc for Lsat= 0.0771206729941898 \n",
      "acc for Psat= 0.10144083665476905 \n",
      "acc for optim= 0.13869796780248483\n",
      "Epoch:631/1000\n",
      "Loss on train= 0.009916817769408226\n",
      "Loss on test= 0.0176576878875494\n",
      "acc for Lsat= 0.07663383831580481 \n",
      "acc for Psat= 0.11354715724786121 \n",
      "acc for optim= 0.13827346857223244\n",
      "Epoch:632/1000\n",
      "Loss on train= 0.010106219910085201\n",
      "Loss on test= 0.01593778096139431\n",
      "acc for Lsat= 0.06619106150335735 \n",
      "acc for Psat= 0.095439522796207 \n",
      "acc for optim= 0.13851104834013514\n",
      "Epoch:633/1000\n",
      "Loss on train= 0.010170632041990757\n",
      "Loss on test= 0.01685183309018612\n",
      "acc for Lsat= 0.07851942992872664 \n",
      "acc for Psat= 0.10081576473183101 \n",
      "acc for optim= 0.13897202303633097\n",
      "Epoch:634/1000\n",
      "Loss on train= 0.010044881142675877\n",
      "Loss on test= 0.01606602966785431\n",
      "acc for Lsat= 0.07095341665877236 \n",
      "acc for Psat= 0.09957646893130409 \n",
      "acc for optim= 0.13921185969892477\n",
      "Epoch:635/1000\n",
      "Loss on train= 0.010076496750116348\n",
      "Loss on test= 0.01655065268278122\n",
      "acc for Lsat= 0.07126053058438832 \n",
      "acc for Psat= 0.10357348654005262 \n",
      "acc for optim= 0.13576125279068943\n",
      "Epoch:636/1000\n",
      "Loss on train= 0.01041578222066164\n",
      "Loss on test= 0.01656443253159523\n",
      "acc for Lsat= 0.07272606276803546 \n",
      "acc for Psat= 0.10598175095187294 \n",
      "acc for optim= 0.13694627303630114\n",
      "Epoch:637/1000\n",
      "Loss on train= 0.010389933362603188\n",
      "Loss on test= 0.015580583363771439\n",
      "acc for Lsat= 0.07698119117154018 \n",
      "acc for Psat= 0.09461097783512538 \n",
      "acc for optim= 0.13685973207983704\n",
      "Epoch:638/1000\n",
      "Loss on train= 0.010036653839051723\n",
      "Loss on test= 0.015017394907772541\n",
      "acc for Lsat= 0.07289398809274038 \n",
      "acc for Psat= 0.09737350510226354 \n",
      "acc for optim= 0.14092010193400914\n",
      "Epoch:639/1000\n",
      "Loss on train= 0.010294093750417233\n",
      "Loss on test= 0.01573966257274151\n",
      "acc for Lsat= 0.06743166314231025 \n",
      "acc for Psat= 0.09476185027096007 \n",
      "acc for optim= 0.13884548660781648\n",
      "Epoch:640/1000\n",
      "Loss on train= 0.010134264826774597\n",
      "Loss on test= 0.016854921355843544\n",
      "acc for Lsat= 0.07218109601073794 \n",
      "acc for Psat= 0.10405663185649448 \n",
      "acc for optim= 0.1386645893773271\n",
      "Epoch:641/1000\n",
      "Loss on train= 0.009923514910042286\n",
      "Loss on test= 0.01624288782477379\n",
      "acc for Lsat= 0.0688495483663347 \n",
      "acc for Psat= 0.0941023470626937 \n",
      "acc for optim= 0.14073741185582345\n",
      "Epoch:642/1000\n",
      "Loss on train= 0.00989698525518179\n",
      "Loss on test= 0.016546502709388733\n",
      "acc for Lsat= 0.06950061122576395 \n",
      "acc for Psat= 0.09265154633257124 \n",
      "acc for optim= 0.13848978716673124\n",
      "Epoch:643/1000\n",
      "Loss on train= 0.010379076935350895\n",
      "Loss on test= 0.016271673142910004\n",
      "acc for Lsat= 0.07920735561185412 \n",
      "acc for Psat= 0.10260890026887258 \n",
      "acc for optim= 0.1407149065285921\n",
      "Epoch:644/1000\n",
      "Loss on train= 0.010968605987727642\n",
      "Loss on test= 0.01709285005927086\n",
      "acc for Lsat= 0.06867925110790465 \n",
      "acc for Psat= 0.1096635002228949 \n",
      "acc for optim= 0.14053585508631333\n",
      "Epoch:645/1000\n",
      "Loss on train= 0.010280502960085869\n",
      "Loss on test= 0.0165008082985878\n",
      "acc for Lsat= 0.06920232541031307 \n",
      "acc for Psat= 0.10732526299026277 \n",
      "acc for optim= 0.140628515308102\n",
      "Epoch:646/1000\n",
      "Loss on train= 0.009936386719346046\n",
      "Loss on test= 0.016510367393493652\n",
      "acc for Lsat= 0.07959302730030482 \n",
      "acc for Psat= 0.10928508440653482 \n",
      "acc for optim= 0.13913700344661872\n",
      "Epoch:647/1000\n",
      "Loss on train= 0.010555949993431568\n",
      "Loss on test= 0.016720790416002274\n",
      "acc for Lsat= 0.08463850286271837 \n",
      "acc for Psat= 0.11270514130592346 \n",
      "acc for optim= 0.14032504318488973\n",
      "Epoch:648/1000\n",
      "Loss on train= 0.009975734166800976\n",
      "Loss on test= 0.016006767749786377\n",
      "acc for Lsat= 0.07018458661105897 \n",
      "acc for Psat= 0.10595987190802891 \n",
      "acc for optim= 0.14251115971969233\n",
      "Epoch:649/1000\n",
      "Loss on train= 0.010115016251802444\n",
      "Loss on test= 0.016033262014389038\n",
      "acc for Lsat= 0.06430625683731502 \n",
      "acc for Psat= 0.09257554875479804 \n",
      "acc for optim= 0.13904762825825148\n",
      "Epoch:650/1000\n",
      "Loss on train= 0.009916257113218307\n",
      "Loss on test= 0.016385557129979134\n",
      "acc for Lsat= 0.06872756398386426 \n",
      "acc for Psat= 0.09447437193658617 \n",
      "acc for optim= 0.1378438772737152\n",
      "Epoch:651/1000\n",
      "Loss on train= 0.010190286673605442\n",
      "Loss on test= 0.01658696122467518\n",
      "acc for Lsat= 0.07740622378057904 \n",
      "acc for Psat= 0.11117114722728728 \n",
      "acc for optim= 0.14064421583380965\n",
      "Epoch:652/1000\n",
      "Loss on train= 0.009956239722669125\n",
      "Loss on test= 0.01650691032409668\n",
      "acc for Lsat= 0.07886341826783287 \n",
      "acc for Psat= 0.09700758871104982 \n",
      "acc for optim= 0.1391036977577541\n",
      "Epoch:653/1000\n",
      "Loss on train= 0.010056502185761929\n",
      "Loss on test= 0.015650488436222076\n",
      "acc for Lsat= 0.06827796598275503 \n",
      "acc for Psat= 0.09484937538703281 \n",
      "acc for optim= 0.14002826495303047\n",
      "Epoch:654/1000\n",
      "Loss on train= 0.009786535054445267\n",
      "Loss on test= 0.016157666221261024\n",
      "acc for Lsat= 0.06928731915023592 \n",
      "acc for Psat= 0.09496249589655134 \n",
      "acc for optim= 0.13945082620614105\n",
      "Epoch:655/1000\n",
      "Loss on train= 0.010491795837879181\n",
      "Loss on test= 0.01798926293849945\n",
      "acc for Lsat= 0.08256325672070183 \n",
      "acc for Psat= 0.11411034299267664 \n",
      "acc for optim= 0.13978271318806543\n",
      "Epoch:656/1000\n",
      "Loss on train= 0.010207666084170341\n",
      "Loss on test= 0.01753467507660389\n",
      "acc for Lsat= 0.07038531485531065 \n",
      "acc for Psat= 0.09758856942256292 \n",
      "acc for optim= 0.13973814787136185\n",
      "Epoch:657/1000\n",
      "Loss on train= 0.009679259732365608\n",
      "Loss on test= 0.0163494274020195\n",
      "acc for Lsat= 0.07485570907592774 \n",
      "acc for Psat= 0.11388841867446899 \n",
      "acc for optim= 0.13849889019297232\n",
      "Epoch:658/1000\n",
      "Loss on train= 0.009867429733276367\n",
      "Loss on test= 0.017268355935811996\n",
      "acc for Lsat= 0.06603180749548805 \n",
      "acc for Psat= 0.10163287288612789 \n",
      "acc for optim= 0.1380156152571241\n",
      "Epoch:659/1000\n",
      "Loss on train= 0.010026603005826473\n",
      "Loss on test= 0.014812626875936985\n",
      "acc for Lsat= 0.06835361272096635 \n",
      "acc for Psat= 0.097668637169732 \n",
      "acc for optim= 0.13906790738304456\n",
      "Epoch:660/1000\n",
      "Loss on train= 0.010072409175336361\n",
      "Loss on test= 0.016079716384410858\n",
      "acc for Lsat= 0.07971714834372205 \n",
      "acc for Psat= 0.10750958091682856 \n",
      "acc for optim= 0.13802057483957875\n",
      "Epoch:661/1000\n",
      "Loss on train= 0.010220970027148724\n",
      "Loss on test= 0.01513095386326313\n",
      "acc for Lsat= 0.08716420614057116 \n",
      "acc for Psat= 0.10242406957679323 \n",
      "acc for optim= 0.1383451848394341\n",
      "Epoch:662/1000\n",
      "Loss on train= 0.010000990703701973\n",
      "Loss on test= 0.016269734129309654\n",
      "acc for Lsat= 0.07273956504133013 \n",
      "acc for Psat= 0.09382345858547421 \n",
      "acc for optim= 0.13782012766848006\n",
      "Epoch:663/1000\n",
      "Loss on train= 0.010049133561551571\n",
      "Loss on test= 0.016693729907274246\n",
      "acc for Lsat= 0.07527705596552954 \n",
      "acc for Psat= 0.10781490769651202 \n",
      "acc for optim= 0.13706804023434718\n",
      "Epoch:664/1000\n",
      "Loss on train= 0.01025073230266571\n",
      "Loss on test= 0.01688859798014164\n",
      "acc for Lsat= 0.07374220490455628 \n",
      "acc for Psat= 0.10432461036576164 \n",
      "acc for optim= 0.13809780618693265\n",
      "Epoch:665/1000\n",
      "Loss on train= 0.009852280840277672\n",
      "Loss on test= 0.016102999448776245\n",
      "acc for Lsat= 0.06707477420568467 \n",
      "acc for Psat= 0.09280819329950546 \n",
      "acc for optim= 0.14153465686572925\n",
      "Epoch:666/1000\n",
      "Loss on train= 0.01020924560725689\n",
      "Loss on test= 0.014947731047868729\n",
      "acc for Lsat= 0.0715527006321483 \n",
      "acc for Psat= 0.09996075828870137 \n",
      "acc for optim= 0.13948513788895472\n",
      "Epoch:667/1000\n",
      "Loss on train= 0.010001394897699356\n",
      "Loss on test= 0.016274670138955116\n",
      "acc for Lsat= 0.0780048562420739 \n",
      "acc for Psat= 0.09554072784052954 \n",
      "acc for optim= 0.13834423147555855\n",
      "Epoch:668/1000\n",
      "Loss on train= 0.010725367814302444\n",
      "Loss on test= 0.015464873984456062\n",
      "acc for Lsat= 0.07071684582365884 \n",
      "acc for Psat= 0.09213746156957413 \n",
      "acc for optim= 0.13789654463633064\n",
      "Epoch:669/1000\n",
      "Loss on train= 0.010146427899599075\n",
      "Loss on test= 0.016199830919504166\n",
      "acc for Lsat= 0.07097664011849297 \n",
      "acc for Psat= 0.09953756100601621 \n",
      "acc for optim= 0.138473361875448\n",
      "Epoch:670/1000\n",
      "Loss on train= 0.009703568182885647\n",
      "Loss on test= 0.01582343317568302\n",
      "acc for Lsat= 0.0739004557331403 \n",
      "acc for Psat= 0.09373266051212946 \n",
      "acc for optim= 0.1398202433354325\n",
      "Epoch:671/1000\n",
      "Loss on train= 0.010362735949456692\n",
      "Loss on test= 0.014709358103573322\n",
      "acc for Lsat= 0.07230397909879684 \n",
      "acc for Psat= 0.10261932611465455 \n",
      "acc for optim= 0.13624576925196577\n",
      "Epoch:672/1000\n",
      "Loss on train= 0.00975688174366951\n",
      "Loss on test= 0.015941791236400604\n",
      "acc for Lsat= 0.07329155918624666 \n",
      "acc for Psat= 0.10712223284774355 \n",
      "acc for optim= 0.13883210981471675\n",
      "Epoch:673/1000\n",
      "Loss on train= 0.010419080965220928\n",
      "Loss on test= 0.015191195532679558\n",
      "acc for Lsat= 0.07281442963414723 \n",
      "acc for Psat= 0.10678175654676227 \n",
      "acc for optim= 0.1381768987617559\n",
      "Epoch:674/1000\n",
      "Loss on train= 0.009925642050802708\n",
      "Loss on test= 0.01651783101260662\n",
      "acc for Lsat= 0.08019871032900282 \n",
      "acc for Psat= 0.10068908300664689 \n",
      "acc for optim= 0.14105825254486667\n",
      "Epoch:675/1000\n",
      "Loss on train= 0.009907826781272888\n",
      "Loss on test= 0.01639934629201889\n",
      "acc for Lsat= 0.07049565944406722 \n",
      "acc for Psat= 0.10150154911809497 \n",
      "acc for optim= 0.14085341253214412\n",
      "Epoch:676/1000\n",
      "Loss on train= 0.00991430226713419\n",
      "Loss on test= 0.016336075961589813\n",
      "acc for Lsat= 0.07215286956893073 \n",
      "acc for Psat= 0.1126081830925412 \n",
      "acc for optim= 0.13860405157837602\n",
      "Epoch:677/1000\n",
      "Loss on train= 0.009695946238934994\n",
      "Loss on test= 0.015616844408214092\n",
      "acc for Lsat= 0.07532623956600824 \n",
      "acc for Psat= 0.10409109426869285 \n",
      "acc for optim= 0.13851468354049656\n",
      "Epoch:678/1000\n",
      "Loss on train= 0.00998906884342432\n",
      "Loss on test= 0.015719009563326836\n",
      "acc for Lsat= 0.07586756133370928 \n",
      "acc for Psat= 0.10455980863836076 \n",
      "acc for optim= 0.13629590194258429\n",
      "Epoch:679/1000\n",
      "Loss on train= 0.009548645466566086\n",
      "Loss on test= 0.017214184626936913\n",
      "acc for Lsat= 0.071348742975129 \n",
      "acc for Psat= 0.10103118386533526 \n",
      "acc for optim= 0.13714954406023025\n",
      "Epoch:680/1000\n",
      "Loss on train= 0.009876088239252567\n",
      "Loss on test= 0.0157353226095438\n",
      "acc for Lsat= 0.07452963838974636 \n",
      "acc for Psat= 0.0983369227912691 \n",
      "acc for optim= 0.13764373167521426\n",
      "Epoch:681/1000\n",
      "Loss on train= 0.009659537114202976\n",
      "Loss on test= 0.014815966598689556\n",
      "acc for Lsat= 0.07082134402460523 \n",
      "acc for Psat= 0.09404917657375336 \n",
      "acc for optim= 0.1374190175905824\n",
      "Epoch:682/1000\n",
      "Loss on train= 0.009600906632840633\n",
      "Loss on test= 0.01588691771030426\n",
      "acc for Lsat= 0.0768418186240726 \n",
      "acc for Psat= 0.09898558855056762 \n",
      "acc for optim= 0.13855385086693184\n",
      "Epoch:683/1000\n",
      "Loss on train= 0.009575474075973034\n",
      "Loss on test= 0.017050575464963913\n",
      "acc for Lsat= 0.07755481600761413 \n",
      "acc for Psat= 0.09534709254900614 \n",
      "acc for optim= 0.1403147308776776\n",
      "Epoch:684/1000\n",
      "Loss on train= 0.009724692441523075\n",
      "Loss on test= 0.016999932006001472\n",
      "acc for Lsat= 0.06860226359632281 \n",
      "acc for Psat= 0.09207337184084786 \n",
      "acc for optim= 0.14000851048363577\n",
      "Epoch:685/1000\n",
      "Loss on train= 0.00966744776815176\n",
      "Loss on test= 0.01636054739356041\n",
      "acc for Lsat= 0.06656709776984321 \n",
      "acc for Psat= 0.0956180994709333 \n",
      "acc for optim= 0.137227524485853\n",
      "Epoch:686/1000\n",
      "Loss on train= 0.009800847619771957\n",
      "Loss on test= 0.01649043709039688\n",
      "acc for Lsat= 0.06755839669042163 \n",
      "acc for Psat= 0.09862248533301882 \n",
      "acc for optim= 0.1387089832375447\n",
      "Epoch:687/1000\n",
      "Loss on train= 0.00963270291686058\n",
      "Loss on test= 0.016676586121320724\n",
      "acc for Lsat= 0.07781646152337392 \n",
      "acc for Psat= 0.11153659654988182 \n",
      "acc for optim= 0.14225768786337642\n",
      "Epoch:688/1000\n",
      "Loss on train= 0.009721293114125729\n",
      "Loss on test= 0.01671750470995903\n",
      "acc for Lsat= 0.0839694619178772 \n",
      "acc for Psat= 0.10831484297911326 \n",
      "acc for optim= 0.14058987696965536\n",
      "Epoch:689/1000\n",
      "Loss on train= 0.009965785779058933\n",
      "Loss on test= 0.01585989072918892\n",
      "acc for Lsat= 0.07653903596931035 \n",
      "acc for Psat= 0.09824464238352246 \n",
      "acc for optim= 0.13960356530216006\n",
      "Epoch:690/1000\n",
      "Loss on train= 0.009861396625638008\n",
      "Loss on test= 0.016223929822444916\n",
      "acc for Lsat= 0.06623514990011851 \n",
      "acc for Psat= 0.09168847683403228 \n",
      "acc for optim= 0.14033495535453164\n",
      "Epoch:691/1000\n",
      "Loss on train= 0.009435933083295822\n",
      "Loss on test= 0.01704859361052513\n",
      "acc for Lsat= 0.06894917885462443 \n",
      "acc for Psat= 0.09554316533936395 \n",
      "acc for optim= 0.14093840825888843\n",
      "Epoch:692/1000\n",
      "Loss on train= 0.00998989399522543\n",
      "Loss on test= 0.015644779428839684\n",
      "acc for Lsat= 0.06740167389313381 \n",
      "acc for Psat= 0.09666221340497333 \n",
      "acc for optim= 0.14063311711781554\n",
      "Epoch:693/1000\n",
      "Loss on train= 0.009844952262938023\n",
      "Loss on test= 0.016766952350735664\n",
      "acc for Lsat= 0.07473439590798484 \n",
      "acc for Psat= 0.10997876425584158 \n",
      "acc for optim= 0.14037961963978077\n",
      "Epoch:694/1000\n",
      "Loss on train= 0.009703979827463627\n",
      "Loss on test= 0.015152111649513245\n",
      "acc for Lsat= 0.0701026462846332 \n",
      "acc for Psat= 0.10015454490979513 \n",
      "acc for optim= 0.1406468753599458\n",
      "Epoch:695/1000\n",
      "Loss on train= 0.009635794907808304\n",
      "Loss on test= 0.017168976366519928\n",
      "acc for Lsat= 0.06819988720946842 \n",
      "acc for Psat= 0.10262347923384774 \n",
      "acc for optim= 0.13946079282710946\n",
      "Epoch:696/1000\n",
      "Loss on train= 0.009753972291946411\n",
      "Loss on test= 0.01588614657521248\n",
      "acc for Lsat= 0.07001596871349547 \n",
      "acc for Psat= 0.09544033590290282 \n",
      "acc for optim= 0.13935142231898176\n",
      "Epoch:697/1000\n",
      "Loss on train= 0.010031669400632381\n",
      "Loss on test= 0.015936193987727165\n",
      "acc for Lsat= 0.07196619560321173 \n",
      "acc for Psat= 0.10374318857987722 \n",
      "acc for optim= 0.1392080526178082\n",
      "Epoch:698/1000\n",
      "Loss on train= 0.009895320981740952\n",
      "Loss on test= 0.015345362946391106\n",
      "acc for Lsat= 0.06625890632470449 \n",
      "acc for Psat= 0.0971793297264311 \n",
      "acc for optim= 0.13916724001367886\n",
      "Epoch:699/1000\n",
      "Loss on train= 0.009847518056631088\n",
      "Loss on test= 0.01671375147998333\n",
      "acc for Lsat= 0.06947824805974961 \n",
      "acc for Psat= 0.09580266906155478 \n",
      "acc for optim= 0.13684146936155026\n",
      "Epoch:700/1000\n",
      "Loss on train= 0.009901827201247215\n",
      "Loss on test= 0.015071517787873745\n",
      "acc for Lsat= 0.06733049617873298 \n",
      "acc for Psat= 0.09562214712301892 \n",
      "acc for optim= 0.137577091757622\n",
      "Epoch:701/1000\n",
      "Loss on train= 0.0096001410856843\n",
      "Loss on test= 0.016084246337413788\n",
      "acc for Lsat= 0.06506185630957287 \n",
      "acc for Psat= 0.10325189597076839 \n",
      "acc for optim= 0.14005684326920248\n",
      "Epoch:702/1000\n",
      "Loss on train= 0.009627877734601498\n",
      "Loss on test= 0.01587674207985401\n",
      "acc for Lsat= 0.06611566593249638 \n",
      "acc for Psat= 0.09801224238342708 \n",
      "acc for optim= 0.13776570751021308\n",
      "Epoch:703/1000\n",
      "Loss on train= 0.009523014537990093\n",
      "Loss on test= 0.016993116587400436\n",
      "acc for Lsat= 0.06792408542500601 \n",
      "acc for Psat= 0.09435433116224078 \n",
      "acc for optim= 0.13988754385047486\n",
      "Epoch:704/1000\n",
      "Loss on train= 0.01041081640869379\n",
      "Loss on test= 0.015983358025550842\n",
      "acc for Lsat= 0.07128691871960957 \n",
      "acc for Psat= 0.10110549860530431 \n",
      "acc for optim= 0.14034129509495366\n",
      "Epoch:705/1000\n",
      "Loss on train= 0.009390001185238361\n",
      "Loss on test= 0.017661361023783684\n",
      "acc for Lsat= 0.06547747767633862 \n",
      "acc for Psat= 0.09760390321413676 \n",
      "acc for optim= 0.1376626733276579\n",
      "Epoch:706/1000\n",
      "Loss on train= 0.009490136057138443\n",
      "Loss on test= 0.01572675257921219\n",
      "acc for Lsat= 0.07329601364003287 \n",
      "acc for Psat= 0.09896709902418983 \n",
      "acc for optim= 0.14182901283105215\n",
      "Epoch:707/1000\n",
      "Loss on train= 0.009424970485270023\n",
      "Loss on test= 0.015749838203191757\n",
      "acc for Lsat= 0.06970430711905162 \n",
      "acc for Psat= 0.10218613346417743 \n",
      "acc for optim= 0.13809586346356406\n",
      "Epoch:708/1000\n",
      "Loss on train= 0.009612259455025196\n",
      "Loss on test= 0.016520356759428978\n",
      "acc for Lsat= 0.07286835428741244 \n",
      "acc for Psat= 0.1025639851888021 \n",
      "acc for optim= 0.14087060872051446\n",
      "Epoch:709/1000\n",
      "Loss on train= 0.009926517494022846\n",
      "Loss on test= 0.01485428586602211\n",
      "acc for Lsat= 0.07004773169755935 \n",
      "acc for Psat= 0.09263671785593032 \n",
      "acc for optim= 0.14038058436579173\n",
      "Epoch:710/1000\n",
      "Loss on train= 0.010054979473352432\n",
      "Loss on test= 0.015853576362133026\n",
      "acc for Lsat= 0.07141412132316166 \n",
      "acc for Psat= 0.1031992392407523 \n",
      "acc for optim= 0.13751722344507775\n",
      "Epoch:711/1000\n",
      "Loss on train= 0.009648596867918968\n",
      "Loss on test= 0.015869973227381706\n",
      "acc for Lsat= 0.06861139287551245 \n",
      "acc for Psat= 0.09852370851569706 \n",
      "acc for optim= 0.13760429158962018\n",
      "Epoch:712/1000\n",
      "Loss on train= 0.009551049210131168\n",
      "Loss on test= 0.015010288916528225\n",
      "acc for Lsat= 0.069495370818509 \n",
      "acc for Psat= 0.0980599211321937 \n",
      "acc for optim= 0.13838797588315271\n",
      "Epoch:713/1000\n",
      "Loss on train= 0.009786883369088173\n",
      "Loss on test= 0.016582854092121124\n",
      "acc for Lsat= 0.07457327925496632 \n",
      "acc for Psat= 0.0992655278907882 \n",
      "acc for optim= 0.1383590784337786\n",
      "Epoch:714/1000\n",
      "Loss on train= 0.009618939831852913\n",
      "Loss on test= 0.01563245803117752\n",
      "acc for Lsat= 0.06656311319933998 \n",
      "acc for Psat= 0.09867576228247749 \n",
      "acc for optim= 0.13972040166457492\n",
      "Epoch:715/1000\n",
      "Loss on train= 0.009762166999280453\n",
      "Loss on test= 0.016125986352562904\n",
      "acc for Lsat= 0.07451309495502047 \n",
      "acc for Psat= 0.10054135703378254 \n",
      "acc for optim= 0.1388658039530532\n",
      "Epoch:716/1000\n",
      "Loss on train= 0.009628375992178917\n",
      "Loss on test= 0.016289886087179184\n",
      "acc for Lsat= 0.06784656180275811 \n",
      "acc for Psat= 0.0960729201634725 \n",
      "acc for optim= 0.13832044998804732\n",
      "Epoch:717/1000\n",
      "Loss on train= 0.009679174050688744\n",
      "Loss on test= 0.016263095661997795\n",
      "acc for Lsat= 0.07030616021818585 \n",
      "acc for Psat= 0.10166225996282366 \n",
      "acc for optim= 0.13919030353426934\n",
      "Epoch:718/1000\n",
      "Loss on train= 0.009864257648587227\n",
      "Loss on test= 0.015051535330712795\n",
      "acc for Lsat= 0.06833712706963221 \n",
      "acc for Psat= 0.09413223067919414 \n",
      "acc for optim= 0.13906869537507494\n",
      "Epoch:719/1000\n",
      "Loss on train= 0.009746957570314407\n",
      "Loss on test= 0.016610272228717804\n",
      "acc for Lsat= 0.08316110438770717 \n",
      "acc for Psat= 0.0993207057317098 \n",
      "acc for optim= 0.13792191000862253\n",
      "Epoch:720/1000\n",
      "Loss on train= 0.009972230531275272\n",
      "Loss on test= 0.016022544354200363\n",
      "acc for Lsat= 0.07424526413281758 \n",
      "acc for Psat= 0.10599305530389151 \n",
      "acc for optim= 0.13803391005429957\n",
      "Epoch:721/1000\n",
      "Loss on train= 0.009695074521005154\n",
      "Loss on test= 0.015268783085048199\n",
      "acc for Lsat= 0.06774876928991741 \n",
      "acc for Psat= 0.09407249821556939 \n",
      "acc for optim= 0.1431476587222682\n",
      "Epoch:722/1000\n",
      "Loss on train= 0.009970065206289291\n",
      "Loss on test= 0.016545001417398453\n",
      "acc for Lsat= 0.06702103664477667 \n",
      "acc for Psat= 0.09877501395013596 \n",
      "acc for optim= 0.13752053786690036\n",
      "Epoch:723/1000\n",
      "Loss on train= 0.009460818953812122\n",
      "Loss on test= 0.015241553075611591\n",
      "acc for Lsat= 0.07137114670541551 \n",
      "acc for Psat= 0.10042657719718084 \n",
      "acc for optim= 0.13959611337631941\n",
      "Epoch:724/1000\n",
      "Loss on train= 0.009573375806212425\n",
      "Loss on test= 0.015247638337314129\n",
      "acc for Lsat= 0.07457749711142646 \n",
      "acc for Psat= 0.10016092658042908 \n",
      "acc for optim= 0.14025890313916733\n",
      "Epoch:725/1000\n",
      "Loss on train= 0.009431452490389347\n",
      "Loss on test= 0.016799800097942352\n",
      "acc for Lsat= 0.07333067423767513 \n",
      "acc for Psat= 0.09883141650093927 \n",
      "acc for optim= 0.13735705375050505\n",
      "Epoch:726/1000\n",
      "Loss on train= 0.009823598898947239\n",
      "Loss on test= 0.016716621816158295\n",
      "acc for Lsat= 0.07491344660520555 \n",
      "acc for Psat= 0.09567667047182718 \n",
      "acc for optim= 0.14017871630688508\n",
      "Epoch:727/1000\n",
      "Loss on train= 0.00966832134872675\n",
      "Loss on test= 0.015239741653203964\n",
      "acc for Lsat= 0.0756437341372172 \n",
      "acc for Psat= 0.11344955298635694 \n",
      "acc for optim= 0.14076957669523027\n",
      "Epoch:728/1000\n",
      "Loss on train= 0.0093211829662323\n",
      "Loss on test= 0.01499059796333313\n",
      "acc for Lsat= 0.07137476437621645 \n",
      "acc for Psat= 0.09912629392411973 \n",
      "acc for optim= 0.13818159019574522\n",
      "Epoch:729/1000\n",
      "Loss on train= 0.009752203710377216\n",
      "Loss on test= 0.015316120348870754\n",
      "acc for Lsat= 0.06974183585908679 \n",
      "acc for Psat= 0.09533796972698635 \n",
      "acc for optim= 0.13996481984439824\n",
      "Epoch:730/1000\n",
      "Loss on train= 0.00972003024071455\n",
      "Loss on test= 0.015962423756718636\n",
      "acc for Lsat= 0.06927014721764459 \n",
      "acc for Psat= 0.0964183067282041 \n",
      "acc for optim= 0.13855718469454187\n",
      "Epoch:731/1000\n",
      "Loss on train= 0.009625288657844067\n",
      "Loss on test= 0.01505148783326149\n",
      "acc for Lsat= 0.07223464565144647 \n",
      "acc for Psat= 0.11678814556863573 \n",
      "acc for optim= 0.14013815133108035\n",
      "Epoch:732/1000\n",
      "Loss on train= 0.009421121329069138\n",
      "Loss on test= 0.015496172942221165\n",
      "acc for Lsat= 0.07592707127332687 \n",
      "acc for Psat= 0.10268818918201661 \n",
      "acc for optim= 0.14080888490296073\n",
      "Epoch:733/1000\n",
      "Loss on train= 0.009473961777985096\n",
      "Loss on test= 0.016549693420529366\n",
      "acc for Lsat= 0.0752570985092057 \n",
      "acc for Psat= 0.11764404376347858 \n",
      "acc for optim= 0.1431099350253741\n",
      "Epoch:734/1000\n",
      "Loss on train= 0.00939651858061552\n",
      "Loss on test= 0.01674758456647396\n",
      "acc for Lsat= 0.07430055141448974 \n",
      "acc for Psat= 0.09924117260509066 \n",
      "acc for optim= 0.1376278813928366\n",
      "Epoch:735/1000\n",
      "Loss on train= 0.009233095683157444\n",
      "Loss on test= 0.016894301399588585\n",
      "acc for Lsat= 0.08028985162576038 \n",
      "acc for Psat= 0.1040906987256474 \n",
      "acc for optim= 0.13747303226134838\n",
      "Epoch:736/1000\n",
      "Loss on train= 0.009765342809259892\n",
      "Loss on test= 0.015983911231160164\n",
      "acc for Lsat= 0.0737370390031073 \n",
      "acc for Psat= 0.09433022489150367 \n",
      "acc for optim= 0.137370859619437\n",
      "Epoch:737/1000\n",
      "Loss on train= 0.009830477647483349\n",
      "Loss on test= 0.01551942341029644\n",
      "acc for Lsat= 0.06857937905523512 \n",
      "acc for Psat= 0.09646516094605129 \n",
      "acc for optim= 0.13885256807940702\n",
      "Epoch:738/1000\n",
      "Loss on train= 0.009737426415085793\n",
      "Loss on test= 0.01724078878760338\n",
      "acc for Lsat= 0.07364373008410136 \n",
      "acc for Psat= 0.11209427515665688 \n",
      "acc for optim= 0.13871058122151425\n",
      "Epoch:739/1000\n",
      "Loss on train= 0.01040865108370781\n",
      "Loss on test= 0.015529642812907696\n",
      "acc for Lsat= 0.06930031643973456 \n",
      "acc for Psat= 0.09817272921403249 \n",
      "acc for optim= 0.13819450893335872\n",
      "Epoch:740/1000\n",
      "Loss on train= 0.009671582840383053\n",
      "Loss on test= 0.015009189955890179\n",
      "acc for Lsat= 0.06657097621096504 \n",
      "acc for Psat= 0.09439863347344928 \n",
      "acc for optim= 0.13786308223174676\n",
      "Epoch:741/1000\n",
      "Loss on train= 0.009458144195377827\n",
      "Loss on test= 0.015829164534807205\n",
      "acc for Lsat= 0.07128296643495559 \n",
      "acc for Psat= 0.09720956732829414 \n",
      "acc for optim= 0.1391487364553743\n",
      "Epoch:742/1000\n",
      "Loss on train= 0.009656877256929874\n",
      "Loss on test= 0.016863994300365448\n",
      "acc for Lsat= 0.07752558870448006 \n",
      "acc for Psat= 0.09318251659472782 \n",
      "acc for optim= 0.1397133213053975\n",
      "Epoch:743/1000\n",
      "Loss on train= 0.009660884737968445\n",
      "Loss on test= 0.01650283671915531\n",
      "acc for Lsat= 0.07236690355671777 \n",
      "acc for Psat= 0.10189840710825389 \n",
      "acc for optim= 0.1405849027964804\n",
      "Epoch:744/1000\n",
      "Loss on train= 0.009398204274475574\n",
      "Loss on test= 0.016526097431778908\n",
      "acc for Lsat= 0.06985303345653746 \n",
      "acc for Psat= 0.10048659907446968 \n",
      "acc for optim= 0.13981549696375928\n",
      "Epoch:745/1000\n",
      "Loss on train= 0.009297155775129795\n",
      "Loss on test= 0.015652336180210114\n",
      "acc for Lsat= 0.07054857280519272 \n",
      "acc for Psat= 0.09585737503237196 \n",
      "acc for optim= 0.1387819044705894\n",
      "Epoch:746/1000\n",
      "Loss on train= 0.00956492405384779\n",
      "Loss on test= 0.01534546073526144\n",
      "acc for Lsat= 0.07429739899105496 \n",
      "acc for Psat= 0.09955212871233622 \n",
      "acc for optim= 0.13806768961043822\n",
      "Epoch:747/1000\n",
      "Loss on train= 0.010142534039914608\n",
      "Loss on test= 0.01629558578133583\n",
      "acc for Lsat= 0.07882266872458989 \n",
      "acc for Psat= 0.09534213162130781 \n",
      "acc for optim= 0.13879850496434504\n",
      "Epoch:748/1000\n",
      "Loss on train= 0.009416695684194565\n",
      "Loss on test= 0.016237305477261543\n",
      "acc for Lsat= 0.07357913934522203 \n",
      "acc for Psat= 0.10101866705550089 \n",
      "acc for optim= 0.13883624997817806\n",
      "Epoch:749/1000\n",
      "Loss on train= 0.00917170848697424\n",
      "Loss on test= 0.015945186838507652\n",
      "acc for Lsat= 0.0689682886004448 \n",
      "acc for Psat= 0.09680227355824578 \n",
      "acc for optim= 0.13778071584593918\n",
      "Epoch:750/1000\n",
      "Loss on train= 0.009604603983461857\n",
      "Loss on test= 0.016826903447508812\n",
      "acc for Lsat= 0.0715840286678738 \n",
      "acc for Psat= 0.1030828297138214 \n",
      "acc for optim= 0.13974307328462604\n",
      "Epoch:751/1000\n",
      "Loss on train= 0.009359030984342098\n",
      "Loss on test= 0.01643930748105049\n",
      "acc for Lsat= 0.07508472849925357 \n",
      "acc for Psat= 0.09556414120727114 \n",
      "acc for optim= 0.13777298270207314\n",
      "Epoch:752/1000\n",
      "Loss on train= 0.009467330761253834\n",
      "Loss on test= 0.01605643704533577\n",
      "acc for Lsat= 0.07764986438883675 \n",
      "acc for Psat= 0.10633320278591579 \n",
      "acc for optim= 0.13812913965019916\n",
      "Epoch:753/1000\n",
      "Loss on train= 0.009524833410978317\n",
      "Loss on test= 0.017039325088262558\n",
      "acc for Lsat= 0.07440296510855356 \n",
      "acc for Psat= 0.10842178795072768 \n",
      "acc for optim= 0.1411237249357833\n",
      "Epoch:754/1000\n",
      "Loss on train= 0.009423977695405483\n",
      "Loss on test= 0.0160868838429451\n",
      "acc for Lsat= 0.07313371698061627 \n",
      "acc for Psat= 0.10125176111857098 \n",
      "acc for optim= 0.1390334901710351\n",
      "Epoch:755/1000\n",
      "Loss on train= 0.009276055730879307\n",
      "Loss on test= 0.01794004626572132\n",
      "acc for Lsat= 0.0761393878195021 \n",
      "acc for Psat= 0.09507610946893692 \n",
      "acc for optim= 0.1387766846973035\n",
      "Epoch:756/1000\n",
      "Loss on train= 0.009143791161477566\n",
      "Loss on test= 0.01717621646821499\n",
      "acc for Lsat= 0.07400163809458415 \n",
      "acc for Psat= 0.10118516667021646 \n",
      "acc for optim= 0.14030428392191727\n",
      "Epoch:757/1000\n",
      "Loss on train= 0.009498187340795994\n",
      "Loss on test= 0.015540357679128647\n",
      "acc for Lsat= 0.06679740266667472 \n",
      "acc for Psat= 0.09778572271267574 \n",
      "acc for optim= 0.1383173821287023\n",
      "Epoch:758/1000\n",
      "Loss on train= 0.009682944044470787\n",
      "Loss on test= 0.015538924373686314\n",
      "acc for Lsat= 0.06804377271069421 \n",
      "acc for Psat= 0.11105333699120416 \n",
      "acc for optim= 0.138118358867036\n",
      "Epoch:759/1000\n",
      "Loss on train= 0.009340272285044193\n",
      "Loss on test= 0.017031945288181305\n",
      "acc for Lsat= 0.06481917070017922 \n",
      "acc for Psat= 0.0966464693347613 \n",
      "acc for optim= 0.13709586304095056\n",
      "Epoch:760/1000\n",
      "Loss on train= 0.009342380799353123\n",
      "Loss on test= 0.016708718612790108\n",
      "acc for Lsat= 0.06844242099258635 \n",
      "acc for Psat= 0.09772380656666227 \n",
      "acc for optim= 0.14083495491908657\n",
      "Epoch:761/1000\n",
      "Loss on train= 0.009199243038892746\n",
      "Loss on test= 0.01663133315742016\n",
      "acc for Lsat= 0.07238905049032635 \n",
      "acc for Psat= 0.10402589076095156 \n",
      "acc for optim= 0.1376647289531926\n",
      "Epoch:762/1000\n",
      "Loss on train= 0.009318850003182888\n",
      "Loss on test= 0.01698087900876999\n",
      "acc for Lsat= 0.07468625373310511 \n",
      "acc for Psat= 0.10481242802408006 \n",
      "acc for optim= 0.13949577833215399\n",
      "Epoch:763/1000\n",
      "Loss on train= 0.00938971433788538\n",
      "Loss on test= 0.01634526625275612\n",
      "acc for Lsat= 0.06783623380793465 \n",
      "acc for Psat= 0.10322328789366617 \n",
      "acc for optim= 0.1397949379351404\n",
      "Epoch:764/1000\n",
      "Loss on train= 0.009553996846079826\n",
      "Loss on test= 0.015688778832554817\n",
      "acc for Lsat= 0.06890373229980468 \n",
      "acc for Psat= 0.10034447047445506 \n",
      "acc for optim= 0.13876100730978783\n",
      "Epoch:765/1000\n",
      "Loss on train= 0.009795955382287502\n",
      "Loss on test= 0.015322033315896988\n",
      "acc for Lsat= 0.069148817161719 \n",
      "acc for Psat= 0.09387023515171473 \n",
      "acc for optim= 0.1389389704085059\n",
      "Epoch:766/1000\n",
      "Loss on train= 0.009419326670467854\n",
      "Loss on test= 0.01593993417918682\n",
      "acc for Lsat= 0.0721245848470264 \n",
      "acc for Psat= 0.10508160160647496 \n",
      "acc for optim= 0.14156195980807149\n",
      "Epoch:767/1000\n",
      "Loss on train= 0.009802923537790775\n",
      "Loss on test= 0.015810353681445122\n",
      "acc for Lsat= 0.06448943018913268 \n",
      "acc for Psat= 0.09242124905188878 \n",
      "acc for optim= 0.13758539742169276\n",
      "Epoch:768/1000\n",
      "Loss on train= 0.00934196263551712\n",
      "Loss on test= 0.015953144058585167\n",
      "acc for Lsat= 0.0672620231906573 \n",
      "acc for Psat= 0.09921920415427948 \n",
      "acc for optim= 0.14175380443533261\n",
      "Epoch:769/1000\n",
      "Loss on train= 0.009445506148040295\n",
      "Loss on test= 0.015651263296604156\n",
      "acc for Lsat= 0.06570234911309349 \n",
      "acc for Psat= 0.09631292356385124 \n",
      "acc for optim= 0.13766057367126147\n",
      "Epoch:770/1000\n",
      "Loss on train= 0.009568054229021072\n",
      "Loss on test= 0.014736016280949116\n",
      "acc for Lsat= 0.06991410470671125 \n",
      "acc for Psat= 0.09673447708288828 \n",
      "acc for optim= 0.13898123477896054\n",
      "Epoch:771/1000\n",
      "Loss on train= 0.009736407548189163\n",
      "Loss on test= 0.016549712046980858\n",
      "acc for Lsat= 0.0685731690790918 \n",
      "acc for Psat= 0.10314444452524188 \n",
      "acc for optim= 0.13862175326794388\n",
      "Epoch:772/1000\n",
      "Loss on train= 0.009805948473513126\n",
      "Loss on test= 0.01595494896173477\n",
      "acc for Lsat= 0.06867796878019969 \n",
      "acc for Psat= 0.09726973921060561 \n",
      "acc for optim= 0.13926524644096694\n",
      "Epoch:773/1000\n",
      "Loss on train= 0.008943669497966766\n",
      "Loss on test= 0.017042431980371475\n",
      "acc for Lsat= 0.0697591761747996 \n",
      "acc for Psat= 0.1034025771750344 \n",
      "acc for optim= 0.13974115684007604\n",
      "Epoch:774/1000\n",
      "Loss on train= 0.009290986694395542\n",
      "Loss on test= 0.015177988447248936\n",
      "acc for Lsat= 0.06920077684852811 \n",
      "acc for Psat= 0.09899343483977846 \n",
      "acc for optim= 0.13746781360047558\n",
      "Epoch:775/1000\n",
      "Loss on train= 0.009505645371973515\n",
      "Loss on test= 0.015610190108418465\n",
      "acc for Lsat= 0.06581419557332992 \n",
      "acc for Psat= 0.09701573650042217 \n",
      "acc for optim= 0.13870501055563283\n",
      "Epoch:776/1000\n",
      "Loss on train= 0.009552313014864922\n",
      "Loss on test= 0.015908651053905487\n",
      "acc for Lsat= 0.06678964296976725 \n",
      "acc for Psat= 0.09632325619459152 \n",
      "acc for optim= 0.13950925370057424\n",
      "Epoch:777/1000\n",
      "Loss on train= 0.009186899289488792\n",
      "Loss on test= 0.015923870727419853\n",
      "acc for Lsat= 0.06648535562886132 \n",
      "acc for Psat= 0.09243111958106359 \n",
      "acc for optim= 0.13813317424307267\n",
      "Epoch:778/1000\n",
      "Loss on train= 0.009416990913450718\n",
      "Loss on test= 0.015837475657463074\n",
      "acc for Lsat= 0.07165782418515948 \n",
      "acc for Psat= 0.10586743884616429 \n",
      "acc for optim= 0.1403908926993608\n",
      "Epoch:779/1000\n",
      "Loss on train= 0.009185473434627056\n",
      "Loss on test= 0.015943488106131554\n",
      "acc for Lsat= 0.067444456451469 \n",
      "acc for Psat= 0.09438034014569391 \n",
      "acc for optim= 0.13927868770228494\n",
      "Epoch:780/1000\n",
      "Loss on train= 0.009309113956987858\n",
      "Loss on test= 0.015968013554811478\n",
      "acc for Lsat= 0.0688231627146403 \n",
      "acc for Psat= 0.09625855700837242 \n",
      "acc for optim= 0.13963370041714773\n",
      "Epoch:781/1000\n",
      "Loss on train= 0.00956759974360466\n",
      "Loss on test= 0.016553236171603203\n",
      "acc for Lsat= 0.06890038665797976 \n",
      "acc for Psat= 0.09576292054520712 \n",
      "acc for optim= 0.13791361918879882\n",
      "Epoch:782/1000\n",
      "Loss on train= 0.009628471918404102\n",
      "Loss on test= 0.016220301389694214\n",
      "acc for Lsat= 0.07226291365093654 \n",
      "acc for Psat= 0.1032181633843316 \n",
      "acc for optim= 0.13678617604956445\n",
      "Epoch:783/1000\n",
      "Loss on train= 0.009199782274663448\n",
      "Loss on test= 0.01701866276562214\n",
      "acc for Lsat= 0.0715090071161588 \n",
      "acc for Psat= 0.10768734415372212 \n",
      "acc for optim= 0.14051790974206393\n",
      "Epoch:784/1000\n",
      "Loss on train= 0.009200407192111015\n",
      "Loss on test= 0.0169981699436903\n",
      "acc for Lsat= 0.06792887896299363 \n",
      "acc for Psat= 0.1061405412024922 \n",
      "acc for optim= 0.13901077064995965\n",
      "Epoch:785/1000\n",
      "Loss on train= 0.00929831899702549\n",
      "Loss on test= 0.016496963798999786\n",
      "acc for Lsat= 0.06810196108288236 \n",
      "acc for Psat= 0.10215620862113103 \n",
      "acc for optim= 0.13989264956778952\n",
      "Epoch:786/1000\n",
      "Loss on train= 0.00931420549750328\n",
      "Loss on test= 0.016175322234630585\n",
      "acc for Lsat= 0.06841921905676523 \n",
      "acc for Psat= 0.10399199095037247 \n",
      "acc for optim= 0.13930012082888021\n",
      "Epoch:787/1000\n",
      "Loss on train= 0.009007665328681469\n",
      "Loss on test= 0.016498588025569916\n",
      "acc for Lsat= 0.06724046435621049 \n",
      "acc for Psat= 0.1021513087881936 \n",
      "acc for optim= 0.13937232688897186\n",
      "Epoch:788/1000\n",
      "Loss on train= 0.009333447553217411\n",
      "Loss on test= 0.015672989189624786\n",
      "acc for Lsat= 0.06978993018468221 \n",
      "acc for Psat= 0.10574903322590723 \n",
      "acc for optim= 0.1415471435834964\n",
      "Epoch:789/1000\n",
      "Loss on train= 0.009594772011041641\n",
      "Loss on test= 0.016663571819663048\n",
      "acc for Lsat= 0.0677489694621828 \n",
      "acc for Psat= 0.09689243518643909 \n",
      "acc for optim= 0.14050891012367275\n",
      "Epoch:790/1000\n",
      "Loss on train= 0.009497052989900112\n",
      "Loss on test= 0.016039857640862465\n",
      "acc for Lsat= 0.06877394649717543 \n",
      "acc for Psat= 0.09490041219525863 \n",
      "acc for optim= 0.1382122866602408\n",
      "Epoch:791/1000\n",
      "Loss on train= 0.009379124268889427\n",
      "Loss on test= 0.01577933505177498\n",
      "acc for Lsat= 0.07188425974713432 \n",
      "acc for Psat= 0.09991567962699466 \n",
      "acc for optim= 0.13899530944310956\n",
      "Epoch:792/1000\n",
      "Loss on train= 0.009240876883268356\n",
      "Loss on test= 0.015141066163778305\n",
      "acc for Lsat= 0.07746541400750478 \n",
      "acc for Psat= 0.10907535884115432 \n",
      "acc for optim= 0.13861259478661747\n",
      "Epoch:793/1000\n",
      "Loss on train= 0.009273231029510498\n",
      "Loss on test= 0.01629122532904148\n",
      "acc for Lsat= 0.06798779517412185 \n",
      "acc for Psat= 0.10144173469808365 \n",
      "acc for optim= 0.14021977161367735\n",
      "Epoch:794/1000\n",
      "Loss on train= 0.009478681720793247\n",
      "Loss on test= 0.016363518312573433\n",
      "acc for Lsat= 0.06713069826364518 \n",
      "acc for Psat= 0.09349873314301173 \n",
      "acc for optim= 0.14123098059660857\n",
      "Epoch:795/1000\n",
      "Loss on train= 0.009502367116510868\n",
      "Loss on test= 0.01576692797243595\n",
      "acc for Lsat= 0.06855674386024474 \n",
      "acc for Psat= 0.09635728945334751 \n",
      "acc for optim= 0.13883236532823906\n",
      "Epoch:796/1000\n",
      "Loss on train= 0.009083006531000137\n",
      "Loss on test= 0.018062643706798553\n",
      "acc for Lsat= 0.07122160494327545 \n",
      "acc for Psat= 0.10982407563262517 \n",
      "acc for optim= 0.14045712567038005\n",
      "Epoch:797/1000\n",
      "Loss on train= 0.009254730306565762\n",
      "Loss on test= 0.015443791635334492\n",
      "acc for Lsat= 0.07128882871733773 \n",
      "acc for Psat= 0.10131659640206231 \n",
      "acc for optim= 0.1390449442797237\n",
      "Epoch:798/1000\n",
      "Loss on train= 0.009708065539598465\n",
      "Loss on test= 0.015872035175561905\n",
      "acc for Lsat= 0.06657991939120822 \n",
      "acc for Psat= 0.09503195848729876 \n",
      "acc for optim= 0.1415016142444478\n",
      "Epoch:799/1000\n",
      "Loss on train= 0.009175650775432587\n",
      "Loss on test= 0.016428843140602112\n",
      "acc for Lsat= 0.06659730490711 \n",
      "acc for Psat= 0.09967263407177397 \n",
      "acc for optim= 0.13881874200370578\n",
      "Epoch:800/1000\n",
      "Loss on train= 0.009029821492731571\n",
      "Loss on test= 0.016026802361011505\n",
      "acc for Lsat= 0.06976294484403398 \n",
      "acc for Psat= 0.10750065743923187 \n",
      "acc for optim= 0.13745202271060808\n",
      "Epoch:801/1000\n",
      "Loss on train= 0.008983972482383251\n",
      "Loss on test= 0.016216985881328583\n",
      "acc for Lsat= 0.06604535679022472 \n",
      "acc for Psat= 0.10437843948602676 \n",
      "acc for optim= 0.14057252912057774\n",
      "Epoch:802/1000\n",
      "Loss on train= 0.009197499603033066\n",
      "Loss on test= 0.01625913195312023\n",
      "acc for Lsat= 0.06675186256567638 \n",
      "acc for Psat= 0.09568059477541181 \n",
      "acc for optim= 0.13914695800178578\n",
      "Epoch:803/1000\n",
      "Loss on train= 0.008896874263882637\n",
      "Loss on test= 0.015427027828991413\n",
      "acc for Lsat= 0.07538107319010628 \n",
      "acc for Psat= 0.09975535190767712 \n",
      "acc for optim= 0.1398196047792832\n",
      "Epoch:804/1000\n",
      "Loss on train= 0.009738404303789139\n",
      "Loss on test= 0.016610469669103622\n",
      "acc for Lsat= 0.07119709187083774 \n",
      "acc for Psat= 0.09885810448063745 \n",
      "acc for optim= 0.13908520916787287\n",
      "Epoch:805/1000\n",
      "Loss on train= 0.009576982818543911\n",
      "Loss on test= 0.016106929630041122\n",
      "acc for Lsat= 0.07215205662780338 \n",
      "acc for Psat= 0.10113685660892063 \n",
      "acc for optim= 0.141098521463573\n",
      "Epoch:806/1000\n",
      "Loss on train= 0.009431719779968262\n",
      "Loss on test= 0.01692325621843338\n",
      "acc for Lsat= 0.06979116234514449 \n",
      "acc for Psat= 0.1033730238676071 \n",
      "acc for optim= 0.13930775954698524\n",
      "Epoch:807/1000\n",
      "Loss on train= 0.00918642058968544\n",
      "Loss on test= 0.01647764816880226\n",
      "acc for Lsat= 0.06883999374177721 \n",
      "acc for Psat= 0.09856515659226311 \n",
      "acc for optim= 0.13973274847699535\n",
      "Epoch:808/1000\n",
      "Loss on train= 0.009239253588020802\n",
      "Loss on test= 0.01569426618516445\n",
      "acc for Lsat= 0.0694782437549697 \n",
      "acc for Psat= 0.10196823494301901 \n",
      "acc for optim= 0.1396048768113057\n",
      "Epoch:809/1000\n",
      "Loss on train= 0.00896285381168127\n",
      "Loss on test= 0.016212990507483482\n",
      "acc for Lsat= 0.06641203198168014 \n",
      "acc for Psat= 0.10725751287407345 \n",
      "acc for optim= 0.13825365779921409\n",
      "Epoch:810/1000\n",
      "Loss on train= 0.009376117028295994\n",
      "Loss on test= 0.016718866303563118\n",
      "acc for Lsat= 0.06811502608988021 \n",
      "acc for Psat= 0.09637137121624416 \n",
      "acc for optim= 0.13934011817392378\n",
      "Epoch:811/1000\n",
      "Loss on train= 0.00921053346246481\n",
      "Loss on test= 0.01577632687985897\n",
      "acc for Lsat= 0.06699896189901564 \n",
      "acc for Psat= 0.10445186760690478 \n",
      "acc for optim= 0.13932909751310943\n",
      "Epoch:812/1000\n",
      "Loss on train= 0.009177722968161106\n",
      "Loss on test= 0.015632852911949158\n",
      "acc for Lsat= 0.0682268778483073 \n",
      "acc for Psat= 0.09794606864452361 \n",
      "acc for optim= 0.13917615781538192\n",
      "Epoch:813/1000\n",
      "Loss on train= 0.009581097401678562\n",
      "Loss on test= 0.017996283248066902\n",
      "acc for Lsat= 0.07277449501885308 \n",
      "acc for Psat= 0.10500114957491558 \n",
      "acc for optim= 0.141828606898586\n",
      "Epoch:814/1000\n",
      "Loss on train= 0.009593998081982136\n",
      "Loss on test= 0.016072578728199005\n",
      "acc for Lsat= 0.06485685888263915 \n",
      "acc for Psat= 0.09339973496066198 \n",
      "acc for optim= 0.13879765853699713\n",
      "Epoch:815/1000\n",
      "Loss on train= 0.009274407289922237\n",
      "Loss on test= 0.016592321917414665\n",
      "acc for Lsat= 0.06654462830887901 \n",
      "acc for Psat= 0.09994294444719949 \n",
      "acc for optim= 0.1406964005695449\n",
      "Epoch:816/1000\n",
      "Loss on train= 0.009105561301112175\n",
      "Loss on test= 0.015765272080898285\n",
      "acc for Lsat= 0.06644124372137918 \n",
      "acc for Psat= 0.09695877283811569 \n",
      "acc for optim= 0.14008766350646815\n",
      "Epoch:817/1000\n",
      "Loss on train= 0.008869525045156479\n",
      "Loss on test= 0.017432818189263344\n",
      "acc for Lsat= 0.06551105048921374 \n",
      "acc for Psat= 0.10070508254898919 \n",
      "acc for optim= 0.143448460350434\n",
      "Epoch:818/1000\n",
      "Loss on train= 0.009292328730225563\n",
      "Loss on test= 0.016876930370926857\n",
      "acc for Lsat= 0.0666456831826104 \n",
      "acc for Psat= 0.09673688179916806 \n",
      "acc for optim= 0.13881192200545533\n",
      "Epoch:819/1000\n",
      "Loss on train= 0.009194836020469666\n",
      "Loss on test= 0.01702651008963585\n",
      "acc for Lsat= 0.06786715702878104 \n",
      "acc for Psat= 0.09675141473611196 \n",
      "acc for optim= 0.13882366975562438\n",
      "Epoch:820/1000\n",
      "Loss on train= 0.008827123790979385\n",
      "Loss on test= 0.017210351303219795\n",
      "acc for Lsat= 0.06786020646492642 \n",
      "acc for Psat= 0.09506195386250813 \n",
      "acc for optim= 0.13932269277154574\n",
      "Epoch:821/1000\n",
      "Loss on train= 0.009600725024938583\n",
      "Loss on test= 0.015806928277015686\n",
      "acc for Lsat= 0.06772351496749454 \n",
      "acc for Psat= 0.09555260423156951 \n",
      "acc for optim= 0.13896094647546609\n",
      "Epoch:822/1000\n",
      "Loss on train= 0.00943478848785162\n",
      "Loss on test= 0.015780076384544373\n",
      "acc for Lsat= 0.07207729948891534 \n",
      "acc for Psat= 0.09625476532512241 \n",
      "acc for optim= 0.13882868593144748\n",
      "Epoch:823/1000\n",
      "Loss on train= 0.009415916167199612\n",
      "Loss on test= 0.015834994614124298\n",
      "acc for Lsat= 0.06674623075458738 \n",
      "acc for Psat= 0.10225364880429372 \n",
      "acc for optim= 0.1409934200760391\n",
      "Epoch:824/1000\n",
      "Loss on train= 0.009079311043024063\n",
      "Loss on test= 0.016387535259127617\n",
      "acc for Lsat= 0.067803931567404 \n",
      "acc for Psat= 0.09891654302676517 \n",
      "acc for optim= 0.1396445914895998\n",
      "Epoch:825/1000\n",
      "Loss on train= 0.009314414113759995\n",
      "Loss on test= 0.015816189348697662\n",
      "acc for Lsat= 0.06400960700379478 \n",
      "acc for Psat= 0.09149971538119847 \n",
      "acc for optim= 0.13809393507221507\n",
      "Epoch:826/1000\n",
      "Loss on train= 0.009136730805039406\n",
      "Loss on test= 0.016662590205669403\n",
      "acc for Lsat= 0.07866773853699366 \n",
      "acc for Psat= 0.10632791883415645 \n",
      "acc for optim= 0.1381444533872935\n",
      "Epoch:827/1000\n",
      "Loss on train= 0.00910882093012333\n",
      "Loss on test= 0.0159591231495142\n",
      "acc for Lsat= 0.07522021796968248 \n",
      "acc for Psat= 0.10791792670885719 \n",
      "acc for optim= 0.13922453450245992\n",
      "Epoch:828/1000\n",
      "Loss on train= 0.00906798429787159\n",
      "Loss on test= 0.016028566285967827\n",
      "acc for Lsat= 0.07229803933037653 \n",
      "acc for Psat= 0.10225649608506099 \n",
      "acc for optim= 0.14198873353501162\n",
      "Epoch:829/1000\n",
      "Loss on train= 0.008905015885829926\n",
      "Loss on test= 0.01657550036907196\n",
      "acc for Lsat= 0.06673588140143288 \n",
      "acc for Psat= 0.09764531751473744 \n",
      "acc for optim= 0.1398261134409242\n",
      "Epoch:830/1000\n",
      "Loss on train= 0.009550943970680237\n",
      "Loss on test= 0.015956483781337738\n",
      "acc for Lsat= 0.06991003072924083 \n",
      "acc for Psat= 0.09843172199196286 \n",
      "acc for optim= 0.13824817368553743\n",
      "Epoch:831/1000\n",
      "Loss on train= 0.009293920360505581\n",
      "Loss on test= 0.016670748591423035\n",
      "acc for Lsat= 0.07082689222362307 \n",
      "acc for Psat= 0.09995175782177185 \n",
      "acc for optim= 0.1392602525651455\n",
      "Epoch:832/1000\n",
      "Loss on train= 0.009494767524302006\n",
      "Loss on test= 0.016148658469319344\n",
      "acc for Lsat= 0.06657884981897141 \n",
      "acc for Psat= 0.09573611103826099 \n",
      "acc for optim= 0.13813570621940827\n",
      "Epoch:833/1000\n",
      "Loss on train= 0.00929189007729292\n",
      "Loss on test= 0.0167046207934618\n",
      "acc for Lsat= 0.06647698912355635 \n",
      "acc for Psat= 0.09800573769542906 \n",
      "acc for optim= 0.13866863743298583\n",
      "Epoch:834/1000\n",
      "Loss on train= 0.009302548132836819\n",
      "Loss on test= 0.016725480556488037\n",
      "acc for Lsat= 0.07276133414771821 \n",
      "acc for Psat= 0.10476503537760841 \n",
      "acc for optim= 0.13983463779505756\n",
      "Epoch:835/1000\n",
      "Loss on train= 0.009255505166947842\n",
      "Loss on test= 0.015438507311046124\n",
      "acc for Lsat= 0.06591941366593043 \n",
      "acc for Psat= 0.0984223147233327 \n",
      "acc for optim= 0.13876834969139765\n",
      "Epoch:836/1000\n",
      "Loss on train= 0.008881689049303532\n",
      "Loss on test= 0.01593608781695366\n",
      "acc for Lsat= 0.06595705449581146 \n",
      "acc for Psat= 0.1000044263071484 \n",
      "acc for optim= 0.1389297574137648\n",
      "Epoch:837/1000\n",
      "Loss on train= 0.00892500951886177\n",
      "Loss on test= 0.015337560325860977\n",
      "acc for Lsat= 0.06516996439960268 \n",
      "acc for Psat= 0.09654432733853657 \n",
      "acc for optim= 0.1408478015412887\n",
      "Epoch:838/1000\n",
      "Loss on train= 0.009127994999289513\n",
      "Loss on test= 0.01503083948045969\n",
      "acc for Lsat= 0.0660057180457645 \n",
      "acc for Psat= 0.10200319323274826 \n",
      "acc for optim= 0.13856895138613054\n",
      "Epoch:839/1000\n",
      "Loss on train= 0.008781865239143372\n",
      "Loss on test= 0.015207510441541672\n",
      "acc for Lsat= 0.06760835200548171 \n",
      "acc for Psat= 0.09593843039539124 \n",
      "acc for optim= 0.13805523487842744\n",
      "Epoch:840/1000\n",
      "Loss on train= 0.008948327042162418\n",
      "Loss on test= 0.016316208988428116\n",
      "acc for Lsat= 0.06873749097188314 \n",
      "acc for Psat= 0.0960889894101355 \n",
      "acc for optim= 0.13772813516358534\n",
      "Epoch:841/1000\n",
      "Loss on train= 0.008934096433222294\n",
      "Loss on test= 0.016669990494847298\n",
      "acc for Lsat= 0.06910317507055071 \n",
      "acc for Psat= 0.09828397350178825 \n",
      "acc for optim= 0.13895094871469255\n",
      "Epoch:842/1000\n",
      "Loss on train= 0.009161537513136864\n",
      "Loss on test= 0.015900878235697746\n",
      "acc for Lsat= 0.06782645301686394 \n",
      "acc for Psat= 0.09582697318659888 \n",
      "acc for optim= 0.14000459520353215\n",
      "Epoch:843/1000\n",
      "Loss on train= 0.009238973259925842\n",
      "Loss on test= 0.01626167818903923\n",
      "acc for Lsat= 0.06874211198753782 \n",
      "acc for Psat= 0.10110025571452247 \n",
      "acc for optim= 0.14011756777763365\n",
      "Epoch:844/1000\n",
      "Loss on train= 0.00890918355435133\n",
      "Loss on test= 0.01632167026400566\n",
      "acc for Lsat= 0.06927017668883005 \n",
      "acc for Psat= 0.10196546663840611 \n",
      "acc for optim= 0.1390750113874674\n",
      "Epoch:845/1000\n",
      "Loss on train= 0.008970866911113262\n",
      "Loss on test= 0.014903907664120197\n",
      "acc for Lsat= 0.07045953157875273 \n",
      "acc for Psat= 0.10004421687788434 \n",
      "acc for optim= 0.1394707483963834\n",
      "Epoch:846/1000\n",
      "Loss on train= 0.008869263343513012\n",
      "Loss on test= 0.01651744544506073\n",
      "acc for Lsat= 0.07027373810609182 \n",
      "acc for Psat= 0.10749028027057649 \n",
      "acc for optim= 0.14083555036534864\n",
      "Epoch:847/1000\n",
      "Loss on train= 0.009645979851484299\n",
      "Loss on test= 0.016332175582647324\n",
      "acc for Lsat= 0.07117834836244584 \n",
      "acc for Psat= 0.1041325075758828 \n",
      "acc for optim= 0.13891504879898597\n",
      "Epoch:848/1000\n",
      "Loss on train= 0.009247672744095325\n",
      "Loss on test= 0.01611357368528843\n",
      "acc for Lsat= 0.06786830342478223 \n",
      "acc for Psat= 0.10944011973010169 \n",
      "acc for optim= 0.14003784636863403\n",
      "Epoch:849/1000\n",
      "Loss on train= 0.008918817155063152\n",
      "Loss on test= 0.016138602048158646\n",
      "acc for Lsat= 0.0692783623933792 \n",
      "acc for Psat= 0.1020373132493761 \n",
      "acc for optim= 0.14001205157902505\n",
      "Epoch:850/1000\n",
      "Loss on train= 0.00912292767316103\n",
      "Loss on test= 0.015899404883384705\n",
      "acc for Lsat= 0.06863047397798963 \n",
      "acc for Psat= 0.09927108105685974 \n",
      "acc for optim= 0.14078048701501555\n",
      "Epoch:851/1000\n",
      "Loss on train= 0.008926528505980968\n",
      "Loss on test= 0.016585521399974823\n",
      "acc for Lsat= 0.06988817403713861 \n",
      "acc for Psat= 0.10973805520269606 \n",
      "acc for optim= 0.13898835835150544\n",
      "Epoch:852/1000\n",
      "Loss on train= 0.009093228727579117\n",
      "Loss on test= 0.01671174168586731\n",
      "acc for Lsat= 0.07293271968762079 \n",
      "acc for Psat= 0.10861577788988748 \n",
      "acc for optim= 0.13950854448808564\n",
      "Epoch:853/1000\n",
      "Loss on train= 0.009112874045968056\n",
      "Loss on test= 0.01573154889047146\n",
      "acc for Lsat= 0.068257884018951 \n",
      "acc for Psat= 0.09778978808058633 \n",
      "acc for optim= 0.14199141179107955\n",
      "Epoch:854/1000\n",
      "Loss on train= 0.009333598427474499\n",
      "Loss on test= 0.016493268311023712\n",
      "acc for Lsat= 0.07050160583522586 \n",
      "acc for Psat= 0.10552222165796492 \n",
      "acc for optim= 0.13957052311549584\n",
      "Epoch:855/1000\n",
      "Loss on train= 0.008986777625977993\n",
      "Loss on test= 0.016892891377210617\n",
      "acc for Lsat= 0.0695876161257426 \n",
      "acc for Psat= 0.09861911038557687 \n",
      "acc for optim= 0.14062399427509967\n",
      "Epoch:856/1000\n",
      "Loss on train= 0.008776159025728703\n",
      "Loss on test= 0.01613352634012699\n",
      "acc for Lsat= 0.06969875279400084 \n",
      "acc for Psat= 0.09781470364994474 \n",
      "acc for optim= 0.13816576967688485\n",
      "Epoch:857/1000\n",
      "Loss on train= 0.008764920756220818\n",
      "Loss on test= 0.016828861087560654\n",
      "acc for Lsat= 0.06761688904629813 \n",
      "acc for Psat= 0.10162448287010192 \n",
      "acc for optim= 0.14059430352515645\n",
      "Epoch:858/1000\n",
      "Loss on train= 0.008759131655097008\n",
      "Loss on test= 0.016151446849107742\n",
      "acc for Lsat= 0.06676473634110557 \n",
      "acc for Psat= 0.09452680548032126 \n",
      "acc for optim= 0.14045906623618473\n",
      "Epoch:859/1000\n",
      "Loss on train= 0.008707904256880283\n",
      "Loss on test= 0.015553398057818413\n",
      "acc for Lsat= 0.06752377715375688 \n",
      "acc for Psat= 0.09775552418496872 \n",
      "acc for optim= 0.1377781900835948\n",
      "Epoch:860/1000\n",
      "Loss on train= 0.00889844074845314\n",
      "Loss on test= 0.01722586713731289\n",
      "acc for Lsat= 0.07112729019588894 \n",
      "acc for Psat= 0.10426719586054484 \n",
      "acc for optim= 0.13984638655351272\n",
      "Epoch:861/1000\n",
      "Loss on train= 0.009010822512209415\n",
      "Loss on test= 0.017368923872709274\n",
      "acc for Lsat= 0.06760368661748038 \n",
      "acc for Psat= 0.09540212783548568 \n",
      "acc for optim= 0.14027154313193427\n",
      "Epoch:862/1000\n",
      "Loss on train= 0.00875119585543871\n",
      "Loss on test= 0.016339194029569626\n",
      "acc for Lsat= 0.07377885083357494 \n",
      "acc for Psat= 0.10333480354812412 \n",
      "acc for optim= 0.14185017235577105\n",
      "Epoch:863/1000\n",
      "Loss on train= 0.008992642164230347\n",
      "Loss on test= 0.016153782606124878\n",
      "acc for Lsat= 0.06839785907003615 \n",
      "acc for Psat= 0.09820892512798308 \n",
      "acc for optim= 0.14102961561746066\n",
      "Epoch:864/1000\n",
      "Loss on train= 0.008972495794296265\n",
      "Loss on test= 0.01527420338243246\n",
      "acc for Lsat= 0.06856980572144192 \n",
      "acc for Psat= 0.10563765631781685 \n",
      "acc for optim= 0.1409548978010814\n",
      "Epoch:865/1000\n",
      "Loss on train= 0.009174996986985207\n",
      "Loss on test= 0.01687578484416008\n",
      "acc for Lsat= 0.07522473682959874 \n",
      "acc for Psat= 0.10589394321044285 \n",
      "acc for optim= 0.1403792582038376\n",
      "Epoch:866/1000\n",
      "Loss on train= 0.009046248160302639\n",
      "Loss on test= 0.015987271443009377\n",
      "acc for Lsat= 0.07030956877602472 \n",
      "acc for Psat= 0.09350209931532542 \n",
      "acc for optim= 0.14121309880995087\n",
      "Epoch:867/1000\n",
      "Loss on train= 0.008715764619410038\n",
      "Loss on test= 0.015420500189065933\n",
      "acc for Lsat= 0.06472487068838544 \n",
      "acc for Psat= 0.09437016348044078 \n",
      "acc for optim= 0.14003125754081544\n",
      "Epoch:868/1000\n",
      "Loss on train= 0.008904628455638885\n",
      "Loss on test= 0.015844590961933136\n",
      "acc for Lsat= 0.06607454220453898 \n",
      "acc for Psat= 0.09646858639187283 \n",
      "acc for optim= 0.1410992498199145\n",
      "Epoch:869/1000\n",
      "Loss on train= 0.008655763231217861\n",
      "Loss on test= 0.015690956264734268\n",
      "acc for Lsat= 0.07027632958359188 \n",
      "acc for Psat= 0.09567396607663897 \n",
      "acc for optim= 0.1393853340918819\n",
      "Epoch:870/1000\n",
      "Loss on train= 0.00920728500932455\n",
      "Loss on test= 0.016017312183976173\n",
      "acc for Lsat= 0.07332416027784347 \n",
      "acc for Psat= 0.10086995462576548 \n",
      "acc for optim= 0.1403971350648337\n",
      "Epoch:871/1000\n",
      "Loss on train= 0.008828348480165005\n",
      "Loss on test= 0.017092779278755188\n",
      "acc for Lsat= 0.06902682102388807 \n",
      "acc for Psat= 0.1009641198648347 \n",
      "acc for optim= 0.1406523759994242\n",
      "Epoch:872/1000\n",
      "Loss on train= 0.008928322233259678\n",
      "Loss on test= 0.015434147790074348\n",
      "acc for Lsat= 0.07010008560286628 \n",
      "acc for Psat= 0.09946867509020699 \n",
      "acc for optim= 0.1401827576984134\n",
      "Epoch:873/1000\n",
      "Loss on train= 0.008865827694535255\n",
      "Loss on test= 0.016040466725826263\n",
      "acc for Lsat= 0.06748938974406984 \n",
      "acc for Psat= 0.0961432178815206 \n",
      "acc for optim= 0.14062876858645015\n",
      "Epoch:874/1000\n",
      "Loss on train= 0.008682962507009506\n",
      "Loss on test= 0.01584382727742195\n",
      "acc for Lsat= 0.06861173361539841 \n",
      "acc for Psat= 0.100869130425983 \n",
      "acc for optim= 0.1387807104840047\n",
      "Epoch:875/1000\n",
      "Loss on train= 0.00881035253405571\n",
      "Loss on test= 0.01700657233595848\n",
      "acc for Lsat= 0.0676076074441274 \n",
      "acc for Psat= 0.09535620162884394 \n",
      "acc for optim= 0.14072177263183727\n",
      "Epoch:876/1000\n",
      "Loss on train= 0.00929662398993969\n",
      "Loss on test= 0.015913987532258034\n",
      "acc for Lsat= 0.06581577509641647 \n",
      "acc for Psat= 0.1024150205983056 \n",
      "acc for optim= 0.14127775273389287\n",
      "Epoch:877/1000\n",
      "Loss on train= 0.009219050407409668\n",
      "Loss on test= 0.01621251367032528\n",
      "acc for Lsat= 0.0668669021791882 \n",
      "acc for Psat= 0.09365616457329856 \n",
      "acc for optim= 0.14124937334822282\n",
      "Epoch:878/1000\n",
      "Loss on train= 0.008836625143885612\n",
      "Loss on test= 0.016112638637423515\n",
      "acc for Lsat= 0.07256806062327492 \n",
      "acc for Psat= 0.09677488671408757 \n",
      "acc for optim= 0.1401460834882326\n",
      "Epoch:879/1000\n",
      "Loss on train= 0.00909801758825779\n",
      "Loss on test= 0.016216279938817024\n",
      "acc for Lsat= 0.07162852568758858 \n",
      "acc for Psat= 0.0979916195074717 \n",
      "acc for optim= 0.13944234715567697\n",
      "Epoch:880/1000\n",
      "Loss on train= 0.008837422356009483\n",
      "Loss on test= 0.01551620289683342\n",
      "acc for Lsat= 0.07195626695950824 \n",
      "acc for Psat= 0.10023807154761422 \n",
      "acc for optim= 0.14233103286888862\n",
      "Epoch:881/1000\n",
      "Loss on train= 0.008686934597790241\n",
      "Loss on test= 0.016225801780819893\n",
      "acc for Lsat= 0.06877546691232256 \n",
      "acc for Psat= 0.10061255461639827 \n",
      "acc for optim= 0.14041475750919843\n",
      "Epoch:882/1000\n",
      "Loss on train= 0.008643871173262596\n",
      "Loss on test= 0.01592816598713398\n",
      "acc for Lsat= 0.07069826241996553 \n",
      "acc for Psat= 0.09698002901342181 \n",
      "acc for optim= 0.14247374907135965\n",
      "Epoch:883/1000\n",
      "Loss on train= 0.008695010095834732\n",
      "Loss on test= 0.01601618528366089\n",
      "acc for Lsat= 0.07296142412556543 \n",
      "acc for Psat= 0.09478013780381943 \n",
      "acc for optim= 0.14036741556806695\n",
      "Epoch:884/1000\n",
      "Loss on train= 0.008951527066528797\n",
      "Loss on test= 0.017983468249440193\n",
      "acc for Lsat= 0.06934398843182459 \n",
      "acc for Psat= 0.0963266564740075 \n",
      "acc for optim= 0.141911749003662\n",
      "Epoch:885/1000\n",
      "Loss on train= 0.008770493790507317\n",
      "Loss on test= 0.016354167833924294\n",
      "acc for Lsat= 0.0708388974269231 \n",
      "acc for Psat= 0.09561965995364717 \n",
      "acc for optim= 0.14006064414150182\n",
      "Epoch:886/1000\n",
      "Loss on train= 0.008795865811407566\n",
      "Loss on test= 0.015565996058285236\n",
      "acc for Lsat= 0.07029110292593638 \n",
      "acc for Psat= 0.09896085411310197 \n",
      "acc for optim= 0.1407282816246152\n",
      "Epoch:887/1000\n",
      "Loss on train= 0.008912039920687675\n",
      "Loss on test= 0.016626276075839996\n",
      "acc for Lsat= 0.06889618006017474 \n",
      "acc for Psat= 0.09960752427577972 \n",
      "acc for optim= 0.1419953399234348\n",
      "Epoch:888/1000\n",
      "Loss on train= 0.00917048379778862\n",
      "Loss on test= 0.01631896011531353\n",
      "acc for Lsat= 0.06974185953537623 \n",
      "acc for Psat= 0.091980532473988 \n",
      "acc for optim= 0.1397385561838746\n",
      "Epoch:889/1000\n",
      "Loss on train= 0.008763836696743965\n",
      "Loss on test= 0.016980916261672974\n",
      "acc for Lsat= 0.06453452342086369 \n",
      "acc for Psat= 0.09380183964967728 \n",
      "acc for optim= 0.13999539764804972\n",
      "Epoch:890/1000\n",
      "Loss on train= 0.008970038965344429\n",
      "Loss on test= 0.016134431585669518\n",
      "acc for Lsat= 0.06844084246291054 \n",
      "acc for Psat= 0.09620240181684495 \n",
      "acc for optim= 0.14049056580083238\n",
      "Epoch:891/1000\n",
      "Loss on train= 0.009132233448326588\n",
      "Loss on test= 0.016225002706050873\n",
      "acc for Lsat= 0.06604078991545571 \n",
      "acc for Psat= 0.0956818343864547 \n",
      "acc for optim= 0.14079781340228192\n",
      "Epoch:892/1000\n",
      "Loss on train= 0.00900302641093731\n",
      "Loss on test= 0.015729527920484543\n",
      "acc for Lsat= 0.06573340776893827 \n",
      "acc for Psat= 0.09541199836466047 \n",
      "acc for optim= 0.1397643756367163\n",
      "Epoch:893/1000\n",
      "Loss on train= 0.008893218822777271\n",
      "Loss on test= 0.016480693593621254\n",
      "acc for Lsat= 0.06650720602936215 \n",
      "acc for Psat= 0.10631170107258689 \n",
      "acc for optim= 0.14015867298261986\n",
      "Epoch:894/1000\n",
      "Loss on train= 0.008751440793275833\n",
      "Loss on test= 0.015987439081072807\n",
      "acc for Lsat= 0.07235781864987478 \n",
      "acc for Psat= 0.10195347368717193 \n",
      "acc for optim= 0.13844250637210076\n",
      "Epoch:895/1000\n",
      "Loss on train= 0.008977211080491543\n",
      "Loss on test= 0.01515278872102499\n",
      "acc for Lsat= 0.07594546824693679 \n",
      "acc for Psat= 0.10282268093691932 \n",
      "acc for optim= 0.13893622412449785\n",
      "Epoch:896/1000\n",
      "Loss on train= 0.009044591337442398\n",
      "Loss on test= 0.015651898458600044\n",
      "acc for Lsat= 0.06816124237245985 \n",
      "acc for Psat= 0.10363707178168827 \n",
      "acc for optim= 0.14044859612153637\n",
      "Epoch:897/1000\n",
      "Loss on train= 0.008716360665857792\n",
      "Loss on test= 0.016719801351428032\n",
      "acc for Lsat= 0.06734985030359691 \n",
      "acc for Psat= 0.09987111936012903 \n",
      "acc for optim= 0.14083806888924708\n",
      "Epoch:898/1000\n",
      "Loss on train= 0.00928563717752695\n",
      "Loss on test= 0.01629345864057541\n",
      "acc for Lsat= 0.07088820023669136 \n",
      "acc for Psat= 0.0970440364546246 \n",
      "acc for optim= 0.14189979086319604\n",
      "Epoch:899/1000\n",
      "Loss on train= 0.008707979694008827\n",
      "Loss on test= 0.015999699011445045\n",
      "acc for Lsat= 0.06694384862979254 \n",
      "acc for Psat= 0.09684693680869208 \n",
      "acc for optim= 0.14019228377275997\n",
      "Epoch:900/1000\n",
      "Loss on train= 0.008871604688465595\n",
      "Loss on test= 0.016317225992679596\n",
      "acc for Lsat= 0.06942993071344164 \n",
      "acc for Psat= 0.09468473113245433 \n",
      "acc for optim= 0.13971090676883857\n",
      "Epoch:901/1000\n",
      "Loss on train= 0.00903545692563057\n",
      "Loss on test= 0.016624296084046364\n",
      "acc for Lsat= 0.06934197313255734 \n",
      "acc for Psat= 0.09613435020049416 \n",
      "acc for optim= 0.1423926864647203\n",
      "Epoch:902/1000\n",
      "Loss on train= 0.008908097632229328\n",
      "Loss on test= 0.01616213470697403\n",
      "acc for Lsat= 0.07036603656080034 \n",
      "acc for Psat= 0.09930033600992626 \n",
      "acc for optim= 0.14078278607792324\n",
      "Epoch:903/1000\n",
      "Loss on train= 0.008928324095904827\n",
      "Loss on test= 0.01611923612654209\n",
      "acc for Lsat= 0.0713892976442973 \n",
      "acc for Psat= 0.09501405159632365 \n",
      "acc for optim= 0.14127990189525816\n",
      "Epoch:904/1000\n",
      "Loss on train= 0.008990884758532047\n",
      "Loss on test= 0.015231652185320854\n",
      "acc for Lsat= 0.06639326396915646 \n",
      "acc for Psat= 0.0966261843840281 \n",
      "acc for optim= 0.13980234836538633\n",
      "Epoch:905/1000\n",
      "Loss on train= 0.00898958183825016\n",
      "Loss on test= 0.016374288126826286\n",
      "acc for Lsat= 0.0695674576693111 \n",
      "acc for Psat= 0.09887038386530345 \n",
      "acc for optim= 0.13913569849812324\n",
      "Epoch:906/1000\n",
      "Loss on train= 0.008706901222467422\n",
      "Loss on test= 0.015211272984743118\n",
      "acc for Lsat= 0.06683492081032859 \n",
      "acc for Psat= 0.09659487985902364 \n",
      "acc for optim= 0.13862831600838235\n",
      "Epoch:907/1000\n",
      "Loss on train= 0.008845219388604164\n",
      "Loss on test= 0.016725121065974236\n",
      "acc for Lsat= 0.06595032082663642 \n",
      "acc for Psat= 0.09350842204358842 \n",
      "acc for optim= 0.14074404775682423\n",
      "Epoch:908/1000\n",
      "Loss on train= 0.008556990884244442\n",
      "Loss on test= 0.01561952754855156\n",
      "acc for Lsat= 0.06613889386256536 \n",
      "acc for Psat= 0.09542109072208405 \n",
      "acc for optim= 0.13922429783269763\n",
      "Epoch:909/1000\n",
      "Loss on train= 0.009048638865351677\n",
      "Loss on test= 0.01660139299929142\n",
      "acc for Lsat= 0.07065480550130208 \n",
      "acc for Psat= 0.0970205914643076 \n",
      "acc for optim= 0.1398646785567204\n",
      "Epoch:910/1000\n",
      "Loss on train= 0.008630561642348766\n",
      "Loss on test= 0.01613793708384037\n",
      "acc for Lsat= 0.07279378672440848 \n",
      "acc for Psat= 0.09708634598387612 \n",
      "acc for optim= 0.1381415353053146\n",
      "Epoch:911/1000\n",
      "Loss on train= 0.009158050641417503\n",
      "Loss on test= 0.016368545591831207\n",
      "acc for Lsat= 0.07105924801694023 \n",
      "acc for Psat= 0.10290235247876907 \n",
      "acc for optim= 0.14019967301024333\n",
      "Epoch:912/1000\n",
      "Loss on train= 0.008847039192914963\n",
      "Loss on test= 0.01611674204468727\n",
      "acc for Lsat= 0.06838123814927206 \n",
      "acc for Psat= 0.10397208862834506 \n",
      "acc for optim= 0.1400538770481944\n",
      "Epoch:913/1000\n",
      "Loss on train= 0.009081750176846981\n",
      "Loss on test= 0.01655539683997631\n",
      "acc for Lsat= 0.0669751803080241 \n",
      "acc for Psat= 0.09600546144776873 \n",
      "acc for optim= 0.14215240486794048\n",
      "Epoch:914/1000\n",
      "Loss on train= 0.0086897574365139\n",
      "Loss on test= 0.015825863927602768\n",
      "acc for Lsat= 0.06648853570222855 \n",
      "acc for Psat= 0.09477554708719253 \n",
      "acc for optim= 0.141353410979112\n",
      "Epoch:915/1000\n",
      "Loss on train= 0.008829319849610329\n",
      "Loss on test= 0.0173498447984457\n",
      "acc for Lsat= 0.07404291662904951 \n",
      "acc for Psat= 0.10443531490034526 \n",
      "acc for optim= 0.14121179268178013\n",
      "Epoch:916/1000\n",
      "Loss on train= 0.00869195070117712\n",
      "Loss on test= 0.015003742650151253\n",
      "acc for Lsat= 0.06979713704850939 \n",
      "acc for Psat= 0.09678676641649671 \n",
      "acc for optim= 0.14056507771213847\n",
      "Epoch:917/1000\n",
      "Loss on train= 0.008977729827165604\n",
      "Loss on test= 0.01566174440085888\n",
      "acc for Lsat= 0.06946979893578423 \n",
      "acc for Psat= 0.09901165001922183 \n",
      "acc for optim= 0.13936734669324424\n",
      "Epoch:918/1000\n",
      "Loss on train= 0.008897248655557632\n",
      "Loss on test= 0.01608002930879593\n",
      "acc for Lsat= 0.0680726259946823 \n",
      "acc for Psat= 0.09694867812924914 \n",
      "acc for optim= 0.1392964582890272\n",
      "Epoch:919/1000\n",
      "Loss on train= 0.008792120963335037\n",
      "Loss on test= 0.016810428351163864\n",
      "acc for Lsat= 0.07121697035100726 \n",
      "acc for Psat= 0.10376410252518123 \n",
      "acc for optim= 0.1392434291334616\n",
      "Epoch:920/1000\n",
      "Loss on train= 0.008930143900215626\n",
      "Loss on test= 0.016457758843898773\n",
      "acc for Lsat= 0.06934649778736962 \n",
      "acc for Psat= 0.09740660985310871 \n",
      "acc for optim= 0.14105775993731287\n",
      "Epoch:921/1000\n",
      "Loss on train= 0.008720039390027523\n",
      "Loss on test= 0.016437795013189316\n",
      "acc for Lsat= 0.06792653732829623 \n",
      "acc for Psat= 0.09958776334921518 \n",
      "acc for optim= 0.14062464143046072\n",
      "Epoch:922/1000\n",
      "Loss on train= 0.008381186053156853\n",
      "Loss on test= 0.01591574214398861\n",
      "acc for Lsat= 0.06500774522622425 \n",
      "acc for Psat= 0.09461966603994368 \n",
      "acc for optim= 0.14022620430009233\n",
      "Epoch:923/1000\n",
      "Loss on train= 0.008765295147895813\n",
      "Loss on test= 0.01613791659474373\n",
      "acc for Lsat= 0.0676856815814972 \n",
      "acc for Psat= 0.0946118242210812 \n",
      "acc for optim= 0.13890308136534357\n",
      "Epoch:924/1000\n",
      "Loss on train= 0.008544398471713066\n",
      "Loss on test= 0.015588992275297642\n",
      "acc for Lsat= 0.07269937760300108 \n",
      "acc for Psat= 0.09641794645124009 \n",
      "acc for optim= 0.1399966540022029\n",
      "Epoch:925/1000\n",
      "Loss on train= 0.00849683117121458\n",
      "Loss on test= 0.016149025410413742\n",
      "acc for Lsat= 0.06645605514446894 \n",
      "acc for Psat= 0.09766529434257083 \n",
      "acc for optim= 0.1394983953278926\n",
      "Epoch:926/1000\n",
      "Loss on train= 0.00863364152610302\n",
      "Loss on test= 0.01563924364745617\n",
      "acc for Lsat= 0.06779200169775221 \n",
      "acc for Psat= 0.09470847447713217 \n",
      "acc for optim= 0.1398133626000749\n",
      "Epoch:927/1000\n",
      "Loss on train= 0.008785377256572247\n",
      "Loss on test= 0.015955694019794464\n",
      "acc for Lsat= 0.06844324552350574 \n",
      "acc for Psat= 0.09281704475482305 \n",
      "acc for optim= 0.1397349875834253\n",
      "Epoch:928/1000\n",
      "Loss on train= 0.008958139456808567\n",
      "Loss on test= 0.016372989863157272\n",
      "acc for Lsat= 0.0653630405664444 \n",
      "acc for Psat= 0.09455705516868167 \n",
      "acc for optim= 0.13948036479867165\n",
      "Epoch:929/1000\n",
      "Loss on train= 0.008811204694211483\n",
      "Loss on test= 0.015867486596107483\n",
      "acc for Lsat= 0.0685771041446262 \n",
      "acc for Psat= 0.09667744802104103 \n",
      "acc for optim= 0.13871853428168432\n",
      "Epoch:930/1000\n",
      "Loss on train= 0.008759882301092148\n",
      "Loss on test= 0.016538430005311966\n",
      "acc for Lsat= 0.06835342976782058 \n",
      "acc for Psat= 0.09330820375018649 \n",
      "acc for optim= 0.14215569549964535\n",
      "Epoch:931/1000\n",
      "Loss on train= 0.008717781864106655\n",
      "Loss on test= 0.01643463596701622\n",
      "acc for Lsat= 0.06667806026008394 \n",
      "acc for Psat= 0.09964530716339746 \n",
      "acc for optim= 0.14022192019555307\n",
      "Epoch:932/1000\n",
      "Loss on train= 0.008963923901319504\n",
      "Loss on test= 0.01737876795232296\n",
      "acc for Lsat= 0.06659180902772481 \n",
      "acc for Psat= 0.10095710837178759 \n",
      "acc for optim= 0.1404718358069658\n",
      "Epoch:933/1000\n",
      "Loss on train= 0.008696329779922962\n",
      "Loss on test= 0.015512671321630478\n",
      "acc for Lsat= 0.06984775116046268 \n",
      "acc for Psat= 0.09784706864092085 \n",
      "acc for optim= 0.1385889986116025\n",
      "Epoch:934/1000\n",
      "Loss on train= 0.008722356520593166\n",
      "Loss on test= 0.01558422576636076\n",
      "acc for Lsat= 0.0672237969107098 \n",
      "acc for Psat= 0.09467803107367621 \n",
      "acc for optim= 0.1402303338050842\n",
      "Epoch:935/1000\n",
      "Loss on train= 0.00896479468792677\n",
      "Loss on test= 0.015401259064674377\n",
      "acc for Lsat= 0.06550023704767227 \n",
      "acc for Psat= 0.09511870907412634 \n",
      "acc for optim= 0.13906511925160883\n",
      "Epoch:936/1000\n",
      "Loss on train= 0.008966848254203796\n",
      "Loss on test= 0.016617892310023308\n",
      "acc for Lsat= 0.06906867755783928 \n",
      "acc for Psat= 0.10401759379439884 \n",
      "acc for optim= 0.13938973897861112\n",
      "Epoch:937/1000\n",
      "Loss on train= 0.008710465393960476\n",
      "Loss on test= 0.015471319667994976\n",
      "acc for Lsat= 0.07063826951715682 \n",
      "acc for Psat= 0.09578267551130719 \n",
      "acc for optim= 0.13947978487445248\n",
      "Epoch:938/1000\n",
      "Loss on train= 0.008983657695353031\n",
      "Loss on test= 0.01619161292910576\n",
      "acc for Lsat= 0.06864843484428193 \n",
      "acc for Psat= 0.09415621558825175 \n",
      "acc for optim= 0.13863785607973117\n",
      "Epoch:939/1000\n",
      "Loss on train= 0.008708016015589237\n",
      "Loss on test= 0.015484543517231941\n",
      "acc for Lsat= 0.0742361358470387 \n",
      "acc for Psat= 0.09873183038499621 \n",
      "acc for optim= 0.1404923444406854\n",
      "Epoch:940/1000\n",
      "Loss on train= 0.008512202650308609\n",
      "Loss on test= 0.015525009483098984\n",
      "acc for Lsat= 0.06504680580563016 \n",
      "acc for Psat= 0.09901736312442355 \n",
      "acc for optim= 0.14129371990760167\n",
      "Epoch:941/1000\n",
      "Loss on train= 0.008743497543036938\n",
      "Loss on test= 0.016988731920719147\n",
      "acc for Lsat= 0.07188478459914525 \n",
      "acc for Psat= 0.09858395755290984 \n",
      "acc for optim= 0.14106017365637755\n",
      "Epoch:942/1000\n",
      "Loss on train= 0.008734666742384434\n",
      "Loss on test= 0.015531846322119236\n",
      "acc for Lsat= 0.07008154210117129 \n",
      "acc for Psat= 0.10080907179249658 \n",
      "acc for optim= 0.14138320394688184\n",
      "Epoch:943/1000\n",
      "Loss on train= 0.008362870663404465\n",
      "Loss on test= 0.016032852232456207\n",
      "acc for Lsat= 0.06801305661598842 \n",
      "acc for Psat= 0.10139694313208263 \n",
      "acc for optim= 0.14061866017679372\n",
      "Epoch:944/1000\n",
      "Loss on train= 0.008438598364591599\n",
      "Loss on test= 0.016901426017284393\n",
      "acc for Lsat= 0.0669798203640514 \n",
      "acc for Psat= 0.09885566516055001 \n",
      "acc for optim= 0.14083170783188603\n",
      "Epoch:945/1000\n",
      "Loss on train= 0.00868216622620821\n",
      "Loss on test= 0.016569728031754494\n",
      "acc for Lsat= 0.06774603244331148 \n",
      "acc for Psat= 0.09951375093724993 \n",
      "acc for optim= 0.14001616202294828\n",
      "Epoch:946/1000\n",
      "Loss on train= 0.008535587228834629\n",
      "Loss on test= 0.016172869130969048\n",
      "acc for Lsat= 0.06595769458346896 \n",
      "acc for Psat= 0.09469089524613486 \n",
      "acc for optim= 0.13952859470413792\n",
      "Epoch:947/1000\n",
      "Loss on train= 0.008404628373682499\n",
      "Loss on test= 0.017406467348337173\n",
      "acc for Lsat= 0.06628074910905626 \n",
      "acc for Psat= 0.09522956427600648 \n",
      "acc for optim= 0.14052897596524824\n",
      "Epoch:948/1000\n",
      "Loss on train= 0.00907324068248272\n",
      "Loss on test= 0.0169311985373497\n",
      "acc for Lsat= 0.06745066195726396 \n",
      "acc for Psat= 0.10067906131347021 \n",
      "acc for optim= 0.1405201342784696\n",
      "Epoch:949/1000\n",
      "Loss on train= 0.008808717131614685\n",
      "Loss on test= 0.0160411074757576\n",
      "acc for Lsat= 0.07033613903654946 \n",
      "acc for Psat= 0.09811184042029913 \n",
      "acc for optim= 0.1407404516720109\n",
      "Epoch:950/1000\n",
      "Loss on train= 0.008684157393872738\n",
      "Loss on test= 0.01559925451874733\n",
      "acc for Lsat= 0.06886163453261057 \n",
      "acc for Psat= 0.09543550064166387 \n",
      "acc for optim= 0.14006927760524884\n",
      "Epoch:951/1000\n",
      "Loss on train= 0.008851646445691586\n",
      "Loss on test= 0.016404785215854645\n",
      "acc for Lsat= 0.07028263078795538 \n",
      "acc for Psat= 0.09925456990798316 \n",
      "acc for optim= 0.13933823659188216\n",
      "Epoch:952/1000\n",
      "Loss on train= 0.008917962200939655\n",
      "Loss on test= 0.01633302867412567\n",
      "acc for Lsat= 0.06668528980678981 \n",
      "acc for Psat= 0.09501076721482805 \n",
      "acc for optim= 0.14227700229320261\n",
      "Epoch:953/1000\n",
      "Loss on train= 0.008610951714217663\n",
      "Loss on test= 0.016795633360743523\n",
      "acc for Lsat= 0.07180161525805791 \n",
      "acc for Psat= 0.10064108139938777 \n",
      "acc for optim= 0.14089827657573756\n",
      "Epoch:954/1000\n",
      "Loss on train= 0.00869066920131445\n",
      "Loss on test= 0.01643672212958336\n",
      "acc for Lsat= 0.06846602873669731 \n",
      "acc for Psat= 0.09810781445768144 \n",
      "acc for optim= 0.14118003884537356\n",
      "Epoch:955/1000\n",
      "Loss on train= 0.008886093273758888\n",
      "Loss on test= 0.016195887699723244\n",
      "acc for Lsat= 0.07442429694864484 \n",
      "acc for Psat= 0.10097505102554957 \n",
      "acc for optim= 0.14054370168596508\n",
      "Epoch:956/1000\n",
      "Loss on train= 0.009032972157001495\n",
      "Loss on test= 0.01554181519895792\n",
      "acc for Lsat= 0.0733937735358874 \n",
      "acc for Psat= 0.09875400960445405 \n",
      "acc for optim= 0.14110883621291984\n",
      "Epoch:957/1000\n",
      "Loss on train= 0.008748609572649002\n",
      "Loss on test= 0.01677626371383667\n",
      "acc for Lsat= 0.07737835099299747 \n",
      "acc for Psat= 0.10751264525784388 \n",
      "acc for optim= 0.14026924806336563\n",
      "Epoch:958/1000\n",
      "Loss on train= 0.008745706640183926\n",
      "Loss on test= 0.015964722260832787\n",
      "acc for Lsat= 0.06763385865423414 \n",
      "acc for Psat= 0.10068696604834663 \n",
      "acc for optim= 0.14050177422662577\n",
      "Epoch:959/1000\n",
      "Loss on train= 0.008894313126802444\n",
      "Loss on test= 0.017177008092403412\n",
      "acc for Lsat= 0.07352708114518061 \n",
      "acc for Psat= 0.10291408581866159 \n",
      "acc for optim= 0.1419354982674122\n",
      "Epoch:960/1000\n",
      "Loss on train= 0.008562518283724785\n",
      "Loss on test= 0.016370665282011032\n",
      "acc for Lsat= 0.07036278098821641 \n",
      "acc for Psat= 0.10115183956093257 \n",
      "acc for optim= 0.1413906121833457\n",
      "Epoch:961/1000\n",
      "Loss on train= 0.00924456212669611\n",
      "Loss on test= 0.01640392653644085\n",
      "acc for Lsat= 0.07601334734095468 \n",
      "acc for Psat= 0.10039267639319102 \n",
      "acc for optim= 0.142970877347721\n",
      "Epoch:962/1000\n",
      "Loss on train= 0.00866975262761116\n",
      "Loss on test= 0.016183458268642426\n",
      "acc for Lsat= 0.06569968842797809 \n",
      "acc for Psat= 0.10479617648654511 \n",
      "acc for optim= 0.1404808553556601\n",
      "Epoch:963/1000\n",
      "Loss on train= 0.009037843905389309\n",
      "Loss on test= 0.016159718856215477\n",
      "acc for Lsat= 0.07168009479840595 \n",
      "acc for Psat= 0.10249638524320392 \n",
      "acc for optim= 0.14132284331652853\n",
      "Epoch:964/1000\n",
      "Loss on train= 0.008818409405648708\n",
      "Loss on test= 0.01633247174322605\n",
      "acc for Lsat= 0.0681453835633066 \n",
      "acc for Psat= 0.09739844120211073 \n",
      "acc for optim= 0.13850820685426396\n",
      "Epoch:965/1000\n",
      "Loss on train= 0.008647138252854347\n",
      "Loss on test= 0.016881020739674568\n",
      "acc for Lsat= 0.0683409238855044 \n",
      "acc for Psat= 0.10002718998326195 \n",
      "acc for optim= 0.13995190821588038\n",
      "Epoch:966/1000\n",
      "Loss on train= 0.008563029579818249\n",
      "Loss on test= 0.01617073453962803\n",
      "acc for Lsat= 0.06665444324413934 \n",
      "acc for Psat= 0.09798733327123854 \n",
      "acc for optim= 0.14179392788145276\n",
      "Epoch:967/1000\n",
      "Loss on train= 0.00885685347020626\n",
      "Loss on test= 0.017101667821407318\n",
      "acc for Lsat= 0.06766727599832748 \n",
      "acc for Psat= 0.09467131826612683 \n",
      "acc for optim= 0.14149035302301247\n",
      "Epoch:968/1000\n",
      "Loss on train= 0.008391528390347958\n",
      "Loss on test= 0.01577794924378395\n",
      "acc for Lsat= 0.0701653066608641 \n",
      "acc for Psat= 0.10071637878815333 \n",
      "acc for optim= 0.14418647670083576\n",
      "Epoch:969/1000\n",
      "Loss on train= 0.008488022722303867\n",
      "Loss on test= 0.015916334465146065\n",
      "acc for Lsat= 0.06743425760004255 \n",
      "acc for Psat= 0.10222475777069728 \n",
      "acc for optim= 0.14093751145733727\n",
      "Epoch:970/1000\n",
      "Loss on train= 0.008537660352885723\n",
      "Loss on test= 0.01547637116163969\n",
      "acc for Lsat= 0.06618216435114542 \n",
      "acc for Psat= 0.10131352610058254 \n",
      "acc for optim= 0.14262958359387184\n",
      "Epoch:971/1000\n",
      "Loss on train= 0.008498961105942726\n",
      "Loss on test= 0.015926547348499298\n",
      "acc for Lsat= 0.06864502098825243 \n",
      "acc for Psat= 0.09959034588601855 \n",
      "acc for optim= 0.14108566662503616\n",
      "Epoch:972/1000\n",
      "Loss on train= 0.008419439196586609\n",
      "Loss on test= 0.01602417416870594\n",
      "acc for Lsat= 0.0674755311674542 \n",
      "acc for Psat= 0.09625303530030782 \n",
      "acc for optim= 0.14235112952689327\n",
      "Epoch:973/1000\n",
      "Loss on train= 0.008702080696821213\n",
      "Loss on test= 0.01683485135436058\n",
      "acc for Lsat= 0.0657409613331159 \n",
      "acc for Psat= 0.09868406554063162 \n",
      "acc for optim= 0.1423439529620939\n",
      "Epoch:974/1000\n",
      "Loss on train= 0.008664960972964764\n",
      "Loss on test= 0.01683483086526394\n",
      "acc for Lsat= 0.06940828661123911 \n",
      "acc for Psat= 0.10836450821823544 \n",
      "acc for optim= 0.1420813348144293\n",
      "Epoch:975/1000\n",
      "Loss on train= 0.008122867904603481\n",
      "Loss on test= 0.016259534284472466\n",
      "acc for Lsat= 0.0664347756240103 \n",
      "acc for Psat= 0.09610529906219906 \n",
      "acc for optim= 0.14242986982895267\n",
      "Epoch:976/1000\n",
      "Loss on train= 0.00856489036232233\n",
      "Loss on test= 0.017260489985346794\n",
      "acc for Lsat= 0.0667128783133295 \n",
      "acc for Psat= 0.09729684591293336 \n",
      "acc for optim= 0.14253167269958392\n",
      "Epoch:977/1000\n",
      "Loss on train= 0.008729781955480576\n",
      "Loss on test= 0.017038999125361443\n",
      "acc for Lsat= 0.07087457842297024 \n",
      "acc for Psat= 0.09979720744821759 \n",
      "acc for optim= 0.14148318250146175\n",
      "Epoch:978/1000\n",
      "Loss on train= 0.008294942788779736\n",
      "Loss on test= 0.01649007573723793\n",
      "acc for Lsat= 0.06616387052668465 \n",
      "acc for Psat= 0.09523980402284199 \n",
      "acc for optim= 0.13990961905154917\n",
      "Epoch:979/1000\n",
      "Loss on train= 0.008666190318763256\n",
      "Loss on test= 0.017339453101158142\n",
      "acc for Lsat= 0.0648210444384151 \n",
      "acc for Psat= 0.09609620604250164 \n",
      "acc for optim= 0.14161204886105327\n",
      "Epoch:980/1000\n",
      "Loss on train= 0.008826371282339096\n",
      "Loss on test= 0.01611534133553505\n",
      "acc for Lsat= 0.068057352801164 \n",
      "acc for Psat= 0.10660671591758729 \n",
      "acc for optim= 0.1419485592593749\n",
      "Epoch:981/1000\n",
      "Loss on train= 0.008315613493323326\n",
      "Loss on test= 0.015565169975161552\n",
      "acc for Lsat= 0.07216349707709419 \n",
      "acc for Psat= 0.09758044232924777 \n",
      "acc for optim= 0.14064544199241533\n",
      "Epoch:982/1000\n",
      "Loss on train= 0.00844529364258051\n",
      "Loss on test= 0.0166240856051445\n",
      "acc for Lsat= 0.06781967563761605 \n",
      "acc for Psat= 0.10047605352269279 \n",
      "acc for optim= 0.1413114750550853\n",
      "Epoch:983/1000\n",
      "Loss on train= 0.008461062796413898\n",
      "Loss on test= 0.015945827588438988\n",
      "acc for Lsat= 0.06635704636573792 \n",
      "acc for Psat= 0.09618813743193942 \n",
      "acc for optim= 0.14094589000774757\n",
      "Epoch:984/1000\n",
      "Loss on train= 0.0085212467238307\n",
      "Loss on test= 0.015223206952214241\n",
      "acc for Lsat= 0.06615302678611543 \n",
      "acc for Psat= 0.09537796907954746 \n",
      "acc for optim= 0.14077840662664834\n",
      "Epoch:985/1000\n",
      "Loss on train= 0.008878621272742748\n",
      "Loss on test= 0.014971526339650154\n",
      "acc for Lsat= 0.06526840875546137 \n",
      "acc for Psat= 0.09639328999651803 \n",
      "acc for optim= 0.13994375906056827\n",
      "Epoch:986/1000\n",
      "Loss on train= 0.008763574995100498\n",
      "Loss on test= 0.018064729869365692\n",
      "acc for Lsat= 0.06800278060966068 \n",
      "acc for Psat= 0.10320004125436147 \n",
      "acc for optim= 0.14366910010576248\n",
      "Epoch:987/1000\n",
      "Loss on train= 0.008366340771317482\n",
      "Loss on test= 0.016712605953216553\n",
      "acc for Lsat= 0.06934091763363943 \n",
      "acc for Psat= 0.09958586560355293 \n",
      "acc for optim= 0.14098961340884364\n",
      "Epoch:988/1000\n",
      "Loss on train= 0.008646228350698948\n",
      "Loss on test= 0.016641046851873398\n",
      "acc for Lsat= 0.0706606603331036 \n",
      "acc for Psat= 0.1053202693661054 \n",
      "acc for optim= 0.14259035144415166\n",
      "Epoch:989/1000\n",
      "Loss on train= 0.008194288238883018\n",
      "Loss on test= 0.016201550140976906\n",
      "acc for Lsat= 0.07368162936634487 \n",
      "acc for Psat= 0.09916971607340708 \n",
      "acc for optim= 0.14274229278994932\n",
      "Epoch:990/1000\n",
      "Loss on train= 0.008365343324840069\n",
      "Loss on test= 0.017286701127886772\n",
      "acc for Lsat= 0.07519860118627548 \n",
      "acc for Psat= 0.10614771743615468 \n",
      "acc for optim= 0.1419733728799555\n",
      "Epoch:991/1000\n",
      "Loss on train= 0.008286393247544765\n",
      "Loss on test= 0.01593206636607647\n",
      "acc for Lsat= 0.06775781710942587 \n",
      "acc for Psat= 0.10128880242506663 \n",
      "acc for optim= 0.1417973341006372\n",
      "Epoch:992/1000\n",
      "Loss on train= 0.008844833821058273\n",
      "Loss on test= 0.016627227887511253\n",
      "acc for Lsat= 0.07092943804131614 \n",
      "acc for Psat= 0.09976202117072212 \n",
      "acc for optim= 0.1410210919256012\n",
      "Epoch:993/1000\n",
      "Loss on train= 0.008589626289904118\n",
      "Loss on test= 0.016845164820551872\n",
      "acc for Lsat= 0.06967835293875799 \n",
      "acc for Psat= 0.09973058932357363 \n",
      "acc for optim= 0.14135809060600069\n",
      "Epoch:994/1000\n",
      "Loss on train= 0.008567036129534245\n",
      "Loss on test= 0.01597553677856922\n",
      "acc for Lsat= 0.06573860860533186 \n",
      "acc for Psat= 0.09635293119483525 \n",
      "acc for optim= 0.1400282429738177\n",
      "Epoch:995/1000\n",
      "Loss on train= 0.008874603547155857\n",
      "Loss on test= 0.015962380915880203\n",
      "acc for Lsat= 0.06816712369521459 \n",
      "acc for Psat= 0.09561978131532671 \n",
      "acc for optim= 0.14055824846857123\n",
      "Epoch:996/1000\n",
      "Loss on train= 0.00860508531332016\n",
      "Loss on test= 0.016555245965719223\n",
      "acc for Lsat= 0.06632241441143884 \n",
      "acc for Psat= 0.09844793279965719 \n",
      "acc for optim= 0.1414093407077922\n",
      "Epoch:997/1000\n",
      "Loss on train= 0.008505471050739288\n",
      "Loss on test= 0.016272922977805138\n",
      "acc for Lsat= 0.06668606185250812 \n",
      "acc for Psat= 0.09500563227468067 \n",
      "acc for optim= 0.14264446720480917\n",
      "Epoch:998/1000\n",
      "Loss on train= 0.008442978374660015\n",
      "Loss on test= 0.017879726365208626\n",
      "acc for Lsat= 0.06862879627280766 \n",
      "acc for Psat= 0.0983936831355095 \n",
      "acc for optim= 0.14222936415010026\n",
      "Epoch:999/1000\n",
      "Loss on train= 0.008521843701601028\n",
      "Loss on test= 0.018108902499079704\n",
      "acc for Lsat= 0.06987993849648372 \n",
      "acc for Psat= 0.09704367518424988 \n",
      "acc for optim= 0.14062112582226594\n",
      "Epoch:1000/1000\n",
      "Loss on train= 0.008980762213468552\n",
      "Loss on test= 0.015961406752467155\n",
      "acc for Lsat= 0.06783773634168838 \n",
      "acc for Psat= 0.09517051743136513 \n",
      "acc for optim= 0.1395013721038898\n",
      "Fold 5\n",
      "Epoch:1/1000\n",
      "Loss on train= 0.2201782763004303\n",
      "Loss on test= 0.09857825934886932\n",
      "acc for Lsat= 0.36173642658525046 \n",
      "acc for Psat= 0.49956732193628944 \n",
      "acc for optim= 0.21699551310804158\n",
      "Epoch:2/1000\n",
      "Loss on train= 0.08236485719680786\n",
      "Loss on test= 0.07086734473705292\n",
      "acc for Lsat= 0.2801433854218987 \n",
      "acc for Psat= 0.474497661822372 \n",
      "acc for optim= 0.21744474867979685\n",
      "Epoch:3/1000\n",
      "Loss on train= 0.06359834223985672\n",
      "Loss on test= 0.05608251318335533\n",
      "acc for Lsat= 0.2626545525259442 \n",
      "acc for Psat= 0.4018782153725624 \n",
      "acc for optim= 0.17535113675726788\n",
      "Epoch:4/1000\n",
      "Loss on train= 0.05906606838107109\n",
      "Loss on test= 0.05554382503032684\n",
      "acc for Lsat= 0.2849452104005548 \n",
      "acc for Psat= 0.39033485137754026 \n",
      "acc for optim= 0.17519999593496316\n",
      "Epoch:5/1000\n",
      "Loss on train= 0.05451323091983795\n",
      "Loss on test= 0.051784541457891464\n",
      "acc for Lsat= 0.275402679377132 \n",
      "acc for Psat= 0.35397137138578627 \n",
      "acc for optim= 0.17263634370432962\n",
      "Epoch:6/1000\n",
      "Loss on train= 0.05041715130209923\n",
      "Loss on test= 0.04997527599334717\n",
      "acc for Lsat= 0.2773458811971876 \n",
      "acc for Psat= 0.34872129062811535 \n",
      "acc for optim= 0.16950917293628054\n",
      "Epoch:7/1000\n",
      "Loss on train= 0.05067664384841919\n",
      "Loss on test= 0.04400213435292244\n",
      "acc for Lsat= 0.2688665320475896 \n",
      "acc for Psat= 0.34041274214784306 \n",
      "acc for optim= 0.16663514922062553\n",
      "Epoch:8/1000\n",
      "Loss on train= 0.04807312414050102\n",
      "Loss on test= 0.04867454245686531\n",
      "acc for Lsat= 0.27188416620095573 \n",
      "acc for Psat= 0.40542308522595305 \n",
      "acc for optim= 0.18256682720449233\n",
      "Epoch:9/1000\n",
      "Loss on train= 0.05089247226715088\n",
      "Loss on test= 0.04758939892053604\n",
      "acc for Lsat= 0.30562484148475855 \n",
      "acc for Psat= 0.32880815805660357 \n",
      "acc for optim= 0.1715278875496653\n",
      "Epoch:10/1000\n",
      "Loss on train= 0.04682894051074982\n",
      "Loss on test= 0.05079477280378342\n",
      "acc for Lsat= 0.3075183431307475 \n",
      "acc for Psat= 0.3173708616859383 \n",
      "acc for optim= 0.16931751585668986\n",
      "Epoch:11/1000\n",
      "Loss on train= 0.04554181173443794\n",
      "Loss on test= 0.046361640095710754\n",
      "acc for Lsat= 0.27754247155454415 \n",
      "acc for Psat= 0.32396002320779693 \n",
      "acc for optim= 0.17834113852845299\n",
      "Epoch:12/1000\n",
      "Loss on train= 0.04422462359070778\n",
      "Loss on test= 0.04429064691066742\n",
      "acc for Lsat= 0.2704187874992689 \n",
      "acc for Psat= 0.3216427817940712 \n",
      "acc for optim= 0.17317919482787447\n",
      "Epoch:13/1000\n",
      "Loss on train= 0.04479949176311493\n",
      "Loss on test= 0.043398212641477585\n",
      "acc for Lsat= 0.271760157081816 \n",
      "acc for Psat= 0.33279941793945095 \n",
      "acc for optim= 0.18206734028127458\n",
      "Epoch:14/1000\n",
      "Loss on train= 0.046350061893463135\n",
      "Loss on test= 0.04619360342621803\n",
      "acc for Lsat= 0.26672891279061633 \n",
      "acc for Psat= 0.33282527476549156 \n",
      "acc for optim= 0.1722945652074284\n",
      "Epoch:15/1000\n",
      "Loss on train= 0.04372701793909073\n",
      "Loss on test= 0.04256382957100868\n",
      "acc for Lsat= 0.2690603921810786 \n",
      "acc for Psat= 0.30362215944462345 \n",
      "acc for optim= 0.19493418865733678\n",
      "Epoch:16/1000\n",
      "Loss on train= 0.043569665402173996\n",
      "Loss on test= 0.04360402002930641\n",
      "acc for Lsat= 0.2674431565735075 \n",
      "acc for Psat= 0.35693461034033036 \n",
      "acc for optim= 0.1755417240990533\n",
      "Epoch:17/1000\n",
      "Loss on train= 0.04269908368587494\n",
      "Loss on test= 0.04209202527999878\n",
      "acc for Lsat= 0.25823703789048724 \n",
      "acc for Psat= 0.3081077287594478 \n",
      "acc for optim= 0.18519311911529968\n",
      "Epoch:18/1000\n",
      "Loss on train= 0.041828740388154984\n",
      "Loss on test= 0.041352760046720505\n",
      "acc for Lsat= 0.273795552055041 \n",
      "acc for Psat= 0.31311573518647084 \n",
      "acc for optim= 0.16694148166312112\n",
      "Epoch:19/1000\n",
      "Loss on train= 0.04213051497936249\n",
      "Loss on test= 0.03950946033000946\n",
      "acc for Lsat= 0.26635484562979805 \n",
      "acc for Psat= 0.2968872868352466 \n",
      "acc for optim= 0.1717321480313937\n",
      "Epoch:20/1000\n",
      "Loss on train= 0.04272637888789177\n",
      "Loss on test= 0.04453246295452118\n",
      "acc for Lsat= 0.2752095038692156 \n",
      "acc for Psat= 0.29505011952585647 \n",
      "acc for optim= 0.1851554079188241\n",
      "Epoch:21/1000\n",
      "Loss on train= 0.04084531217813492\n",
      "Loss on test= 0.043668922036886215\n",
      "acc for Lsat= 0.25647324820359546 \n",
      "acc for Psat= 0.3714827948146396 \n",
      "acc for optim= 0.17300572610563708\n",
      "Epoch:22/1000\n",
      "Loss on train= 0.042577698826789856\n",
      "Loss on test= 0.04014860466122627\n",
      "acc for Lsat= 0.2863287781675657 \n",
      "acc for Psat= 0.3109582465555933 \n",
      "acc for optim= 0.17294537027676898\n",
      "Epoch:23/1000\n",
      "Loss on train= 0.04082275182008743\n",
      "Loss on test= 0.0391450934112072\n",
      "acc for Lsat= 0.3002716726726956 \n",
      "acc for Psat= 0.2900662802159786 \n",
      "acc for optim= 0.18307073646121558\n",
      "Epoch:24/1000\n",
      "Loss on train= 0.040873169898986816\n",
      "Loss on test= 0.04120876267552376\n",
      "acc for Lsat= 0.3060360395246081 \n",
      "acc for Psat= 0.2858165615962611 \n",
      "acc for optim= 0.18059857818815445\n",
      "Epoch:25/1000\n",
      "Loss on train= 0.039233386516571045\n",
      "Loss on test= 0.03730401396751404\n",
      "acc for Lsat= 0.2565727515353097 \n",
      "acc for Psat= 0.32009973526000973 \n",
      "acc for optim= 0.17954475780328116\n",
      "Epoch:26/1000\n",
      "Loss on train= 0.04168463125824928\n",
      "Loss on test= 0.043211959302425385\n",
      "acc for Lsat= 0.3322670286728276 \n",
      "acc for Psat= 0.2845151931875282 \n",
      "acc for optim= 0.16853549364540316\n",
      "Epoch:27/1000\n",
      "Loss on train= 0.04087195172905922\n",
      "Loss on test= 0.037890199571847916\n",
      "acc for Lsat= 0.2529902471436395 \n",
      "acc for Psat= 0.2886233392688963 \n",
      "acc for optim= 0.17285658104552165\n",
      "Epoch:28/1000\n",
      "Loss on train= 0.03760362043976784\n",
      "Loss on test= 0.038279399275779724\n",
      "acc for Lsat= 0.25386629386080634 \n",
      "acc for Psat= 0.2911606532004145 \n",
      "acc for optim= 0.18655107418696085\n",
      "Epoch:29/1000\n",
      "Loss on train= 0.03804301843047142\n",
      "Loss on test= 0.03496866673231125\n",
      "acc for Lsat= 0.25495272013876175 \n",
      "acc for Psat= 0.28852260129319296 \n",
      "acc for optim= 0.16411501367886863\n",
      "Epoch:30/1000\n",
      "Loss on train= 0.037929706275463104\n",
      "Loss on test= 0.03504820540547371\n",
      "acc for Lsat= 0.2549520562092463 \n",
      "acc for Psat= 0.28880758384863536 \n",
      "acc for optim= 0.17578244705994925\n",
      "Epoch:31/1000\n",
      "Loss on train= 0.03748340904712677\n",
      "Loss on test= 0.03842730075120926\n",
      "acc for Lsat= 0.25264280935128525 \n",
      "acc for Psat= 0.30893221762445244 \n",
      "acc for optim= 0.17482206705543735\n",
      "Epoch:32/1000\n",
      "Loss on train= 0.037947073578834534\n",
      "Loss on test= 0.03706831485033035\n",
      "acc for Lsat= 0.24054953952630365 \n",
      "acc for Psat= 0.2904408653577169 \n",
      "acc for optim= 0.17127655926677912\n",
      "Epoch:33/1000\n",
      "Loss on train= 0.036578256636857986\n",
      "Loss on test= 0.04010117053985596\n",
      "acc for Lsat= 0.28730712164607314 \n",
      "acc for Psat= 0.2645437225699425 \n",
      "acc for optim= 0.1813264035516315\n",
      "Epoch:34/1000\n",
      "Loss on train= 0.03655257076025009\n",
      "Loss on test= 0.03401365131139755\n",
      "acc for Lsat= 0.257281404071384 \n",
      "acc for Psat= 0.2882279379500283 \n",
      "acc for optim= 0.16540980339050293\n",
      "Epoch:35/1000\n",
      "Loss on train= 0.03600333258509636\n",
      "Loss on test= 0.03613327071070671\n",
      "acc for Lsat= 0.2456305869751507 \n",
      "acc for Psat= 0.28250509334935076 \n",
      "acc for optim= 0.18282495041688282\n",
      "Epoch:36/1000\n",
      "Loss on train= 0.03557458147406578\n",
      "Loss on test= 0.0336240753531456\n",
      "acc for Lsat= 0.23386606226364773 \n",
      "acc for Psat= 0.2699359133011765 \n",
      "acc for optim= 0.17122733212179608\n",
      "Epoch:37/1000\n",
      "Loss on train= 0.03673236817121506\n",
      "Loss on test= 0.03605295345187187\n",
      "acc for Lsat= 0.2584919643070963 \n",
      "acc for Psat= 0.2568610856930415 \n",
      "acc for optim= 0.17610355599059002\n",
      "Epoch:38/1000\n",
      "Loss on train= 0.03597668930888176\n",
      "Loss on test= 0.03429831564426422\n",
      "acc for Lsat= 0.23489480962355933 \n",
      "acc for Psat= 0.2860100259383519 \n",
      "acc for optim= 0.16945628341701297\n",
      "Epoch:39/1000\n",
      "Loss on train= 0.03534170985221863\n",
      "Loss on test= 0.033176589757204056\n",
      "acc for Lsat= 0.22854323089122774 \n",
      "acc for Psat= 0.28879655235343504 \n",
      "acc for optim= 0.16570300012826922\n",
      "Epoch:40/1000\n",
      "Loss on train= 0.03472195565700531\n",
      "Loss on test= 0.030579455196857452\n",
      "acc for Lsat= 0.2679191753268242 \n",
      "acc for Psat= 0.24674541188610924 \n",
      "acc for optim= 0.16962747292386166\n",
      "Epoch:41/1000\n",
      "Loss on train= 0.0362834595143795\n",
      "Loss on test= 0.035703711211681366\n",
      "acc for Lsat= 0.24894623094134863 \n",
      "acc for Psat= 0.3187882635328505 \n",
      "acc for optim= 0.18833315504921805\n",
      "Epoch:42/1000\n",
      "Loss on train= 0.03559960424900055\n",
      "Loss on test= 0.03373505547642708\n",
      "acc for Lsat= 0.24366462892956203 \n",
      "acc for Psat= 0.2629355788230896 \n",
      "acc for optim= 0.16558709988991419\n",
      "Epoch:43/1000\n",
      "Loss on train= 0.03444978967308998\n",
      "Loss on test= 0.0355580635368824\n",
      "acc for Lsat= 0.26188065078523426 \n",
      "acc for Psat= 0.25258697006437514 \n",
      "acc for optim= 0.16807026333279076\n",
      "Epoch:44/1000\n",
      "Loss on train= 0.03313153609633446\n",
      "Loss on test= 0.03405417501926422\n",
      "acc for Lsat= 0.28004489921861225 \n",
      "acc for Psat= 0.2389059707522392 \n",
      "acc for optim= 0.17021966377894085\n",
      "Epoch:45/1000\n",
      "Loss on train= 0.03318969905376434\n",
      "Loss on test= 0.03345515578985214\n",
      "acc for Lsat= 0.2757743492722511 \n",
      "acc for Psat= 0.24149681089652908 \n",
      "acc for optim= 0.16374480227629343\n",
      "Epoch:46/1000\n",
      "Loss on train= 0.03377939760684967\n",
      "Loss on test= 0.03555908426642418\n",
      "acc for Lsat= 0.2658884864714411 \n",
      "acc for Psat= 0.24817021323574917 \n",
      "acc for optim= 0.1892830391724904\n",
      "Epoch:47/1000\n",
      "Loss on train= 0.034002140164375305\n",
      "Loss on test= 0.03237910196185112\n",
      "acc for Lsat= 0.2514686794744597 \n",
      "acc for Psat= 0.2728258874681261 \n",
      "acc for optim= 0.16481546345684264\n",
      "Epoch:48/1000\n",
      "Loss on train= 0.033117201179265976\n",
      "Loss on test= 0.034331128001213074\n",
      "acc for Lsat= 0.23960595561398396 \n",
      "acc for Psat= 0.25574592451254524 \n",
      "acc for optim= 0.18596748279200662\n",
      "Epoch:49/1000\n",
      "Loss on train= 0.03273330256342888\n",
      "Loss on test= 0.03223548084497452\n",
      "acc for Lsat= 0.24553260207176214 \n",
      "acc for Psat= 0.2379265561699867 \n",
      "acc for optim= 0.16385574489831922\n",
      "Epoch:50/1000\n",
      "Loss on train= 0.03288310393691063\n",
      "Loss on test= 0.0339508131146431\n",
      "acc for Lsat= 0.25450438194804714 \n",
      "acc for Psat= 0.2719104558229446 \n",
      "acc for optim= 0.16918279313378867\n",
      "Epoch:51/1000\n",
      "Loss on train= 0.03329087421298027\n",
      "Loss on test= 0.034275978803634644\n",
      "acc for Lsat= 0.24553374846776327 \n",
      "acc for Psat= 0.2666738162438075 \n",
      "acc for optim= 0.1718685156769223\n",
      "Epoch:52/1000\n",
      "Loss on train= 0.03154423460364342\n",
      "Loss on test= 0.032269492745399475\n",
      "acc for Lsat= 0.2589205445514785 \n",
      "acc for Psat= 0.24311545325650108 \n",
      "acc for optim= 0.17285348094171946\n",
      "Epoch:53/1000\n",
      "Loss on train= 0.03201417997479439\n",
      "Loss on test= 0.033164773136377335\n",
      "acc for Lsat= 0.2539289252625571 \n",
      "acc for Psat= 0.23838075747092569 \n",
      "acc for optim= 0.16554273929860858\n",
      "Epoch:54/1000\n",
      "Loss on train= 0.03213139995932579\n",
      "Loss on test= 0.033509932458400726\n",
      "acc for Lsat= 0.2478755119774077 \n",
      "acc for Psat= 0.27636088430881506 \n",
      "acc for optim= 0.16789115634229448\n",
      "Epoch:55/1000\n",
      "Loss on train= 0.03228610381484032\n",
      "Loss on test= 0.03222866356372833\n",
      "acc for Lsat= 0.2535666119721201 \n",
      "acc for Psat= 0.22578283689088297 \n",
      "acc for optim= 0.17143394615915086\n",
      "Epoch:56/1000\n",
      "Loss on train= 0.032231900840997696\n",
      "Loss on test= 0.031978242099285126\n",
      "acc for Lsat= 0.24330587072504894 \n",
      "acc for Psat= 0.25044237607055236 \n",
      "acc for optim= 0.18225680192311602\n",
      "Epoch:57/1000\n",
      "Loss on train= 0.0320657342672348\n",
      "Loss on test= 0.03282110393047333\n",
      "acc for Lsat= 0.24814379033115175 \n",
      "acc for Psat= 0.24788092441029017 \n",
      "acc for optim= 0.1757860246631834\n",
      "Epoch:58/1000\n",
      "Loss on train= 0.03217644989490509\n",
      "Loss on test= 0.03132883086800575\n",
      "acc for Lsat= 0.2529122407237689 \n",
      "acc for Psat= 0.24967345595359797 \n",
      "acc for optim= 0.16865945955117542\n",
      "Epoch:59/1000\n",
      "Loss on train= 0.030746478587388992\n",
      "Loss on test= 0.03046213649213314\n",
      "acc for Lsat= 0.22665021684434686 \n",
      "acc for Psat= 0.24842151237858673 \n",
      "acc for optim= 0.1735925124751197\n",
      "Epoch:60/1000\n",
      "Loss on train= 0.03109118528664112\n",
      "Loss on test= 0.030134953558444977\n",
      "acc for Lsat= 0.2413130104541778 \n",
      "acc for Psat= 0.24081187778049046 \n",
      "acc for optim= 0.16731416698959134\n",
      "Epoch:61/1000\n",
      "Loss on train= 0.03027404099702835\n",
      "Loss on test= 0.032791122794151306\n",
      "acc for Lsat= 0.25979420575830675 \n",
      "acc for Psat= 0.24808868567148845 \n",
      "acc for optim= 0.17329572902785412\n",
      "Epoch:62/1000\n",
      "Loss on train= 0.03014320693910122\n",
      "Loss on test= 0.0297765601426363\n",
      "acc for Lsat= 0.2489763364195824 \n",
      "acc for Psat= 0.2507102057337761 \n",
      "acc for optim= 0.16324822091394\n",
      "Epoch:63/1000\n",
      "Loss on train= 0.030592484399676323\n",
      "Loss on test= 0.02930551767349243\n",
      "acc for Lsat= 0.23961439165804116 \n",
      "acc for Psat= 0.22803490592373737 \n",
      "acc for optim= 0.16976574261983235\n",
      "Epoch:64/1000\n",
      "Loss on train= 0.03034588135778904\n",
      "Loss on test= 0.03033149614930153\n",
      "acc for Lsat= 0.22187858174244562 \n",
      "acc for Psat= 0.24391878777080114 \n",
      "acc for optim= 0.170409247941441\n",
      "Epoch:65/1000\n",
      "Loss on train= 0.030176186934113503\n",
      "Loss on test= 0.03344936668872833\n",
      "acc for Lsat= 0.24277730054325528 \n",
      "acc for Psat= 0.30408916274706527 \n",
      "acc for optim= 0.18699818849563596\n",
      "Epoch:66/1000\n",
      "Loss on train= 0.03143933787941933\n",
      "Loss on test= 0.040388159453868866\n",
      "acc for Lsat= 0.3366885356605053 \n",
      "acc for Psat= 0.291191603243351 \n",
      "acc for optim= 0.16310695028967326\n",
      "Epoch:67/1000\n",
      "Loss on train= 0.03183024376630783\n",
      "Loss on test= 0.032531626522541046\n",
      "acc for Lsat= 0.22456972185108393 \n",
      "acc for Psat= 0.26241535544395445 \n",
      "acc for optim= 0.1712158623668883\n",
      "Epoch:68/1000\n",
      "Loss on train= 0.030582698062062263\n",
      "Loss on test= 0.030136428773403168\n",
      "acc for Lsat= 0.2553922255006102 \n",
      "acc for Psat= 0.22544003940290874 \n",
      "acc for optim= 0.16941936728027132\n",
      "Epoch:69/1000\n",
      "Loss on train= 0.030840860679745674\n",
      "Loss on test= 0.030315794050693512\n",
      "acc for Lsat= 0.25372931352920003 \n",
      "acc for Psat= 0.2286938806374868 \n",
      "acc for optim= 0.16495916810300615\n",
      "Epoch:70/1000\n",
      "Loss on train= 0.02793346717953682\n",
      "Loss on test= 0.029509995132684708\n",
      "acc for Lsat= 0.21024480114380523 \n",
      "acc for Psat= 0.231195698512925 \n",
      "acc for optim= 0.1728118790520562\n",
      "Epoch:71/1000\n",
      "Loss on train= 0.028620155528187752\n",
      "Loss on test= 0.029189765453338623\n",
      "acc for Lsat= 0.21246455096536213 \n",
      "acc for Psat= 0.2160918724205759 \n",
      "acc for optim= 0.16762528005573485\n",
      "Epoch:72/1000\n",
      "Loss on train= 0.029472915455698967\n",
      "Loss on test= 0.03159715235233307\n",
      "acc for Lsat= 0.23486189511087208 \n",
      "acc for Psat= 0.22836639318201277 \n",
      "acc for optim= 0.16321310549974444\n",
      "Epoch:73/1000\n",
      "Loss on train= 0.028573816642165184\n",
      "Loss on test= 0.02995733730494976\n",
      "acc for Lsat= 0.21707310014300876 \n",
      "acc for Psat= 0.25367135008176167 \n",
      "acc for optim= 0.17262134220865039\n",
      "Epoch:74/1000\n",
      "Loss on train= 0.030580885708332062\n",
      "Loss on test= 0.031098803505301476\n",
      "acc for Lsat= 0.2077330542935266 \n",
      "acc for Psat= 0.22087265186839633 \n",
      "acc for optim= 0.17890351017316186\n",
      "Epoch:75/1000\n",
      "Loss on train= 0.028700411319732666\n",
      "Loss on test= 0.02630811557173729\n",
      "acc for Lsat= 0.21061159272988642 \n",
      "acc for Psat= 0.2126932673984104 \n",
      "acc for optim= 0.16383149574200312\n",
      "Epoch:76/1000\n",
      "Loss on train= 0.028870414942502975\n",
      "Loss on test= 0.027602553367614746\n",
      "acc for Lsat= 0.19457957926723696 \n",
      "acc for Psat= 0.22356195847193394 \n",
      "acc for optim= 0.16893514974249732\n",
      "Epoch:77/1000\n",
      "Loss on train= 0.028254715725779533\n",
      "Loss on test= 0.027038756757974625\n",
      "acc for Lsat= 0.20792675746811762 \n",
      "acc for Psat= 0.21639917095502217 \n",
      "acc for optim= 0.17026943481630746\n",
      "Epoch:78/1000\n",
      "Loss on train= 0.0271344855427742\n",
      "Loss on test= 0.026247840374708176\n",
      "acc for Lsat= 0.18529057055711745 \n",
      "acc for Psat= 0.2135614865356022 \n",
      "acc for optim= 0.16616627756092284\n",
      "Epoch:79/1000\n",
      "Loss on train= 0.027245469391345978\n",
      "Loss on test= 0.02840046025812626\n",
      "acc for Lsat= 0.18166018393304614 \n",
      "acc for Psat= 0.2143649379412333 \n",
      "acc for optim= 0.17440375652578144\n",
      "Epoch:80/1000\n",
      "Loss on train= 0.02718537487089634\n",
      "Loss on test= 0.027002280578017235\n",
      "acc for Lsat= 0.17960729797681177 \n",
      "acc for Psat= 0.1989623857869042 \n",
      "acc for optim= 0.17772503197193146\n",
      "Epoch:81/1000\n",
      "Loss on train= 0.02674204856157303\n",
      "Loss on test= 0.025948116555809975\n",
      "acc for Lsat= 0.16856430338488684 \n",
      "acc for Psat= 0.22638611363040073 \n",
      "acc for optim= 0.17093628280692633\n",
      "Epoch:82/1000\n",
      "Loss on train= 0.026604177430272102\n",
      "Loss on test= 0.028851104900240898\n",
      "acc for Lsat= 0.20194926460584003 \n",
      "acc for Psat= 0.20218350109126837 \n",
      "acc for optim= 0.16730171690384546\n",
      "Epoch:83/1000\n",
      "Loss on train= 0.02693183906376362\n",
      "Loss on test= 0.027100546285510063\n",
      "acc for Lsat= 0.1796607242690192 \n",
      "acc for Psat= 0.24019905991024437 \n",
      "acc for optim= 0.17114821639325883\n",
      "Epoch:84/1000\n",
      "Loss on train= 0.02584533765912056\n",
      "Loss on test= 0.02645890973508358\n",
      "acc for Lsat= 0.17539029916127521 \n",
      "acc for Psat= 0.18870421250661218 \n",
      "acc for optim= 0.16527349319722914\n",
      "Epoch:85/1000\n",
      "Loss on train= 0.02624589204788208\n",
      "Loss on test= 0.026700599119067192\n",
      "acc for Lsat= 0.19092588308784697 \n",
      "acc for Psat= 0.19556111792723332 \n",
      "acc for optim= 0.1656387196646796\n",
      "Epoch:86/1000\n",
      "Loss on train= 0.026135843247175217\n",
      "Loss on test= 0.027390381321310997\n",
      "acc for Lsat= 0.17111459904246856 \n",
      "acc for Psat= 0.24016913904084103 \n",
      "acc for optim= 0.17299866312079962\n",
      "Epoch:87/1000\n",
      "Loss on train= 0.027084069326519966\n",
      "Loss on test= 0.026275774464011192\n",
      "acc for Lsat= 0.17866101894113748 \n",
      "acc for Psat= 0.19923454191949633 \n",
      "acc for optim= 0.16728476302491294\n",
      "Epoch:88/1000\n",
      "Loss on train= 0.026197774335741997\n",
      "Loss on test= 0.024788664653897285\n",
      "acc for Lsat= 0.14515652689668868 \n",
      "acc for Psat= 0.2244005885389116 \n",
      "acc for optim= 0.16566372613112132\n",
      "Epoch:89/1000\n",
      "Loss on train= 0.02468988299369812\n",
      "Loss on test= 0.02676384523510933\n",
      "acc for Lsat= 0.14778477682007682 \n",
      "acc for Psat= 0.22752427193853592 \n",
      "acc for optim= 0.16649917198552025\n",
      "Epoch:90/1000\n",
      "Loss on train= 0.024876348674297333\n",
      "Loss on test= 0.024711254984140396\n",
      "acc for Lsat= 0.15828074349297416 \n",
      "acc for Psat= 0.18886013461483847 \n",
      "acc for optim= 0.16667777581347362\n",
      "Epoch:91/1000\n",
      "Loss on train= 0.024267900735139847\n",
      "Loss on test= 0.024873873218894005\n",
      "acc for Lsat= 0.13733371529314253 \n",
      "acc for Psat= 0.20594977835814154 \n",
      "acc for optim= 0.16662661217980915\n",
      "Epoch:92/1000\n",
      "Loss on train= 0.024944638833403587\n",
      "Loss on test= 0.026331761851906776\n",
      "acc for Lsat= 0.13536187211672465 \n",
      "acc for Psat= 0.1794082303841909 \n",
      "acc for optim= 0.1680644128057692\n",
      "Epoch:93/1000\n",
      "Loss on train= 0.024965573102235794\n",
      "Loss on test= 0.025732876732945442\n",
      "acc for Lsat= 0.1285089545779758 \n",
      "acc for Psat= 0.19229414992862273 \n",
      "acc for optim= 0.1695685918132464\n",
      "Epoch:94/1000\n",
      "Loss on train= 0.023651743307709694\n",
      "Loss on test= 0.02594844065606594\n",
      "acc for Lsat= 0.13823439280192057 \n",
      "acc for Psat= 0.1602055793007215 \n",
      "acc for optim= 0.1699853347407447\n",
      "Epoch:95/1000\n",
      "Loss on train= 0.02376359887421131\n",
      "Loss on test= 0.02286802977323532\n",
      "acc for Lsat= 0.1367831809653176 \n",
      "acc for Psat= 0.18091391424338024 \n",
      "acc for optim= 0.17150726152790916\n",
      "Epoch:96/1000\n",
      "Loss on train= 0.02337905578315258\n",
      "Loss on test= 0.027257032692432404\n",
      "acc for Lsat= 0.13742764559057025 \n",
      "acc for Psat= 0.16818594336509707 \n",
      "acc for optim= 0.17730670207076601\n",
      "Epoch:97/1000\n",
      "Loss on train= 0.02388869598507881\n",
      "Loss on test= 0.02295800670981407\n",
      "acc for Lsat= 0.1331394828028149 \n",
      "acc for Psat= 0.16347162690427566 \n",
      "acc for optim= 0.16721352222892974\n",
      "Epoch:98/1000\n",
      "Loss on train= 0.02330423705279827\n",
      "Loss on test= 0.02317941188812256\n",
      "acc for Lsat= 0.12051571408907573 \n",
      "acc for Psat= 0.16467607683605617 \n",
      "acc for optim= 0.16804324686527253\n",
      "Epoch:99/1000\n",
      "Loss on train= 0.02322204038500786\n",
      "Loss on test= 0.022226858884096146\n",
      "acc for Lsat= 0.1272051284710566 \n",
      "acc for Psat= 0.20871945950720042 \n",
      "acc for optim= 0.16758294105529783\n",
      "Epoch:100/1000\n",
      "Loss on train= 0.023100731894373894\n",
      "Loss on test= 0.02275793068110943\n",
      "acc for Lsat= 0.12041484547985926 \n",
      "acc for Psat= 0.18141399290826588 \n",
      "acc for optim= 0.16825798087649874\n",
      "Epoch:101/1000\n",
      "Loss on train= 0.023384027183055878\n",
      "Loss on test= 0.02430545538663864\n",
      "acc for Lsat= 0.14093807174099815 \n",
      "acc for Psat= 0.1833202557431327 \n",
      "acc for optim= 0.16902448849545584\n",
      "Epoch:102/1000\n",
      "Loss on train= 0.02265009470283985\n",
      "Loss on test= 0.023465564474463463\n",
      "acc for Lsat= 0.13113095810015996 \n",
      "acc for Psat= 0.17055389251973896 \n",
      "acc for optim= 0.164217417438825\n",
      "Epoch:103/1000\n",
      "Loss on train= 0.023360200226306915\n",
      "Loss on test= 0.023256070911884308\n",
      "acc for Lsat= 0.10739732798602847 \n",
      "acc for Psat= 0.16559438705444335 \n",
      "acc for optim= 0.1779953234725528\n",
      "Epoch:104/1000\n",
      "Loss on train= 0.02271401509642601\n",
      "Loss on test= 0.024308430030941963\n",
      "acc for Lsat= 0.14255732496579487 \n",
      "acc for Psat= 0.2290132290787167 \n",
      "acc for optim= 0.17040506634447314\n",
      "Epoch:105/1000\n",
      "Loss on train= 0.022333882749080658\n",
      "Loss on test= 0.02102617546916008\n",
      "acc for Lsat= 0.12498033659325708 \n",
      "acc for Psat= 0.16308105670743514 \n",
      "acc for optim= 0.15863937868012323\n",
      "Epoch:106/1000\n",
      "Loss on train= 0.022003939375281334\n",
      "Loss on test= 0.021040253341197968\n",
      "acc for Lsat= 0.12951679593986934 \n",
      "acc for Psat= 0.19999985761112637 \n",
      "acc for optim= 0.16155277871423301\n",
      "Epoch:107/1000\n",
      "Loss on train= 0.02232188731431961\n",
      "Loss on test= 0.02246551215648651\n",
      "acc for Lsat= 0.12768310374683806 \n",
      "acc for Psat= 0.18249940640396545 \n",
      "acc for optim= 0.16770161158508726\n",
      "Epoch:108/1000\n",
      "Loss on train= 0.021877681836485863\n",
      "Loss on test= 0.021912289783358574\n",
      "acc for Lsat= 0.11759141882260643 \n",
      "acc for Psat= 0.19513329797320894 \n",
      "acc for optim= 0.16421475013097128\n",
      "Epoch:109/1000\n",
      "Loss on train= 0.021983163431286812\n",
      "Loss on test= 0.023176752030849457\n",
      "acc for Lsat= 0.12926071253087787 \n",
      "acc for Psat= 0.19430323640505473 \n",
      "acc for optim= 0.1730245490868886\n",
      "Epoch:110/1000\n",
      "Loss on train= 0.0226250272244215\n",
      "Loss on test= 0.02180948480963707\n",
      "acc for Lsat= 0.11881366736359067 \n",
      "acc for Psat= 0.16661898791790009 \n",
      "acc for optim= 0.16426141344838674\n",
      "Epoch:111/1000\n",
      "Loss on train= 0.022298002615571022\n",
      "Loss on test= 0.021357303485274315\n",
      "acc for Lsat= 0.10644721554385292 \n",
      "acc for Psat= 0.1496357689301173 \n",
      "acc for optim= 0.1601716947224405\n",
      "Epoch:112/1000\n",
      "Loss on train= 0.021418295800685883\n",
      "Loss on test= 0.023670436814427376\n",
      "acc for Lsat= 0.11455780814091367 \n",
      "acc for Psat= 0.1581972865594758 \n",
      "acc for optim= 0.1668040563662847\n",
      "Epoch:113/1000\n",
      "Loss on train= 0.021086949855089188\n",
      "Loss on test= 0.024135034531354904\n",
      "acc for Lsat= 0.13011767996682064 \n",
      "acc for Psat= 0.2197718030876584 \n",
      "acc for optim= 0.16634100443787045\n",
      "Epoch:114/1000\n",
      "Loss on train= 0.02142074704170227\n",
      "Loss on test= 0.020424742251634598\n",
      "acc for Lsat= 0.10191183951165941 \n",
      "acc for Psat= 0.15232613368166814 \n",
      "acc for optim= 0.16627763625648287\n",
      "Epoch:115/1000\n",
      "Loss on train= 0.021442800760269165\n",
      "Loss on test= 0.020207801833748817\n",
      "acc for Lsat= 0.1129919403129154 \n",
      "acc for Psat= 0.17813885311285657 \n",
      "acc for optim= 0.1634386087457339\n",
      "Epoch:116/1000\n",
      "Loss on train= 0.021703360602259636\n",
      "Loss on test= 0.024420756846666336\n",
      "acc for Lsat= 0.19921246916055682 \n",
      "acc for Psat= 0.16980158388614655 \n",
      "acc for optim= 0.161457582977083\n",
      "Epoch:117/1000\n",
      "Loss on train= 0.021441880613565445\n",
      "Loss on test= 0.021026942878961563\n",
      "acc for Lsat= 0.10354788998762769 \n",
      "acc for Psat= 0.16114110251267752 \n",
      "acc for optim= 0.1656897776656681\n",
      "Epoch:118/1000\n",
      "Loss on train= 0.02101474069058895\n",
      "Loss on test= 0.020881211385130882\n",
      "acc for Lsat= 0.10119886563883887 \n",
      "acc for Psat= 0.15220550729168786 \n",
      "acc for optim= 0.1603221651580598\n",
      "Epoch:119/1000\n",
      "Loss on train= 0.02114713005721569\n",
      "Loss on test= 0.02083980292081833\n",
      "acc for Lsat= 0.10168178959025277 \n",
      "acc for Psat= 0.15495125022199419 \n",
      "acc for optim= 0.16339818421337343\n",
      "Epoch:120/1000\n",
      "Loss on train= 0.021634580567479134\n",
      "Loss on test= 0.022688768804073334\n",
      "acc for Lsat= 0.11102234489387935 \n",
      "acc for Psat= 0.18993825448883908 \n",
      "acc for optim= 0.16624498400423265\n",
      "Epoch:121/1000\n",
      "Loss on train= 0.01969996839761734\n",
      "Loss on test= 0.022677725180983543\n",
      "acc for Lsat= 0.12754491137133703 \n",
      "acc for Psat= 0.1998319844404856 \n",
      "acc for optim= 0.1629995031489266\n",
      "Epoch:122/1000\n",
      "Loss on train= 0.02010473981499672\n",
      "Loss on test= 0.02137206308543682\n",
      "acc for Lsat= 0.11836162259181343 \n",
      "acc for Psat= 0.1494649324152205 \n",
      "acc for optim= 0.16180141303274367\n",
      "Epoch:123/1000\n",
      "Loss on train= 0.02082142047584057\n",
      "Loss on test= 0.021613840013742447\n",
      "acc for Lsat= 0.10556591583622826 \n",
      "acc for Psat= 0.17652929094102646 \n",
      "acc for optim= 0.16261354237794873\n",
      "Epoch:124/1000\n",
      "Loss on train= 0.021139610558748245\n",
      "Loss on test= 0.020865976810455322\n",
      "acc for Lsat= 0.10515373084280226 \n",
      "acc for Psat= 0.16346013810899523 \n",
      "acc for optim= 0.1692555510335498\n",
      "Epoch:125/1000\n",
      "Loss on train= 0.02036517672240734\n",
      "Loss on test= 0.023170042783021927\n",
      "acc for Lsat= 0.09923249102301067 \n",
      "acc for Psat= 0.16183590624067523 \n",
      "acc for optim= 0.1789792213175032\n",
      "Epoch:126/1000\n",
      "Loss on train= 0.019822001457214355\n",
      "Loss on test= 0.019688734784722328\n",
      "acc for Lsat= 0.09668809274832407 \n",
      "acc for Psat= 0.16534932851791379 \n",
      "acc for optim= 0.16960425708029006\n",
      "Epoch:127/1000\n",
      "Loss on train= 0.020303746685385704\n",
      "Loss on test= 0.020203081890940666\n",
      "acc for Lsat= 0.0986102196905348 \n",
      "acc for Psat= 0.14547911816173129 \n",
      "acc for optim= 0.15659605926937528\n",
      "Epoch:128/1000\n",
      "Loss on train= 0.020391330122947693\n",
      "Loss on test= 0.018704146146774292\n",
      "acc for Lsat= 0.0969587203529146 \n",
      "acc for Psat= 0.16346700257725186 \n",
      "acc for optim= 0.15966956979698607\n",
      "Epoch:129/1000\n",
      "Loss on train= 0.020246967673301697\n",
      "Loss on test= 0.02022577077150345\n",
      "acc for Lsat= 0.13315461327632266 \n",
      "acc for Psat= 0.16326813730928633 \n",
      "acc for optim= 0.16144715232981574\n",
      "Epoch:130/1000\n",
      "Loss on train= 0.01952637918293476\n",
      "Loss on test= 0.0187100637704134\n",
      "acc for Lsat= 0.10314223832554287 \n",
      "acc for Psat= 0.15002572668923275 \n",
      "acc for optim= 0.15990803721878263\n",
      "Epoch:131/1000\n",
      "Loss on train= 0.019638817757368088\n",
      "Loss on test= 0.019269220530986786\n",
      "acc for Lsat= 0.1025366932153702 \n",
      "acc for Psat= 0.16779838436179695 \n",
      "acc for optim= 0.15851051608721414\n",
      "Epoch:132/1000\n",
      "Loss on train= 0.019544634968042374\n",
      "Loss on test= 0.020378464832901955\n",
      "acc for Lsat= 0.10150052524275249 \n",
      "acc for Psat= 0.1652913434637917 \n",
      "acc for optim= 0.15505659298764335\n",
      "Epoch:133/1000\n",
      "Loss on train= 0.019524715840816498\n",
      "Loss on test= 0.0208622794598341\n",
      "acc for Lsat= 0.10653733445538414 \n",
      "acc for Psat= 0.19066767493883768 \n",
      "acc for optim= 0.16628709948725173\n",
      "Epoch:134/1000\n",
      "Loss on train= 0.020086372271180153\n",
      "Loss on test= 0.019664041697978973\n",
      "acc for Lsat= 0.10298381812042662 \n",
      "acc for Psat= 0.17192219793796537 \n",
      "acc for optim= 0.16609574125872717\n",
      "Epoch:135/1000\n",
      "Loss on train= 0.019714873284101486\n",
      "Loss on test= 0.024287177249789238\n",
      "acc for Lsat= 0.12604419158564673 \n",
      "acc for Psat= 0.19636743797196282 \n",
      "acc for optim= 0.1634109179178874\n",
      "Epoch:136/1000\n",
      "Loss on train= 0.019251273944973946\n",
      "Loss on test= 0.019862372428178787\n",
      "acc for Lsat= 0.10138705985413657 \n",
      "acc for Psat= 0.14440864457024472 \n",
      "acc for optim= 0.15803750985198545\n",
      "Epoch:137/1000\n",
      "Loss on train= 0.019994646310806274\n",
      "Loss on test= 0.01951051503419876\n",
      "acc for Lsat= 0.10180881371100742 \n",
      "acc for Psat= 0.15402944882710778 \n",
      "acc for optim= 0.15559702780511644\n",
      "Epoch:138/1000\n",
      "Loss on train= 0.019351564347743988\n",
      "Loss on test= 0.018193522468209267\n",
      "acc for Lsat= 0.10631838175985547 \n",
      "acc for Psat= 0.14457702702946135 \n",
      "acc for optim= 0.1601112425327301\n",
      "Epoch:139/1000\n",
      "Loss on train= 0.019550301134586334\n",
      "Loss on test= 0.020511381328105927\n",
      "acc for Lsat= 0.09775759180386862 \n",
      "acc for Psat= 0.15375864373313053 \n",
      "acc for optim= 0.15786932094229592\n",
      "Epoch:140/1000\n",
      "Loss on train= 0.019890455529093742\n",
      "Loss on test= 0.019418824464082718\n",
      "acc for Lsat= 0.10346955888801151 \n",
      "acc for Psat= 0.15792501933044856 \n",
      "acc for optim= 0.15702255765597026\n",
      "Epoch:141/1000\n",
      "Loss on train= 0.019481034949421883\n",
      "Loss on test= 0.01954762265086174\n",
      "acc for Lsat= 0.10146341224511464 \n",
      "acc for Psat= 0.14413782010475795 \n",
      "acc for optim= 0.15618986205922233\n",
      "Epoch:142/1000\n",
      "Loss on train= 0.019431736320257187\n",
      "Loss on test= 0.01837439276278019\n",
      "acc for Lsat= 0.09175973600811428 \n",
      "acc for Psat= 0.14649462037616304 \n",
      "acc for optim= 0.1577151349849171\n",
      "Epoch:143/1000\n",
      "Loss on train= 0.018739987164735794\n",
      "Loss on test= 0.020129641517996788\n",
      "acc for Lsat= 0.10241379621956082 \n",
      "acc for Psat= 0.15446258054839238 \n",
      "acc for optim= 0.16199526704019968\n",
      "Epoch:144/1000\n",
      "Loss on train= 0.019530555233359337\n",
      "Loss on test= 0.02139718271791935\n",
      "acc for Lsat= 0.09272923601998222 \n",
      "acc for Psat= 0.1451911957727538 \n",
      "acc for optim= 0.16962953872150846\n",
      "Epoch:145/1000\n",
      "Loss on train= 0.019625233486294746\n",
      "Loss on test= 0.019737325608730316\n",
      "acc for Lsat= 0.08653370870484245 \n",
      "acc for Psat= 0.16700987517833707 \n",
      "acc for optim= 0.1572885248396132\n",
      "Epoch:146/1000\n",
      "Loss on train= 0.01906205527484417\n",
      "Loss on test= 0.020348327234387398\n",
      "acc for Lsat= 0.09982877373695374 \n",
      "acc for Psat= 0.13657731033033793 \n",
      "acc for optim= 0.15461305280526477\n",
      "Epoch:147/1000\n",
      "Loss on train= 0.01894361712038517\n",
      "Loss on test= 0.020599013194441795\n",
      "acc for Lsat= 0.09847192880180147 \n",
      "acc for Psat= 0.1554433011346393 \n",
      "acc for optim= 0.1653763539261288\n",
      "Epoch:148/1000\n",
      "Loss on train= 0.018433786928653717\n",
      "Loss on test= 0.020213833078742027\n",
      "acc for Lsat= 0.10250948866208393 \n",
      "acc for Psat= 0.17436599002944098 \n",
      "acc for optim= 0.16191381596856647\n",
      "Epoch:149/1000\n",
      "Loss on train= 0.018849747255444527\n",
      "Loss on test= 0.01895292103290558\n",
      "acc for Lsat= 0.09033137692345512 \n",
      "acc for Psat= 0.13069925159215928 \n",
      "acc for optim= 0.15556735727522109\n",
      "Epoch:150/1000\n",
      "Loss on train= 0.018299199640750885\n",
      "Loss on test= 0.020535830408334732\n",
      "acc for Lsat= 0.1250184416770935 \n",
      "acc for Psat= 0.15215858585304684 \n",
      "acc for optim= 0.16782307989067505\n",
      "Epoch:151/1000\n",
      "Loss on train= 0.018793174996972084\n",
      "Loss on test= 0.01823377050459385\n",
      "acc for Lsat= 0.09623691158162223 \n",
      "acc for Psat= 0.13749629788928563 \n",
      "acc for optim= 0.151136941379971\n",
      "Epoch:152/1000\n",
      "Loss on train= 0.018670549616217613\n",
      "Loss on test= 0.018280409276485443\n",
      "acc for Lsat= 0.09773957812123829 \n",
      "acc for Psat= 0.14054264492458768 \n",
      "acc for optim= 0.15792339377933082\n",
      "Epoch:153/1000\n",
      "Loss on train= 0.018726138398051262\n",
      "Loss on test= 0.018883051350712776\n",
      "acc for Lsat= 0.10942240456740061 \n",
      "acc for Psat= 0.1758301612403658 \n",
      "acc for optim= 0.15547200623485774\n",
      "Epoch:154/1000\n",
      "Loss on train= 0.018751945346593857\n",
      "Loss on test= 0.01804962381720543\n",
      "acc for Lsat= 0.09851566834582223 \n",
      "acc for Psat= 0.1435123314460119 \n",
      "acc for optim= 0.15472733543978795\n",
      "Epoch:155/1000\n",
      "Loss on train= 0.017874158918857574\n",
      "Loss on test= 0.019819969311356544\n",
      "acc for Lsat= 0.08504305995172924 \n",
      "acc for Psat= 0.1385489293270641 \n",
      "acc for optim= 0.1596160324083434\n",
      "Epoch:156/1000\n",
      "Loss on train= 0.018706420436501503\n",
      "Loss on test= 0.018590904772281647\n",
      "acc for Lsat= 0.09588625464174484 \n",
      "acc for Psat= 0.15284095042281678 \n",
      "acc for optim= 0.1648347579770618\n",
      "Epoch:157/1000\n",
      "Loss on train= 0.0178285613656044\n",
      "Loss on test= 0.01835455186665058\n",
      "acc for Lsat= 0.08388629770941206 \n",
      "acc for Psat= 0.12860149542490643 \n",
      "acc for optim= 0.15403137918975618\n",
      "Epoch:158/1000\n",
      "Loss on train= 0.017608053982257843\n",
      "Loss on test= 0.01686163991689682\n",
      "acc for Lsat= 0.08677939954731201 \n",
      "acc for Psat= 0.13391011936797034 \n",
      "acc for optim= 0.155545180870427\n",
      "Epoch:159/1000\n",
      "Loss on train= 0.017623133957386017\n",
      "Loss on test= 0.017480002716183662\n",
      "acc for Lsat= 0.08883677572011946 \n",
      "acc for Psat= 0.13356555898984274 \n",
      "acc for optim= 0.15874684204657874\n",
      "Epoch:160/1000\n",
      "Loss on train= 0.01801810972392559\n",
      "Loss on test= 0.019251272082328796\n",
      "acc for Lsat= 0.09500270320309534 \n",
      "acc for Psat= 0.15728550023502774 \n",
      "acc for optim= 0.1569663006398413\n",
      "Epoch:161/1000\n",
      "Loss on train= 0.018015192821621895\n",
      "Loss on test= 0.020798036828637123\n",
      "acc for Lsat= 0.1202112376689911 \n",
      "acc for Psat= 0.1677259292867449 \n",
      "acc for optim= 0.17956173121929173\n",
      "Epoch:162/1000\n",
      "Loss on train= 0.018644852563738823\n",
      "Loss on test= 0.0180050041526556\n",
      "acc for Lsat= 0.09638474848535325 \n",
      "acc for Psat= 0.14312696986728246 \n",
      "acc for optim= 0.1559654023912218\n",
      "Epoch:163/1000\n",
      "Loss on train= 0.017866872251033783\n",
      "Loss on test= 0.01687314547598362\n",
      "acc for Lsat= 0.08553898500071633 \n",
      "acc for Psat= 0.12771111379067102 \n",
      "acc for optim= 0.15563279903597305\n",
      "Epoch:164/1000\n",
      "Loss on train= 0.01859847828745842\n",
      "Loss on test= 0.021067138761281967\n",
      "acc for Lsat= 0.10940019090970358 \n",
      "acc for Psat= 0.17556038830015394 \n",
      "acc for optim= 0.15706863751014077\n",
      "Epoch:165/1000\n",
      "Loss on train= 0.017708389088511467\n",
      "Loss on test= 0.022721843793988228\n",
      "acc for Lsat= 0.11625394788053299 \n",
      "acc for Psat= 0.22571050392256842 \n",
      "acc for optim= 0.1495575525694423\n",
      "Epoch:166/1000\n",
      "Loss on train= 0.018107648938894272\n",
      "Loss on test= 0.01855221949517727\n",
      "acc for Lsat= 0.09882398429844114 \n",
      "acc for Psat= 0.13422893020841814 \n",
      "acc for optim= 0.14759871645106207\n",
      "Epoch:167/1000\n",
      "Loss on train= 0.017340684309601784\n",
      "Loss on test= 0.01937391422688961\n",
      "acc for Lsat= 0.0913790937927034 \n",
      "acc for Psat= 0.16161512070231968 \n",
      "acc for optim= 0.15806346899933285\n",
      "Epoch:168/1000\n",
      "Loss on train= 0.018013671040534973\n",
      "Loss on test= 0.01770932786166668\n",
      "acc for Lsat= 0.08386827343040043 \n",
      "acc for Psat= 0.12553467883004082 \n",
      "acc for optim= 0.1513548915584882\n",
      "Epoch:169/1000\n",
      "Loss on train= 0.01774655655026436\n",
      "Loss on test= 0.017320580780506134\n",
      "acc for Lsat= 0.0846534581647979 \n",
      "acc for Psat= 0.12562419606579675 \n",
      "acc for optim= 0.15418538749217986\n",
      "Epoch:170/1000\n",
      "Loss on train= 0.01746421493589878\n",
      "Loss on test= 0.019089195877313614\n",
      "acc for Lsat= 0.0978316315346294 \n",
      "acc for Psat= 0.1409814476966858 \n",
      "acc for optim= 0.15394111226002374\n",
      "Epoch:171/1000\n",
      "Loss on train= 0.01820465922355652\n",
      "Loss on test= 0.01884564943611622\n",
      "acc for Lsat= 0.09483888116147783 \n",
      "acc for Psat= 0.16624345746305258 \n",
      "acc for optim= 0.1607526464594735\n",
      "Epoch:172/1000\n",
      "Loss on train= 0.017954973503947258\n",
      "Loss on test= 0.018066436052322388\n",
      "acc for Lsat= 0.09558287643724017 \n",
      "acc for Psat= 0.1581965019305547 \n",
      "acc for optim= 0.152969838016563\n",
      "Epoch:173/1000\n",
      "Loss on train= 0.018286488950252533\n",
      "Loss on test= 0.017333172261714935\n",
      "acc for Lsat= 0.09141695747772853 \n",
      "acc for Psat= 0.13252230253484515 \n",
      "acc for optim= 0.1513646235068639\n",
      "Epoch:174/1000\n",
      "Loss on train= 0.017740091308951378\n",
      "Loss on test= 0.017200641334056854\n",
      "acc for Lsat= 0.12891198347012203 \n",
      "acc for Psat= 0.1458486835161845 \n",
      "acc for optim= 0.1497475986679395\n",
      "Epoch:175/1000\n",
      "Loss on train= 0.01784157007932663\n",
      "Loss on test= 0.017871102318167686\n",
      "acc for Lsat= 0.0985000063975652 \n",
      "acc for Psat= 0.1385788099633323 \n",
      "acc for optim= 0.15255498008595572\n",
      "Epoch:176/1000\n",
      "Loss on train= 0.017444562166929245\n",
      "Loss on test= 0.017374256625771523\n",
      "acc for Lsat= 0.09254679050710465 \n",
      "acc for Psat= 0.1273927700188425 \n",
      "acc for optim= 0.15067892687188256\n",
      "Epoch:177/1000\n",
      "Loss on train= 0.01787833869457245\n",
      "Loss on test= 0.01746469736099243\n",
      "acc for Lsat= 0.09789012902312808 \n",
      "acc for Psat= 0.1229309305548668 \n",
      "acc for optim= 0.14909383406241739\n",
      "Epoch:178/1000\n",
      "Loss on train= 0.016844367608428\n",
      "Loss on test= 0.01932971365749836\n",
      "acc for Lsat= 0.1031657639476988 \n",
      "acc for Psat= 0.13684687548213537 \n",
      "acc for optim= 0.15323163270950318\n",
      "Epoch:179/1000\n",
      "Loss on train= 0.01674203760921955\n",
      "Loss on test= 0.016711805015802383\n",
      "acc for Lsat= 0.0819468362463845 \n",
      "acc for Psat= 0.1324255496263504 \n",
      "acc for optim= 0.14714899791611566\n",
      "Epoch:180/1000\n",
      "Loss on train= 0.01677815429866314\n",
      "Loss on test= 0.01829327642917633\n",
      "acc for Lsat= 0.0822533345884747 \n",
      "acc for Psat= 0.1505245463715659 \n",
      "acc for optim= 0.15299122250742384\n",
      "Epoch:181/1000\n",
      "Loss on train= 0.016853196546435356\n",
      "Loss on test= 0.018442772328853607\n",
      "acc for Lsat= 0.08249890291028551 \n",
      "acc for Psat= 0.12142560796605216 \n",
      "acc for optim= 0.15625372992621528\n",
      "Epoch:182/1000\n",
      "Loss on train= 0.01715197041630745\n",
      "Loss on test= 0.018445394933223724\n",
      "acc for Lsat= 0.1014334950182173 \n",
      "acc for Psat= 0.14982158342997234 \n",
      "acc for optim= 0.16723218063513443\n",
      "Epoch:183/1000\n",
      "Loss on train= 0.01695672981441021\n",
      "Loss on test= 0.017013436183333397\n",
      "acc for Lsat= 0.10550085372394985 \n",
      "acc for Psat= 0.11768656935956742 \n",
      "acc for optim= 0.15502765062782495\n",
      "Epoch:184/1000\n",
      "Loss on train= 0.016706952825188637\n",
      "Loss on test= 0.01671743392944336\n",
      "acc for Lsat= 0.08650411731666988 \n",
      "acc for Psat= 0.13258323238955605 \n",
      "acc for optim= 0.1522155500120587\n",
      "Epoch:185/1000\n",
      "Loss on train= 0.016832537949085236\n",
      "Loss on test= 0.017524631693959236\n",
      "acc for Lsat= 0.08863785184091992 \n",
      "acc for Psat= 0.12479270199934639 \n",
      "acc for optim= 0.14924116532007856\n",
      "Epoch:186/1000\n",
      "Loss on train= 0.017071349546313286\n",
      "Loss on test= 0.016837188974022865\n",
      "acc for Lsat= 0.09944267835881976 \n",
      "acc for Psat= 0.13552402671840455 \n",
      "acc for optim= 0.15040587915314563\n",
      "Epoch:187/1000\n",
      "Loss on train= 0.017337564378976822\n",
      "Loss on test= 0.016698982566595078\n",
      "acc for Lsat= 0.09417836467425028 \n",
      "acc for Psat= 0.12037836826509898 \n",
      "acc for optim= 0.15152412917878894\n",
      "Epoch:188/1000\n",
      "Loss on train= 0.017198840156197548\n",
      "Loss on test= 0.018848925828933716\n",
      "acc for Lsat= 0.10207372042867874 \n",
      "acc for Psat= 0.15522341562641995 \n",
      "acc for optim= 0.1526274123125606\n",
      "Epoch:189/1000\n",
      "Loss on train= 0.017209492623806\n",
      "Loss on test= 0.018115496262907982\n",
      "acc for Lsat= 0.08814159532388051 \n",
      "acc for Psat= 0.12556505501270293 \n",
      "acc for optim= 0.14811584800481803\n",
      "Epoch:190/1000\n",
      "Loss on train= 0.016497302800416946\n",
      "Loss on test= 0.016715116798877716\n",
      "acc for Lsat= 0.09575253725051881 \n",
      "acc for Psat= 0.14251057306925458 \n",
      "acc for optim= 0.1507538788848453\n",
      "Epoch:191/1000\n",
      "Loss on train= 0.01687719114124775\n",
      "Loss on test= 0.016083508729934692\n",
      "acc for Lsat= 0.08352775673071544 \n",
      "acc for Psat= 0.13970459120141135 \n",
      "acc for optim= 0.14838102575805454\n",
      "Epoch:192/1000\n",
      "Loss on train= 0.017229128628969193\n",
      "Loss on test= 0.018448444083333015\n",
      "acc for Lsat= 0.0940457315908538 \n",
      "acc for Psat= 0.13273960716194577 \n",
      "acc for optim= 0.15930539386139975\n",
      "Epoch:193/1000\n",
      "Loss on train= 0.017378486692905426\n",
      "Loss on test= 0.01760254055261612\n",
      "acc for Lsat= 0.09825292792585161 \n",
      "acc for Psat= 0.13183257910940382 \n",
      "acc for optim= 0.15054426838954293\n",
      "Epoch:194/1000\n",
      "Loss on train= 0.016354599967598915\n",
      "Loss on test= 0.01588418520987034\n",
      "acc for Lsat= 0.07751509149869283 \n",
      "acc for Psat= 0.12414695868889487 \n",
      "acc for optim= 0.14769303301970166\n",
      "Epoch:195/1000\n",
      "Loss on train= 0.016311487182974815\n",
      "Loss on test= 0.01717381738126278\n",
      "acc for Lsat= 0.09047010557519063 \n",
      "acc for Psat= 0.12379854288366104 \n",
      "acc for optim= 0.15521320021814772\n",
      "Epoch:196/1000\n",
      "Loss on train= 0.01656881347298622\n",
      "Loss on test= 0.015769407153129578\n",
      "acc for Lsat= 0.09893886744976042 \n",
      "acc for Psat= 0.13163287192583087 \n",
      "acc for optim= 0.1453490247329076\n",
      "Epoch:197/1000\n",
      "Loss on train= 0.01687290519475937\n",
      "Loss on test= 0.01722005195915699\n",
      "acc for Lsat= 0.08038474437263277 \n",
      "acc for Psat= 0.1504922009176678 \n",
      "acc for optim= 0.149120573202769\n",
      "Epoch:198/1000\n",
      "Loss on train= 0.016657374799251556\n",
      "Loss on test= 0.016851667314767838\n",
      "acc for Lsat= 0.09194784462451934 \n",
      "acc for Psat= 0.12825768788655598 \n",
      "acc for optim= 0.15194990949498283\n",
      "Epoch:199/1000\n",
      "Loss on train= 0.016697973012924194\n",
      "Loss on test= 0.018563631922006607\n",
      "acc for Lsat= 0.09292529854509565 \n",
      "acc for Psat= 0.1394860797458225 \n",
      "acc for optim= 0.15147327267461352\n",
      "Epoch:200/1000\n",
      "Loss on train= 0.016378561034798622\n",
      "Loss on test= 0.017067106440663338\n",
      "acc for Lsat= 0.09661465982596079 \n",
      "acc for Psat= 0.14513516989019182 \n",
      "acc for optim= 0.14893454362948735\n",
      "Epoch:201/1000\n",
      "Loss on train= 0.016849348321557045\n",
      "Loss on test= 0.018125690519809723\n",
      "acc for Lsat= 0.09047193908029133 \n",
      "acc for Psat= 0.16153829793135324 \n",
      "acc for optim= 0.15005813688039782\n",
      "Epoch:202/1000\n",
      "Loss on train= 0.016317516565322876\n",
      "Loss on test= 0.017679540440440178\n",
      "acc for Lsat= 0.08845130734973485 \n",
      "acc for Psat= 0.11891976379685931 \n",
      "acc for optim= 0.1491703694065412\n",
      "Epoch:203/1000\n",
      "Loss on train= 0.01653408259153366\n",
      "Loss on test= 0.017178423702716827\n",
      "acc for Lsat= 0.100924737420347 \n",
      "acc for Psat= 0.13529557519488866 \n",
      "acc for optim= 0.15932672934399716\n",
      "Epoch:204/1000\n",
      "Loss on train= 0.016871770843863487\n",
      "Loss on test= 0.01590525358915329\n",
      "acc for Lsat= 0.0843554135825899 \n",
      "acc for Psat= 0.1314873761600918 \n",
      "acc for optim= 0.15210358103116356\n",
      "Epoch:205/1000\n",
      "Loss on train= 0.016625480726361275\n",
      "Loss on test= 0.016338804736733437\n",
      "acc for Lsat= 0.09265377107593747 \n",
      "acc for Psat= 0.11476084109809664 \n",
      "acc for optim= 0.1465505212545395\n",
      "Epoch:206/1000\n",
      "Loss on train= 0.016763634979724884\n",
      "Loss on test= 0.017120355740189552\n",
      "acc for Lsat= 0.09282483226723141 \n",
      "acc for Psat= 0.12267825139893428 \n",
      "acc for optim= 0.16247237837976874\n",
      "Epoch:207/1000\n",
      "Loss on train= 0.016638899222016335\n",
      "Loss on test= 0.01705656200647354\n",
      "acc for Lsat= 0.08568083660470115 \n",
      "acc for Psat= 0.1359030157327652 \n",
      "acc for optim= 0.15860030319955615\n",
      "Epoch:208/1000\n",
      "Loss on train= 0.01634242758154869\n",
      "Loss on test= 0.01696699485182762\n",
      "acc for Lsat= 0.07969920254415937 \n",
      "acc for Psat= 0.1210559265481101 \n",
      "acc for optim= 0.1472902246647411\n",
      "Epoch:209/1000\n",
      "Loss on train= 0.015980221331119537\n",
      "Loss on test= 0.017168652266263962\n",
      "acc for Lsat= 0.08609796745909586 \n",
      "acc for Psat= 0.11937284966309865 \n",
      "acc for optim= 0.1510687313146061\n",
      "Epoch:210/1000\n",
      "Loss on train= 0.016195081174373627\n",
      "Loss on test= 0.01635575480759144\n",
      "acc for Lsat= 0.09199859433703951 \n",
      "acc for Psat= 0.11949242187870872 \n",
      "acc for optim= 0.14563377731376226\n",
      "Epoch:211/1000\n",
      "Loss on train= 0.015991542488336563\n",
      "Loss on test= 0.016717180609703064\n",
      "acc for Lsat= 0.0821732148528099 \n",
      "acc for Psat= 0.12756746378209857 \n",
      "acc for optim= 0.14946143527825673\n",
      "Epoch:212/1000\n",
      "Loss on train= 0.01607242412865162\n",
      "Loss on test= 0.017232464626431465\n",
      "acc for Lsat= 0.0835134713186158 \n",
      "acc for Psat= 0.11372861514488856 \n",
      "acc for optim= 0.1558883488178253\n",
      "Epoch:213/1000\n",
      "Loss on train= 0.016350235790014267\n",
      "Loss on test= 0.016713155433535576\n",
      "acc for Lsat= 0.09374725570281345 \n",
      "acc for Psat= 0.11627669864230687 \n",
      "acc for optim= 0.14650414122475514\n",
      "Epoch:214/1000\n",
      "Loss on train= 0.016533391550183296\n",
      "Loss on test= 0.01645577698945999\n",
      "acc for Lsat= 0.08590578354067271 \n",
      "acc for Psat= 0.1384381569094128 \n",
      "acc for optim= 0.14785911407735614\n",
      "Epoch:215/1000\n",
      "Loss on train= 0.016031796112656593\n",
      "Loss on test= 0.01619229093194008\n",
      "acc for Lsat= 0.09049584103955163 \n",
      "acc for Psat= 0.13652909348408382 \n",
      "acc for optim= 0.1473198902275827\n",
      "Epoch:216/1000\n",
      "Loss on train= 0.015879902988672256\n",
      "Loss on test= 0.016336144879460335\n",
      "acc for Lsat= 0.08224764168262483 \n",
      "acc for Psat= 0.14178358713785805 \n",
      "acc for optim= 0.1478099683920542\n",
      "Epoch:217/1000\n",
      "Loss on train= 0.01557664480060339\n",
      "Loss on test= 0.015948111191391945\n",
      "acc for Lsat= 0.08361393908659616 \n",
      "acc for Psat= 0.11492066532373427 \n",
      "acc for optim= 0.14528837634457484\n",
      "Epoch:218/1000\n",
      "Loss on train= 0.01581014320254326\n",
      "Loss on test= 0.016650862991809845\n",
      "acc for Lsat= 0.09215464062160915 \n",
      "acc for Psat= 0.11449718359443878 \n",
      "acc for optim= 0.15357975926664136\n",
      "Epoch:219/1000\n",
      "Loss on train= 0.015611154027283192\n",
      "Loss on test= 0.016725001856684685\n",
      "acc for Lsat= 0.08642446878883575 \n",
      "acc for Psat= 0.11530866672595343 \n",
      "acc for optim= 0.15734496663014094\n",
      "Epoch:220/1000\n",
      "Loss on train= 0.015465912409126759\n",
      "Loss on test= 0.016998328268527985\n",
      "acc for Lsat= 0.0944897461268637 \n",
      "acc for Psat= 0.13868431680732304 \n",
      "acc for optim= 0.15569688545333016\n",
      "Epoch:221/1000\n",
      "Loss on train= 0.015698738396167755\n",
      "Loss on test= 0.016306979581713676\n",
      "acc for Lsat= 0.08035306516620849 \n",
      "acc for Psat= 0.1272722231017219 \n",
      "acc for optim= 0.15589647243420285\n",
      "Epoch:222/1000\n",
      "Loss on train= 0.015591133385896683\n",
      "Loss on test= 0.01679726503789425\n",
      "acc for Lsat= 0.07885073141919242 \n",
      "acc for Psat= 0.1188861456182268 \n",
      "acc for optim= 0.14707281986872356\n",
      "Epoch:223/1000\n",
      "Loss on train= 0.015900705009698868\n",
      "Loss on test= 0.01623399369418621\n",
      "acc for Lsat= 0.07740872701009115 \n",
      "acc for Psat= 0.11145370387368733 \n",
      "acc for optim= 0.14883748590946197\n",
      "Epoch:224/1000\n",
      "Loss on train= 0.015758689492940903\n",
      "Loss on test= 0.016383972018957138\n",
      "acc for Lsat= 0.09158789813518524 \n",
      "acc for Psat= 0.11229779140816791 \n",
      "acc for optim= 0.1476161723335584\n",
      "Epoch:225/1000\n",
      "Loss on train= 0.015817342326045036\n",
      "Loss on test= 0.015704195946455002\n",
      "acc for Lsat= 0.09175625311003789 \n",
      "acc for Psat= 0.12363678581184812 \n",
      "acc for optim= 0.14522834552658928\n",
      "Epoch:226/1000\n",
      "Loss on train= 0.01565810851752758\n",
      "Loss on test= 0.017018571496009827\n",
      "acc for Lsat= 0.08270060701502695 \n",
      "acc for Psat= 0.1195955988433626 \n",
      "acc for optim= 0.1605286611451043\n",
      "Epoch:227/1000\n",
      "Loss on train= 0.0158681683242321\n",
      "Loss on test= 0.015873076394200325\n",
      "acc for Lsat= 0.09001389924022887 \n",
      "acc for Psat= 0.11618776834673354 \n",
      "acc for optim= 0.1519863368736373\n",
      "Epoch:228/1000\n",
      "Loss on train= 0.015479829162359238\n",
      "Loss on test= 0.01431792601943016\n",
      "acc for Lsat= 0.08168346418274776 \n",
      "acc for Psat= 0.12190941737757788 \n",
      "acc for optim= 0.14773070282406278\n",
      "Epoch:229/1000\n",
      "Loss on train= 0.015135440044105053\n",
      "Loss on test= 0.01596757210791111\n",
      "acc for Lsat= 0.09443090822961595 \n",
      "acc for Psat= 0.13757133781909942 \n",
      "acc for optim= 0.14381145139535267\n",
      "Epoch:230/1000\n",
      "Loss on train= 0.015462576411664486\n",
      "Loss on test= 0.015343483537435532\n",
      "acc for Lsat= 0.0781001607577006 \n",
      "acc for Psat= 0.12659527709086735 \n",
      "acc for optim= 0.14621023171477848\n",
      "Epoch:231/1000\n",
      "Loss on train= 0.015329539775848389\n",
      "Loss on test= 0.0168082844465971\n",
      "acc for Lsat= 0.08420233726501467 \n",
      "acc for Psat= 0.11014131440056694 \n",
      "acc for optim= 0.14480072342687186\n",
      "Epoch:232/1000\n",
      "Loss on train= 0.015668636187911034\n",
      "Loss on test= 0.015746647492051125\n",
      "acc for Lsat= 0.09108316666550106 \n",
      "acc for Psat= 0.11101313597626157 \n",
      "acc for optim= 0.14810073375701902\n",
      "Epoch:233/1000\n",
      "Loss on train= 0.015199634246528149\n",
      "Loss on test= 0.0162948090583086\n",
      "acc for Lsat= 0.07893333550956515 \n",
      "acc for Psat= 0.12061126314931446 \n",
      "acc for optim= 0.14802909460332664\n",
      "Epoch:234/1000\n",
      "Loss on train= 0.014906196855008602\n",
      "Loss on test= 0.015972929075360298\n",
      "acc for Lsat= 0.09276663909355799 \n",
      "acc for Psat= 0.13167825672361586 \n",
      "acc for optim= 0.15708760056230753\n",
      "Epoch:235/1000\n",
      "Loss on train= 0.015909366309642792\n",
      "Loss on test= 0.015242691151797771\n",
      "acc for Lsat= 0.08874098492993249 \n",
      "acc for Psat= 0.11231589284208082 \n",
      "acc for optim= 0.15039748466677133\n",
      "Epoch:236/1000\n",
      "Loss on train= 0.015512418001890182\n",
      "Loss on test= 0.01622605137526989\n",
      "acc for Lsat= 0.08116972280873194 \n",
      "acc for Psat= 0.1183681297633383 \n",
      "acc for optim= 0.15459710508584976\n",
      "Epoch:237/1000\n",
      "Loss on train= 0.015447854064404964\n",
      "Loss on test= 0.0162922665476799\n",
      "acc for Lsat= 0.08219599905941223 \n",
      "acc for Psat= 0.1224745591481527 \n",
      "acc for optim= 0.15427844557497236\n",
      "Epoch:238/1000\n",
      "Loss on train= 0.015865612775087357\n",
      "Loss on test= 0.017804861068725586\n",
      "acc for Lsat= 0.0830603258477317 \n",
      "acc for Psat= 0.12360444002681309 \n",
      "acc for optim= 0.16016719308164382\n",
      "Epoch:239/1000\n",
      "Loss on train= 0.01511811651289463\n",
      "Loss on test= 0.01586780697107315\n",
      "acc for Lsat= 0.07837666869163512 \n",
      "acc for Psat= 0.10770303010940555 \n",
      "acc for optim= 0.14395593901475268\n",
      "Epoch:240/1000\n",
      "Loss on train= 0.015371731482446194\n",
      "Loss on test= 0.01588323526084423\n",
      "acc for Lsat= 0.08687402589453591 \n",
      "acc for Psat= 0.118015890651279 \n",
      "acc for optim= 0.14833725227249991\n",
      "Epoch:241/1000\n",
      "Loss on train= 0.015247717499732971\n",
      "Loss on test= 0.015925683081150055\n",
      "acc for Lsat= 0.08093189150094987 \n",
      "acc for Psat= 0.13534276949034796 \n",
      "acc for optim= 0.15201152082946565\n",
      "Epoch:242/1000\n",
      "Loss on train= 0.015203038230538368\n",
      "Loss on test= 0.01817595586180687\n",
      "acc for Lsat= 0.09012537648280462 \n",
      "acc for Psat= 0.1250599225362142 \n",
      "acc for optim= 0.14936996466583677\n",
      "Epoch:243/1000\n",
      "Loss on train= 0.014911249279975891\n",
      "Loss on test= 0.015499141067266464\n",
      "acc for Lsat= 0.07793568438953824 \n",
      "acc for Psat= 0.12135981089539 \n",
      "acc for optim= 0.15214741594261594\n",
      "Epoch:244/1000\n",
      "Loss on train= 0.015295266173779964\n",
      "Loss on test= 0.016120046377182007\n",
      "acc for Lsat= 0.08508855700492858 \n",
      "acc for Psat= 0.1379641662041346 \n",
      "acc for optim= 0.15083079884449643\n",
      "Epoch:245/1000\n",
      "Loss on train= 0.015447399578988552\n",
      "Loss on test= 0.015556910075247288\n",
      "acc for Lsat= 0.0808608344859547 \n",
      "acc for Psat= 0.11258370015356275 \n",
      "acc for optim= 0.14663815415567824\n",
      "Epoch:246/1000\n",
      "Loss on train= 0.015061285346746445\n",
      "Loss on test= 0.015510248020291328\n",
      "acc for Lsat= 0.08313740806447135 \n",
      "acc for Psat= 0.12143121626642017 \n",
      "acc for optim= 0.14935790714290406\n",
      "Epoch:247/1000\n",
      "Loss on train= 0.015478449873626232\n",
      "Loss on test= 0.016586432233452797\n",
      "acc for Lsat= 0.11120497253206042 \n",
      "acc for Psat= 0.15668277442455292 \n",
      "acc for optim= 0.15163104782501857\n",
      "Epoch:248/1000\n",
      "Loss on train= 0.01549710612744093\n",
      "Loss on test= 0.017272822558879852\n",
      "acc for Lsat= 0.08036281300915613 \n",
      "acc for Psat= 0.1391607854101393 \n",
      "acc for optim= 0.15682479259040621\n",
      "Epoch:249/1000\n",
      "Loss on train= 0.01540434081107378\n",
      "Loss on test= 0.017074573785066605\n",
      "acc for Lsat= 0.08900435980823304 \n",
      "acc for Psat= 0.14046715895334877 \n",
      "acc for optim= 0.1519568701585134\n",
      "Epoch:250/1000\n",
      "Loss on train= 0.015411756001412868\n",
      "Loss on test= 0.014594610780477524\n",
      "acc for Lsat= 0.08287763529353673 \n",
      "acc for Psat= 0.11386682457394068 \n",
      "acc for optim= 0.14807060923841264\n",
      "Epoch:251/1000\n",
      "Loss on train= 0.014887935481965542\n",
      "Loss on test= 0.015842515975236893\n",
      "acc for Lsat= 0.0868244876464208 \n",
      "acc for Psat= 0.11494058469931286 \n",
      "acc for optim= 0.14832864883873198\n",
      "Epoch:252/1000\n",
      "Loss on train= 0.015229619108140469\n",
      "Loss on test= 0.015458798967301846\n",
      "acc for Lsat= 0.10086224029461542 \n",
      "acc for Psat= 0.11861987643771703 \n",
      "acc for optim= 0.14662828942139944\n",
      "Epoch:253/1000\n",
      "Loss on train= 0.015126433223485947\n",
      "Loss on test= 0.015977995470166206\n",
      "acc for Lsat= 0.08696728613641527 \n",
      "acc for Psat= 0.1536736786365509 \n",
      "acc for optim= 0.14735569705565765\n",
      "Epoch:254/1000\n",
      "Loss on train= 0.014768699184060097\n",
      "Loss on test= 0.016457686200737953\n",
      "acc for Lsat= 0.09476395001014074 \n",
      "acc for Psat= 0.11823007828659479 \n",
      "acc for optim= 0.14553390161858665\n",
      "Epoch:255/1000\n",
      "Loss on train= 0.015253078192472458\n",
      "Loss on test= 0.015903372317552567\n",
      "acc for Lsat= 0.10107471197843551 \n",
      "acc for Psat= 0.11152442743380865 \n",
      "acc for optim= 0.1475561441646682\n",
      "Epoch:256/1000\n",
      "Loss on train= 0.015295767225325108\n",
      "Loss on test= 0.016458196565508842\n",
      "acc for Lsat= 0.10657791511880026 \n",
      "acc for Psat= 0.11157355937692855 \n",
      "acc for optim= 0.14916687193844055\n",
      "Epoch:257/1000\n",
      "Loss on train= 0.014378699474036694\n",
      "Loss on test= 0.016744427382946014\n",
      "acc for Lsat= 0.0875456240442064 \n",
      "acc for Psat= 0.12643235872189204 \n",
      "acc for optim= 0.14816455327802236\n",
      "Epoch:258/1000\n",
      "Loss on train= 0.014663955196738243\n",
      "Loss on test= 0.015714576467871666\n",
      "acc for Lsat= 0.08662781665722527 \n",
      "acc for Psat= 0.11198459035820428 \n",
      "acc for optim= 0.14364440772268505\n",
      "Epoch:259/1000\n",
      "Loss on train= 0.015535090118646622\n",
      "Loss on test= 0.015491068363189697\n",
      "acc for Lsat= 0.08742399745517307 \n",
      "acc for Psat= 0.11379953275124234 \n",
      "acc for optim= 0.14945116705364656\n",
      "Epoch:260/1000\n",
      "Loss on train= 0.014683078974485397\n",
      "Loss on test= 0.016035171225667\n",
      "acc for Lsat= 0.0795589827828937 \n",
      "acc for Psat= 0.10552146202988094 \n",
      "acc for optim= 0.15444599986076357\n",
      "Epoch:261/1000\n",
      "Loss on train= 0.014655029401183128\n",
      "Loss on test= 0.016445305198431015\n",
      "acc for Lsat= 0.0842729346619712 \n",
      "acc for Psat= 0.1108098910190165 \n",
      "acc for optim= 0.14861564983924228\n",
      "Epoch:262/1000\n",
      "Loss on train= 0.014619843102991581\n",
      "Loss on test= 0.015566711314022541\n",
      "acc for Lsat= 0.09550993674331242 \n",
      "acc for Psat= 0.10526249864035181 \n",
      "acc for optim= 0.15265786117977567\n",
      "Epoch:263/1000\n",
      "Loss on train= 0.014921396039426327\n",
      "Loss on test= 0.01603700965642929\n",
      "acc for Lsat= 0.0906301733520296 \n",
      "acc for Psat= 0.11749861935774485 \n",
      "acc for optim= 0.15261371731758117\n",
      "Epoch:264/1000\n",
      "Loss on train= 0.015497595071792603\n",
      "Loss on test= 0.015605449676513672\n",
      "acc for Lsat= 0.08500124298863941 \n",
      "acc for Psat= 0.1256302134858237 \n",
      "acc for optim= 0.14492020375198789\n",
      "Epoch:265/1000\n",
      "Loss on train= 0.014406364411115646\n",
      "Loss on test= 0.015447099693119526\n",
      "acc for Lsat= 0.07592880162927838 \n",
      "acc for Psat= 0.10152106442385249 \n",
      "acc for optim= 0.14671444677644305\n",
      "Epoch:266/1000\n",
      "Loss on train= 0.014539451338350773\n",
      "Loss on test= 0.01450127363204956\n",
      "acc for Lsat= 0.08348613513840569 \n",
      "acc for Psat= 0.10790312505430645 \n",
      "acc for optim= 0.15359623995092184\n",
      "Epoch:267/1000\n",
      "Loss on train= 0.015370864421129227\n",
      "Loss on test= 0.01598203182220459\n",
      "acc for Lsat= 0.08372986730602053 \n",
      "acc for Psat= 0.12535339097181955 \n",
      "acc for optim= 0.15291959643363953\n",
      "Epoch:268/1000\n",
      "Loss on train= 0.015152418985962868\n",
      "Loss on test= 0.016019299626350403\n",
      "acc for Lsat= 0.08879418389664756 \n",
      "acc for Psat= 0.12442221972677443 \n",
      "acc for optim= 0.15121438105901083\n",
      "Epoch:269/1000\n",
      "Loss on train= 0.01474218349903822\n",
      "Loss on test= 0.015691928565502167\n",
      "acc for Lsat= 0.08006727513339786 \n",
      "acc for Psat= 0.12123248245981004 \n",
      "acc for optim= 0.1522267484002643\n",
      "Epoch:270/1000\n",
      "Loss on train= 0.014866705983877182\n",
      "Loss on test= 0.015745021402835846\n",
      "acc for Lsat= 0.082383417752054 \n",
      "acc for Psat= 0.10414087043868168 \n",
      "acc for optim= 0.1529116920299\n",
      "Epoch:271/1000\n",
      "Loss on train= 0.014614247716963291\n",
      "Loss on test= 0.016237949952483177\n",
      "acc for Lsat= 0.08534445928202736 \n",
      "acc for Psat= 0.1193380289607578 \n",
      "acc for optim= 0.15125646210379068\n",
      "Epoch:272/1000\n",
      "Loss on train= 0.014896423555910587\n",
      "Loss on test= 0.015651710331439972\n",
      "acc for Lsat= 0.07794562843110825 \n",
      "acc for Psat= 0.13044151928689743 \n",
      "acc for optim= 0.14865051011244457\n",
      "Epoch:273/1000\n",
      "Loss on train= 0.01517778355628252\n",
      "Loss on test= 0.01610463485121727\n",
      "acc for Lsat= 0.08002925001912647 \n",
      "acc for Psat= 0.10861683603790069 \n",
      "acc for optim= 0.1527321770787239\n",
      "Epoch:274/1000\n",
      "Loss on train= 0.014716935344040394\n",
      "Loss on test= 0.01510736346244812\n",
      "acc for Lsat= 0.08121967050764295 \n",
      "acc for Psat= 0.11341774827904172 \n",
      "acc for optim= 0.14582508073912728\n",
      "Epoch:275/1000\n",
      "Loss on train= 0.01459547784179449\n",
      "Loss on test= 0.015495717525482178\n",
      "acc for Lsat= 0.0825432745946778 \n",
      "acc for Psat= 0.11354245377911464 \n",
      "acc for optim= 0.14707076532973182\n",
      "Epoch:276/1000\n",
      "Loss on train= 0.014453396201133728\n",
      "Loss on test= 0.015566001646220684\n",
      "acc for Lsat= 0.08484942697816426 \n",
      "acc for Psat= 0.13480620483557385 \n",
      "acc for optim= 0.15153315332200792\n",
      "Epoch:277/1000\n",
      "Loss on train= 0.014281004667282104\n",
      "Loss on test= 0.014803288504481316\n",
      "acc for Lsat= 0.09198187490304312 \n",
      "acc for Psat= 0.1171956772605578 \n",
      "acc for optim= 0.15234153154823513\n",
      "Epoch:278/1000\n",
      "Loss on train= 0.01510631199926138\n",
      "Loss on test= 0.01539874728769064\n",
      "acc for Lsat= 0.0823625256617864 \n",
      "acc for Psat= 0.10574030213885838 \n",
      "acc for optim= 0.15474276310867732\n",
      "Epoch:279/1000\n",
      "Loss on train= 0.014963511377573013\n",
      "Loss on test= 0.015346045605838299\n",
      "acc for Lsat= 0.0770037516951561 \n",
      "acc for Psat= 0.10734576268328559 \n",
      "acc for optim= 0.14565025468667347\n",
      "Epoch:280/1000\n",
      "Loss on train= 0.014302670955657959\n",
      "Loss on test= 0.01464142743498087\n",
      "acc for Lsat= 0.08419794605837927 \n",
      "acc for Psat= 0.11239904314279557 \n",
      "acc for optim= 0.14557641661829418\n",
      "Epoch:281/1000\n",
      "Loss on train= 0.014075437560677528\n",
      "Loss on test= 0.015768710523843765\n",
      "acc for Lsat= 0.08812507738669713 \n",
      "acc for Psat= 0.10692309505409664 \n",
      "acc for optim= 0.14446107496817906\n",
      "Epoch:282/1000\n",
      "Loss on train= 0.014800370670855045\n",
      "Loss on test= 0.015784459188580513\n",
      "acc for Lsat= 0.07532089203596115 \n",
      "acc for Psat= 0.09516765429741807 \n",
      "acc for optim= 0.149622384707133\n",
      "Epoch:283/1000\n",
      "Loss on train= 0.01474368292838335\n",
      "Loss on test= 0.015540655702352524\n",
      "acc for Lsat= 0.07792782055007089 \n",
      "acc for Psat= 0.10494067503346337 \n",
      "acc for optim= 0.1532509939538108\n",
      "Epoch:284/1000\n",
      "Loss on train= 0.014908850193023682\n",
      "Loss on test= 0.015993908047676086\n",
      "acc for Lsat= 0.08356343491209878 \n",
      "acc for Psat= 0.10611565891239375 \n",
      "acc for optim= 0.15188452783558104\n",
      "Epoch:285/1000\n",
      "Loss on train= 0.014214552007615566\n",
      "Loss on test= 0.016458576545119286\n",
      "acc for Lsat= 0.07937061505185233 \n",
      "acc for Psat= 0.12168656521373326 \n",
      "acc for optim= 0.15552749600675372\n",
      "Epoch:286/1000\n",
      "Loss on train= 0.014212995767593384\n",
      "Loss on test= 0.014518694952130318\n",
      "acc for Lsat= 0.07753139866722954 \n",
      "acc for Psat= 0.09975438912709556 \n",
      "acc for optim= 0.14845895651313995\n",
      "Epoch:287/1000\n",
      "Loss on train= 0.014718100428581238\n",
      "Loss on test= 0.015121949836611748\n",
      "acc for Lsat= 0.08630457603269154 \n",
      "acc for Psat= 0.11553568624787862 \n",
      "acc for optim= 0.14990251660346987\n",
      "Epoch:288/1000\n",
      "Loss on train= 0.014220079407095909\n",
      "Loss on test= 0.016423918306827545\n",
      "acc for Lsat= 0.08861947026517657 \n",
      "acc for Psat= 0.11209704710377585 \n",
      "acc for optim= 0.14527530703279712\n",
      "Epoch:289/1000\n",
      "Loss on train= 0.014438793994486332\n",
      "Loss on test= 0.01593594253063202\n",
      "acc for Lsat= 0.07545565234290229 \n",
      "acc for Psat= 0.10423205544551213 \n",
      "acc for optim= 0.15108545621236166\n",
      "Epoch:290/1000\n",
      "Loss on train= 0.014316115528345108\n",
      "Loss on test= 0.015225199982523918\n",
      "acc for Lsat= 0.07847710400819778 \n",
      "acc for Psat= 0.11148338052961562 \n",
      "acc for optim= 0.15275482535362242\n",
      "Epoch:291/1000\n",
      "Loss on train= 0.014287833124399185\n",
      "Loss on test= 0.015155606903135777\n",
      "acc for Lsat= 0.08358687543206743 \n",
      "acc for Psat= 0.1167692313591639 \n",
      "acc for optim= 0.15076968322197595\n",
      "Epoch:292/1000\n",
      "Loss on train= 0.014005575329065323\n",
      "Loss on test= 0.01583358459174633\n",
      "acc for Lsat= 0.08486919320291943 \n",
      "acc for Psat= 0.137925237748358 \n",
      "acc for optim= 0.1513504809803433\n",
      "Epoch:293/1000\n",
      "Loss on train= 0.014470243826508522\n",
      "Loss on test= 0.016306588426232338\n",
      "acc for Lsat= 0.08458280397786033 \n",
      "acc for Psat= 0.11590891049967873 \n",
      "acc for optim= 0.15612998306751252\n",
      "Epoch:294/1000\n",
      "Loss on train= 0.014711013063788414\n",
      "Loss on test= 0.016146736219525337\n",
      "acc for Lsat= 0.09240178912878036 \n",
      "acc for Psat= 0.12377307630247539 \n",
      "acc for optim= 0.1473360944125387\n",
      "Epoch:295/1000\n",
      "Loss on train= 0.014558836817741394\n",
      "Loss on test= 0.01842503994703293\n",
      "acc for Lsat= 0.08413776738776102 \n",
      "acc for Psat= 0.11950740036037232 \n",
      "acc for optim= 0.14915626909997728\n",
      "Epoch:296/1000\n",
      "Loss on train= 0.01448036078363657\n",
      "Loss on test= 0.01595919579267502\n",
      "acc for Lsat= 0.08140974707073634 \n",
      "acc for Psat= 0.10514712300565508 \n",
      "acc for optim= 0.1472286550535096\n",
      "Epoch:297/1000\n",
      "Loss on train= 0.01406390406191349\n",
      "Loss on test= 0.016447363421320915\n",
      "acc for Lsat= 0.09232819626728693 \n",
      "acc for Psat= 0.10618808103932273 \n",
      "acc for optim= 0.1560811233189371\n",
      "Epoch:298/1000\n",
      "Loss on train= 0.014082351699471474\n",
      "Loss on test= 0.016929786652326584\n",
      "acc for Lsat= 0.08615569720665613 \n",
      "acc for Psat= 0.10810064574082695 \n",
      "acc for optim= 0.15831052561601006\n",
      "Epoch:299/1000\n",
      "Loss on train= 0.014227225445210934\n",
      "Loss on test= 0.014367584139108658\n",
      "acc for Lsat= 0.09192197836107677 \n",
      "acc for Psat= 0.12976081636216905 \n",
      "acc for optim= 0.14293122672372396\n",
      "Epoch:300/1000\n",
      "Loss on train= 0.014445066452026367\n",
      "Loss on test= 0.014870992861688137\n",
      "acc for Lsat= 0.0899864966670672 \n",
      "acc for Psat= 0.12268614106708103 \n",
      "acc for optim= 0.1443899441096518\n",
      "Epoch:301/1000\n",
      "Loss on train= 0.013852057047188282\n",
      "Loss on test= 0.015408406965434551\n",
      "acc for Lsat= 0.08709770540396372 \n",
      "acc for Psat= 0.13764155242178175 \n",
      "acc for optim= 0.14498584800296357\n",
      "Epoch:302/1000\n",
      "Loss on train= 0.01429752353578806\n",
      "Loss on test= 0.01487762201577425\n",
      "acc for Lsat= 0.08329305350780486 \n",
      "acc for Psat= 0.1249820994006263 \n",
      "acc for optim= 0.14962274332841236\n",
      "Epoch:303/1000\n",
      "Loss on train= 0.01398989837616682\n",
      "Loss on test= 0.015551620163023472\n",
      "acc for Lsat= 0.08053283575508331 \n",
      "acc for Psat= 0.11053678244352341 \n",
      "acc for optim= 0.15609829061561162\n",
      "Epoch:304/1000\n",
      "Loss on train= 0.014288601465523243\n",
      "Loss on test= 0.014024719595909119\n",
      "acc for Lsat= 0.08080876304043665 \n",
      "acc for Psat= 0.09757283942566977 \n",
      "acc for optim= 0.14920968065659204\n",
      "Epoch:305/1000\n",
      "Loss on train= 0.014109640382230282\n",
      "Loss on test= 0.014616699889302254\n",
      "acc for Lsat= 0.08340458886490928 \n",
      "acc for Psat= 0.1153317246172163 \n",
      "acc for optim= 0.14224708080291745\n",
      "Epoch:306/1000\n",
      "Loss on train= 0.014454564079642296\n",
      "Loss on test= 0.015043243765830994\n",
      "acc for Lsat= 0.08866176207860312 \n",
      "acc for Psat= 0.11495159831311969 \n",
      "acc for optim= 0.1489726972248819\n",
      "Epoch:307/1000\n",
      "Loss on train= 0.013763803988695145\n",
      "Loss on test= 0.016016416251659393\n",
      "acc for Lsat= 0.09124532192945482 \n",
      "acc for Psat= 0.11284393105241991 \n",
      "acc for optim= 0.14354536864492629\n",
      "Epoch:308/1000\n",
      "Loss on train= 0.013531237840652466\n",
      "Loss on test= 0.015806715935468674\n",
      "acc for Lsat= 0.07924325929747686 \n",
      "acc for Psat= 0.09683627337217332 \n",
      "acc for optim= 0.14610074493620132\n",
      "Epoch:309/1000\n",
      "Loss on train= 0.013749373145401478\n",
      "Loss on test= 0.015489737503230572\n",
      "acc for Lsat= 0.09123079710536533 \n",
      "acc for Psat= 0.13382662071122062 \n",
      "acc for optim= 0.1464391976594925\n",
      "Epoch:310/1000\n",
      "Loss on train= 0.014164265245199203\n",
      "Loss on test= 0.016771655529737473\n",
      "acc for Lsat= 0.08767736123667821 \n",
      "acc for Psat= 0.14175435105959575 \n",
      "acc for optim= 0.1487713580330213\n",
      "Epoch:311/1000\n",
      "Loss on train= 0.013882255181670189\n",
      "Loss on test= 0.01529941987246275\n",
      "acc for Lsat= 0.07447846863004896 \n",
      "acc for Psat= 0.1104093702303039 \n",
      "acc for optim= 0.14712051136626136\n",
      "Epoch:312/1000\n",
      "Loss on train= 0.013771495781838894\n",
      "Loss on test= 0.014086276292800903\n",
      "acc for Lsat= 0.07356553922096887 \n",
      "acc for Psat= 0.10059050760335393 \n",
      "acc for optim= 0.1489028395877944\n",
      "Epoch:313/1000\n",
      "Loss on train= 0.013998991809785366\n",
      "Loss on test= 0.016208019107580185\n",
      "acc for Lsat= 0.07772639625602298 \n",
      "acc for Psat= 0.10968465689155792 \n",
      "acc for optim= 0.14991534004608786\n",
      "Epoch:314/1000\n",
      "Loss on train= 0.01372058317065239\n",
      "Loss on test= 0.015208250842988491\n",
      "acc for Lsat= 0.07185550779104233 \n",
      "acc for Psat= 0.09931123488479189 \n",
      "acc for optim= 0.14718832522630693\n",
      "Epoch:315/1000\n",
      "Loss on train= 0.013777226209640503\n",
      "Loss on test= 0.015969382598996162\n",
      "acc for Lsat= 0.09005155083205967 \n",
      "acc for Psat= 0.11746967666678959 \n",
      "acc for optim= 0.150639741619428\n",
      "Epoch:316/1000\n",
      "Loss on train= 0.013711202889680862\n",
      "Loss on test= 0.014319676905870438\n",
      "acc for Lsat= 0.07719448722071118 \n",
      "acc for Psat= 0.10525986469454235 \n",
      "acc for optim= 0.14709655708736843\n",
      "Epoch:317/1000\n",
      "Loss on train= 0.014048933982849121\n",
      "Loss on test= 0.01512719877064228\n",
      "acc for Lsat= 0.08720353030496174 \n",
      "acc for Psat= 0.09785744390553901 \n",
      "acc for optim= 0.14836025635401406\n",
      "Epoch:318/1000\n",
      "Loss on train= 0.01404353603720665\n",
      "Loss on test= 0.017421932891011238\n",
      "acc for Lsat= 0.08119964334699842 \n",
      "acc for Psat= 0.11410703278250163 \n",
      "acc for optim= 0.14745547456873787\n",
      "Epoch:319/1000\n",
      "Loss on train= 0.01371197123080492\n",
      "Loss on test= 0.01705833338201046\n",
      "acc for Lsat= 0.08864171869224971 \n",
      "acc for Psat= 0.12392814490530224 \n",
      "acc for optim= 0.15133892910348046\n",
      "Epoch:320/1000\n",
      "Loss on train= 0.013510894030332565\n",
      "Loss on test= 0.015291217714548111\n",
      "acc for Lsat= 0.08668387118313049 \n",
      "acc for Psat= 0.10902768986092673 \n",
      "acc for optim= 0.14478775759538015\n",
      "Epoch:321/1000\n",
      "Loss on train= 0.013547489419579506\n",
      "Loss on test= 0.016331732273101807\n",
      "acc for Lsat= 0.07947317328717975 \n",
      "acc for Psat= 0.10635903345213994 \n",
      "acc for optim= 0.14640019171767765\n",
      "Epoch:322/1000\n",
      "Loss on train= 0.013973466120660305\n",
      "Loss on test= 0.013864172622561455\n",
      "acc for Lsat= 0.07932631489303377 \n",
      "acc for Psat= 0.10940157125393551 \n",
      "acc for optim= 0.14571019129620655\n",
      "Epoch:323/1000\n",
      "Loss on train= 0.014370238408446312\n",
      "Loss on test= 0.014582674950361252\n",
      "acc for Lsat= 0.07962191303571065 \n",
      "acc for Psat= 0.11289427942699855 \n",
      "acc for optim= 0.1474209353327751\n",
      "Epoch:324/1000\n",
      "Loss on train= 0.013976985588669777\n",
      "Loss on test= 0.015982596203684807\n",
      "acc for Lsat= 0.08892949372529983 \n",
      "acc for Psat= 0.11607574207915199 \n",
      "acc for optim= 0.1497165516018867\n",
      "Epoch:325/1000\n",
      "Loss on train= 0.014053950086236\n",
      "Loss on test= 0.015935352072119713\n",
      "acc for Lsat= 0.0963916735516654 \n",
      "acc for Psat= 0.10511147446102569 \n",
      "acc for optim= 0.15082285636001166\n",
      "Epoch:326/1000\n",
      "Loss on train= 0.013653839007019997\n",
      "Loss on test= 0.014795277267694473\n",
      "acc for Lsat= 0.0821731726328532 \n",
      "acc for Psat= 0.1029791221022606 \n",
      "acc for optim= 0.15322566065523363\n",
      "Epoch:327/1000\n",
      "Loss on train= 0.013511291705071926\n",
      "Loss on test= 0.014756820164620876\n",
      "acc for Lsat= 0.07591488328244951 \n",
      "acc for Psat= 0.11602331035666996 \n",
      "acc for optim= 0.14992002712355718\n",
      "Epoch:328/1000\n",
      "Loss on train= 0.01371998805552721\n",
      "Loss on test= 0.015396409668028355\n",
      "acc for Lsat= 0.08221437848276564 \n",
      "acc for Psat= 0.09842984577020009 \n",
      "acc for optim= 0.1550327984823121\n",
      "Epoch:329/1000\n",
      "Loss on train= 0.013766681775450706\n",
      "Loss on test= 0.015238608233630657\n",
      "acc for Lsat= 0.081497885286808 \n",
      "acc for Psat= 0.11920901586612069 \n",
      "acc for optim= 0.15045016904671987\n",
      "Epoch:330/1000\n",
      "Loss on train= 0.013502857647836208\n",
      "Loss on test= 0.01615103706717491\n",
      "acc for Lsat= 0.07852146145370272 \n",
      "acc for Psat= 0.1110787004232407 \n",
      "acc for optim= 0.14961707707908423\n",
      "Epoch:331/1000\n",
      "Loss on train= 0.013473212718963623\n",
      "Loss on test= 0.0163063183426857\n",
      "acc for Lsat= 0.08859909723202387 \n",
      "acc for Psat= 0.11229309406545429 \n",
      "acc for optim= 0.1475517686870363\n",
      "Epoch:332/1000\n",
      "Loss on train= 0.013446497730910778\n",
      "Loss on test= 0.015545457601547241\n",
      "acc for Lsat= 0.07487642698817781 \n",
      "acc for Psat= 0.09759136935075124 \n",
      "acc for optim= 0.14540958023733563\n",
      "Epoch:333/1000\n",
      "Loss on train= 0.013660687021911144\n",
      "Loss on test= 0.014882253482937813\n",
      "acc for Lsat= 0.07300244255198372 \n",
      "acc for Psat= 0.09838096814023123 \n",
      "acc for optim= 0.14847213592794206\n",
      "Epoch:334/1000\n",
      "Loss on train= 0.013751483522355556\n",
      "Loss on test= 0.015398915857076645\n",
      "acc for Lsat= 0.08735062562757069 \n",
      "acc for Psat= 0.12738403611712984 \n",
      "acc for optim= 0.14584010971917047\n",
      "Epoch:335/1000\n",
      "Loss on train= 0.013725847005844116\n",
      "Loss on test= 0.014672782272100449\n",
      "acc for Lsat= 0.07918067276477814 \n",
      "acc for Psat= 0.10735725164413452 \n",
      "acc for optim= 0.1493708292643229\n",
      "Epoch:336/1000\n",
      "Loss on train= 0.013458117842674255\n",
      "Loss on test= 0.0150421392172575\n",
      "acc for Lsat= 0.09706232299407323 \n",
      "acc for Psat= 0.10882703363895416 \n",
      "acc for optim= 0.1489339186085595\n",
      "Epoch:337/1000\n",
      "Loss on train= 0.013393462635576725\n",
      "Loss on test= 0.01534202042967081\n",
      "acc for Lsat= 0.07537977529896629 \n",
      "acc for Psat= 0.1064091157582071 \n",
      "acc for optim= 0.14794256455368468\n",
      "Epoch:338/1000\n",
      "Loss on train= 0.013733730651438236\n",
      "Loss on test= 0.01648777164518833\n",
      "acc for Lsat= 0.07559581398963929 \n",
      "acc for Psat= 0.09844509445958667 \n",
      "acc for optim= 0.14664406345950232\n",
      "Epoch:339/1000\n",
      "Loss on train= 0.013610536232590675\n",
      "Loss on test= 0.014519495889544487\n",
      "acc for Lsat= 0.0952928708659278 \n",
      "acc for Psat= 0.10226707226700252 \n",
      "acc for optim= 0.1466709895266427\n",
      "Epoch:340/1000\n",
      "Loss on train= 0.013639700599014759\n",
      "Loss on test= 0.015013810247182846\n",
      "acc for Lsat= 0.0849439142478837 \n",
      "acc for Psat= 0.10848540266354881 \n",
      "acc for optim= 0.14654792646567028\n",
      "Epoch:341/1000\n",
      "Loss on train= 0.013359840959310532\n",
      "Loss on test= 0.017476491630077362\n",
      "acc for Lsat= 0.07604153818554349 \n",
      "acc for Psat= 0.10521119982004166 \n",
      "acc for optim= 0.1510157356659571\n",
      "Epoch:342/1000\n",
      "Loss on train= 0.012998610734939575\n",
      "Loss on test= 0.01431781705468893\n",
      "acc for Lsat= 0.07737034807602565 \n",
      "acc for Psat= 0.11942984826034969 \n",
      "acc for optim= 0.14733538462056053\n",
      "Epoch:343/1000\n",
      "Loss on train= 0.013333801180124283\n",
      "Loss on test= 0.015541129745543003\n",
      "acc for Lsat= 0.08385544882880318 \n",
      "acc for Psat= 0.12857039405239953 \n",
      "acc for optim= 0.15092180040147568\n",
      "Epoch:344/1000\n",
      "Loss on train= 0.013451069593429565\n",
      "Loss on test= 0.015954528003931046\n",
      "acc for Lsat= 0.08499162329567804 \n",
      "acc for Psat= 0.1261339796913995 \n",
      "acc for optim= 0.15030003984769186\n",
      "Epoch:345/1000\n",
      "Loss on train= 0.013424628414213657\n",
      "Loss on test= 0.014576592482626438\n",
      "acc for Lsat= 0.07316201163662803 \n",
      "acc for Psat= 0.10004620220926075 \n",
      "acc for optim= 0.14738515930043328\n",
      "Epoch:346/1000\n",
      "Loss on train= 0.013265454210340977\n",
      "Loss on test= 0.014919351786375046\n",
      "acc for Lsat= 0.08772003451983135 \n",
      "acc for Psat= 0.13508406943745085 \n",
      "acc for optim= 0.14824624094698166\n",
      "Epoch:347/1000\n",
      "Loss on train= 0.01311984658241272\n",
      "Loss on test= 0.014898298308253288\n",
      "acc for Lsat= 0.07721058991220263 \n",
      "acc for Psat= 0.09746157543526753 \n",
      "acc for optim= 0.14535890056027304\n",
      "Epoch:348/1000\n",
      "Loss on train= 0.013440481387078762\n",
      "Loss on test= 0.015500073321163654\n",
      "acc for Lsat= 0.08556724869542653 \n",
      "acc for Psat= 0.11885219415028889 \n",
      "acc for optim= 0.14796654615137314\n",
      "Epoch:349/1000\n",
      "Loss on train= 0.013849854469299316\n",
      "Loss on test= 0.015632567927241325\n",
      "acc for Lsat= 0.07399161772595512 \n",
      "acc for Psat= 0.10295839094453389 \n",
      "acc for optim= 0.14552960644165672\n",
      "Epoch:350/1000\n",
      "Loss on train= 0.013364509679377079\n",
      "Loss on test= 0.014976110309362411\n",
      "acc for Lsat= 0.09085811989174947 \n",
      "acc for Psat= 0.1092222198843956 \n",
      "acc for optim= 0.1488488328125742\n",
      "Epoch:351/1000\n",
      "Loss on train= 0.013801146298646927\n",
      "Loss on test= 0.014480371959507465\n",
      "acc for Lsat= 0.07722775787115098 \n",
      "acc for Psat= 0.12862407498889503 \n",
      "acc for optim= 0.14611541314257517\n",
      "Epoch:352/1000\n",
      "Loss on train= 0.013327095657587051\n",
      "Loss on test= 0.015822717919945717\n",
      "acc for Lsat= 0.07399745401408936 \n",
      "acc for Psat= 0.101330024169551 \n",
      "acc for optim= 0.14788785742388832\n",
      "Epoch:353/1000\n",
      "Loss on train= 0.01322202105075121\n",
      "Loss on test= 0.014536827802658081\n",
      "acc for Lsat= 0.07672494765785005 \n",
      "acc for Psat= 0.11065437810288534 \n",
      "acc for optim= 0.14528684500190947\n",
      "Epoch:354/1000\n",
      "Loss on train= 0.013562668114900589\n",
      "Loss on test= 0.014717940241098404\n",
      "acc for Lsat= 0.08828302754296197 \n",
      "acc for Psat= 0.11407684021525914 \n",
      "acc for optim= 0.14689918077654313\n",
      "Epoch:355/1000\n",
      "Loss on train= 0.013359805569052696\n",
      "Loss on test= 0.014573480933904648\n",
      "acc for Lsat= 0.07533509184916815 \n",
      "acc for Psat= 0.10777765363454819 \n",
      "acc for optim= 0.1472842208213276\n",
      "Epoch:356/1000\n",
      "Loss on train= 0.01287017110735178\n",
      "Loss on test= 0.014263118617236614\n",
      "acc for Lsat= 0.07114201767577064 \n",
      "acc for Psat= 0.10335343993372387 \n",
      "acc for optim= 0.14829107820987705\n",
      "Epoch:357/1000\n",
      "Loss on train= 0.013258527964353561\n",
      "Loss on test= 0.015616551041603088\n",
      "acc for Lsat= 0.07116475817230013 \n",
      "acc for Psat= 0.09920144908958012 \n",
      "acc for optim= 0.14729098396168816\n",
      "Epoch:358/1000\n",
      "Loss on train= 0.012831167317926884\n",
      "Loss on test= 0.01472418662160635\n",
      "acc for Lsat= 0.07423460731903712 \n",
      "acc for Psat= 0.1152124075425996 \n",
      "acc for optim= 0.15227691796090864\n",
      "Epoch:359/1000\n",
      "Loss on train= 0.013237741775810719\n",
      "Loss on test= 0.013969866558909416\n",
      "acc for Lsat= 0.08077803055445353 \n",
      "acc for Psat= 0.10846361054314507 \n",
      "acc for optim= 0.14399555259280736\n",
      "Epoch:360/1000\n",
      "Loss on train= 0.012764772400259972\n",
      "Loss on test= 0.015513488091528416\n",
      "acc for Lsat= 0.0760145374470287 \n",
      "acc for Psat= 0.10658952312337028 \n",
      "acc for optim= 0.14771859629286663\n",
      "Epoch:361/1000\n",
      "Loss on train= 0.0127173513174057\n",
      "Loss on test= 0.015687737613916397\n",
      "acc for Lsat= 0.07435194071796204 \n",
      "acc for Psat= 0.09635116193029616 \n",
      "acc for optim= 0.14792811787790722\n",
      "Epoch:362/1000\n",
      "Loss on train= 0.012798353098332882\n",
      "Loss on test= 0.013894109055399895\n",
      "acc for Lsat= 0.07822543101178275 \n",
      "acc for Psat= 0.09457411550813252 \n",
      "acc for optim= 0.14799515042040082\n",
      "Epoch:363/1000\n",
      "Loss on train= 0.013589185662567616\n",
      "Loss on test= 0.015469435602426529\n",
      "acc for Lsat= 0.08319182561503517 \n",
      "acc for Psat= 0.10007117589314779 \n",
      "acc for optim= 0.15681314170360564\n",
      "Epoch:364/1000\n",
      "Loss on train= 0.013224751688539982\n",
      "Loss on test= 0.01502038724720478\n",
      "acc for Lsat= 0.07972779340214199 \n",
      "acc for Psat= 0.10914172182480493 \n",
      "acc for optim= 0.1442188958326976\n",
      "Epoch:365/1000\n",
      "Loss on train= 0.013355979695916176\n",
      "Loss on test= 0.016130385920405388\n",
      "acc for Lsat= 0.08375688327683344 \n",
      "acc for Psat= 0.1283515555991067 \n",
      "acc for optim= 0.1469598255223698\n",
      "Epoch:366/1000\n",
      "Loss on train= 0.012955183163285255\n",
      "Loss on test= 0.015103353187441826\n",
      "acc for Lsat= 0.08053742763068941 \n",
      "acc for Psat= 0.1074116736650467 \n",
      "acc for optim= 0.14513069921069677\n",
      "Epoch:367/1000\n",
      "Loss on train= 0.012775620445609093\n",
      "Loss on test= 0.01410186942666769\n",
      "acc for Lsat= 0.08006663570801416 \n",
      "acc for Psat= 0.10669234378470314 \n",
      "acc for optim= 0.1454180364807447\n",
      "Epoch:368/1000\n",
      "Loss on train= 0.013060322031378746\n",
      "Loss on test= 0.015110854059457779\n",
      "acc for Lsat= 0.07512929356760449 \n",
      "acc for Psat= 0.1115276167790095 \n",
      "acc for optim= 0.1461300712492731\n",
      "Epoch:369/1000\n",
      "Loss on train= 0.013133049011230469\n",
      "Loss on test= 0.014781215228140354\n",
      "acc for Lsat= 0.10394034402238 \n",
      "acc for Psat= 0.12244026793373955 \n",
      "acc for optim= 0.14470617291000154\n",
      "Epoch:370/1000\n",
      "Loss on train= 0.012800821103155613\n",
      "Loss on test= 0.015406913124024868\n",
      "acc for Lsat= 0.07369232128063836 \n",
      "acc for Psat= 0.09624664452340867 \n",
      "acc for optim= 0.14611448347568515\n",
      "Epoch:371/1000\n",
      "Loss on train= 0.013144394382834435\n",
      "Loss on test= 0.01428412739187479\n",
      "acc for Lsat= 0.08182368261946574 \n",
      "acc for Psat= 0.11027786764833664 \n",
      "acc for optim= 0.14846097876628242\n",
      "Epoch:372/1000\n",
      "Loss on train= 0.013348096050322056\n",
      "Loss on test= 0.014581810683012009\n",
      "acc for Lsat= 0.07119826955927742 \n",
      "acc for Psat= 0.09855154173241722 \n",
      "acc for optim= 0.14445960240231626\n",
      "Epoch:373/1000\n",
      "Loss on train= 0.01307517196983099\n",
      "Loss on test= 0.015613434836268425\n",
      "acc for Lsat= 0.07452973922093709 \n",
      "acc for Psat= 0.10854941589964763 \n",
      "acc for optim= 0.15183674461311764\n",
      "Epoch:374/1000\n",
      "Loss on train= 0.013084412552416325\n",
      "Loss on test= 0.01518690399825573\n",
      "acc for Lsat= 0.07625623328818215 \n",
      "acc for Psat= 0.10025377786821789 \n",
      "acc for optim= 0.1472054087453418\n",
      "Epoch:375/1000\n",
      "Loss on train= 0.012981485575437546\n",
      "Loss on test= 0.014917590655386448\n",
      "acc for Lsat= 0.07959318144453896 \n",
      "acc for Psat= 0.09933232797516717 \n",
      "acc for optim= 0.14785150902138816\n",
      "Epoch:376/1000\n",
      "Loss on train= 0.012802463956177235\n",
      "Loss on test= 0.015685822814702988\n",
      "acc for Lsat= 0.0799673393368721 \n",
      "acc for Psat= 0.1134598155816396 \n",
      "acc for optim= 0.1479367623726527\n",
      "Epoch:377/1000\n",
      "Loss on train= 0.012795389629900455\n",
      "Loss on test= 0.01763809658586979\n",
      "acc for Lsat= 0.08007442173030642 \n",
      "acc for Psat= 0.1148576302660836 \n",
      "acc for optim= 0.14708857470088532\n",
      "Epoch:378/1000\n",
      "Loss on train= 0.012910769321024418\n",
      "Loss on test= 0.014731194823980331\n",
      "acc for Lsat= 0.0715085721678204 \n",
      "acc for Psat= 0.10266392181317012 \n",
      "acc for optim= 0.1462566784686512\n",
      "Epoch:379/1000\n",
      "Loss on train= 0.01326726283878088\n",
      "Loss on test= 0.016237569972872734\n",
      "acc for Lsat= 0.07778226484855016 \n",
      "acc for Psat= 0.10993786023722756 \n",
      "acc for optim= 0.14824673632780713\n",
      "Epoch:380/1000\n",
      "Loss on train= 0.013144630938768387\n",
      "Loss on test= 0.015892699360847473\n",
      "acc for Lsat= 0.07251028898689481 \n",
      "acc for Psat= 0.11119625220696132 \n",
      "acc for optim= 0.14492328150404823\n",
      "Epoch:381/1000\n",
      "Loss on train= 0.012719755992293358\n",
      "Loss on test= 0.015349096618592739\n",
      "acc for Lsat= 0.0705271624856525 \n",
      "acc for Psat= 0.10146745559242037 \n",
      "acc for optim= 0.14769541405969192\n",
      "Epoch:382/1000\n",
      "Loss on train= 0.01276851911097765\n",
      "Loss on test= 0.014594988897442818\n",
      "acc for Lsat= 0.07413751019371878 \n",
      "acc for Psat= 0.10409012138843537 \n",
      "acc for optim= 0.1491616442799568\n",
      "Epoch:383/1000\n",
      "Loss on train= 0.012850149534642696\n",
      "Loss on test= 0.014810522086918354\n",
      "acc for Lsat= 0.0744001713063982 \n",
      "acc for Psat= 0.10472654634051853 \n",
      "acc for optim= 0.14744693620337382\n",
      "Epoch:384/1000\n",
      "Loss on train= 0.012559395283460617\n",
      "Loss on test= 0.014522765763103962\n",
      "acc for Lsat= 0.07540781928433311 \n",
      "acc for Psat= 0.11155885789129469 \n",
      "acc for optim= 0.14482294436958104\n",
      "Epoch:385/1000\n",
      "Loss on train= 0.012871864251792431\n",
      "Loss on test= 0.015359263867139816\n",
      "acc for Lsat= 0.07365304314427905 \n",
      "acc for Psat= 0.11452444394429524 \n",
      "acc for optim= 0.1472540660036935\n",
      "Epoch:386/1000\n",
      "Loss on train= 0.012642192654311657\n",
      "Loss on test= 0.015291682444512844\n",
      "acc for Lsat= 0.0907044334544076 \n",
      "acc for Psat= 0.10006639319989417 \n",
      "acc for optim= 0.1440348908305168\n",
      "Epoch:387/1000\n",
      "Loss on train= 0.012568330392241478\n",
      "Loss on test= 0.01645827293395996\n",
      "acc for Lsat= 0.07621490442090563 \n",
      "acc for Psat= 0.10287516067425412 \n",
      "acc for optim= 0.15248148971133765\n",
      "Epoch:388/1000\n",
      "Loss on train= 0.01290210708975792\n",
      "Loss on test= 0.01459426712244749\n",
      "acc for Lsat= 0.07982729309135013 \n",
      "acc for Psat= 0.09812097946802775 \n",
      "acc for optim= 0.14560580154259994\n",
      "Epoch:389/1000\n",
      "Loss on train= 0.012879541143774986\n",
      "Loss on test= 0.01500814501196146\n",
      "acc for Lsat= 0.08835857295327715 \n",
      "acc for Psat= 0.10895879914363227 \n",
      "acc for optim= 0.14650754349099263\n",
      "Epoch:390/1000\n",
      "Loss on train= 0.01308821514248848\n",
      "Loss on test= 0.015502829104661942\n",
      "acc for Lsat= 0.07437461333142387 \n",
      "acc for Psat= 0.10200934261083602 \n",
      "acc for optim= 0.1436395989523994\n",
      "Epoch:391/1000\n",
      "Loss on train= 0.012642490677535534\n",
      "Loss on test= 0.013846338726580143\n",
      "acc for Lsat= 0.07595087703731324 \n",
      "acc for Psat= 0.10808040996392569 \n",
      "acc for optim= 0.14829966856373683\n",
      "Epoch:392/1000\n",
      "Loss on train= 0.012764201499521732\n",
      "Loss on test= 0.01647181622684002\n",
      "acc for Lsat= 0.09061445991198222 \n",
      "acc for Psat= 0.1126373916864395 \n",
      "acc for optim= 0.1460605803463194\n",
      "Epoch:393/1000\n",
      "Loss on train= 0.013047606684267521\n",
      "Loss on test= 0.014910757541656494\n",
      "acc for Lsat= 0.0848572939634323 \n",
      "acc for Psat= 0.12829301522837747 \n",
      "acc for optim= 0.14447593126032088\n",
      "Epoch:394/1000\n",
      "Loss on train= 0.013145238161087036\n",
      "Loss on test= 0.014767197892069817\n",
      "acc for Lsat= 0.08229609264267815 \n",
      "acc for Psat= 0.11541937407520085 \n",
      "acc for optim= 0.1489635788732105\n",
      "Epoch:395/1000\n",
      "Loss on train= 0.013034581206738949\n",
      "Loss on test= 0.01512167975306511\n",
      "acc for Lsat= 0.07490338252650368 \n",
      "acc for Psat= 0.11150397890143922 \n",
      "acc for optim= 0.14660447984933855\n",
      "Epoch:396/1000\n",
      "Loss on train= 0.012686398811638355\n",
      "Loss on test= 0.014865790493786335\n",
      "acc for Lsat= 0.08936631364954843 \n",
      "acc for Psat= 0.12872189084688823 \n",
      "acc for optim= 0.14381252626578012\n",
      "Epoch:397/1000\n",
      "Loss on train= 0.012653105892241001\n",
      "Loss on test= 0.015821438282728195\n",
      "acc for Lsat= 0.07565118736690946 \n",
      "acc for Psat= 0.11705299913883212 \n",
      "acc for optim= 0.14544380456209183\n",
      "Epoch:398/1000\n",
      "Loss on train= 0.012483459897339344\n",
      "Loss on test= 0.014150639064610004\n",
      "acc for Lsat= 0.07438584135638343 \n",
      "acc for Psat= 0.10383461581336127 \n",
      "acc for optim= 0.14217933283911813\n",
      "Epoch:399/1000\n",
      "Loss on train= 0.012804385274648666\n",
      "Loss on test= 0.015716366469860077\n",
      "acc for Lsat= 0.08968804693884322 \n",
      "acc for Psat= 0.10092202673355738 \n",
      "acc for optim= 0.14611562059985267\n",
      "Epoch:400/1000\n",
      "Loss on train= 0.01286743301898241\n",
      "Loss on test= 0.014439402148127556\n",
      "acc for Lsat= 0.07582777904139623 \n",
      "acc for Psat= 0.10650393333699967 \n",
      "acc for optim= 0.145095981657505\n",
      "Epoch:401/1000\n",
      "Loss on train= 0.012656038627028465\n",
      "Loss on test= 0.01638137549161911\n",
      "acc for Lsat= 0.08392138017548455 \n",
      "acc for Psat= 0.10188241551319759 \n",
      "acc for optim= 0.14947528127166962\n",
      "Epoch:402/1000\n",
      "Loss on train= 0.012456252239644527\n",
      "Loss on test= 0.01447939034551382\n",
      "acc for Lsat= 0.09542606009377375 \n",
      "acc for Psat= 0.10932808849546645 \n",
      "acc for optim= 0.1477689877152443\n",
      "Epoch:403/1000\n",
      "Loss on train= 0.012595171108841896\n",
      "Loss on test= 0.014665484428405762\n",
      "acc for Lsat= 0.08853730426894292 \n",
      "acc for Psat= 0.1318292475408978 \n",
      "acc for optim= 0.14513650553094015\n",
      "Epoch:404/1000\n",
      "Loss on train= 0.012921452522277832\n",
      "Loss on test= 0.015242058783769608\n",
      "acc for Lsat= 0.11138782434993318 \n",
      "acc for Psat= 0.14222250050968596 \n",
      "acc for optim= 0.14546448505587048\n",
      "Epoch:405/1000\n",
      "Loss on train= 0.012523140758275986\n",
      "Loss on test= 0.014396293088793755\n",
      "acc for Lsat= 0.06904182467195723 \n",
      "acc for Psat= 0.10035457528299757 \n",
      "acc for optim= 0.14862942314810224\n",
      "Epoch:406/1000\n",
      "Loss on train= 0.012871078215539455\n",
      "Loss on test= 0.01555513683706522\n",
      "acc for Lsat= 0.07237488925457002 \n",
      "acc for Psat= 0.0953589936097463 \n",
      "acc for optim= 0.1480783273776372\n",
      "Epoch:407/1000\n",
      "Loss on train= 0.012969832867383957\n",
      "Loss on test= 0.014606392942368984\n",
      "acc for Lsat= 0.08353340642319786 \n",
      "acc for Psat= 0.0986361958914333 \n",
      "acc for optim= 0.14608572986390855\n",
      "Epoch:408/1000\n",
      "Loss on train= 0.012469202280044556\n",
      "Loss on test= 0.015024312771856785\n",
      "acc for Lsat= 0.0704710950454076 \n",
      "acc for Psat= 0.10495029273960325 \n",
      "acc for optim= 0.14834378378258808\n",
      "Epoch:409/1000\n",
      "Loss on train= 0.01282536331564188\n",
      "Loss on test= 0.016470525413751602\n",
      "acc for Lsat= 0.09203641513983409 \n",
      "acc for Psat= 0.1293239262368944 \n",
      "acc for optim= 0.15294434527556103\n",
      "Epoch:410/1000\n",
      "Loss on train= 0.01254422590136528\n",
      "Loss on test= 0.01458379253745079\n",
      "acc for Lsat= 0.08861669500668844 \n",
      "acc for Psat= 0.10960821128553816 \n",
      "acc for optim= 0.14540722022453945\n",
      "Epoch:411/1000\n",
      "Loss on train= 0.012572254985570908\n",
      "Loss on test= 0.014738086611032486\n",
      "acc for Lsat= 0.08104484577973683 \n",
      "acc for Psat= 0.09976281772057215 \n",
      "acc for optim= 0.14603346950478024\n",
      "Epoch:412/1000\n",
      "Loss on train= 0.012952917255461216\n",
      "Loss on test= 0.014820612035691738\n",
      "acc for Lsat= 0.07183943374289407 \n",
      "acc for Psat= 0.10927969366312025 \n",
      "acc for optim= 0.1495076268911362\n",
      "Epoch:413/1000\n",
      "Loss on train= 0.01278622355312109\n",
      "Loss on test= 0.0138670913875103\n",
      "acc for Lsat= 0.07738475932015314 \n",
      "acc for Psat= 0.11626513302326202 \n",
      "acc for optim= 0.14336697459220885\n",
      "Epoch:414/1000\n",
      "Loss on train= 0.012378249317407608\n",
      "Loss on test= 0.01453398633748293\n",
      "acc for Lsat= 0.07453957233164046 \n",
      "acc for Psat= 0.10863117310735915 \n",
      "acc for optim= 0.14384550369448135\n",
      "Epoch:415/1000\n",
      "Loss on train= 0.012636110186576843\n",
      "Loss on test= 0.01574525237083435\n",
      "acc for Lsat= 0.08022577150000468 \n",
      "acc for Psat= 0.10620846715238358 \n",
      "acc for optim= 0.14747615738047493\n",
      "Epoch:416/1000\n",
      "Loss on train= 0.012456056661903858\n",
      "Loss on test= 0.015817122533917427\n",
      "acc for Lsat= 0.0864375455511941 \n",
      "acc for Psat= 0.10855603880352444 \n",
      "acc for optim= 0.14926330082946354\n",
      "Epoch:417/1000\n",
      "Loss on train= 0.011936121620237827\n",
      "Loss on test= 0.01456486713141203\n",
      "acc for Lsat= 0.07445406466722487 \n",
      "acc for Psat= 0.10001704427931044 \n",
      "acc for optim= 0.14288827114635044\n",
      "Epoch:418/1000\n",
      "Loss on train= 0.012501486577093601\n",
      "Loss on test= 0.014581587165594101\n",
      "acc for Lsat= 0.08013699866003461 \n",
      "acc for Psat= 0.10237589379151661 \n",
      "acc for optim= 0.14565183520317074\n",
      "Epoch:419/1000\n",
      "Loss on train= 0.012237900868058205\n",
      "Loss on test= 0.015231933444738388\n",
      "acc for Lsat= 0.08071186708079443 \n",
      "acc for Psat= 0.10166341811418532 \n",
      "acc for optim= 0.149040824174881\n",
      "Epoch:420/1000\n",
      "Loss on train= 0.012242318131029606\n",
      "Loss on test= 0.014560865238308907\n",
      "acc for Lsat= 0.07665439314312406 \n",
      "acc for Psat= 0.09874041742748683 \n",
      "acc for optim= 0.1470366307430797\n",
      "Epoch:421/1000\n",
      "Loss on train= 0.012121199630200863\n",
      "Loss on test= 0.014929817989468575\n",
      "acc for Lsat= 0.07823128766483732 \n",
      "acc for Psat= 0.11407337254948084 \n",
      "acc for optim= 0.1438386763135592\n",
      "Epoch:422/1000\n",
      "Loss on train= 0.011757319793105125\n",
      "Loss on test= 0.015630146488547325\n",
      "acc for Lsat= 0.07552426490518782 \n",
      "acc for Psat= 0.10266943027575813 \n",
      "acc for optim= 0.14871235224935744\n",
      "Epoch:423/1000\n",
      "Loss on train= 0.01255727093666792\n",
      "Loss on test= 0.014814083464443684\n",
      "acc for Lsat= 0.07787871642245187 \n",
      "acc for Psat= 0.10829389194647471 \n",
      "acc for optim= 0.15058293408817716\n",
      "Epoch:424/1000\n",
      "Loss on train= 0.012632425874471664\n",
      "Loss on test= 0.014711891300976276\n",
      "acc for Lsat= 0.07322329613897537 \n",
      "acc for Psat= 0.10422263840834299 \n",
      "acc for optim= 0.14492759274111855\n",
      "Epoch:425/1000\n",
      "Loss on train= 0.012604408897459507\n",
      "Loss on test= 0.01425305474549532\n",
      "acc for Lsat= 0.08417667978339725 \n",
      "acc for Psat= 0.10464315795236165 \n",
      "acc for optim= 0.1426768887374136\n",
      "Epoch:426/1000\n",
      "Loss on train= 0.012095245532691479\n",
      "Loss on test= 0.015127609483897686\n",
      "acc for Lsat= 0.08555189188983706 \n",
      "acc for Psat= 0.10598543617460464 \n",
      "acc for optim= 0.144333946870433\n",
      "Epoch:427/1000\n",
      "Loss on train= 0.012127253226935863\n",
      "Loss on test= 0.013950569555163383\n",
      "acc for Lsat= 0.07908491575055651 \n",
      "acc for Psat= 0.10373385068443088 \n",
      "acc for optim= 0.14397358546654382\n",
      "Epoch:428/1000\n",
      "Loss on train= 0.012372691184282303\n",
      "Loss on test= 0.014938210137188435\n",
      "acc for Lsat= 0.08506471978293526 \n",
      "acc for Psat= 0.10005956672959856 \n",
      "acc for optim= 0.14451969083812502\n",
      "Epoch:429/1000\n",
      "Loss on train= 0.012827050872147083\n",
      "Loss on test= 0.014910485595464706\n",
      "acc for Lsat= 0.0901973863442739 \n",
      "acc for Psat= 0.12145856850677067 \n",
      "acc for optim= 0.14631476634078555\n",
      "Epoch:430/1000\n",
      "Loss on train= 0.012150820344686508\n",
      "Loss on test= 0.014454479329288006\n",
      "acc for Lsat= 0.0711327569352256 \n",
      "acc for Psat= 0.10089757591485976 \n",
      "acc for optim= 0.14967865645885467\n",
      "Epoch:431/1000\n",
      "Loss on train= 0.012370649725198746\n",
      "Loss on test= 0.01613696850836277\n",
      "acc for Lsat= 0.08557025227281781 \n",
      "acc for Psat= 0.10965819259484609 \n",
      "acc for optim= 0.15012453281217153\n",
      "Epoch:432/1000\n",
      "Loss on train= 0.012375983409583569\n",
      "Loss on test= 0.015786653384566307\n",
      "acc for Lsat= 0.07249161021576987 \n",
      "acc for Psat= 0.10890501687924065 \n",
      "acc for optim= 0.14559217459625665\n",
      "Epoch:433/1000\n",
      "Loss on train= 0.012213380075991154\n",
      "Loss on test= 0.015469943173229694\n",
      "acc for Lsat= 0.07645633800162209 \n",
      "acc for Psat= 0.11125673684808943 \n",
      "acc for optim= 0.14543440159824156\n",
      "Epoch:434/1000\n",
      "Loss on train= 0.012086743488907814\n",
      "Loss on test= 0.014444227330386639\n",
      "acc for Lsat= 0.07223135282595952 \n",
      "acc for Psat= 0.10568828632434209 \n",
      "acc for optim= 0.14532664285765756\n",
      "Epoch:435/1000\n",
      "Loss on train= 0.012093966826796532\n",
      "Loss on test= 0.014580592513084412\n",
      "acc for Lsat= 0.07495757904317643 \n",
      "acc for Psat= 0.10346378518475427 \n",
      "acc for optim= 0.14452194306585522\n",
      "Epoch:436/1000\n",
      "Loss on train= 0.011690586805343628\n",
      "Loss on test= 0.015735933557152748\n",
      "acc for Lsat= 0.08724361062049865 \n",
      "acc for Psat= 0.1188310066858927 \n",
      "acc for optim= 0.14855992148319883\n",
      "Epoch:437/1000\n",
      "Loss on train= 0.012158472090959549\n",
      "Loss on test= 0.014643496833741665\n",
      "acc for Lsat= 0.07676590383052825 \n",
      "acc for Psat= 0.09233548988898593 \n",
      "acc for optim= 0.14881114463011422\n",
      "Epoch:438/1000\n",
      "Loss on train= 0.01247128564864397\n",
      "Loss on test= 0.014701442793011665\n",
      "acc for Lsat= 0.06810870766639711 \n",
      "acc for Psat= 0.09761010491185718 \n",
      "acc for optim= 0.14718879097037843\n",
      "Epoch:439/1000\n",
      "Loss on train= 0.012467221356928349\n",
      "Loss on test= 0.014778544194996357\n",
      "acc for Lsat= 0.07551494704352484 \n",
      "acc for Psat= 0.0966879571477572 \n",
      "acc for optim= 0.14639475329054724\n",
      "Epoch:440/1000\n",
      "Loss on train= 0.012042667716741562\n",
      "Loss on test= 0.014247894287109375\n",
      "acc for Lsat= 0.07483712997701433 \n",
      "acc for Psat= 0.0965112790465355 \n",
      "acc for optim= 0.14406673361857736\n",
      "Epoch:441/1000\n",
      "Loss on train= 0.012293397448956966\n",
      "Loss on test= 0.0143355093896389\n",
      "acc for Lsat= 0.08602056900660197 \n",
      "acc for Psat= 0.10711434980233508 \n",
      "acc for optim= 0.14437044031090207\n",
      "Epoch:442/1000\n",
      "Loss on train= 0.012314248830080032\n",
      "Loss on test= 0.014983203262090683\n",
      "acc for Lsat= 0.0722106150454945 \n",
      "acc for Psat= 0.10029342257314258 \n",
      "acc for optim= 0.1473261250389947\n",
      "Epoch:443/1000\n",
      "Loss on train= 0.01224928256124258\n",
      "Loss on test= 0.014664754271507263\n",
      "acc for Lsat= 0.09550534486770629 \n",
      "acc for Psat= 0.1393194321129057 \n",
      "acc for optim= 0.1475530814793375\n",
      "Epoch:444/1000\n",
      "Loss on train= 0.012209729291498661\n",
      "Loss on test= 0.015144716016948223\n",
      "acc for Lsat= 0.08551047411229876 \n",
      "acc for Psat= 0.12553938660356734 \n",
      "acc for optim= 0.14715244289901522\n",
      "Epoch:445/1000\n",
      "Loss on train= 0.01205720379948616\n",
      "Loss on test= 0.014565893448889256\n",
      "acc for Lsat= 0.08142300446828205 \n",
      "acc for Psat= 0.10586560269196826 \n",
      "acc for optim= 0.14335056824816597\n",
      "Epoch:446/1000\n",
      "Loss on train= 0.01210422720760107\n",
      "Loss on test= 0.015761155635118484\n",
      "acc for Lsat= 0.08181956758101781 \n",
      "acc for Psat= 0.12977586653497483 \n",
      "acc for optim= 0.14643630435069405\n",
      "Epoch:447/1000\n",
      "Loss on train= 0.012021374888718128\n",
      "Loss on test= 0.01581941545009613\n",
      "acc for Lsat= 0.07041286958588494 \n",
      "acc for Psat= 0.10348026007413863 \n",
      "acc for optim= 0.15046374797821047\n",
      "Epoch:448/1000\n",
      "Loss on train= 0.012114012613892555\n",
      "Loss on test= 0.014377294108271599\n",
      "acc for Lsat= 0.0729864928457472 \n",
      "acc for Psat= 0.10380270034074783 \n",
      "acc for optim= 0.14710051748487685\n",
      "Epoch:449/1000\n",
      "Loss on train= 0.011851796880364418\n",
      "Loss on test= 0.014170861802995205\n",
      "acc for Lsat= 0.07220199207464854 \n",
      "acc for Psat= 0.09847398615545697 \n",
      "acc for optim= 0.14595480031437344\n",
      "Epoch:450/1000\n",
      "Loss on train= 0.012327402830123901\n",
      "Loss on test= 0.01529762614518404\n",
      "acc for Lsat= 0.07935551636748843 \n",
      "acc for Psat= 0.1209630098607805 \n",
      "acc for optim= 0.1464190714889103\n",
      "Epoch:451/1000\n",
      "Loss on train= 0.012271539308130741\n",
      "Loss on test= 0.014340470544993877\n",
      "acc for Lsat= 0.07369308753146066 \n",
      "acc for Psat= 0.09712992641660903 \n",
      "acc for optim= 0.14968471527099608\n",
      "Epoch:452/1000\n",
      "Loss on train= 0.011892047710716724\n",
      "Loss on test= 0.014350869692862034\n",
      "acc for Lsat= 0.07696572260724172 \n",
      "acc for Psat= 0.09741128981113435 \n",
      "acc for optim= 0.14743757231367957\n",
      "Epoch:453/1000\n",
      "Loss on train= 0.011945689097046852\n",
      "Loss on test= 0.014541945420205593\n",
      "acc for Lsat= 0.07007346169816124 \n",
      "acc for Psat= 0.10471028370989693 \n",
      "acc for optim= 0.14915793074501885\n",
      "Epoch:454/1000\n",
      "Loss on train= 0.011917677707970142\n",
      "Loss on test= 0.014274254441261292\n",
      "acc for Lsat= 0.0731640610429976 \n",
      "acc for Psat= 0.10195735179715686 \n",
      "acc for optim= 0.14647485282686018\n",
      "Epoch:455/1000\n",
      "Loss on train= 0.01167114358395338\n",
      "Loss on test= 0.015034773387014866\n",
      "acc for Lsat= 0.07604721552795833 \n",
      "acc for Psat= 0.09669390983051725 \n",
      "acc for optim= 0.14668166438738503\n",
      "Epoch:456/1000\n",
      "Loss on train= 0.012107334099709988\n",
      "Loss on test= 0.014454805292189121\n",
      "acc for Lsat= 0.07835579348935022 \n",
      "acc for Psat= 0.0948881533410814 \n",
      "acc for optim= 0.14848853730493122\n",
      "Epoch:457/1000\n",
      "Loss on train= 0.01192596834152937\n",
      "Loss on test= 0.015069644898176193\n",
      "acc for Lsat= 0.08273808244201872 \n",
      "acc for Psat= 0.11859674553076428 \n",
      "acc for optim= 0.14601174493630728\n",
      "Epoch:458/1000\n",
      "Loss on train= 0.01213901862502098\n",
      "Loss on test= 0.01528792455792427\n",
      "acc for Lsat= 0.07182297822501924 \n",
      "acc for Psat= 0.10158206638362671 \n",
      "acc for optim= 0.14745482951402666\n",
      "Epoch:459/1000\n",
      "Loss on train= 0.011564742773771286\n",
      "Loss on test= 0.015335675328969955\n",
      "acc for Lsat= 0.07416402664449478 \n",
      "acc for Psat= 0.10654042926099566 \n",
      "acc for optim= 0.14913511541154648\n",
      "Epoch:460/1000\n",
      "Loss on train= 0.012461885809898376\n",
      "Loss on test= 0.015469498001039028\n",
      "acc for Lsat= 0.06846549014250439 \n",
      "acc for Psat= 0.09375280588865281 \n",
      "acc for optim= 0.15058302813106114\n",
      "Epoch:461/1000\n",
      "Loss on train= 0.011732101440429688\n",
      "Loss on test= 0.014799833297729492\n",
      "acc for Lsat= 0.07376702891455757 \n",
      "acc for Psat= 0.10255129817459319 \n",
      "acc for optim= 0.14523468150032895\n",
      "Epoch:462/1000\n",
      "Loss on train= 0.011999783106148243\n",
      "Loss on test= 0.015277642756700516\n",
      "acc for Lsat= 0.07620200183656482 \n",
      "acc for Psat= 0.10301094932688608 \n",
      "acc for optim= 0.1463200118806627\n",
      "Epoch:463/1000\n",
      "Loss on train= 0.011532869189977646\n",
      "Loss on test= 0.013659530319273472\n",
      "acc for Lsat= 0.08123906817701126 \n",
      "acc for Psat= 0.09094197452068327 \n",
      "acc for optim= 0.1450405670536889\n",
      "Epoch:464/1000\n",
      "Loss on train= 0.01159065030515194\n",
      "Loss on test= 0.01459351647645235\n",
      "acc for Lsat= 0.07892920159631306 \n",
      "acc for Psat= 0.09750165740648906 \n",
      "acc for optim= 0.14594846334722308\n",
      "Epoch:465/1000\n",
      "Loss on train= 0.011343543417751789\n",
      "Loss on test= 0.014385304413735867\n",
      "acc for Lsat= 0.07847879115078184 \n",
      "acc for Psat= 0.09912426554494433 \n",
      "acc for optim= 0.14313687003321116\n",
      "Epoch:466/1000\n",
      "Loss on train= 0.011909198947250843\n",
      "Loss on test= 0.015654034912586212\n",
      "acc for Lsat= 0.07328399337000317 \n",
      "acc for Psat= 0.09896897806061639 \n",
      "acc for optim= 0.15094365709357793\n",
      "Epoch:467/1000\n",
      "Loss on train= 0.011591422371566296\n",
      "Loss on test= 0.014148415997624397\n",
      "acc for Lsat= 0.07515016280942494 \n",
      "acc for Psat= 0.10834774060381784 \n",
      "acc for optim= 0.1443657320406702\n",
      "Epoch:468/1000\n",
      "Loss on train= 0.012017813511192799\n",
      "Loss on test= 0.014593970030546188\n",
      "acc for Lsat= 0.08451988945404688 \n",
      "acc for Psat= 0.11722399989763896 \n",
      "acc for optim= 0.14579256839222376\n",
      "Epoch:469/1000\n",
      "Loss on train= 0.01235260721296072\n",
      "Loss on test= 0.0146098667755723\n",
      "acc for Lsat= 0.07001836862828997 \n",
      "acc for Psat= 0.09828533877929052 \n",
      "acc for optim= 0.146189818614059\n",
      "Epoch:470/1000\n",
      "Loss on train= 0.011753073893487453\n",
      "Loss on test= 0.015999747440218925\n",
      "acc for Lsat= 0.07795074433088303 \n",
      "acc for Psat= 0.09917472071117824 \n",
      "acc for optim= 0.14424114243851766\n",
      "Epoch:471/1000\n",
      "Loss on train= 0.011344012804329395\n",
      "Loss on test= 0.01475695800036192\n",
      "acc for Lsat= 0.08118926998641755 \n",
      "acc for Psat= 0.12258743478192224 \n",
      "acc for optim= 0.14696413642830314\n",
      "Epoch:472/1000\n",
      "Loss on train= 0.01158494595438242\n",
      "Loss on test= 0.014812593348324299\n",
      "acc for Lsat= 0.07700391908486684 \n",
      "acc for Psat= 0.10442587253120211 \n",
      "acc for optim= 0.14273808466063603\n",
      "Epoch:473/1000\n",
      "Loss on train= 0.011652570217847824\n",
      "Loss on test= 0.015187003649771214\n",
      "acc for Lsat= 0.0737809975941976 \n",
      "acc for Psat= 0.0996201346317927 \n",
      "acc for optim= 0.14488731258445317\n",
      "Epoch:474/1000\n",
      "Loss on train= 0.011848139576613903\n",
      "Loss on test= 0.01480686292052269\n",
      "acc for Lsat= 0.06763944576183954 \n",
      "acc for Psat= 0.10137041840288374 \n",
      "acc for optim= 0.1447929119070371\n",
      "Epoch:475/1000\n",
      "Loss on train= 0.01173007395118475\n",
      "Loss on test= 0.014257315546274185\n",
      "acc for Lsat= 0.07741611997286481 \n",
      "acc for Psat= 0.09910730752680037 \n",
      "acc for optim= 0.1455916215976079\n",
      "Epoch:476/1000\n",
      "Loss on train= 0.011621827259659767\n",
      "Loss on test= 0.014697468839585781\n",
      "acc for Lsat= 0.07289826456043454 \n",
      "acc for Psat= 0.10143726352188323 \n",
      "acc for optim= 0.1448400398095449\n",
      "Epoch:477/1000\n",
      "Loss on train= 0.011638381518423557\n",
      "Loss on test= 0.014562860131263733\n",
      "acc for Lsat= 0.06916435990068646 \n",
      "acc for Psat= 0.09343238257699543 \n",
      "acc for optim= 0.14281677918301688\n",
      "Epoch:478/1000\n",
      "Loss on train= 0.01166151836514473\n",
      "Loss on test= 0.014098022133111954\n",
      "acc for Lsat= 0.07539899249871572 \n",
      "acc for Psat= 0.10555861360496945 \n",
      "acc for optim= 0.1434420026010937\n",
      "Epoch:479/1000\n",
      "Loss on train= 0.011897153221070766\n",
      "Loss on test= 0.01466725580394268\n",
      "acc for Lsat= 0.07341979874504936 \n",
      "acc for Psat= 0.0950288959675365 \n",
      "acc for optim= 0.1454900241560406\n",
      "Epoch:480/1000\n",
      "Loss on train= 0.011719786562025547\n",
      "Loss on test= 0.015673812478780746\n",
      "acc for Lsat= 0.07316210899088117 \n",
      "acc for Psat= 0.10882449646790822 \n",
      "acc for optim= 0.14585123078690634\n",
      "Epoch:481/1000\n",
      "Loss on train= 0.011890048161149025\n",
      "Loss on test= 0.014083183370530605\n",
      "acc for Lsat= 0.07580807755390803 \n",
      "acc for Psat= 0.09961158649788962 \n",
      "acc for optim= 0.1465856846835878\n",
      "Epoch:482/1000\n",
      "Loss on train= 0.011862758547067642\n",
      "Loss on test= 0.014549857936799526\n",
      "acc for Lsat= 0.08389877776304881 \n",
      "acc for Psat= 0.10404909898837408 \n",
      "acc for optim= 0.14291058232386905\n",
      "Epoch:483/1000\n",
      "Loss on train= 0.011845115572214127\n",
      "Loss on test= 0.013914035633206367\n",
      "acc for Lsat= 0.07911181863811281 \n",
      "acc for Psat= 0.10861380265818701 \n",
      "acc for optim= 0.14168439838621352\n",
      "Epoch:484/1000\n",
      "Loss on train= 0.011861611157655716\n",
      "Loss on test= 0.015148763544857502\n",
      "acc for Lsat= 0.07231475346618227 \n",
      "acc for Psat= 0.09239074455367194 \n",
      "acc for optim= 0.14372383372651204\n",
      "Epoch:485/1000\n",
      "Loss on train= 0.011497818864881992\n",
      "Loss on test= 0.016228940337896347\n",
      "acc for Lsat= 0.09029876424206627 \n",
      "acc for Psat= 0.10897788835896388 \n",
      "acc for optim= 0.14755946944157283\n",
      "Epoch:486/1000\n",
      "Loss on train= 0.011598044075071812\n",
      "Loss on test= 0.016194986179471016\n",
      "acc for Lsat= 0.08362163934442732 \n",
      "acc for Psat= 0.11680040823088753 \n",
      "acc for optim= 0.14852939314312405\n",
      "Epoch:487/1000\n",
      "Loss on train= 0.011704719625413418\n",
      "Loss on test= 0.01364163588732481\n",
      "acc for Lsat= 0.07082377903991276 \n",
      "acc for Psat= 0.10583158764574264 \n",
      "acc for optim= 0.14221305251121522\n",
      "Epoch:488/1000\n",
      "Loss on train= 0.01156518142670393\n",
      "Loss on test= 0.014907985925674438\n",
      "acc for Lsat= 0.07075529909796184 \n",
      "acc for Psat= 0.09836795015467537 \n",
      "acc for optim= 0.1459700173801846\n",
      "Epoch:489/1000\n",
      "Loss on train= 0.011536043137311935\n",
      "Loss on test= 0.016072647646069527\n",
      "acc for Lsat= 0.08107263098160426 \n",
      "acc for Psat= 0.11320752832624649 \n",
      "acc for optim= 0.1419350582692358\n",
      "Epoch:490/1000\n",
      "Loss on train= 0.011635595001280308\n",
      "Loss on test= 0.015386205166578293\n",
      "acc for Lsat= 0.06818113409810594 \n",
      "acc for Psat= 0.09274590429332523 \n",
      "acc for optim= 0.14791806240876518\n",
      "Epoch:491/1000\n",
      "Loss on train= 0.01185218058526516\n",
      "Loss on test= 0.015698961913585663\n",
      "acc for Lsat= 0.08039873159594008 \n",
      "acc for Psat= 0.12578943189647462 \n",
      "acc for optim= 0.14763116157717177\n",
      "Epoch:492/1000\n",
      "Loss on train= 0.011556118726730347\n",
      "Loss on test= 0.014544349163770676\n",
      "acc for Lsat= 0.07218726807170445 \n",
      "acc for Psat= 0.10140679313076867 \n",
      "acc for optim= 0.1414163985186153\n",
      "Epoch:493/1000\n",
      "Loss on train= 0.011901342310011387\n",
      "Loss on test= 0.014111917465925217\n",
      "acc for Lsat= 0.07111210525035858 \n",
      "acc for Psat= 0.09918401423427793 \n",
      "acc for optim= 0.14743894951211078\n",
      "Epoch:494/1000\n",
      "Loss on train= 0.010939382947981358\n",
      "Loss on test= 0.014750725589692593\n",
      "acc for Lsat= 0.07235169808069865 \n",
      "acc for Psat= 0.09760763545831044 \n",
      "acc for optim= 0.15068855600224604\n",
      "Epoch:495/1000\n",
      "Loss on train= 0.011295396834611893\n",
      "Loss on test= 0.014597538858652115\n",
      "acc for Lsat= 0.07346904310915206 \n",
      "acc for Psat= 0.09915637936857015 \n",
      "acc for optim= 0.14632053739494746\n",
      "Epoch:496/1000\n",
      "Loss on train= 0.012180084362626076\n",
      "Loss on test= 0.015851173549890518\n",
      "acc for Lsat= 0.0771495246224933 \n",
      "acc for Psat= 0.1057828426361084 \n",
      "acc for optim= 0.14328514801131353\n",
      "Epoch:497/1000\n",
      "Loss on train= 0.011547792702913284\n",
      "Loss on test= 0.015650251880288124\n",
      "acc for Lsat= 0.07944365474912855 \n",
      "acc for Psat= 0.11273868232965469 \n",
      "acc for optim= 0.14798483533991708\n",
      "Epoch:498/1000\n",
      "Loss on train= 0.011844014748930931\n",
      "Loss on test= 0.015039030462503433\n",
      "acc for Lsat= 0.07311027066575156 \n",
      "acc for Psat= 0.10153007838461134 \n",
      "acc for optim= 0.14299046645561858\n",
      "Epoch:499/1000\n",
      "Loss on train= 0.011462814174592495\n",
      "Loss on test= 0.014648079872131348\n",
      "acc for Lsat= 0.07420006642738979 \n",
      "acc for Psat= 0.1144093483686447 \n",
      "acc for optim= 0.14656312465667728\n",
      "Epoch:500/1000\n",
      "Loss on train= 0.011126169003546238\n",
      "Loss on test= 0.015843527391552925\n",
      "acc for Lsat= 0.07046535164117812 \n",
      "acc for Psat= 0.09616117063495849 \n",
      "acc for optim= 0.1479646881421407\n",
      "Epoch:501/1000\n",
      "Loss on train= 0.011784294620156288\n",
      "Loss on test= 0.014764485880732536\n",
      "acc for Lsat= 0.07093097749683591 \n",
      "acc for Psat= 0.09367957297298642 \n",
      "acc for optim= 0.1447315227654245\n",
      "Epoch:502/1000\n",
      "Loss on train= 0.011663380078971386\n",
      "Loss on test= 0.01632719114422798\n",
      "acc for Lsat= 0.07631468143728043 \n",
      "acc for Psat= 0.10954384207725525 \n",
      "acc for optim= 0.1420477191607157\n",
      "Epoch:503/1000\n",
      "Loss on train= 0.01139784138649702\n",
      "Loss on test= 0.015324215404689312\n",
      "acc for Lsat= 0.07219929181867175 \n",
      "acc for Psat= 0.10047934585147436 \n",
      "acc for optim= 0.14351457075940238\n",
      "Epoch:504/1000\n",
      "Loss on train= 0.011624312959611416\n",
      "Loss on test= 0.016543563455343246\n",
      "acc for Lsat= 0.08319889373249478 \n",
      "acc for Psat= 0.09664637545744578 \n",
      "acc for optim= 0.14833720790015326\n",
      "Epoch:505/1000\n",
      "Loss on train= 0.011494276113808155\n",
      "Loss on test= 0.016337517648935318\n",
      "acc for Lsat= 0.08517848286363813 \n",
      "acc for Psat= 0.11175705989201863 \n",
      "acc for optim= 0.14450093756119411\n",
      "Epoch:506/1000\n",
      "Loss on train= 0.01137281022965908\n",
      "Loss on test= 0.014188162051141262\n",
      "acc for Lsat= 0.07277661280499564 \n",
      "acc for Psat= 0.09422972400983173 \n",
      "acc for optim= 0.14387898445129393\n",
      "Epoch:507/1000\n",
      "Loss on train= 0.011324204504489899\n",
      "Loss on test= 0.015013744123280048\n",
      "acc for Lsat= 0.08379995326201121 \n",
      "acc for Psat= 0.09155115377571849 \n",
      "acc for optim= 0.14704664581351812\n",
      "Epoch:508/1000\n",
      "Loss on train= 0.011511248536407948\n",
      "Loss on test= 0.01588774472475052\n",
      "acc for Lsat= 0.07591819448603525 \n",
      "acc for Psat= 0.09464153564638561 \n",
      "acc for optim= 0.1448883102999793\n",
      "Epoch:509/1000\n",
      "Loss on train= 0.011012891307473183\n",
      "Loss on test= 0.014666857197880745\n",
      "acc for Lsat= 0.0747160866856575 \n",
      "acc for Psat= 0.10662001768747967 \n",
      "acc for optim= 0.14553184741073183\n",
      "Epoch:510/1000\n",
      "Loss on train= 0.0113694928586483\n",
      "Loss on test= 0.013538721017539501\n",
      "acc for Lsat= 0.07163147727648417 \n",
      "acc for Psat= 0.0987743318080902 \n",
      "acc for optim= 0.14355800251166026\n",
      "Epoch:511/1000\n",
      "Loss on train= 0.011612236499786377\n",
      "Loss on test= 0.013679474592208862\n",
      "acc for Lsat= 0.07280789415041605 \n",
      "acc for Psat= 0.09970334536499448 \n",
      "acc for optim= 0.14347562061415778\n",
      "Epoch:512/1000\n",
      "Loss on train= 0.011330354027450085\n",
      "Loss on test= 0.014776830561459064\n",
      "acc for Lsat= 0.07473054561350082 \n",
      "acc for Psat= 0.09272042496336831 \n",
      "acc for optim= 0.14594792789883082\n",
      "Epoch:513/1000\n",
      "Loss on train= 0.011794107966125011\n",
      "Loss on test= 0.01396999228745699\n",
      "acc for Lsat= 0.08069671955373553 \n",
      "acc for Psat= 0.10267797758181889 \n",
      "acc for optim= 0.14423097968101503\n",
      "Epoch:514/1000\n",
      "Loss on train= 0.011306922882795334\n",
      "Loss on test= 0.013531181029975414\n",
      "acc for Lsat= 0.07913389636410607 \n",
      "acc for Psat= 0.10042491919464534 \n",
      "acc for optim= 0.1406908647881614\n",
      "Epoch:515/1000\n",
      "Loss on train= 0.011524808593094349\n",
      "Loss on test= 0.014464429579675198\n",
      "acc for Lsat= 0.08560018671883476 \n",
      "acc for Psat= 0.12925546997123294 \n",
      "acc for optim= 0.1419903929034869\n",
      "Epoch:516/1000\n",
      "Loss on train= 0.011120812967419624\n",
      "Loss on test= 0.015424360521137714\n",
      "acc for Lsat= 0.08071892493300968 \n",
      "acc for Psat= 0.10143830014599693 \n",
      "acc for optim= 0.14164201617240907\n",
      "Epoch:517/1000\n",
      "Loss on train= 0.011144748888909817\n",
      "Loss on test= 0.014640147797763348\n",
      "acc for Lsat= 0.07182752440373102 \n",
      "acc for Psat= 0.09660651766591605 \n",
      "acc for optim= 0.14395218706793259\n",
      "Epoch:518/1000\n",
      "Loss on train= 0.010912402532994747\n",
      "Loss on test= 0.01510072872042656\n",
      "acc for Lsat= 0.07387604316075642 \n",
      "acc for Psat= 0.09724842839770848 \n",
      "acc for optim= 0.14491997791661157\n",
      "Epoch:519/1000\n",
      "Loss on train= 0.011363320983946323\n",
      "Loss on test= 0.013968710787594318\n",
      "acc for Lsat= 0.075507690012455 \n",
      "acc for Psat= 0.10160277750757005 \n",
      "acc for optim= 0.1444180044862959\n",
      "Epoch:520/1000\n",
      "Loss on train= 0.011329619213938713\n",
      "Loss on test= 0.015495551750063896\n",
      "acc for Lsat= 0.0709286650021871 \n",
      "acc for Psat= 0.09958042932881252 \n",
      "acc for optim= 0.14436351309219997\n",
      "Epoch:521/1000\n",
      "Loss on train= 0.011102764867246151\n",
      "Loss on test= 0.01503063552081585\n",
      "acc for Lsat= 0.076365155643887 \n",
      "acc for Psat= 0.11055760333935422 \n",
      "acc for optim= 0.13968494054343963\n",
      "Epoch:522/1000\n",
      "Loss on train= 0.011038798838853836\n",
      "Loss on test= 0.015257241204380989\n",
      "acc for Lsat= 0.07461623069312837 \n",
      "acc for Psat= 0.09820315308041043 \n",
      "acc for optim= 0.1482413695918189\n",
      "Epoch:523/1000\n",
      "Loss on train= 0.011195878498256207\n",
      "Loss on test= 0.014786498621106148\n",
      "acc for Lsat= 0.08079708268245062 \n",
      "acc for Psat= 0.09271826694409052 \n",
      "acc for optim= 0.1431475690669484\n",
      "Epoch:524/1000\n",
      "Loss on train= 0.011281422339379787\n",
      "Loss on test= 0.013903889805078506\n",
      "acc for Lsat= 0.07481305781337949 \n",
      "acc for Psat= 0.09867442432377073 \n",
      "acc for optim= 0.14348675741089714\n",
      "Epoch:525/1000\n",
      "Loss on train= 0.011388705112040043\n",
      "Loss on test= 0.014898765832185745\n",
      "acc for Lsat= 0.07027966446346706 \n",
      "acc for Psat= 0.0990121907658047 \n",
      "acc for optim= 0.140824313627349\n",
      "Epoch:526/1000\n",
      "Loss on train= 0.01136244460940361\n",
      "Loss on test= 0.014870456419885159\n",
      "acc for Lsat= 0.07764501753780577 \n",
      "acc for Psat= 0.1089664348297649 \n",
      "acc for optim= 0.14472928163078097\n",
      "Epoch:527/1000\n",
      "Loss on train= 0.01150297187268734\n",
      "Loss on test= 0.015923241153359413\n",
      "acc for Lsat= 0.07960601233773762 \n",
      "acc for Psat= 0.11060623361004723 \n",
      "acc for optim= 0.14864845159980988\n",
      "Epoch:528/1000\n",
      "Loss on train= 0.01138221099972725\n",
      "Loss on test= 0.014195475727319717\n",
      "acc for Lsat= 0.07978011088238822 \n",
      "acc for Psat= 0.11087859802775912 \n",
      "acc for optim= 0.14452700714270278\n",
      "Epoch:529/1000\n",
      "Loss on train= 0.01142410933971405\n",
      "Loss on test= 0.016046548262238503\n",
      "acc for Lsat= 0.09000758628050484 \n",
      "acc for Psat= 0.1009250260061688 \n",
      "acc for optim= 0.14477227131525672\n",
      "Epoch:530/1000\n",
      "Loss on train= 0.011004623025655746\n",
      "Loss on test= 0.014113711193203926\n",
      "acc for Lsat= 0.07024120754665798 \n",
      "acc for Psat= 0.09339335229661729 \n",
      "acc for optim= 0.14237967282533648\n",
      "Epoch:531/1000\n",
      "Loss on train= 0.011060606688261032\n",
      "Loss on test= 0.014633115381002426\n",
      "acc for Lsat= 0.07272460245423847 \n",
      "acc for Psat= 0.10210560742351744 \n",
      "acc for optim= 0.1417200156384044\n",
      "Epoch:532/1000\n",
      "Loss on train= 0.011191458441317081\n",
      "Loss on test= 0.0140687245875597\n",
      "acc for Lsat= 0.08039345343907675 \n",
      "acc for Psat= 0.1013804202278455 \n",
      "acc for optim= 0.141576903561751\n",
      "Epoch:533/1000\n",
      "Loss on train= 0.011292714625597\n",
      "Loss on test= 0.015109932981431484\n",
      "acc for Lsat= 0.07743807617161008 \n",
      "acc for Psat= 0.09491983850797019 \n",
      "acc for optim= 0.1459790446692043\n",
      "Epoch:534/1000\n",
      "Loss on train= 0.011270375922322273\n",
      "Loss on test= 0.01407688669860363\n",
      "acc for Lsat= 0.07503184427817662 \n",
      "acc for Psat= 0.0933040572537316 \n",
      "acc for optim= 0.14147098975049122\n",
      "Epoch:535/1000\n",
      "Loss on train= 0.011647829785943031\n",
      "Loss on test= 0.01565135270357132\n",
      "acc for Lsat= 0.08190661205185784 \n",
      "acc for Psat= 0.11036007288429472 \n",
      "acc for optim= 0.14184911184840732\n",
      "Epoch:536/1000\n",
      "Loss on train= 0.011229034513235092\n",
      "Loss on test= 0.01413978636264801\n",
      "acc for Lsat= 0.0725763491458363 \n",
      "acc for Psat= 0.0998669390877088 \n",
      "acc for optim= 0.13982586703366703\n",
      "Epoch:537/1000\n",
      "Loss on train= 0.010829079896211624\n",
      "Loss on test= 0.01375147420912981\n",
      "acc for Lsat= 0.08117446568277148 \n",
      "acc for Psat= 0.10700081868304147 \n",
      "acc for optim= 0.14346555636988745\n",
      "Epoch:538/1000\n",
      "Loss on train= 0.011018565855920315\n",
      "Loss on test= 0.014370002783834934\n",
      "acc for Lsat= 0.07588552451796002 \n",
      "acc for Psat= 0.09755268262492284 \n",
      "acc for optim= 0.1419400625758701\n",
      "Epoch:539/1000\n",
      "Loss on train= 0.010924977250397205\n",
      "Loss on test= 0.01634690910577774\n",
      "acc for Lsat= 0.07485544433196385 \n",
      "acc for Psat= 0.11004867206017176 \n",
      "acc for optim= 0.14513815475834738\n",
      "Epoch:540/1000\n",
      "Loss on train= 0.011363040655851364\n",
      "Loss on test= 0.014688791707158089\n",
      "acc for Lsat= 0.07189175950156318 \n",
      "acc for Psat= 0.09554717044035593 \n",
      "acc for optim= 0.145200534330474\n",
      "Epoch:541/1000\n",
      "Loss on train= 0.01088960375636816\n",
      "Loss on test= 0.014032612554728985\n",
      "acc for Lsat= 0.07500195470121171 \n",
      "acc for Psat= 0.09624237798982198 \n",
      "acc for optim= 0.1449196270770497\n",
      "Epoch:542/1000\n",
      "Loss on train= 0.010999930091202259\n",
      "Loss on test= 0.015086865983903408\n",
      "acc for Lsat= 0.09052723911073474 \n",
      "acc for Psat= 0.10765970630778206 \n",
      "acc for optim= 0.1431272162331475\n",
      "Epoch:543/1000\n",
      "Loss on train= 0.01097716111689806\n",
      "Loss on test= 0.015820257365703583\n",
      "acc for Lsat= 0.08012130094899073 \n",
      "acc for Psat= 0.1095168070660697 \n",
      "acc for optim= 0.1444655085603396\n",
      "Epoch:544/1000\n",
      "Loss on train= 0.011256332509219646\n",
      "Loss on test= 0.015662826597690582\n",
      "acc for Lsat= 0.08203077018260956 \n",
      "acc for Psat= 0.10742635081211727 \n",
      "acc for optim= 0.1448370180196232\n",
      "Epoch:545/1000\n",
      "Loss on train= 0.010759622789919376\n",
      "Loss on test= 0.014870541170239449\n",
      "acc for Lsat= 0.08915707137849596 \n",
      "acc for Psat= 0.09612873709864085 \n",
      "acc for optim= 0.14354652927981482\n",
      "Epoch:546/1000\n",
      "Loss on train= 0.010887423530220985\n",
      "Loss on test= 0.015057636424899101\n",
      "acc for Lsat= 0.08207309279176925 \n",
      "acc for Psat= 0.1192825241221322 \n",
      "acc for optim= 0.14175919327470993\n",
      "Epoch:547/1000\n",
      "Loss on train= 0.011118844151496887\n",
      "Loss on test= 0.01477380283176899\n",
      "acc for Lsat= 0.0708750448293156 \n",
      "acc for Psat= 0.09626111371649636 \n",
      "acc for optim= 0.1444422726829847\n",
      "Epoch:548/1000\n",
      "Loss on train= 0.011224305257201195\n",
      "Loss on test= 0.015305866487324238\n",
      "acc for Lsat= 0.07561589413219028 \n",
      "acc for Psat= 0.10558604747056963 \n",
      "acc for optim= 0.14281661974059212\n",
      "Epoch:549/1000\n",
      "Loss on train= 0.010689234361052513\n",
      "Loss on test= 0.015239679254591465\n",
      "acc for Lsat= 0.08117042084534963 \n",
      "acc for Psat= 0.11956352757083044 \n",
      "acc for optim= 0.1409445827205976\n",
      "Epoch:550/1000\n",
      "Loss on train= 0.010728717781603336\n",
      "Loss on test= 0.014187191613018513\n",
      "acc for Lsat= 0.0787501648068428 \n",
      "acc for Psat= 0.09548613197273677 \n",
      "acc for optim= 0.14423020647631754\n",
      "Epoch:551/1000\n",
      "Loss on train= 0.01082135271281004\n",
      "Loss on test= 0.014295324683189392\n",
      "acc for Lsat= 0.06733196725447972 \n",
      "acc for Psat= 0.09497990873124863 \n",
      "acc for optim= 0.1426833262046178\n",
      "Epoch:552/1000\n",
      "Loss on train= 0.010826590470969677\n",
      "Loss on test= 0.014736916869878769\n",
      "acc for Lsat= 0.07436443169911702 \n",
      "acc for Psat= 0.10671906669934592 \n",
      "acc for optim= 0.14382783389753767\n",
      "Epoch:553/1000\n",
      "Loss on train= 0.0108025586232543\n",
      "Loss on test= 0.014672011137008667\n",
      "acc for Lsat= 0.08776982128620148 \n",
      "acc for Psat= 0.10528853833675385 \n",
      "acc for optim= 0.1404053697983424\n",
      "Epoch:554/1000\n",
      "Loss on train= 0.010305281728506088\n",
      "Loss on test= 0.015142392367124557\n",
      "acc for Lsat= 0.07516229682498508 \n",
      "acc for Psat= 0.09991237454944185 \n",
      "acc for optim= 0.14295792331298196\n",
      "Epoch:555/1000\n",
      "Loss on train= 0.011206323280930519\n",
      "Loss on test= 0.015436594374477863\n",
      "acc for Lsat= 0.08594116850031747 \n",
      "acc for Psat= 0.09645124624172846 \n",
      "acc for optim= 0.1434934188922246\n",
      "Epoch:556/1000\n",
      "Loss on train= 0.010697158984839916\n",
      "Loss on test= 0.01587541587650776\n",
      "acc for Lsat= 0.08021907574600645 \n",
      "acc for Psat= 0.10214348071151311 \n",
      "acc for optim= 0.14204494224654304\n",
      "Epoch:557/1000\n",
      "Loss on train= 0.011175560764968395\n",
      "Loss on test= 0.014582695439457893\n",
      "acc for Lsat= 0.07920358777046203 \n",
      "acc for Psat= 0.10235170357757144 \n",
      "acc for optim= 0.14650513894028136\n",
      "Epoch:558/1000\n",
      "Loss on train= 0.010703199543058872\n",
      "Loss on test= 0.014691086485981941\n",
      "acc for Lsat= 0.07553215523560841 \n",
      "acc for Psat= 0.11177356673611534 \n",
      "acc for optim= 0.14438855283790164\n",
      "Epoch:559/1000\n",
      "Loss on train= 0.01104729250073433\n",
      "Loss on test= 0.015432197600603104\n",
      "acc for Lsat= 0.07427093419763778 \n",
      "acc for Psat= 0.10216360241174699 \n",
      "acc for optim= 0.1427746781044536\n",
      "Epoch:560/1000\n",
      "Loss on train= 0.01065272931009531\n",
      "Loss on test= 0.015814045444130898\n",
      "acc for Lsat= 0.07862118399805493 \n",
      "acc for Psat= 0.10084193878703647 \n",
      "acc for optim= 0.1392958011892107\n",
      "Epoch:561/1000\n",
      "Loss on train= 0.011241148225963116\n",
      "Loss on test= 0.014633949846029282\n",
      "acc for Lsat= 0.08198779390917883 \n",
      "acc for Psat= 0.11042183140913646 \n",
      "acc for optim= 0.14087789588504368\n",
      "Epoch:562/1000\n",
      "Loss on train= 0.010550830513238907\n",
      "Loss on test= 0.014995566569268703\n",
      "acc for Lsat= 0.07215084400441912 \n",
      "acc for Psat= 0.10329965071545705 \n",
      "acc for optim= 0.13993267979886798\n",
      "Epoch:563/1000\n",
      "Loss on train= 0.011035213246941566\n",
      "Loss on test= 0.015033851377665997\n",
      "acc for Lsat= 0.07796469943390953 \n",
      "acc for Psat= 0.09323305520746444 \n",
      "acc for optim= 0.14365594734748208\n",
      "Epoch:564/1000\n",
      "Loss on train= 0.0107888737693429\n",
      "Loss on test= 0.014206386171281338\n",
      "acc for Lsat= 0.07427914092938105 \n",
      "acc for Psat= 0.09576482656929229 \n",
      "acc for optim= 0.1409674728910128\n",
      "Epoch:565/1000\n",
      "Loss on train= 0.010926461778581142\n",
      "Loss on test= 0.015862485393881798\n",
      "acc for Lsat= 0.0839657833178838 \n",
      "acc for Psat= 0.12107544971836938 \n",
      "acc for optim= 0.14070744382010567\n",
      "Epoch:566/1000\n",
      "Loss on train= 0.010865237563848495\n",
      "Loss on test= 0.014526544138789177\n",
      "acc for Lsat= 0.07382453249560463 \n",
      "acc for Psat= 0.11020553625292248 \n",
      "acc for optim= 0.1421681011716525\n",
      "Epoch:567/1000\n",
      "Loss on train= 0.010712639428675175\n",
      "Loss on test= 0.014526115730404854\n",
      "acc for Lsat= 0.07319222539663314 \n",
      "acc for Psat= 0.09288977765374712 \n",
      "acc for optim= 0.14156728949811723\n",
      "Epoch:568/1000\n",
      "Loss on train= 0.010850462131202221\n",
      "Loss on test= 0.014692040160298347\n",
      "acc for Lsat= 0.09256686336464354 \n",
      "acc for Psat= 0.10818672908676992 \n",
      "acc for optim= 0.140288460916943\n",
      "Epoch:569/1000\n",
      "Loss on train= 0.010972125455737114\n",
      "Loss on test= 0.014554866589605808\n",
      "acc for Lsat= 0.0778596250547303 \n",
      "acc for Psat= 0.09642761713928648 \n",
      "acc for optim= 0.14577603936195374\n",
      "Epoch:570/1000\n",
      "Loss on train= 0.010794163681566715\n",
      "Loss on test= 0.014680776745080948\n",
      "acc for Lsat= 0.07532975408766004 \n",
      "acc for Psat= 0.09700845463408364 \n",
      "acc for optim= 0.14445591304037306\n",
      "Epoch:571/1000\n",
      "Loss on train= 0.010724212974309921\n",
      "Loss on test= 0.015661511570215225\n",
      "acc for Lsat= 0.09141395191351574 \n",
      "acc for Psat= 0.10844199839565488 \n",
      "acc for optim= 0.14342763208680684\n",
      "Epoch:572/1000\n",
      "Loss on train= 0.010884926654398441\n",
      "Loss on test= 0.014033130370080471\n",
      "acc for Lsat= 0.0784297310643726 \n",
      "acc for Psat= 0.10333220693800184 \n",
      "acc for optim= 0.1447877402106921\n",
      "Epoch:573/1000\n",
      "Loss on train= 0.010638435371220112\n",
      "Loss on test= 0.014686046168208122\n",
      "acc for Lsat= 0.07412522253062989 \n",
      "acc for Psat= 0.10151286241081026 \n",
      "acc for optim= 0.14222000539302826\n",
      "Epoch:574/1000\n",
      "Loss on train= 0.010730784386396408\n",
      "Loss on test= 0.014780604280531406\n",
      "acc for Lsat= 0.07014419916603301 \n",
      "acc for Psat= 0.09367122186554801 \n",
      "acc for optim= 0.1445792703164948\n",
      "Epoch:575/1000\n",
      "Loss on train= 0.010670305229723454\n",
      "Loss on test= 0.016047345474362373\n",
      "acc for Lsat= 0.07383181949456533 \n",
      "acc for Psat= 0.09605458974838256 \n",
      "acc for optim= 0.1457869223422474\n",
      "Epoch:576/1000\n",
      "Loss on train= 0.01095670834183693\n",
      "Loss on test= 0.014738953672349453\n",
      "acc for Lsat= 0.07367140485180748 \n",
      "acc for Psat= 0.09428481062253317 \n",
      "acc for optim= 0.14082017888625462\n",
      "Epoch:577/1000\n",
      "Loss on train= 0.01032884418964386\n",
      "Loss on test= 0.014394651167094707\n",
      "acc for Lsat= 0.07548583216137356 \n",
      "acc for Psat= 0.09538129932350582 \n",
      "acc for optim= 0.1469998354713122\n",
      "Epoch:578/1000\n",
      "Loss on train= 0.010717974044382572\n",
      "Loss on test= 0.015279165469110012\n",
      "acc for Lsat= 0.0825323964158694 \n",
      "acc for Psat= 0.1172841128375795 \n",
      "acc for optim= 0.14390125307771892\n",
      "Epoch:579/1000\n",
      "Loss on train= 0.010448531247675419\n",
      "Loss on test= 0.014801952987909317\n",
      "acc for Lsat= 0.07916000419192842 \n",
      "acc for Psat= 0.09297283523612553 \n",
      "acc for optim= 0.14349062691132228\n",
      "Epoch:580/1000\n",
      "Loss on train= 0.010667958296835423\n",
      "Loss on test= 0.015996359288692474\n",
      "acc for Lsat= 0.07442650828096602 \n",
      "acc for Psat= 0.09732643398973678 \n",
      "acc for optim= 0.14216837055153317\n",
      "Epoch:581/1000\n",
      "Loss on train= 0.010435101576149464\n",
      "Loss on test= 0.016614381223917007\n",
      "acc for Lsat= 0.07617788480387794 \n",
      "acc for Psat= 0.089981290532483 \n",
      "acc for optim= 0.14455879860454132\n",
      "Epoch:582/1000\n",
      "Loss on train= 0.010510268621146679\n",
      "Loss on test= 0.01536762248724699\n",
      "acc for Lsat= 0.07811670220560497 \n",
      "acc for Psat= 0.10448230438762242 \n",
      "acc for optim= 0.14471774498621626\n",
      "Epoch:583/1000\n",
      "Loss on train= 0.011042844504117966\n",
      "Loss on test= 0.014711694791913033\n",
      "acc for Lsat= 0.072787692811754 \n",
      "acc for Psat= 0.09608700623114905 \n",
      "acc for optim= 0.14586709489425026\n",
      "Epoch:584/1000\n",
      "Loss on train= 0.01051788218319416\n",
      "Loss on test= 0.01568964123725891\n",
      "acc for Lsat= 0.07466144528653887 \n",
      "acc for Psat= 0.09775894102123049 \n",
      "acc for optim= 0.14447143624226252\n",
      "Epoch:585/1000\n",
      "Loss on train= 0.010226147249341011\n",
      "Loss on test= 0.014857292175292969\n",
      "acc for Lsat= 0.0755689317981402 \n",
      "acc for Psat= 0.09941940274503497 \n",
      "acc for optim= 0.14147788617346022\n",
      "Epoch:586/1000\n",
      "Loss on train= 0.010591679252684116\n",
      "Loss on test= 0.015717506408691406\n",
      "acc for Lsat= 0.07825354552931256 \n",
      "acc for Psat= 0.10367914951509899 \n",
      "acc for optim= 0.14343545089165372\n",
      "Epoch:587/1000\n",
      "Loss on train= 0.010541887022554874\n",
      "Loss on test= 0.014469529502093792\n",
      "acc for Lsat= 0.0724453416135576 \n",
      "acc for Psat= 0.09760807587040794 \n",
      "acc for optim= 0.1433462876412604\n",
      "Epoch:588/1000\n",
      "Loss on train= 0.010371998883783817\n",
      "Loss on test= 0.01408391073346138\n",
      "acc for Lsat= 0.07153119775984022 \n",
      "acc for Psat= 0.10647576285733117 \n",
      "acc for optim= 0.14331678781244492\n",
      "Epoch:589/1000\n",
      "Loss on train= 0.010820908471941948\n",
      "Loss on test= 0.014200745150446892\n",
      "acc for Lsat= 0.07837340434392295 \n",
      "acc for Psat= 0.0984377258353763 \n",
      "acc for optim= 0.1415973017613093\n",
      "Epoch:590/1000\n",
      "Loss on train= 0.010865363292396069\n",
      "Loss on test= 0.014911819249391556\n",
      "acc for Lsat= 0.07134549568096797 \n",
      "acc for Psat= 0.10052138235833911 \n",
      "acc for optim= 0.1466031298041344\n",
      "Epoch:591/1000\n",
      "Loss on train= 0.01038440689444542\n",
      "Loss on test= 0.01571340300142765\n",
      "acc for Lsat= 0.07481685479482016 \n",
      "acc for Psat= 0.09713763015137779 \n",
      "acc for optim= 0.14661952555179597\n",
      "Epoch:592/1000\n",
      "Loss on train= 0.010499300435185432\n",
      "Loss on test= 0.013808142393827438\n",
      "acc for Lsat= 0.08210318916373784 \n",
      "acc for Psat= 0.10527380837334527 \n",
      "acc for optim= 0.14151066193977993\n",
      "Epoch:593/1000\n",
      "Loss on train= 0.010572737082839012\n",
      "Loss on test= 0.01610647700726986\n",
      "acc for Lsat= 0.08067686673667696 \n",
      "acc for Psat= 0.11168817447291479 \n",
      "acc for optim= 0.1451998151010937\n",
      "Epoch:594/1000\n",
      "Loss on train= 0.01052251085639\n",
      "Loss on test= 0.014407387934625149\n",
      "acc for Lsat= 0.07305338664187326 \n",
      "acc for Psat= 0.09996410508950551 \n",
      "acc for optim= 0.14321880059109796\n",
      "Epoch:595/1000\n",
      "Loss on train= 0.010733694769442081\n",
      "Loss on test= 0.016081780195236206\n",
      "acc for Lsat= 0.07140041109588412 \n",
      "acc for Psat= 0.10127929035160277 \n",
      "acc for optim= 0.14290117853217657\n",
      "Epoch:596/1000\n",
      "Loss on train= 0.009972131811082363\n",
      "Loss on test= 0.0153011130169034\n",
      "acc for Lsat= 0.08013215826617348 \n",
      "acc for Psat= 0.10682898577716615 \n",
      "acc for optim= 0.14252969258361392\n",
      "Epoch:597/1000\n",
      "Loss on train= 0.010684624314308167\n",
      "Loss on test= 0.01459654700011015\n",
      "acc for Lsat= 0.07044107202026578 \n",
      "acc for Psat= 0.09713064465257853 \n",
      "acc for optim= 0.14345905714564855\n",
      "Epoch:598/1000\n",
      "Loss on train= 0.01047823391854763\n",
      "Loss on test= 0.014924845658242702\n",
      "acc for Lsat= 0.08020042164458169 \n",
      "acc for Psat= 0.10039883454640708 \n",
      "acc for optim= 0.14309216489394505\n",
      "Epoch:599/1000\n",
      "Loss on train= 0.010561554692685604\n",
      "Loss on test= 0.014805806800723076\n",
      "acc for Lsat= 0.07678559753629896 \n",
      "acc for Psat= 0.10746578358941608 \n",
      "acc for optim= 0.14114214165343175\n",
      "Epoch:600/1000\n",
      "Loss on train= 0.010409322567284107\n",
      "Loss on test= 0.015224931761622429\n",
      "acc for Lsat= 0.06918316549725004 \n",
      "acc for Psat= 0.10438160763846503 \n",
      "acc for optim= 0.14481543120410706\n",
      "Epoch:601/1000\n",
      "Loss on train= 0.010313557460904121\n",
      "Loss on test= 0.014885272830724716\n",
      "acc for Lsat= 0.07539509932200114 \n",
      "acc for Psat= 0.0967615990175141 \n",
      "acc for optim= 0.1448358385099305\n",
      "Epoch:602/1000\n",
      "Loss on train= 0.010377458296716213\n",
      "Loss on test= 0.015171998180449009\n",
      "acc for Lsat= 0.07983620464801788 \n",
      "acc for Psat= 0.11235405007998149 \n",
      "acc for optim= 0.14315076652500366\n",
      "Epoch:603/1000\n",
      "Loss on train= 0.010447746142745018\n",
      "Loss on test= 0.014403954148292542\n",
      "acc for Lsat= 0.07289802432060241 \n",
      "acc for Psat= 0.10986169543531206 \n",
      "acc for optim= 0.14789633949597678\n",
      "Epoch:604/1000\n",
      "Loss on train= 0.010442488826811314\n",
      "Loss on test= 0.014729486778378487\n",
      "acc for Lsat= 0.07214610030253728 \n",
      "acc for Psat= 0.09230929414431255 \n",
      "acc for optim= 0.14321833137008877\n",
      "Epoch:605/1000\n",
      "Loss on train= 0.01085070800036192\n",
      "Loss on test= 0.01547730341553688\n",
      "acc for Lsat= 0.07384025868442323 \n",
      "acc for Psat= 0.09759787685341306 \n",
      "acc for optim= 0.14482861277129916\n",
      "Epoch:606/1000\n",
      "Loss on train= 0.010115050710737705\n",
      "Loss on test= 0.014577772468328476\n",
      "acc for Lsat= 0.07166937953895992 \n",
      "acc for Psat= 0.09771043144994312 \n",
      "acc for optim= 0.1442194854219755\n",
      "Epoch:607/1000\n",
      "Loss on train= 0.010740538127720356\n",
      "Loss on test= 0.015452331863343716\n",
      "acc for Lsat= 0.07136777904298569 \n",
      "acc for Psat= 0.10772104875908958 \n",
      "acc for optim= 0.14353672795825537\n",
      "Epoch:608/1000\n",
      "Loss on train= 0.01080094650387764\n",
      "Loss on test= 0.014705502428114414\n",
      "acc for Lsat= 0.07509282413456174 \n",
      "acc for Psat= 0.10020669251680375 \n",
      "acc for optim= 0.1432882974545161\n",
      "Epoch:609/1000\n",
      "Loss on train= 0.010480585508048534\n",
      "Loss on test= 0.016514623537659645\n",
      "acc for Lsat= 0.08482201331191593 \n",
      "acc for Psat= 0.10329792102177938 \n",
      "acc for optim= 0.14227251178688471\n",
      "Epoch:610/1000\n",
      "Loss on train= 0.01037872489541769\n",
      "Loss on test= 0.014198639430105686\n",
      "acc for Lsat= 0.07244773308436075 \n",
      "acc for Psat= 0.10236142459842892 \n",
      "acc for optim= 0.14422791533999976\n",
      "Epoch:611/1000\n",
      "Loss on train= 0.010578986257314682\n",
      "Loss on test= 0.01421596109867096\n",
      "acc for Lsat= 0.0746186708410581 \n",
      "acc for Psat= 0.10319462335771984 \n",
      "acc for optim= 0.14166907254192565\n",
      "Epoch:612/1000\n",
      "Loss on train= 0.01072391215711832\n",
      "Loss on test= 0.01412890013307333\n",
      "acc for Lsat= 0.07170795285039479 \n",
      "acc for Psat= 0.1029439123140441 \n",
      "acc for optim= 0.1436331167817116\n",
      "Epoch:613/1000\n",
      "Loss on train= 0.010106179863214493\n",
      "Loss on test= 0.015407941304147243\n",
      "acc for Lsat= 0.08785337375269997 \n",
      "acc for Psat= 0.1125687559445699 \n",
      "acc for optim= 0.14283532599608104\n",
      "Epoch:614/1000\n",
      "Loss on train= 0.010653861798346043\n",
      "Loss on test= 0.015224837698042393\n",
      "acc for Lsat= 0.07265745004018148 \n",
      "acc for Psat= 0.10853107008669112 \n",
      "acc for optim= 0.14411721113655301\n",
      "Epoch:615/1000\n",
      "Loss on train= 0.010365392081439495\n",
      "Loss on test= 0.014540559612214565\n",
      "acc for Lsat= 0.06908842209312652 \n",
      "acc for Psat= 0.09420538445313771 \n",
      "acc for optim= 0.14376968774530619\n",
      "Epoch:616/1000\n",
      "Loss on train= 0.010763979516923428\n",
      "Loss on test= 0.017231520265340805\n",
      "acc for Lsat= 0.07191665387815899 \n",
      "acc for Psat= 0.10715570582283868 \n",
      "acc for optim= 0.1468432707919015\n",
      "Epoch:617/1000\n",
      "Loss on train= 0.010599799454212189\n",
      "Loss on test= 0.014381415210664272\n",
      "acc for Lsat= 0.06897292269600762 \n",
      "acc for Psat= 0.09287640369600719 \n",
      "acc for optim= 0.143445113963551\n",
      "Epoch:618/1000\n",
      "Loss on train= 0.010529663413763046\n",
      "Loss on test= 0.014146219938993454\n",
      "acc for Lsat= 0.06935445434517332 \n",
      "acc for Psat= 0.09155494479669464 \n",
      "acc for optim= 0.14192925211456087\n",
      "Epoch:619/1000\n",
      "Loss on train= 0.010504004545509815\n",
      "Loss on test= 0.014751538634300232\n",
      "acc for Lsat= 0.08041982534858916 \n",
      "acc for Psat= 0.10067628340588675 \n",
      "acc for optim= 0.13993197414610123\n",
      "Epoch:620/1000\n",
      "Loss on train= 0.010533811524510384\n",
      "Loss on test= 0.014276598580181599\n",
      "acc for Lsat= 0.0746596293316947 \n",
      "acc for Psat= 0.09129988882276746 \n",
      "acc for optim= 0.13882739361789492\n",
      "Epoch:621/1000\n",
      "Loss on train= 0.010308205150067806\n",
      "Loss on test= 0.01528915949165821\n",
      "acc for Lsat= 0.08484058545695411 \n",
      "acc for Psat= 0.09428621066941155 \n",
      "acc for optim= 0.14240147636996378\n",
      "Epoch:622/1000\n",
      "Loss on train= 0.010440986603498459\n",
      "Loss on test= 0.015052232891321182\n",
      "acc for Lsat= 0.07227207687166003 \n",
      "acc for Psat= 0.09480808079242704 \n",
      "acc for optim= 0.14469614062044356\n",
      "Epoch:623/1000\n",
      "Loss on train= 0.010087263770401478\n",
      "Loss on test= 0.016229979693889618\n",
      "acc for Lsat= 0.08032082749737635 \n",
      "acc for Psat= 0.09925347897741528 \n",
      "acc for optim= 0.13853390514850616\n",
      "Epoch:624/1000\n",
      "Loss on train= 0.010407698340713978\n",
      "Loss on test= 0.01485908031463623\n",
      "acc for Lsat= 0.07327507767412397 \n",
      "acc for Psat= 0.09760690662595961 \n",
      "acc for optim= 0.14302776058514913\n",
      "Epoch:625/1000\n",
      "Loss on train= 0.011077778413891792\n",
      "Loss on test= 0.015487300232052803\n",
      "acc for Lsat= 0.07231893324189716 \n",
      "acc for Psat= 0.0901910390290949 \n",
      "acc for optim= 0.14089135577281317\n",
      "Epoch:626/1000\n",
      "Loss on train= 0.010450955480337143\n",
      "Loss on test= 0.014210909605026245\n",
      "acc for Lsat= 0.06971584558486939 \n",
      "acc for Psat= 0.09312750564681158 \n",
      "acc for optim= 0.14324518508381315\n",
      "Epoch:627/1000\n",
      "Loss on train= 0.010174836963415146\n",
      "Loss on test= 0.014770693145692348\n",
      "acc for Lsat= 0.07508092390166388 \n",
      "acc for Psat= 0.09961259977685079 \n",
      "acc for optim= 0.14239401502741708\n",
      "Epoch:628/1000\n",
      "Loss on train= 0.010652150958776474\n",
      "Loss on test= 0.015998687595129013\n",
      "acc for Lsat= 0.0779175834523307 \n",
      "acc for Psat= 0.11381245570050345 \n",
      "acc for optim= 0.14393388910426036\n",
      "Epoch:629/1000\n",
      "Loss on train= 0.010021593421697617\n",
      "Loss on test= 0.015103073790669441\n",
      "acc for Lsat= 0.07301540788677004 \n",
      "acc for Psat= 0.10012015004952748 \n",
      "acc for optim= 0.14285736646917133\n",
      "Epoch:630/1000\n",
      "Loss on train= 0.010389696806669235\n",
      "Loss on test= 0.016199447214603424\n",
      "acc for Lsat= 0.0736606303188536 \n",
      "acc for Psat= 0.09109057502614128 \n",
      "acc for optim= 0.1437687713238928\n",
      "Epoch:631/1000\n",
      "Loss on train= 0.010179867967963219\n",
      "Loss on test= 0.014540932141244411\n",
      "acc for Lsat= 0.0731370276874966 \n",
      "acc for Psat= 0.09794298890564176 \n",
      "acc for optim= 0.14316893501414196\n",
      "Epoch:632/1000\n",
      "Loss on train= 0.010112138465046883\n",
      "Loss on test= 0.015327692031860352\n",
      "acc for Lsat= 0.07241217758920455 \n",
      "acc for Psat= 0.09630343831247754 \n",
      "acc for optim= 0.1435055911540985\n",
      "Epoch:633/1000\n",
      "Loss on train= 0.010127617977559566\n",
      "Loss on test= 0.014957915060222149\n",
      "acc for Lsat= 0.08195738212929832 \n",
      "acc for Psat= 0.11097326510482365 \n",
      "acc for optim= 0.14285949253373678\n",
      "Epoch:634/1000\n",
      "Loss on train= 0.00993635319173336\n",
      "Loss on test= 0.015022996813058853\n",
      "acc for Lsat= 0.07927728676133687 \n",
      "acc for Psat= 0.10655105958382288 \n",
      "acc for optim= 0.14107828587293622\n",
      "Epoch:635/1000\n",
      "Loss on train= 0.010162543505430222\n",
      "Loss on test= 0.014929736964404583\n",
      "acc for Lsat= 0.07505104922586019 \n",
      "acc for Psat= 0.09117685606082282 \n",
      "acc for optim= 0.14037466297547022\n",
      "Epoch:636/1000\n",
      "Loss on train= 0.009745161049067974\n",
      "Loss on test= 0.014275269582867622\n",
      "acc for Lsat= 0.07790364076693854 \n",
      "acc for Psat= 0.09696615801917183 \n",
      "acc for optim= 0.14243177556329303\n",
      "Epoch:637/1000\n",
      "Loss on train= 0.01038519199937582\n",
      "Loss on test= 0.014749308116734028\n",
      "acc for Lsat= 0.0779964420530531 \n",
      "acc for Psat= 0.09412456783983442 \n",
      "acc for optim= 0.1434394096334775\n",
      "Epoch:638/1000\n",
      "Loss on train= 0.00993497483432293\n",
      "Loss on test= 0.013740696012973785\n",
      "acc for Lsat= 0.07371314333544836 \n",
      "acc for Psat= 0.09649568233225078 \n",
      "acc for optim= 0.1441584090391795\n",
      "Epoch:639/1000\n",
      "Loss on train= 0.010129768401384354\n",
      "Loss on test= 0.014304520562291145\n",
      "acc for Lsat= 0.07328907632165485 \n",
      "acc for Psat= 0.09828591065274345 \n",
      "acc for optim= 0.14118983878029714\n",
      "Epoch:640/1000\n",
      "Loss on train= 0.009914107620716095\n",
      "Loss on test= 0.015357696451246738\n",
      "acc for Lsat= 0.07395042296912935 \n",
      "acc for Psat= 0.09134829209910499 \n",
      "acc for optim= 0.14459254691998163\n",
      "Epoch:641/1000\n",
      "Loss on train= 0.010068303905427456\n",
      "Loss on test= 0.01435274537652731\n",
      "acc for Lsat= 0.07774329698748059 \n",
      "acc for Psat= 0.10295140859153534 \n",
      "acc for optim= 0.13986444903744594\n",
      "Epoch:642/1000\n",
      "Loss on train= 0.009826215915381908\n",
      "Loss on test= 0.015403598546981812\n",
      "acc for Lsat= 0.07533910373846689 \n",
      "acc for Psat= 0.0947677231497235 \n",
      "acc for optim= 0.145506743590037\n",
      "Epoch:643/1000\n",
      "Loss on train= 0.010230340994894505\n",
      "Loss on test= 0.014534877613186836\n",
      "acc for Lsat= 0.07748828132947287 \n",
      "acc for Psat= 0.09939262320597968 \n",
      "acc for optim= 0.14206047770049837\n",
      "Epoch:644/1000\n",
      "Loss on train= 0.010417986661195755\n",
      "Loss on test= 0.014828933402895927\n",
      "acc for Lsat= 0.078981531990899 \n",
      "acc for Psat= 0.10559464378489389 \n",
      "acc for optim= 0.14207353591918948\n",
      "Epoch:645/1000\n",
      "Loss on train= 0.009941201657056808\n",
      "Loss on test= 0.014512645080685616\n",
      "acc for Lsat= 0.07757631209161547 \n",
      "acc for Psat= 0.09258304784695306 \n",
      "acc for optim= 0.14046818299425973\n",
      "Epoch:646/1000\n",
      "Loss on train= 0.009789005853235722\n",
      "Loss on test= 0.015208623372018337\n",
      "acc for Lsat= 0.07655078321695329 \n",
      "acc for Psat= 0.09180123375521765 \n",
      "acc for optim= 0.13885158913003073\n",
      "Epoch:647/1000\n",
      "Loss on train= 0.009929724968969822\n",
      "Loss on test= 0.017524927854537964\n",
      "acc for Lsat= 0.0728016995721393 \n",
      "acc for Psat= 0.09801308628585606 \n",
      "acc for optim= 0.1446391638782289\n",
      "Epoch:648/1000\n",
      "Loss on train= 0.010311545804142952\n",
      "Loss on test= 0.014594950713217258\n",
      "acc for Lsat= 0.07490526719225778 \n",
      "acc for Psat= 0.09867958658271364 \n",
      "acc for optim= 0.14144983308182824\n",
      "Epoch:649/1000\n",
      "Loss on train= 0.01021391898393631\n",
      "Loss on test= 0.014547441154718399\n",
      "acc for Lsat= 0.07721992896662817 \n",
      "acc for Psat= 0.1028384288152059 \n",
      "acc for optim= 0.1402926845682992\n",
      "Epoch:650/1000\n",
      "Loss on train= 0.010043836198747158\n",
      "Loss on test= 0.016526635736227036\n",
      "acc for Lsat= 0.07341673937108781 \n",
      "acc for Psat= 0.09948407510916392 \n",
      "acc for optim= 0.14708557774623235\n",
      "Epoch:651/1000\n",
      "Loss on train= 0.009921512566506863\n",
      "Loss on test= 0.01416051760315895\n",
      "acc for Lsat= 0.0729906451370981 \n",
      "acc for Psat= 0.0977974611851904 \n",
      "acc for optim= 0.13979321983125473\n",
      "Epoch:652/1000\n",
      "Loss on train= 0.00997059978544712\n",
      "Loss on test= 0.01390934456139803\n",
      "acc for Lsat= 0.075788238313463 \n",
      "acc for Psat= 0.09272428138388528 \n",
      "acc for optim= 0.14419558859533732\n",
      "Epoch:653/1000\n",
      "Loss on train= 0.009752719663083553\n",
      "Loss on test= 0.01526375487446785\n",
      "acc for Lsat= 0.07846384180916681 \n",
      "acc for Psat= 0.09541809840334786 \n",
      "acc for optim= 0.14358579383956063\n",
      "Epoch:654/1000\n",
      "Loss on train= 0.01006735023111105\n",
      "Loss on test= 0.014618169516324997\n",
      "acc for Lsat= 0.07967202961444855 \n",
      "acc for Psat= 0.1012342780828476 \n",
      "acc for optim= 0.14005722834004294\n",
      "Epoch:655/1000\n",
      "Loss on train= 0.009848122484982014\n",
      "Loss on test= 0.014228993095457554\n",
      "acc for Lsat= 0.06946958485576842 \n",
      "acc for Psat= 0.08998051981131236 \n",
      "acc for optim= 0.14222235977649691\n",
      "Epoch:656/1000\n",
      "Loss on train= 0.010263722389936447\n",
      "Loss on test= 0.01492306962609291\n",
      "acc for Lsat= 0.07640992734167312 \n",
      "acc for Psat= 0.10533948772483402 \n",
      "acc for optim= 0.14215529031223723\n",
      "Epoch:657/1000\n",
      "Loss on train= 0.010167703032493591\n",
      "Loss on test= 0.014480418525636196\n",
      "acc for Lsat= 0.0766188093357616 \n",
      "acc for Psat= 0.10265899962849087 \n",
      "acc for optim= 0.14406406299935445\n",
      "Epoch:658/1000\n",
      "Loss on train= 0.009689618833363056\n",
      "Loss on test= 0.015475932508707047\n",
      "acc for Lsat= 0.0737073462870386 \n",
      "acc for Psat= 0.11377312077416314 \n",
      "acc for optim= 0.14357249091068905\n",
      "Epoch:659/1000\n",
      "Loss on train= 0.010223754681646824\n",
      "Loss on test= 0.014827711507678032\n",
      "acc for Lsat= 0.07593821850087908 \n",
      "acc for Psat= 0.10695974230766295 \n",
      "acc for optim= 0.1407390968667136\n",
      "Epoch:660/1000\n",
      "Loss on train= 0.009625217877328396\n",
      "Loss on test= 0.015112063847482204\n",
      "acc for Lsat= 0.077619516187244 \n",
      "acc for Psat= 0.10564062380128436 \n",
      "acc for optim= 0.14232404828071593\n",
      "Epoch:661/1000\n",
      "Loss on train= 0.010129415430128574\n",
      "Loss on test= 0.014331449754536152\n",
      "acc for Lsat= 0.07179196940528021 \n",
      "acc for Psat= 0.10008478148116003 \n",
      "acc for optim= 0.14044074416160587\n",
      "Epoch:662/1000\n",
      "Loss on train= 0.009938793256878853\n",
      "Loss on test= 0.014948423020541668\n",
      "acc for Lsat= 0.0720549085074001 \n",
      "acc for Psat= 0.09704568253623115 \n",
      "acc for optim= 0.14164685126807955\n",
      "Epoch:663/1000\n",
      "Loss on train= 0.010059922002255917\n",
      "Loss on test= 0.015123127028346062\n",
      "acc for Lsat= 0.06908601671457291 \n",
      "acc for Psat= 0.09136056933138105 \n",
      "acc for optim= 0.14332575268215603\n",
      "Epoch:664/1000\n",
      "Loss on train= 0.010415853001177311\n",
      "Loss on test= 0.015307486057281494\n",
      "acc for Lsat= 0.07411658929453956 \n",
      "acc for Psat= 0.09354642000463274 \n",
      "acc for optim= 0.1417020650373565\n",
      "Epoch:665/1000\n",
      "Loss on train= 0.009684684686362743\n",
      "Loss on test= 0.015491760335862637\n",
      "acc for Lsat= 0.0731875126560529 \n",
      "acc for Psat= 0.10516859458552466 \n",
      "acc for optim= 0.14310647050539654\n",
      "Epoch:666/1000\n",
      "Loss on train= 0.010228668339550495\n",
      "Loss on test= 0.015356693416833878\n",
      "acc for Lsat= 0.07484512743022706 \n",
      "acc for Psat= 0.10395655797587502 \n",
      "acc for optim= 0.1418894189927313\n",
      "Epoch:667/1000\n",
      "Loss on train= 0.009981516748666763\n",
      "Loss on test= 0.013635823503136635\n",
      "acc for Lsat= 0.07396395868725247 \n",
      "acc for Psat= 0.09481211900711059 \n",
      "acc for optim= 0.1427617518438233\n",
      "Epoch:668/1000\n",
      "Loss on train= 0.01012339722365141\n",
      "Loss on test= 0.015143904834985733\n",
      "acc for Lsat= 0.07271328154537413 \n",
      "acc for Psat= 0.10076749838060803 \n",
      "acc for optim= 0.14121707578500114\n",
      "Epoch:669/1000\n",
      "Loss on train= 0.009979890659451485\n",
      "Loss on test= 0.015150722116231918\n",
      "acc for Lsat= 0.07404327160782284 \n",
      "acc for Psat= 0.09733955015738804 \n",
      "acc for optim= 0.14302780992454955\n",
      "Epoch:670/1000\n",
      "Loss on train= 0.010136005468666553\n",
      "Loss on test= 0.015963448211550713\n",
      "acc for Lsat= 0.07687666813532511 \n",
      "acc for Psat= 0.10201012955771553 \n",
      "acc for optim= 0.14084354440371197\n",
      "Epoch:671/1000\n",
      "Loss on train= 0.00983252003788948\n",
      "Loss on test= 0.01578160934150219\n",
      "acc for Lsat= 0.07450038823816511 \n",
      "acc for Psat= 0.09429888973633446 \n",
      "acc for optim= 0.14211836589707266\n",
      "Epoch:672/1000\n",
      "Loss on train= 0.010061067529022694\n",
      "Loss on test= 0.014308369718492031\n",
      "acc for Lsat= 0.07345396329959235 \n",
      "acc for Psat= 0.09698104427920447 \n",
      "acc for optim= 0.14156556990411545\n",
      "Epoch:673/1000\n",
      "Loss on train= 0.009883057326078415\n",
      "Loss on test= 0.014878327958285809\n",
      "acc for Lsat= 0.07686144891712401 \n",
      "acc for Psat= 0.10567387392123541 \n",
      "acc for optim= 0.14086318363746006\n",
      "Epoch:674/1000\n",
      "Loss on train= 0.010105928406119347\n",
      "Loss on test= 0.013846483081579208\n",
      "acc for Lsat= 0.06907723645369211 \n",
      "acc for Psat= 0.09596877776914176 \n",
      "acc for optim= 0.14178086138433882\n",
      "Epoch:675/1000\n",
      "Loss on train= 0.00974187906831503\n",
      "Loss on test= 0.014734994620084763\n",
      "acc for Lsat= 0.06974913941489326 \n",
      "acc for Psat= 0.09849860982762443 \n",
      "acc for optim= 0.14309449907806185\n",
      "Epoch:676/1000\n",
      "Loss on train= 0.009864713996648788\n",
      "Loss on test= 0.015993233770132065\n",
      "acc for Lsat= 0.07818448262082206 \n",
      "acc for Psat= 0.10566705680555766 \n",
      "acc for optim= 0.1435378220346239\n",
      "Epoch:677/1000\n",
      "Loss on train= 0.010011415928602219\n",
      "Loss on test= 0.013782226480543613\n",
      "acc for Lsat= 0.07340720941623052 \n",
      "acc for Psat= 0.1002844492594401 \n",
      "acc for optim= 0.1412657428118918\n",
      "Epoch:678/1000\n",
      "Loss on train= 0.009679943323135376\n",
      "Loss on test= 0.015063351951539516\n",
      "acc for Lsat= 0.07232996357811822 \n",
      "acc for Psat= 0.09255890978707206 \n",
      "acc for optim= 0.14141442494259937\n",
      "Epoch:679/1000\n",
      "Loss on train= 0.010315679013729095\n",
      "Loss on test= 0.015969695523381233\n",
      "acc for Lsat= 0.07014391471942266 \n",
      "acc for Psat= 0.09194991886615751 \n",
      "acc for optim= 0.14425536410676107\n",
      "Epoch:680/1000\n",
      "Loss on train= 0.009835758246481419\n",
      "Loss on test= 0.015506912022829056\n",
      "acc for Lsat= 0.07793117943737243 \n",
      "acc for Psat= 0.10583232707447476 \n",
      "acc for optim= 0.1437315430906084\n",
      "Epoch:681/1000\n",
      "Loss on train= 0.0098784901201725\n",
      "Loss on test= 0.01603568345308304\n",
      "acc for Lsat= 0.07851735336913003 \n",
      "acc for Psat= 0.10859995931386947 \n",
      "acc for optim= 0.14378159592549006\n",
      "Epoch:682/1000\n",
      "Loss on train= 0.009924855083227158\n",
      "Loss on test= 0.01467802282422781\n",
      "acc for Lsat= 0.07399933023585215 \n",
      "acc for Psat= 0.09517204993300968 \n",
      "acc for optim= 0.13926852113670773\n",
      "Epoch:683/1000\n",
      "Loss on train= 0.010198681615293026\n",
      "Loss on test= 0.01635066419839859\n",
      "acc for Lsat= 0.07498165898852878 \n",
      "acc for Psat= 0.10151858064863417 \n",
      "acc for optim= 0.14185921748479208\n",
      "Epoch:684/1000\n",
      "Loss on train= 0.00995719712227583\n",
      "Loss on test= 0.014926801435649395\n",
      "acc for Lsat= 0.07794508188962936 \n",
      "acc for Psat= 0.0962183874514368 \n",
      "acc for optim= 0.14340477999713686\n",
      "Epoch:685/1000\n",
      "Loss on train= 0.009580829180777073\n",
      "Loss on test= 0.015518400818109512\n",
      "acc for Lsat= 0.07735905067788229 \n",
      "acc for Psat= 0.10829325053426954 \n",
      "acc for optim= 0.14013833767837947\n",
      "Epoch:686/1000\n",
      "Loss on train= 0.009598378092050552\n",
      "Loss on test= 0.014234255068004131\n",
      "acc for Lsat= 0.07210851907730104 \n",
      "acc for Psat= 0.10385106388065551 \n",
      "acc for optim= 0.1434674017959171\n",
      "Epoch:687/1000\n",
      "Loss on train= 0.009928732179105282\n",
      "Loss on test= 0.014947500079870224\n",
      "acc for Lsat= 0.07405541539192201 \n",
      "acc for Psat= 0.09601443029112285 \n",
      "acc for optim= 0.1419126687778367\n",
      "Epoch:688/1000\n",
      "Loss on train= 0.009872904978692532\n",
      "Loss on test= 0.014084650203585625\n",
      "acc for Lsat= 0.07154853625430002 \n",
      "acc for Psat= 0.09321624586979549 \n",
      "acc for optim= 0.14335346205366983\n",
      "Epoch:689/1000\n",
      "Loss on train= 0.00991552509367466\n",
      "Loss on test= 0.015517743304371834\n",
      "acc for Lsat= 0.06954116506708992 \n",
      "acc for Psat= 0.09071476889981164 \n",
      "acc for optim= 0.1410030275583267\n",
      "Epoch:690/1000\n",
      "Loss on train= 0.010029829107224941\n",
      "Loss on test= 0.013832044787704945\n",
      "acc for Lsat= 0.08052443580494988 \n",
      "acc for Psat= 0.09984565956725015 \n",
      "acc for optim= 0.1412936576538616\n",
      "Epoch:691/1000\n",
      "Loss on train= 0.009804983623325825\n",
      "Loss on test= 0.015520145185291767\n",
      "acc for Lsat= 0.07322547286748887 \n",
      "acc for Psat= 0.09566669745577706 \n",
      "acc for optim= 0.13914725888106558\n",
      "Epoch:692/1000\n",
      "Loss on train= 0.009991437196731567\n",
      "Loss on test= 0.015084913931787014\n",
      "acc for Lsat= 0.07228311465846168 \n",
      "acc for Psat= 0.09126416063970991 \n",
      "acc for optim= 0.1443704398141967\n",
      "Epoch:693/1000\n",
      "Loss on train= 0.009594962932169437\n",
      "Loss on test= 0.01503448374569416\n",
      "acc for Lsat= 0.07063704762193893 \n",
      "acc for Psat= 0.09321840935283238 \n",
      "acc for optim= 0.13987857318586772\n",
      "Epoch:694/1000\n",
      "Loss on train= 0.009681538678705692\n",
      "Loss on test= 0.014543362893164158\n",
      "acc for Lsat= 0.07168631454308827 \n",
      "acc for Psat= 0.09700646979941262 \n",
      "acc for optim= 0.14022225273980035\n",
      "Epoch:695/1000\n",
      "Loss on train= 0.009574910625815392\n",
      "Loss on test= 0.015565337613224983\n",
      "acc for Lsat= 0.07441525508960088 \n",
      "acc for Psat= 0.111189361082183 \n",
      "acc for optim= 0.13902819537454184\n",
      "Epoch:696/1000\n",
      "Loss on train= 0.0099797947332263\n",
      "Loss on test= 0.015421677380800247\n",
      "acc for Lsat= 0.07475548966063393 \n",
      "acc for Psat= 0.09976574132839838 \n",
      "acc for optim= 0.14018378572331533\n",
      "Epoch:697/1000\n",
      "Loss on train= 0.010087105445563793\n",
      "Loss on test= 0.01646331697702408\n",
      "acc for Lsat= 0.07313645432392755 \n",
      "acc for Psat= 0.10121507197618484 \n",
      "acc for optim= 0.14340222527583443\n",
      "Epoch:698/1000\n",
      "Loss on train= 0.009341783821582794\n",
      "Loss on test= 0.015236596576869488\n",
      "acc for Lsat= 0.08385237207015356 \n",
      "acc for Psat= 0.11200574470890892 \n",
      "acc for optim= 0.14053584999508328\n",
      "Epoch:699/1000\n",
      "Loss on train= 0.00982371624559164\n",
      "Loss on test= 0.015825781971216202\n",
      "acc for Lsat= 0.06976823856433233 \n",
      "acc for Psat= 0.10349408090114592 \n",
      "acc for optim= 0.14010694755448236\n",
      "Epoch:700/1000\n",
      "Loss on train= 0.009749320335686207\n",
      "Loss on test= 0.013782266527414322\n",
      "acc for Lsat= 0.06872333453761208 \n",
      "acc for Psat= 0.09756774339410995 \n",
      "acc for optim= 0.14050818565818995\n",
      "Epoch:701/1000\n",
      "Loss on train= 0.009396662004292011\n",
      "Loss on test= 0.013950256630778313\n",
      "acc for Lsat= 0.0686400286025471 \n",
      "acc for Psat= 0.0919116761121485 \n",
      "acc for optim= 0.14152437763081657\n",
      "Epoch:702/1000\n",
      "Loss on train= 0.009954548440873623\n",
      "Loss on test= 0.014582391828298569\n",
      "acc for Lsat= 0.07352298216687309 \n",
      "acc for Psat= 0.10748499350415336 \n",
      "acc for optim= 0.14125057872798708\n",
      "Epoch:703/1000\n",
      "Loss on train= 0.009862974286079407\n",
      "Loss on test= 0.015825411304831505\n",
      "acc for Lsat= 0.07537934945689306 \n",
      "acc for Psat= 0.10147554261816873 \n",
      "acc for optim= 0.14163347250885433\n",
      "Epoch:704/1000\n",
      "Loss on train= 0.009514902718365192\n",
      "Loss on test= 0.014366509392857552\n",
      "acc for Lsat= 0.07086830817990834 \n",
      "acc for Psat= 0.09814474003182518 \n",
      "acc for optim= 0.14166172179910871\n",
      "Epoch:705/1000\n",
      "Loss on train= 0.009812409989535809\n",
      "Loss on test= 0.016336195170879364\n",
      "acc for Lsat= 0.07292710940043132 \n",
      "acc for Psat= 0.10126169274250667 \n",
      "acc for optim= 0.14217908630768458\n",
      "Epoch:706/1000\n",
      "Loss on train= 0.009632570669054985\n",
      "Loss on test= 0.015818839892745018\n",
      "acc for Lsat= 0.0762632100118531 \n",
      "acc for Psat= 0.10204121983713574 \n",
      "acc for optim= 0.14025276568200853\n",
      "Epoch:707/1000\n",
      "Loss on train= 0.009823194704949856\n",
      "Loss on test= 0.014463881962001324\n",
      "acc for Lsat= 0.07589884714947807 \n",
      "acc for Psat= 0.09596892313824762 \n",
      "acc for optim= 0.14199810011519326\n",
      "Epoch:708/1000\n",
      "Loss on train= 0.009338285773992538\n",
      "Loss on test= 0.014440621249377728\n",
      "acc for Lsat= 0.07359775801499685 \n",
      "acc for Psat= 0.09484474377499687 \n",
      "acc for optim= 0.13898603469133378\n",
      "Epoch:709/1000\n",
      "Loss on train= 0.009792149066925049\n",
      "Loss on test= 0.015183521434664726\n",
      "acc for Lsat= 0.07567842668957181 \n",
      "acc for Psat= 0.09928933398591148 \n",
      "acc for optim= 0.1437743638952573\n",
      "Epoch:710/1000\n",
      "Loss on train= 0.00994807668030262\n",
      "Loss on test= 0.014461543411016464\n",
      "acc for Lsat= 0.0729649297065205 \n",
      "acc for Psat= 0.09360445655054515 \n",
      "acc for optim= 0.13969584852457045\n",
      "Epoch:711/1000\n",
      "Loss on train= 0.009648025035858154\n",
      "Loss on test= 0.014274071902036667\n",
      "acc for Lsat= 0.07624240616957347 \n",
      "acc for Psat= 0.09699324568112692 \n",
      "acc for optim= 0.14125877055856917\n",
      "Epoch:712/1000\n",
      "Loss on train= 0.009670478291809559\n",
      "Loss on test= 0.01628071814775467\n",
      "acc for Lsat= 0.0710760262277391 \n",
      "acc for Psat= 0.09185000542137357 \n",
      "acc for optim= 0.14537814805905025\n",
      "Epoch:713/1000\n",
      "Loss on train= 0.00984395481646061\n",
      "Loss on test= 0.013778090476989746\n",
      "acc for Lsat= 0.07222617086437014 \n",
      "acc for Psat= 0.09543504714965818 \n",
      "acc for optim= 0.13995922456185023\n",
      "Epoch:714/1000\n",
      "Loss on train= 0.010144572705030441\n",
      "Loss on test= 0.015852507203817368\n",
      "acc for Lsat= 0.07792814075946808 \n",
      "acc for Psat= 0.09389250642723507 \n",
      "acc for optim= 0.13952974528074263\n",
      "Epoch:715/1000\n",
      "Loss on train= 0.009429379366338253\n",
      "Loss on test= 0.015194583684206009\n",
      "acc for Lsat= 0.07138301365905338 \n",
      "acc for Psat= 0.09355358928442001 \n",
      "acc for optim= 0.1402411604921023\n",
      "Epoch:716/1000\n",
      "Loss on train= 0.009867751970887184\n",
      "Loss on test= 0.016371645033359528\n",
      "acc for Lsat= 0.07052839348713556 \n",
      "acc for Psat= 0.10094249463743635 \n",
      "acc for optim= 0.1413391311963399\n",
      "Epoch:717/1000\n",
      "Loss on train= 0.010138881392776966\n",
      "Loss on test= 0.013679901137948036\n",
      "acc for Lsat= 0.06869103593958749 \n",
      "acc for Psat= 0.0968381212817298 \n",
      "acc for optim= 0.1418801117274496\n",
      "Epoch:718/1000\n",
      "Loss on train= 0.00938123557716608\n",
      "Loss on test= 0.01559761818498373\n",
      "acc for Lsat= 0.08190293312072754 \n",
      "acc for Psat= 0.11346343755722045 \n",
      "acc for optim= 0.13844145470195343\n",
      "Epoch:719/1000\n",
      "Loss on train= 0.009979892522096634\n",
      "Loss on test= 0.015880150720477104\n",
      "acc for Lsat= 0.06980203257666694 \n",
      "acc for Psat= 0.08821097215016685 \n",
      "acc for optim= 0.14282872627178828\n",
      "Epoch:720/1000\n",
      "Loss on train= 0.009571913629770279\n",
      "Loss on test= 0.014309670776128769\n",
      "acc for Lsat= 0.07014675521188313 \n",
      "acc for Psat= 0.10022498269875844 \n",
      "acc for optim= 0.1420015898015764\n",
      "Epoch:721/1000\n",
      "Loss on train= 0.009512359276413918\n",
      "Loss on test= 0.014866729266941547\n",
      "acc for Lsat= 0.06758767830000983 \n",
      "acc for Psat= 0.09578877836465838 \n",
      "acc for optim= 0.1392677668068144\n",
      "Epoch:722/1000\n",
      "Loss on train= 0.009623127989470959\n",
      "Loss on test= 0.015309744514524937\n",
      "acc for Lsat= 0.07302458882331847 \n",
      "acc for Psat= 0.10819134927458232 \n",
      "acc for optim= 0.13970699773894413\n",
      "Epoch:723/1000\n",
      "Loss on train= 0.009641042910516262\n",
      "Loss on test= 0.015860265120863914\n",
      "acc for Lsat= 0.07457888126373292 \n",
      "acc for Psat= 0.10540186762809753 \n",
      "acc for optim= 0.14114087522029878\n",
      "Epoch:724/1000\n",
      "Loss on train= 0.0095917247235775\n",
      "Loss on test= 0.014361007139086723\n",
      "acc for Lsat= 0.0782406595018175 \n",
      "acc for Psat= 0.10378450122144485 \n",
      "acc for optim= 0.14381355941295626\n",
      "Epoch:725/1000\n",
      "Loss on train= 0.009605837054550648\n",
      "Loss on test= 0.01472568977624178\n",
      "acc for Lsat= 0.08158935341570113 \n",
      "acc for Psat= 0.11161929302745394 \n",
      "acc for optim= 0.13899383337961302\n",
      "Epoch:726/1000\n",
      "Loss on train= 0.009286166168749332\n",
      "Loss on test= 0.01660642772912979\n",
      "acc for Lsat= 0.07250749882724548 \n",
      "acc for Psat= 0.09941287719541127 \n",
      "acc for optim= 0.14384632127152547\n",
      "Epoch:727/1000\n",
      "Loss on train= 0.009593590162694454\n",
      "Loss on test= 0.015726638957858086\n",
      "acc for Lsat= 0.07273767126931084 \n",
      "acc for Psat= 0.10235623386171128 \n",
      "acc for optim= 0.14148054520289105\n",
      "Epoch:728/1000\n",
      "Loss on train= 0.009700551629066467\n",
      "Loss on test= 0.014104164205491543\n",
      "acc for Lsat= 0.07492208364937039 \n",
      "acc for Psat= 0.08837220991651219 \n",
      "acc for optim= 0.14084893796179032\n",
      "Epoch:729/1000\n",
      "Loss on train= 0.009561329148709774\n",
      "Loss on test= 0.016671959310770035\n",
      "acc for Lsat= 0.07893567598528332 \n",
      "acc for Psat= 0.1023567611972491 \n",
      "acc for optim= 0.1404621200429069\n",
      "Epoch:730/1000\n",
      "Loss on train= 0.009559505619108677\n",
      "Loss on test= 0.016641082242131233\n",
      "acc for Lsat= 0.08062590923574235 \n",
      "acc for Psat= 0.1044625745879279 \n",
      "acc for optim= 0.1445881740914451\n",
      "Epoch:731/1000\n",
      "Loss on train= 0.009551114402711391\n",
      "Loss on test= 0.01551076304167509\n",
      "acc for Lsat= 0.07313643826378717 \n",
      "acc for Psat= 0.09499354445272022 \n",
      "acc for optim= 0.14106160667207504\n",
      "Epoch:732/1000\n",
      "Loss on train= 0.009464416652917862\n",
      "Loss on test= 0.014950303360819817\n",
      "acc for Lsat= 0.07362984584437476 \n",
      "acc for Psat= 0.09527296241786745 \n",
      "acc for optim= 0.1399366815884908\n",
      "Epoch:733/1000\n",
      "Loss on train= 0.009794874116778374\n",
      "Loss on test= 0.01477546151727438\n",
      "acc for Lsat= 0.07518925070762633 \n",
      "acc for Psat= 0.09649525549676681 \n",
      "acc for optim= 0.13977120965719225\n",
      "Epoch:734/1000\n",
      "Loss on train= 0.009343891404569149\n",
      "Loss on test= 0.015181594528257847\n",
      "acc for Lsat= 0.06947142514917586 \n",
      "acc for Psat= 0.09009230625298288 \n",
      "acc for optim= 0.14009308732218215\n",
      "Epoch:735/1000\n",
      "Loss on train= 0.00978632178157568\n",
      "Loss on test= 0.014541666023433208\n",
      "acc for Lsat= 0.07313240965207418 \n",
      "acc for Psat= 0.09096315006415051 \n",
      "acc for optim= 0.14038214468293717\n",
      "Epoch:736/1000\n",
      "Loss on train= 0.009592688642442226\n",
      "Loss on test= 0.013579968363046646\n",
      "acc for Lsat= 0.07367806914779876 \n",
      "acc for Psat= 0.09712249139944713 \n",
      "acc for optim= 0.13876908669869106\n",
      "Epoch:737/1000\n",
      "Loss on train= 0.009312174282968044\n",
      "Loss on test= 0.014907530508935452\n",
      "acc for Lsat= 0.07514145225286482 \n",
      "acc for Psat= 0.09117055378026431 \n",
      "acc for optim= 0.13900132262044484\n",
      "Epoch:738/1000\n",
      "Loss on train= 0.009409676305949688\n",
      "Loss on test= 0.014745030552148819\n",
      "acc for Lsat= 0.0828459304240015 \n",
      "acc for Psat= 0.10296203676197263 \n",
      "acc for optim= 0.13961721228228674\n",
      "Epoch:739/1000\n",
      "Loss on train= 0.009570475667715073\n",
      "Loss on test= 0.014614548534154892\n",
      "acc for Lsat= 0.08175974786281587 \n",
      "acc for Psat= 0.09728079934914907 \n",
      "acc for optim= 0.13908962971634334\n",
      "Epoch:740/1000\n",
      "Loss on train= 0.009718597866594791\n",
      "Loss on test= 0.015511270612478256\n",
      "acc for Lsat= 0.08188695030079945 \n",
      "acc for Psat= 0.10910199052757687 \n",
      "acc for optim= 0.13951321269075076\n",
      "Epoch:741/1000\n",
      "Loss on train= 0.009705551899969578\n",
      "Loss on test= 0.015532540157437325\n",
      "acc for Lsat= 0.07137219061454138 \n",
      "acc for Psat= 0.09530333429574965 \n",
      "acc for optim= 0.14376232143905426\n",
      "Epoch:742/1000\n",
      "Loss on train= 0.00949940737336874\n",
      "Loss on test= 0.01561957597732544\n",
      "acc for Lsat= 0.08834878438048892 \n",
      "acc for Psat= 0.12023347897662057 \n",
      "acc for optim= 0.13998830964167913\n",
      "Epoch:743/1000\n",
      "Loss on train= 0.009200815111398697\n",
      "Loss on test= 0.014735187403857708\n",
      "acc for Lsat= 0.07497840887970394 \n",
      "acc for Psat= 0.09916104094849693 \n",
      "acc for optim= 0.1407241976923413\n",
      "Epoch:744/1000\n",
      "Loss on train= 0.009333536960184574\n",
      "Loss on test= 0.015549391508102417\n",
      "acc for Lsat= 0.07584012630912992 \n",
      "acc for Psat= 0.1046858600444264 \n",
      "acc for optim= 0.13989361938503053\n",
      "Epoch:745/1000\n",
      "Loss on train= 0.009360028430819511\n",
      "Loss on test= 0.016216648742556572\n",
      "acc for Lsat= 0.0712806181775199 \n",
      "acc for Psat= 0.09685405608680514 \n",
      "acc for optim= 0.14206698023610642\n",
      "Epoch:746/1000\n",
      "Loss on train= 0.00946117378771305\n",
      "Loss on test= 0.01411740854382515\n",
      "acc for Lsat= 0.06952137168910769 \n",
      "acc for Psat= 0.09567666517363654 \n",
      "acc for optim= 0.14301185823149154\n",
      "Epoch:747/1000\n",
      "Loss on train= 0.00951776560395956\n",
      "Loss on test= 0.015916602686047554\n",
      "acc for Lsat= 0.06966640717453426 \n",
      "acc for Psat= 0.10027531368864906 \n",
      "acc for optim= 0.1432318079802725\n",
      "Epoch:748/1000\n",
      "Loss on train= 0.00922118965536356\n",
      "Loss on test= 0.01507505401968956\n",
      "acc for Lsat= 0.07634544571240744 \n",
      "acc for Psat= 0.09744760973585978 \n",
      "acc for optim= 0.14369853321048948\n",
      "Epoch:749/1000\n",
      "Loss on train= 0.009629782289266586\n",
      "Loss on test= 0.014597405679523945\n",
      "acc for Lsat= 0.06972703751590519 \n",
      "acc for Psat= 0.09682181610001457 \n",
      "acc for optim= 0.1413921457197931\n",
      "Epoch:750/1000\n",
      "Loss on train= 0.00913961511105299\n",
      "Loss on test= 0.015156920067965984\n",
      "acc for Lsat= 0.07416543546650145 \n",
      "acc for Psat= 0.10855104062292312 \n",
      "acc for optim= 0.14041069895029065\n",
      "Epoch:751/1000\n",
      "Loss on train= 0.00923642423003912\n",
      "Loss on test= 0.015341139398515224\n",
      "acc for Lsat= 0.07036754406160778 \n",
      "acc for Psat= 0.09983466052346758 \n",
      "acc for optim= 0.14014968607160777\n",
      "Epoch:752/1000\n",
      "Loss on train= 0.009285584092140198\n",
      "Loss on test= 0.014697275124490261\n",
      "acc for Lsat= 0.07330068432622487 \n",
      "acc for Psat= 0.09951913224326239 \n",
      "acc for optim= 0.1377234453956286\n",
      "Epoch:753/1000\n",
      "Loss on train= 0.009372555650770664\n",
      "Loss on test= 0.015367355197668076\n",
      "acc for Lsat= 0.07700177215867572 \n",
      "acc for Psat= 0.10306705584128698 \n",
      "acc for optim= 0.1394209729300605\n",
      "Epoch:754/1000\n",
      "Loss on train= 0.00902216974645853\n",
      "Loss on test= 0.014795144088566303\n",
      "acc for Lsat= 0.07051409913433923 \n",
      "acc for Psat= 0.0949423760175705 \n",
      "acc for optim= 0.14015973657369613\n",
      "Epoch:755/1000\n",
      "Loss on train= 0.00948655791580677\n",
      "Loss on test= 0.014345798641443253\n",
      "acc for Lsat= 0.07742057773802015 \n",
      "acc for Psat= 0.1017936971452501 \n",
      "acc for optim= 0.13907218393352297\n",
      "Epoch:756/1000\n",
      "Loss on train= 0.009131592698395252\n",
      "Loss on test= 0.01470409706234932\n",
      "acc for Lsat= 0.07416644063260819 \n",
      "acc for Psat= 0.11025461571084129 \n",
      "acc for optim= 0.13879711743858125\n",
      "Epoch:757/1000\n",
      "Loss on train= 0.009307854808866978\n",
      "Loss on test= 0.014623154886066914\n",
      "acc for Lsat= 0.08128623730606505 \n",
      "acc for Psat= 0.10441492348909379 \n",
      "acc for optim= 0.135667882197433\n",
      "Epoch:758/1000\n",
      "Loss on train= 0.009228256531059742\n",
      "Loss on test= 0.014349358156323433\n",
      "acc for Lsat= 0.07515735675891239 \n",
      "acc for Psat= 0.0945624041888449 \n",
      "acc for optim= 0.1388680249452591\n",
      "Epoch:759/1000\n",
      "Loss on train= 0.008826709352433681\n",
      "Loss on test= 0.014210626482963562\n",
      "acc for Lsat= 0.08127866503265169 \n",
      "acc for Psat= 0.0966407886809773 \n",
      "acc for optim= 0.13985716932349734\n",
      "Epoch:760/1000\n",
      "Loss on train= 0.009132946841418743\n",
      "Loss on test= 0.014685675501823425\n",
      "acc for Lsat= 0.08385043425692452 \n",
      "acc for Psat= 0.09630644470453263 \n",
      "acc for optim= 0.13887950132290522\n",
      "Epoch:761/1000\n",
      "Loss on train= 0.009466125629842281\n",
      "Loss on test= 0.014073982834815979\n",
      "acc for Lsat= 0.07711653543843161 \n",
      "acc for Psat= 0.09532094813055464 \n",
      "acc for optim= 0.13948675791422527\n",
      "Epoch:762/1000\n",
      "Loss on train= 0.009575755335390568\n",
      "Loss on test= 0.014396192505955696\n",
      "acc for Lsat= 0.06928868608342277 \n",
      "acc for Psat= 0.09652465648121306 \n",
      "acc for optim= 0.14110552188422948\n",
      "Epoch:763/1000\n",
      "Loss on train= 0.009291954338550568\n",
      "Loss on test= 0.014561217278242111\n",
      "acc for Lsat= 0.07806136922703849 \n",
      "acc for Psat= 0.09902522845400706 \n",
      "acc for optim= 0.13841114623679057\n",
      "Epoch:764/1000\n",
      "Loss on train= 0.009226610884070396\n",
      "Loss on test= 0.015855198726058006\n",
      "acc for Lsat= 0.07953297197818754 \n",
      "acc for Psat= 0.09517956541644201 \n",
      "acc for optim= 0.1418803667028745\n",
      "Epoch:765/1000\n",
      "Loss on train= 0.009086080826818943\n",
      "Loss on test= 0.01456653606146574\n",
      "acc for Lsat= 0.07589565697643491 \n",
      "acc for Psat= 0.10049787345859741 \n",
      "acc for optim= 0.14184551139672597\n",
      "Epoch:766/1000\n",
      "Loss on train= 0.009540767408907413\n",
      "Loss on test= 0.015215923078358173\n",
      "acc for Lsat= 0.06921436505185233 \n",
      "acc for Psat= 0.09019658640027048 \n",
      "acc for optim= 0.14001967608928678\n",
      "Epoch:767/1000\n",
      "Loss on train= 0.00938514992594719\n",
      "Loss on test= 0.016060078516602516\n",
      "acc for Lsat= 0.08070242073800829 \n",
      "acc for Psat= 0.09148789263433882 \n",
      "acc for optim= 0.13890128748284447\n",
      "Epoch:768/1000\n",
      "Loss on train= 0.009187642484903336\n",
      "Loss on test= 0.016221964731812477\n",
      "acc for Lsat= 0.07376109610001245 \n",
      "acc for Psat= 0.09336099475622177 \n",
      "acc for optim= 0.14070287711090512\n",
      "Epoch:769/1000\n",
      "Loss on train= 0.00938908476382494\n",
      "Loss on test= 0.015466094017028809\n",
      "acc for Lsat= 0.08199492328696781 \n",
      "acc for Psat= 0.09860527631309296 \n",
      "acc for optim= 0.1403862425022655\n",
      "Epoch:770/1000\n",
      "Loss on train= 0.009032870642840862\n",
      "Loss on test= 0.015326370485126972\n",
      "acc for Lsat= 0.0714426757560836 \n",
      "acc for Psat= 0.09944575660758548 \n",
      "acc for optim= 0.1392693551050292\n",
      "Epoch:771/1000\n",
      "Loss on train= 0.009342885576188564\n",
      "Loss on test= 0.015044893138110638\n",
      "acc for Lsat= 0.07547596395015717 \n",
      "acc for Psat= 0.09238959699869155 \n",
      "acc for optim= 0.13882418043083614\n",
      "Epoch:772/1000\n",
      "Loss on train= 0.009329267777502537\n",
      "Loss on test= 0.014483775943517685\n",
      "acc for Lsat= 0.0775306655300988 \n",
      "acc for Psat= 0.09804375403457218 \n",
      "acc for optim= 0.1394186935491032\n",
      "Epoch:773/1000\n",
      "Loss on train= 0.009554825723171234\n",
      "Loss on test= 0.014199246652424335\n",
      "acc for Lsat= 0.07484928717215857 \n",
      "acc for Psat= 0.09344942735301125 \n",
      "acc for optim= 0.1408397578530841\n",
      "Epoch:774/1000\n",
      "Loss on train= 0.009245139546692371\n",
      "Loss on test= 0.015410848893225193\n",
      "acc for Lsat= 0.08200140015946493 \n",
      "acc for Psat= 0.10353399068117142 \n",
      "acc for optim= 0.13938255889548193\n",
      "Epoch:775/1000\n",
      "Loss on train= 0.009423394687473774\n",
      "Loss on test= 0.014227094128727913\n",
      "acc for Lsat= 0.07216425720188352 \n",
      "acc for Psat= 0.09253834651576148 \n",
      "acc for optim= 0.14007739159795973\n",
      "Epoch:776/1000\n",
      "Loss on train= 0.00904081854969263\n",
      "Loss on test= 0.016975069418549538\n",
      "acc for Lsat= 0.07272799445523155 \n",
      "acc for Psat= 0.0902140874001715 \n",
      "acc for optim= 0.13886223435401918\n",
      "Epoch:777/1000\n",
      "Loss on train= 0.009633761830627918\n",
      "Loss on test= 0.015860220417380333\n",
      "acc for Lsat= 0.08187206586201985 \n",
      "acc for Psat= 0.10376104166110357 \n",
      "acc for optim= 0.1410518811808692\n",
      "Epoch:778/1000\n",
      "Loss on train= 0.009070249274373055\n",
      "Loss on test= 0.014787157066166401\n",
      "acc for Lsat= 0.07472950236664878 \n",
      "acc for Psat= 0.09422567205296623 \n",
      "acc for optim= 0.1397102745042907\n",
      "Epoch:779/1000\n",
      "Loss on train= 0.009399822913110256\n",
      "Loss on test= 0.015352766960859299\n",
      "acc for Lsat= 0.06775317854351466 \n",
      "acc for Psat= 0.08877355133493742 \n",
      "acc for optim= 0.14225518604119622\n",
      "Epoch:780/1000\n",
      "Loss on train= 0.009107770398259163\n",
      "Loss on test= 0.014578742906451225\n",
      "acc for Lsat= 0.07178489002916548 \n",
      "acc for Psat= 0.09150709104206826 \n",
      "acc for optim= 0.14018872711393568\n",
      "Epoch:781/1000\n",
      "Loss on train= 0.00930514745414257\n",
      "Loss on test= 0.015628747642040253\n",
      "acc for Lsat= 0.07347957707113689 \n",
      "acc for Psat= 0.09587695433033838 \n",
      "acc for optim= 0.1413571432232857\n",
      "Epoch:782/1000\n",
      "Loss on train= 0.00909525528550148\n",
      "Loss on test= 0.015495236031711102\n",
      "acc for Lsat= 0.07140063411659664 \n",
      "acc for Psat= 0.0925121714671453 \n",
      "acc for optim= 0.14373579041825402\n",
      "Epoch:783/1000\n",
      "Loss on train= 0.00915445201098919\n",
      "Loss on test= 0.015583189204335213\n",
      "acc for Lsat= 0.0769075709912512 \n",
      "acc for Psat= 0.09026500408848129 \n",
      "acc for optim= 0.14098558558358087\n",
      "Epoch:784/1000\n",
      "Loss on train= 0.009266383945941925\n",
      "Loss on test= 0.014632999897003174\n",
      "acc for Lsat= 0.07613548189401625 \n",
      "acc for Psat= 0.09533189783493677 \n",
      "acc for optim= 0.14051084385977852\n",
      "Epoch:785/1000\n",
      "Loss on train= 0.009379717521369457\n",
      "Loss on test= 0.014744404703378677\n",
      "acc for Lsat= 0.07574808398882549 \n",
      "acc for Psat= 0.09922510120603774 \n",
      "acc for optim= 0.1402022891574436\n",
      "Epoch:786/1000\n",
      "Loss on train= 0.008939947001636028\n",
      "Loss on test= 0.015763580799102783\n",
      "acc for Lsat= 0.07039487246010039 \n",
      "acc for Psat= 0.09202386240164438 \n",
      "acc for optim= 0.1397042241361406\n",
      "Epoch:787/1000\n",
      "Loss on train= 0.00920801516622305\n",
      "Loss on test= 0.016215544193983078\n",
      "acc for Lsat= 0.07655591153436236 \n",
      "acc for Psat= 0.09421242020196385 \n",
      "acc for optim= 0.14012714740302828\n",
      "Epoch:788/1000\n",
      "Loss on train= 0.009081883355975151\n",
      "Loss on test= 0.014641101472079754\n",
      "acc for Lsat= 0.07432576616605123 \n",
      "acc for Psat= 0.09331479089127644 \n",
      "acc for optim= 0.1401592317554686\n",
      "Epoch:789/1000\n",
      "Loss on train= 0.009295365773141384\n",
      "Loss on test= 0.014855961315333843\n",
      "acc for Lsat= 0.0747086101108127 \n",
      "acc for Psat= 0.1016169803010093 \n",
      "acc for optim= 0.14058671477768156\n",
      "Epoch:790/1000\n",
      "Loss on train= 0.009394205175340176\n",
      "Loss on test= 0.014706600457429886\n",
      "acc for Lsat= 0.07360407743189071 \n",
      "acc for Psat= 0.11303215821584064 \n",
      "acc for optim= 0.1390103366639879\n",
      "Epoch:791/1000\n",
      "Loss on train= 0.00906377099454403\n",
      "Loss on test= 0.01477015484124422\n",
      "acc for Lsat= 0.07131235814756817 \n",
      "acc for Psat= 0.09725586010350121 \n",
      "acc for optim= 0.13877821829583908\n",
      "Epoch:792/1000\n",
      "Loss on train= 0.009349863976240158\n",
      "Loss on test= 0.01597035676240921\n",
      "acc for Lsat= 0.07120639863941405 \n",
      "acc for Psat= 0.09241360227266948 \n",
      "acc for optim= 0.13786826001273264\n",
      "Epoch:793/1000\n",
      "Loss on train= 0.00917263887822628\n",
      "Loss on test= 0.014990124851465225\n",
      "acc for Lsat= 0.06916509022315342 \n",
      "acc for Psat= 0.09628430356582006 \n",
      "acc for optim= 0.1399687609738774\n",
      "Epoch:794/1000\n",
      "Loss on train= 0.009250003844499588\n",
      "Loss on test= 0.015802694484591484\n",
      "acc for Lsat= 0.07336107045412062 \n",
      "acc for Psat= 0.10016400681601631 \n",
      "acc for optim= 0.13897125787205164\n",
      "Epoch:795/1000\n",
      "Loss on train= 0.008844361640512943\n",
      "Loss on test= 0.01519872434437275\n",
      "acc for Lsat= 0.07443093773391513 \n",
      "acc for Psat= 0.10017148206631343 \n",
      "acc for optim= 0.14293235159582562\n",
      "Epoch:796/1000\n",
      "Loss on train= 0.009305262006819248\n",
      "Loss on test= 0.014526371844112873\n",
      "acc for Lsat= 0.07090288698673247 \n",
      "acc for Psat= 0.08913624650902219 \n",
      "acc for optim= 0.1387513778275914\n",
      "Epoch:797/1000\n",
      "Loss on train= 0.009483696892857552\n",
      "Loss on test= 0.014494936913251877\n",
      "acc for Lsat= 0.07204077707396613 \n",
      "acc for Psat= 0.10441302955150605 \n",
      "acc for optim= 0.141600092417664\n",
      "Epoch:798/1000\n",
      "Loss on train= 0.00883288774639368\n",
      "Loss on test= 0.014977650716900826\n",
      "acc for Lsat= 0.07933675729566152 \n",
      "acc for Psat= 0.10857201119263966 \n",
      "acc for optim= 0.13976741664939457\n",
      "Epoch:799/1000\n",
      "Loss on train= 0.008963707834482193\n",
      "Loss on test= 0.015436003915965557\n",
      "acc for Lsat= 0.07027277201414109 \n",
      "acc for Psat= 0.09851084748903909 \n",
      "acc for optim= 0.14186312672164705\n",
      "Epoch:800/1000\n",
      "Loss on train= 0.008983088657259941\n",
      "Loss on test= 0.014929373748600483\n",
      "acc for Lsat= 0.0694077119231224 \n",
      "acc for Psat= 0.09090469578901925 \n",
      "acc for optim= 0.14074666847785317\n",
      "Epoch:801/1000\n",
      "Loss on train= 0.009282860904932022\n",
      "Loss on test= 0.01561069954186678\n",
      "acc for Lsat= 0.07588623861471812 \n",
      "acc for Psat= 0.09870311733749176 \n",
      "acc for optim= 0.14170686850945155\n",
      "Epoch:802/1000\n",
      "Loss on train= 0.008941060863435268\n",
      "Loss on test= 0.014376630075275898\n",
      "acc for Lsat= 0.07742262896564271 \n",
      "acc for Psat= 0.09870617240667341 \n",
      "acc for optim= 0.1385904542273945\n",
      "Epoch:803/1000\n",
      "Loss on train= 0.009458716958761215\n",
      "Loss on test= 0.014253046363592148\n",
      "acc for Lsat= 0.07231122271882164 \n",
      "acc for Psat= 0.09694172673755222 \n",
      "acc for optim= 0.14002303497658836\n",
      "Epoch:804/1000\n",
      "Loss on train= 0.009364687837660313\n",
      "Loss on test= 0.01401841826736927\n",
      "acc for Lsat= 0.07516479723983341 \n",
      "acc for Psat= 0.09465632074409061 \n",
      "acc for optim= 0.141211375925276\n",
      "Epoch:805/1000\n",
      "Loss on train= 0.009040587581694126\n",
      "Loss on test= 0.015315613709390163\n",
      "acc for Lsat= 0.07611991200182172 \n",
      "acc for Psat= 0.09309696704149245 \n",
      "acc for optim= 0.13969396485222713\n",
      "Epoch:806/1000\n",
      "Loss on train= 0.009071050211787224\n",
      "Loss on test= 0.013991687446832657\n",
      "acc for Lsat= 0.07331825577550465 \n",
      "acc for Psat= 0.09816436270872751 \n",
      "acc for optim= 0.1408883174260457\n",
      "Epoch:807/1000\n",
      "Loss on train= 0.008927695453166962\n",
      "Loss on test= 0.015605833381414413\n",
      "acc for Lsat= 0.07445913785033756 \n",
      "acc for Psat= 0.09244185189406078 \n",
      "acc for optim= 0.14314521385563742\n",
      "Epoch:808/1000\n",
      "Loss on train= 0.009113907814025879\n",
      "Loss on test= 0.015708843246102333\n",
      "acc for Lsat= 0.07235033743911319 \n",
      "acc for Psat= 0.09247107687923643 \n",
      "acc for optim= 0.1421863247950872\n",
      "Epoch:809/1000\n",
      "Loss on train= 0.009216301143169403\n",
      "Loss on test= 0.014983545057475567\n",
      "acc for Lsat= 0.07327036029762692 \n",
      "acc for Psat= 0.09799018767144946 \n",
      "acc for optim= 0.1448216297560268\n",
      "Epoch:810/1000\n",
      "Loss on train= 0.009029798209667206\n",
      "Loss on test= 0.01446583028882742\n",
      "acc for Lsat= 0.06811566236946318 \n",
      "acc for Psat= 0.09049580378664862 \n",
      "acc for optim= 0.14146811366081238\n",
      "Epoch:811/1000\n",
      "Loss on train= 0.009074569679796696\n",
      "Loss on test= 0.015126249752938747\n",
      "acc for Lsat= 0.07006267640325758 \n",
      "acc for Psat= 0.09079491049051285 \n",
      "acc for optim= 0.14247109707858827\n",
      "Epoch:812/1000\n",
      "Loss on train= 0.009112671948969364\n",
      "Loss on test= 0.01578161120414734\n",
      "acc for Lsat= 0.07295663605133691 \n",
      "acc for Psat= 0.09183642235067156 \n",
      "acc for optim= 0.1462538141343329\n",
      "Epoch:813/1000\n",
      "Loss on train= 0.009222058579325676\n",
      "Loss on test= 0.014150364324450493\n",
      "acc for Lsat= 0.07591917994949553 \n",
      "acc for Psat= 0.09114416903919646 \n",
      "acc for optim= 0.13799166248904335\n",
      "Epoch:814/1000\n",
      "Loss on train= 0.009409653954207897\n",
      "Loss on test= 0.015449040569365025\n",
      "acc for Lsat= 0.07128441350327598 \n",
      "acc for Psat= 0.0904991784857379 \n",
      "acc for optim= 0.14105931570132574\n",
      "Epoch:815/1000\n",
      "Loss on train= 0.00888165645301342\n",
      "Loss on test= 0.01589641533792019\n",
      "acc for Lsat= 0.07295034395323859 \n",
      "acc for Psat= 0.09112366512417795 \n",
      "acc for optim= 0.14315658426947067\n",
      "Epoch:816/1000\n",
      "Loss on train= 0.008972927927970886\n",
      "Loss on test= 0.015096996910870075\n",
      "acc for Lsat= 0.07192583829164505 \n",
      "acc for Psat= 0.09672672036621305 \n",
      "acc for optim= 0.13993477357758413\n",
      "Epoch:817/1000\n",
      "Loss on train= 0.009179294109344482\n",
      "Loss on test= 0.015254514291882515\n",
      "acc for Lsat= 0.06895458400249482 \n",
      "acc for Psat= 0.09128144077128834 \n",
      "acc for optim= 0.1419390842318535\n",
      "Epoch:818/1000\n",
      "Loss on train= 0.00922118779271841\n",
      "Loss on test= 0.014454192481935024\n",
      "acc for Lsat= 0.07541240006685256 \n",
      "acc for Psat= 0.09802617645925946 \n",
      "acc for optim= 0.13904005587100984\n",
      "Epoch:819/1000\n",
      "Loss on train= 0.00907929614186287\n",
      "Loss on test= 0.015025083906948566\n",
      "acc for Lsat= 0.07643795940611096 \n",
      "acc for Psat= 0.0978994228773647 \n",
      "acc for optim= 0.14076457106404835\n",
      "Epoch:820/1000\n",
      "Loss on train= 0.009219370782375336\n",
      "Loss on test= 0.015662213787436485\n",
      "acc for Lsat= 0.07573571420378156 \n",
      "acc for Psat= 0.107098551094532 \n",
      "acc for optim= 0.1393561566869418\n",
      "Epoch:821/1000\n",
      "Loss on train= 0.00919705256819725\n",
      "Loss on test= 0.014139380306005478\n",
      "acc for Lsat= 0.07540409002039167 \n",
      "acc for Psat= 0.1020947410000695 \n",
      "acc for optim= 0.139609232544899\n",
      "Epoch:822/1000\n",
      "Loss on train= 0.009086744859814644\n",
      "Loss on test= 0.014921706169843674\n",
      "acc for Lsat= 0.07111627327071296 \n",
      "acc for Psat= 0.09513437234693102 \n",
      "acc for optim= 0.1381463380323516\n",
      "Epoch:823/1000\n",
      "Loss on train= 0.009160605259239674\n",
      "Loss on test= 0.0144241563975811\n",
      "acc for Lsat= 0.07232395029730267 \n",
      "acc for Psat= 0.09808277471197974 \n",
      "acc for optim= 0.14147948970397317\n",
      "Epoch:824/1000\n",
      "Loss on train= 0.009129243902862072\n",
      "Loss on test= 0.01615685597062111\n",
      "acc for Lsat= 0.0712357931666904 \n",
      "acc for Psat= 0.09139166441228655 \n",
      "acc for optim= 0.14043876065148247\n",
      "Epoch:825/1000\n",
      "Loss on train= 0.008942680433392525\n",
      "Loss on test= 0.014399183914065361\n",
      "acc for Lsat= 0.07770139575004577 \n",
      "acc for Psat= 0.1001114050547282 \n",
      "acc for optim= 0.1398854002356529\n",
      "Epoch:826/1000\n",
      "Loss on train= 0.009088688530027866\n",
      "Loss on test= 0.014780700206756592\n",
      "acc for Lsat= 0.07344589001602596 \n",
      "acc for Psat= 0.09985912458764185 \n",
      "acc for optim= 0.1400101280874676\n",
      "Epoch:827/1000\n",
      "Loss on train= 0.009121183305978775\n",
      "Loss on test= 0.014810732565820217\n",
      "acc for Lsat= 0.0731219889389144 \n",
      "acc for Psat= 0.09847677250703175 \n",
      "acc for optim= 0.1386302603615655\n",
      "Epoch:828/1000\n",
      "Loss on train= 0.009389199316501617\n",
      "Loss on test= 0.015169133432209492\n",
      "acc for Lsat= 0.06995702170663409 \n",
      "acc for Psat= 0.09518817481067447 \n",
      "acc for optim= 0.1400860306289461\n",
      "Epoch:829/1000\n",
      "Loss on train= 0.009633442386984825\n",
      "Loss on test= 0.014955462887883186\n",
      "acc for Lsat= 0.07047756562630335 \n",
      "acc for Psat= 0.09361712949143514 \n",
      "acc for optim= 0.1411960471007559\n",
      "Epoch:830/1000\n",
      "Loss on train= 0.009122206829488277\n",
      "Loss on test= 0.014827854000031948\n",
      "acc for Lsat= 0.07404582235548232 \n",
      "acc for Psat= 0.10152905997302798 \n",
      "acc for optim= 0.14214894738462236\n",
      "Epoch:831/1000\n",
      "Loss on train= 0.008440470322966576\n",
      "Loss on test= 0.014374562539160252\n",
      "acc for Lsat= 0.07251567641894023 \n",
      "acc for Psat= 0.1049404693974389 \n",
      "acc for optim= 0.14045387274689147\n",
      "Epoch:832/1000\n",
      "Loss on train= 0.008737836964428425\n",
      "Loss on test= 0.01546955481171608\n",
      "acc for Lsat= 0.07111336770984861 \n",
      "acc for Psat= 0.09740004307693904 \n",
      "acc for optim= 0.14113129062785043\n",
      "Epoch:833/1000\n",
      "Loss on train= 0.008965597487986088\n",
      "Loss on test= 0.015046115964651108\n",
      "acc for Lsat= 0.07281199213531282 \n",
      "acc for Psat= 0.09354337900877001 \n",
      "acc for optim= 0.14219912853505878\n",
      "Epoch:834/1000\n",
      "Loss on train= 0.009428654797375202\n",
      "Loss on test= 0.016801875084638596\n",
      "acc for Lsat= 0.07230727987156974 \n",
      "acc for Psat= 0.09731032417880164 \n",
      "acc for optim= 0.14110302560859259\n",
      "Epoch:835/1000\n",
      "Loss on train= 0.009096993133425713\n",
      "Loss on test= 0.013823662884533405\n",
      "acc for Lsat= 0.07161964376767477 \n",
      "acc for Psat= 0.10575477066967225 \n",
      "acc for optim= 0.14044706374406812\n",
      "Epoch:836/1000\n",
      "Loss on train= 0.008858203887939453\n",
      "Loss on test= 0.01701934263110161\n",
      "acc for Lsat= 0.06927041792207295 \n",
      "acc for Psat= 0.09233117219474583 \n",
      "acc for optim= 0.14397088951534698\n",
      "Epoch:837/1000\n",
      "Loss on train= 0.008823738433420658\n",
      "Loss on test= 0.01385551318526268\n",
      "acc for Lsat= 0.07367358273930019 \n",
      "acc for Psat= 0.10324788457817502 \n",
      "acc for optim= 0.13836532615953023\n",
      "Epoch:838/1000\n",
      "Loss on train= 0.008900939486920834\n",
      "Loss on test= 0.014631337486207485\n",
      "acc for Lsat= 0.07140010297298431 \n",
      "acc for Psat= 0.09367358883221945 \n",
      "acc for optim= 0.1414965248770184\n",
      "Epoch:839/1000\n",
      "Loss on train= 0.008813870139420033\n",
      "Loss on test= 0.015787355601787567\n",
      "acc for Lsat= 0.07172338051928413 \n",
      "acc for Psat= 0.09582259174850254 \n",
      "acc for optim= 0.1424256372782919\n",
      "Epoch:840/1000\n",
      "Loss on train= 0.008768490515649319\n",
      "Loss on test= 0.016362255439162254\n",
      "acc for Lsat= 0.07162139150831434 \n",
      "acc for Psat= 0.10270131362809075 \n",
      "acc for optim= 0.13860393083757824\n",
      "Epoch:841/1000\n",
      "Loss on train= 0.009194789454340935\n",
      "Loss on test= 0.014567013829946518\n",
      "acc for Lsat= 0.07243809915251202 \n",
      "acc for Psat= 0.10022242499722374 \n",
      "acc for optim= 0.14139535725116728\n",
      "Epoch:842/1000\n",
      "Loss on train= 0.00911480188369751\n",
      "Loss on test= 0.016815228387713432\n",
      "acc for Lsat= 0.0706021433075269 \n",
      "acc for Psat= 0.10252300517426596 \n",
      "acc for optim= 0.14198297841681376\n",
      "Epoch:843/1000\n",
      "Loss on train= 0.00893399491906166\n",
      "Loss on test= 0.01530971098691225\n",
      "acc for Lsat= 0.07239269498321746 \n",
      "acc for Psat= 0.10189273787869349 \n",
      "acc for optim= 0.13912995275523926\n",
      "Epoch:844/1000\n",
      "Loss on train= 0.00894892867654562\n",
      "Loss on test= 0.01442795991897583\n",
      "acc for Lsat= 0.07067403097947439 \n",
      "acc for Psat= 0.09531902836428748 \n",
      "acc for optim= 0.14157471507787706\n",
      "Epoch:845/1000\n",
      "Loss on train= 0.008675266057252884\n",
      "Loss on test= 0.015762915834784508\n",
      "acc for Lsat= 0.07533074153794182 \n",
      "acc for Psat= 0.09928038832214144 \n",
      "acc for optim= 0.13953665031327145\n",
      "Epoch:846/1000\n",
      "Loss on train= 0.009055128321051598\n",
      "Loss on test= 0.015450524166226387\n",
      "acc for Lsat= 0.0723982282810741 \n",
      "acc for Psat= 0.09506144705745911 \n",
      "acc for optim= 0.140869592792458\n",
      "Epoch:847/1000\n",
      "Loss on train= 0.00914730317890644\n",
      "Loss on test= 0.015080736950039864\n",
      "acc for Lsat= 0.07116492489973705 \n",
      "acc for Psat= 0.09832291553417843 \n",
      "acc for optim= 0.14102506322993172\n",
      "Epoch:848/1000\n",
      "Loss on train= 0.009310207329690456\n",
      "Loss on test= 0.013761318288743496\n",
      "acc for Lsat= 0.0711982207165824 \n",
      "acc for Psat= 0.10080327557192909 \n",
      "acc for optim= 0.13844102256827887\n",
      "Epoch:849/1000\n",
      "Loss on train= 0.008918100968003273\n",
      "Loss on test= 0.014744619838893414\n",
      "acc for Lsat= 0.07588519818252987 \n",
      "acc for Psat= 0.09975113156769011 \n",
      "acc for optim= 0.13950193093882668\n",
      "Epoch:850/1000\n",
      "Loss on train= 0.009225383400917053\n",
      "Loss on test= 0.01617913320660591\n",
      "acc for Lsat= 0.07359992199473912 \n",
      "acc for Psat= 0.09441731373469033 \n",
      "acc for optim= 0.14016940891742707\n",
      "Epoch:851/1000\n",
      "Loss on train= 0.008940748870372772\n",
      "Loss on test= 0.01530196238309145\n",
      "acc for Lsat= 0.07742405914598043 \n",
      "acc for Psat= 0.09580908202462725 \n",
      "acc for optim= 0.14241676015986335\n",
      "Epoch:852/1000\n",
      "Loss on train= 0.008969560265541077\n",
      "Loss on test= 0.015112970024347305\n",
      "acc for Lsat= 0.07189268784390555 \n",
      "acc for Psat= 0.1019417174988323 \n",
      "acc for optim= 0.1413892462849617\n",
      "Epoch:853/1000\n",
      "Loss on train= 0.008981728926301003\n",
      "Loss on test= 0.014599774032831192\n",
      "acc for Lsat= 0.0715236763159434 \n",
      "acc for Psat= 0.09743511660231484 \n",
      "acc for optim= 0.14198929551574918\n",
      "Epoch:854/1000\n",
      "Loss on train= 0.008895895443856716\n",
      "Loss on test= 0.015030357986688614\n",
      "acc for Lsat= 0.07580858469009401 \n",
      "acc for Psat= 0.09303896443711389 \n",
      "acc for optim= 0.14099843651056287\n",
      "Epoch:855/1000\n",
      "Loss on train= 0.008816077373921871\n",
      "Loss on test= 0.014048056676983833\n",
      "acc for Lsat= 0.06971007717980279 \n",
      "acc for Psat= 0.09431847747829225 \n",
      "acc for optim= 0.1399958155221409\n",
      "Epoch:856/1000\n",
      "Loss on train= 0.009071115404367447\n",
      "Loss on test= 0.016093524172902107\n",
      "acc for Lsat= 0.06977520767185424 \n",
      "acc for Psat= 0.09806079069773357 \n",
      "acc for optim= 0.14072840495242012\n",
      "Epoch:857/1000\n",
      "Loss on train= 0.008996617048978806\n",
      "Loss on test= 0.015529680997133255\n",
      "acc for Lsat= 0.0756931773490376 \n",
      "acc for Psat= 0.1099778786301613 \n",
      "acc for optim= 0.14103320240974426\n",
      "Epoch:858/1000\n",
      "Loss on train= 0.009613825008273125\n",
      "Loss on test= 0.01508315373212099\n",
      "acc for Lsat= 0.07429933084381951 \n",
      "acc for Psat= 0.09405292471249899 \n",
      "acc for optim= 0.14314507924848133\n",
      "Epoch:859/1000\n",
      "Loss on train= 0.008938941173255444\n",
      "Loss on test= 0.015521042980253696\n",
      "acc for Lsat= 0.07404353701406056 \n",
      "acc for Psat= 0.10230122407277426 \n",
      "acc for optim= 0.13943552010589177\n",
      "Epoch:860/1000\n",
      "Loss on train= 0.008484993129968643\n",
      "Loss on test= 0.015302170999348164\n",
      "acc for Lsat= 0.07046965426868862 \n",
      "acc for Psat= 0.09312583191527263 \n",
      "acc for optim= 0.14023796154393092\n",
      "Epoch:861/1000\n",
      "Loss on train= 0.008717537857592106\n",
      "Loss on test= 0.014092897064983845\n",
      "acc for Lsat= 0.07048413571384218 \n",
      "acc for Psat= 0.09476017604271572 \n",
      "acc for optim= 0.14175057841671834\n",
      "Epoch:862/1000\n",
      "Loss on train= 0.008744988590478897\n",
      "Loss on test= 0.014978017657995224\n",
      "acc for Lsat= 0.07718696047862368 \n",
      "acc for Psat= 0.09864635550313525 \n",
      "acc for optim= 0.14331513212786784\n",
      "Epoch:863/1000\n",
      "Loss on train= 0.008660414256155491\n",
      "Loss on test= 0.01571931503713131\n",
      "acc for Lsat= 0.074148561557134 \n",
      "acc for Psat= 0.10777272664838367 \n",
      "acc for optim= 0.1396852617462476\n",
      "Epoch:864/1000\n",
      "Loss on train= 0.009133618324995041\n",
      "Loss on test= 0.015338054858148098\n",
      "acc for Lsat= 0.07109073003133137 \n",
      "acc for Psat= 0.09308321062061521 \n",
      "acc for optim= 0.1418163572748502\n",
      "Epoch:865/1000\n",
      "Loss on train= 0.008953110314905643\n",
      "Loss on test= 0.015600119717419147\n",
      "acc for Lsat= 0.07643259581592349 \n",
      "acc for Psat= 0.09045810798803965 \n",
      "acc for optim= 0.1425036190284623\n",
      "Epoch:866/1000\n",
      "Loss on train= 0.008812868036329746\n",
      "Loss on test= 0.0160810649394989\n",
      "acc for Lsat= 0.07685623599423302 \n",
      "acc for Psat= 0.10341398053699069 \n",
      "acc for optim= 0.1412341578139199\n",
      "Epoch:867/1000\n",
      "Loss on train= 0.008917965926229954\n",
      "Loss on test= 0.014132734388113022\n",
      "acc for Lsat= 0.07176347706052993 \n",
      "acc for Psat= 0.10700916714138456 \n",
      "acc for optim= 0.1400410908791754\n",
      "Epoch:868/1000\n",
      "Loss on train= 0.008331596851348877\n",
      "Loss on test= 0.015604336746037006\n",
      "acc for Lsat= 0.07267393585708407 \n",
      "acc for Psat= 0.09493987659613291 \n",
      "acc for optim= 0.13973247508207956\n",
      "Epoch:869/1000\n",
      "Loss on train= 0.008677220903337002\n",
      "Loss on test= 0.015133397653698921\n",
      "acc for Lsat= 0.08409113238255184 \n",
      "acc for Psat= 0.10177662902408174 \n",
      "acc for optim= 0.13831632054514353\n",
      "Epoch:870/1000\n",
      "Loss on train= 0.008607104420661926\n",
      "Loss on test= 0.014910710975527763\n",
      "acc for Lsat= 0.0758653574519687 \n",
      "acc for Psat= 0.09527595175637141 \n",
      "acc for optim= 0.14089673641655176\n",
      "Epoch:871/1000\n",
      "Loss on train= 0.00868652667850256\n",
      "Loss on test= 0.014185971580445766\n",
      "acc for Lsat= 0.07128858698738945 \n",
      "acc for Psat= 0.0912377647227711 \n",
      "acc for optim= 0.14012583709425397\n",
      "Epoch:872/1000\n",
      "Loss on train= 0.008835269138216972\n",
      "Loss on test= 0.015471205115318298\n",
      "acc for Lsat= 0.07258463766839769 \n",
      "acc for Psat= 0.09765353997548422 \n",
      "acc for optim= 0.13928719427850514\n",
      "Epoch:873/1000\n",
      "Loss on train= 0.008799425326287746\n",
      "Loss on test= 0.016277655959129333\n",
      "acc for Lsat= 0.06983223673370148 \n",
      "acc for Psat= 0.09697310245699355 \n",
      "acc for optim= 0.14266680247253846\n",
      "Epoch:874/1000\n",
      "Loss on train= 0.008715126663446426\n",
      "Loss on test= 0.014884104020893574\n",
      "acc for Lsat= 0.0761282538374265 \n",
      "acc for Psat= 0.09775727672709358 \n",
      "acc for optim= 0.14115169793367388\n",
      "Epoch:875/1000\n",
      "Loss on train= 0.008674796670675278\n",
      "Loss on test= 0.015137644484639168\n",
      "acc for Lsat= 0.07616648674011231 \n",
      "acc for Psat= 0.10221151543988122 \n",
      "acc for optim= 0.1418303734726376\n",
      "Epoch:876/1000\n",
      "Loss on train= 0.00877732876688242\n",
      "Loss on test= 0.015151225961744785\n",
      "acc for Lsat= 0.07323066426648034 \n",
      "acc for Psat= 0.10085831218295625 \n",
      "acc for optim= 0.14539181374841262\n",
      "Epoch:877/1000\n",
      "Loss on train= 0.009069274179637432\n",
      "Loss on test= 0.01573137193918228\n",
      "acc for Lsat= 0.07349292553133435 \n",
      "acc for Psat= 0.0952965526117219 \n",
      "acc for optim= 0.13762166500091555\n",
      "Epoch:878/1000\n",
      "Loss on train= 0.008737574331462383\n",
      "Loss on test= 0.015681911259889603\n",
      "acc for Lsat= 0.07162494775321747 \n",
      "acc for Psat= 0.09453949853777885 \n",
      "acc for optim= 0.14113125834200116\n",
      "Epoch:879/1000\n",
      "Loss on train= 0.008749608881771564\n",
      "Loss on test= 0.014485365711152554\n",
      "acc for Lsat= 0.07622491584883795 \n",
      "acc for Psat= 0.10120942658848234 \n",
      "acc for optim= 0.13928756647639803\n",
      "Epoch:880/1000\n",
      "Loss on train= 0.008773082867264748\n",
      "Loss on test= 0.014832843095064163\n",
      "acc for Lsat= 0.07241847548219893 \n",
      "acc for Psat= 0.09849674271212684 \n",
      "acc for optim= 0.13788058492872451\n",
      "Epoch:881/1000\n",
      "Loss on train= 0.009274611249566078\n",
      "Loss on test= 0.014940770342946053\n",
      "acc for Lsat= 0.08043361422088412 \n",
      "acc for Psat= 0.10892695966694091 \n",
      "acc for optim= 0.13909929196039836\n",
      "Epoch:882/1000\n",
      "Loss on train= 0.008479410782456398\n",
      "Loss on test= 0.014334004372358322\n",
      "acc for Lsat= 0.07006812857256994 \n",
      "acc for Psat= 0.09106734519203503 \n",
      "acc for optim= 0.13821025043725965\n",
      "Epoch:883/1000\n",
      "Loss on train= 0.008775681257247925\n",
      "Loss on test= 0.01697331666946411\n",
      "acc for Lsat= 0.07052127354674868 \n",
      "acc for Psat= 0.09202206681172054 \n",
      "acc for optim= 0.14187211212184694\n",
      "Epoch:884/1000\n",
      "Loss on train= 0.008583981543779373\n",
      "Loss on test= 0.01603041961789131\n",
      "acc for Lsat= 0.07100956622097226 \n",
      "acc for Psat= 0.10508428530560598 \n",
      "acc for optim= 0.13858810994360182\n",
      "Epoch:885/1000\n",
      "Loss on train= 0.008834650740027428\n",
      "Loss on test= 0.014536265283823013\n",
      "acc for Lsat= 0.07257077313131757 \n",
      "acc for Psat= 0.09920149015055763 \n",
      "acc for optim= 0.14115048166778354\n",
      "Epoch:886/1000\n",
      "Loss on train= 0.008816869929432869\n",
      "Loss on test= 0.015755990520119667\n",
      "acc for Lsat= 0.0706777211692598 \n",
      "acc for Psat= 0.09695583432912826 \n",
      "acc for optim= 0.14217935800552367\n",
      "Epoch:887/1000\n",
      "Loss on train= 0.008719055913388729\n",
      "Loss on test= 0.014331493526697159\n",
      "acc for Lsat= 0.07372093730502657 \n",
      "acc for Psat= 0.09444848563936022 \n",
      "acc for optim= 0.14070176763667\n",
      "Epoch:888/1000\n",
      "Loss on train= 0.00881473533809185\n",
      "Loss on test= 0.016309799626469612\n",
      "acc for Lsat= 0.0798078942630026 \n",
      "acc for Psat= 0.10603374838829041 \n",
      "acc for optim= 0.13917414281103346\n",
      "Epoch:889/1000\n",
      "Loss on train= 0.008570627309381962\n",
      "Loss on test= 0.015702566131949425\n",
      "acc for Lsat= 0.07037399212519328 \n",
      "acc for Psat= 0.09588843782742817 \n",
      "acc for optim= 0.1425283400548829\n",
      "Epoch:890/1000\n",
      "Loss on train= 0.008905078284442425\n",
      "Loss on test= 0.014761704951524734\n",
      "acc for Lsat= 0.07165859291950863 \n",
      "acc for Psat= 0.09223623590336905 \n",
      "acc for optim= 0.13965359528859458\n",
      "Epoch:891/1000\n",
      "Loss on train= 0.009387729689478874\n",
      "Loss on test= 0.015754489228129387\n",
      "acc for Lsat= 0.07363981124427584 \n",
      "acc for Psat= 0.09139037463400099 \n",
      "acc for optim= 0.14043603042761485\n",
      "Epoch:892/1000\n",
      "Loss on train= 0.0086764395236969\n",
      "Loss on test= 0.014603685587644577\n",
      "acc for Lsat= 0.07645703223016528 \n",
      "acc for Psat= 0.0979125064280298 \n",
      "acc for optim= 0.14232713083426157\n",
      "Epoch:893/1000\n",
      "Loss on train= 0.00876554660499096\n",
      "Loss on test= 0.015786442905664444\n",
      "acc for Lsat= 0.07150111065970527 \n",
      "acc for Psat= 0.09578726010190115 \n",
      "acc for optim= 0.14101889663272432\n",
      "Epoch:894/1000\n",
      "Loss on train= 0.008517383597791195\n",
      "Loss on test= 0.014896457083523273\n",
      "acc for Lsat= 0.07497114025884205 \n",
      "acc for Psat= 0.09604392382833692 \n",
      "acc for optim= 0.14057744907008274\n",
      "Epoch:895/1000\n",
      "Loss on train= 0.008608219213783741\n",
      "Loss on test= 0.015810741111636162\n",
      "acc for Lsat= 0.07176996204588149 \n",
      "acc for Psat= 0.09341045866409937 \n",
      "acc for optim= 0.1394837647676468\n",
      "Epoch:896/1000\n",
      "Loss on train= 0.009445552714169025\n",
      "Loss on test= 0.014950932934880257\n",
      "acc for Lsat= 0.07697521713044908 \n",
      "acc for Psat= 0.10135670453310014 \n",
      "acc for optim= 0.13987978845834728\n",
      "Epoch:897/1000\n",
      "Loss on train= 0.008935265243053436\n",
      "Loss on test= 0.0161607563495636\n",
      "acc for Lsat= 0.07473317186037698 \n",
      "acc for Psat= 0.09419479966163634 \n",
      "acc for optim= 0.13949592659870783\n",
      "Epoch:898/1000\n",
      "Loss on train= 0.008665814995765686\n",
      "Loss on test= 0.016478074714541435\n",
      "acc for Lsat= 0.07317675683233474 \n",
      "acc for Psat= 0.09547149688005448 \n",
      "acc for optim= 0.13968855904208288\n",
      "Epoch:899/1000\n",
      "Loss on train= 0.00845774170011282\n",
      "Loss on test= 0.016067465767264366\n",
      "acc for Lsat= 0.06988543917735418 \n",
      "acc for Psat= 0.09200399153762392 \n",
      "acc for optim= 0.14215343147516252\n",
      "Epoch:900/1000\n",
      "Loss on train= 0.008410726673901081\n",
      "Loss on test= 0.014407957904040813\n",
      "acc for Lsat= 0.07286318606800504 \n",
      "acc for Psat= 0.10069788528813255 \n",
      "acc for optim= 0.1423856261703703\n",
      "Epoch:901/1000\n",
      "Loss on train= 0.008676636964082718\n",
      "Loss on test= 0.013816971331834793\n",
      "acc for Lsat= 0.07075658457146751 \n",
      "acc for Psat= 0.09076144728395676 \n",
      "acc for optim= 0.1407519653439522\n",
      "Epoch:902/1000\n",
      "Loss on train= 0.00868921261280775\n",
      "Loss on test= 0.01531435176730156\n",
      "acc for Lsat= 0.07815000116825105 \n",
      "acc for Psat= 0.09391532821787728 \n",
      "acc for optim= 0.14422532733943727\n",
      "Epoch:903/1000\n",
      "Loss on train= 0.00852497760206461\n",
      "Loss on test= 0.015870068222284317\n",
      "acc for Lsat= 0.06973721600241131 \n",
      "acc for Psat= 0.09606739001141652 \n",
      "acc for optim= 0.14245544754796557\n",
      "Epoch:904/1000\n",
      "Loss on train= 0.008871686644852161\n",
      "Loss on test= 0.015809912234544754\n",
      "acc for Lsat= 0.0703054525785976 \n",
      "acc for Psat= 0.09497696525520748 \n",
      "acc for optim= 0.14100606557395723\n",
      "Epoch:905/1000\n",
      "Loss on train= 0.008524100296199322\n",
      "Loss on test= 0.015446122735738754\n",
      "acc for Lsat= 0.07091290338171855 \n",
      "acc for Psat= 0.09942703429195617 \n",
      "acc for optim= 0.14094911234246363\n",
      "Epoch:906/1000\n",
      "Loss on train= 0.008798860013484955\n",
      "Loss on test= 0.015933718532323837\n",
      "acc for Lsat= 0.07449873967303171 \n",
      "acc for Psat= 0.10850230819649168 \n",
      "acc for optim= 0.14012081192599404\n",
      "Epoch:907/1000\n",
      "Loss on train= 0.008720590732991695\n",
      "Loss on test= 0.015276145190000534\n",
      "acc for Lsat= 0.07051990843481487 \n",
      "acc for Psat= 0.10046321749687195 \n",
      "acc for optim= 0.14095701840188768\n",
      "Epoch:908/1000\n",
      "Loss on train= 0.008924704045057297\n",
      "Loss on test= 0.01818031445145607\n",
      "acc for Lsat= 0.07492835008435779 \n",
      "acc for Psat= 0.10244145575496887 \n",
      "acc for optim= 0.14096115860674116\n",
      "Epoch:909/1000\n",
      "Loss on train= 0.008673676289618015\n",
      "Loss on test= 0.016012268140912056\n",
      "acc for Lsat= 0.07413401669926113 \n",
      "acc for Psat= 0.10093917366531159 \n",
      "acc for optim= 0.14060723351107704\n",
      "Epoch:910/1000\n",
      "Loss on train= 0.008408067747950554\n",
      "Loss on test= 0.016717087477445602\n",
      "acc for Lsat= 0.07551932169331445 \n",
      "acc for Psat= 0.09286329646905261 \n",
      "acc for optim= 0.14236408869425454\n",
      "Epoch:911/1000\n",
      "Loss on train= 0.00863104872405529\n",
      "Loss on test= 0.013905642554163933\n",
      "acc for Lsat= 0.07059226897027758 \n",
      "acc for Psat= 0.09260545348127684 \n",
      "acc for optim= 0.13943699647982913\n",
      "Epoch:912/1000\n",
      "Loss on train= 0.008811318315565586\n",
      "Loss on test= 0.015302490442991257\n",
      "acc for Lsat= 0.07525024827983645 \n",
      "acc for Psat= 0.09321424861749013 \n",
      "acc for optim= 0.1396713192264239\n",
      "Epoch:913/1000\n",
      "Loss on train= 0.00858242716640234\n",
      "Loss on test= 0.014487255364656448\n",
      "acc for Lsat= 0.07703692631589042 \n",
      "acc for Psat= 0.10349803732501134 \n",
      "acc for optim= 0.13946358462174732\n",
      "Epoch:914/1000\n",
      "Loss on train= 0.008842270821332932\n",
      "Loss on test= 0.01527880597859621\n",
      "acc for Lsat= 0.07582566059297985 \n",
      "acc for Psat= 0.09649343507157433 \n",
      "acc for optim= 0.14355099134975008\n",
      "Epoch:915/1000\n",
      "Loss on train= 0.008515344932675362\n",
      "Loss on test= 0.014578991569578648\n",
      "acc for Lsat= 0.07219369577036963 \n",
      "acc for Psat= 0.09299954606427085 \n",
      "acc for optim= 0.1413051317135493\n",
      "Epoch:916/1000\n",
      "Loss on train= 0.008542402647435665\n",
      "Loss on test= 0.016190029680728912\n",
      "acc for Lsat= 0.07145563678609 \n",
      "acc for Psat= 0.10290033486154344 \n",
      "acc for optim= 0.14164339618550406\n",
      "Epoch:917/1000\n",
      "Loss on train= 0.008489731699228287\n",
      "Loss on test= 0.014884361065924168\n",
      "acc for Lsat= 0.07479509032434888 \n",
      "acc for Psat= 0.09387968828280766 \n",
      "acc for optim= 0.14029960069391464\n",
      "Epoch:918/1000\n",
      "Loss on train= 0.008579074405133724\n",
      "Loss on test= 0.015647057443857193\n",
      "acc for Lsat= 0.07607924391825993 \n",
      "acc for Psat= 0.10472490919960867 \n",
      "acc for optim= 0.1422370250026385\n",
      "Epoch:919/1000\n",
      "Loss on train= 0.008326375856995583\n",
      "Loss on test= 0.014443610794842243\n",
      "acc for Lsat= 0.07683659195899963 \n",
      "acc for Psat= 0.09749339901738696 \n",
      "acc for optim= 0.14041412638293374\n",
      "Epoch:920/1000\n",
      "Loss on train= 0.00856003537774086\n",
      "Loss on test= 0.015374954789876938\n",
      "acc for Lsat= 0.07036460869842105 \n",
      "acc for Psat= 0.09440342237552007 \n",
      "acc for optim= 0.1411162265472942\n",
      "Epoch:921/1000\n",
      "Loss on train= 0.008610949851572514\n",
      "Loss on test= 0.01500821765512228\n",
      "acc for Lsat= 0.07023962852027682 \n",
      "acc for Psat= 0.09808003058036167 \n",
      "acc for optim= 0.1419948869281345\n",
      "Epoch:922/1000\n",
      "Loss on train= 0.008788122795522213\n",
      "Loss on test= 0.015213691629469395\n",
      "acc for Lsat= 0.07067478679948384 \n",
      "acc for Psat= 0.09650324227081405 \n",
      "acc for optim= 0.14176423963573245\n",
      "Epoch:923/1000\n",
      "Loss on train= 0.008355287835001945\n",
      "Loss on test= 0.015346278436481953\n",
      "acc for Lsat= 0.07012665139304268 \n",
      "acc for Psat= 0.09403081519736183 \n",
      "acc for optim= 0.14099819593959384\n",
      "Epoch:924/1000\n",
      "Loss on train= 0.008518261834979057\n",
      "Loss on test= 0.014287088066339493\n",
      "acc for Lsat= 0.07511118451754252 \n",
      "acc for Psat= 0.09590304162767198 \n",
      "acc for optim= 0.1404486710826556\n",
      "Epoch:925/1000\n",
      "Loss on train= 0.008742418140172958\n",
      "Loss on test= 0.01543941255658865\n",
      "acc for Lsat= 0.07345155196057425 \n",
      "acc for Psat= 0.09455707172552746 \n",
      "acc for optim= 0.1394082092576557\n",
      "Epoch:926/1000\n",
      "Loss on train= 0.008696460165083408\n",
      "Loss on test= 0.015044059604406357\n",
      "acc for Lsat= 0.07036041667064032 \n",
      "acc for Psat= 0.09858609305487737 \n",
      "acc for optim= 0.14043596883614862\n",
      "Epoch:927/1000\n",
      "Loss on train= 0.008351524360477924\n",
      "Loss on test= 0.01479111984372139\n",
      "acc for Lsat= 0.07318623479869629 \n",
      "acc for Psat= 0.09104102535380258 \n",
      "acc for optim= 0.1394066479470995\n",
      "Epoch:928/1000\n",
      "Loss on train= 0.008504805155098438\n",
      "Loss on test= 0.01629958301782608\n",
      "acc for Lsat= 0.07079096519284778 \n",
      "acc for Psat= 0.09324754046069253 \n",
      "acc for optim= 0.1411918416619301\n",
      "Epoch:929/1000\n",
      "Loss on train= 0.00863111112266779\n",
      "Loss on test= 0.014680410735309124\n",
      "acc for Lsat= 0.06982374141613641 \n",
      "acc for Psat= 0.09248727576600178 \n",
      "acc for optim= 0.1402139961719513\n",
      "Epoch:930/1000\n",
      "Loss on train= 0.008300459943711758\n",
      "Loss on test= 0.01528322882950306\n",
      "acc for Lsat= 0.07037723196877373 \n",
      "acc for Psat= 0.09083391800522805 \n",
      "acc for optim= 0.1397335653503736\n",
      "Epoch:931/1000\n",
      "Loss on train= 0.008503868244588375\n",
      "Loss on test= 0.01516089029610157\n",
      "acc for Lsat= 0.07226252141926023 \n",
      "acc for Psat= 0.0992250916030672 \n",
      "acc for optim= 0.13951062311728796\n",
      "Epoch:932/1000\n",
      "Loss on train= 0.008393174037337303\n",
      "Loss on test= 0.015197992324829102\n",
      "acc for Lsat= 0.07256360467937258 \n",
      "acc for Psat= 0.09589339842398961 \n",
      "acc for optim= 0.1413227268391185\n",
      "Epoch:933/1000\n",
      "Loss on train= 0.008320105262100697\n",
      "Loss on test= 0.014832104556262493\n",
      "acc for Lsat= 0.07495345291164185 \n",
      "acc for Psat= 0.09594379696581097 \n",
      "acc for optim= 0.14139957427978517\n",
      "Epoch:934/1000\n",
      "Loss on train= 0.008456924930214882\n",
      "Loss on test= 0.014402136206626892\n",
      "acc for Lsat= 0.07864640040530099 \n",
      "acc for Psat= 0.0948496354950799 \n",
      "acc for optim= 0.1405721868077914\n",
      "Epoch:935/1000\n",
      "Loss on train= 0.008596021682024002\n",
      "Loss on test= 0.01587384194135666\n",
      "acc for Lsat= 0.07840215017398197 \n",
      "acc for Psat= 0.10325603253311583 \n",
      "acc for optim= 0.14003985904985006\n",
      "Epoch:936/1000\n",
      "Loss on train= 0.008364922367036343\n",
      "Loss on test= 0.01534197386354208\n",
      "acc for Lsat= 0.07048256165451473 \n",
      "acc for Psat= 0.09778372181786431 \n",
      "acc for optim= 0.1397450264957216\n",
      "Epoch:937/1000\n",
      "Loss on train= 0.008731232024729252\n",
      "Loss on test= 0.01495070569217205\n",
      "acc for Lsat= 0.07515363991260529 \n",
      "acc for Psat= 0.09607401688893635 \n",
      "acc for optim= 0.14114613831043246\n",
      "Epoch:938/1000\n",
      "Loss on train= 0.008871952071785927\n",
      "Loss on test= 0.015405135229229927\n",
      "acc for Lsat= 0.07385235892401801 \n",
      "acc for Psat= 0.0980658652053939 \n",
      "acc for optim= 0.14215636071231627\n",
      "Epoch:939/1000\n",
      "Loss on train= 0.008396821096539497\n",
      "Loss on test= 0.016087111085653305\n",
      "acc for Lsat= 0.07646543582280478 \n",
      "acc for Psat= 0.09548341665003034 \n",
      "acc for optim= 0.14314558588796197\n",
      "Epoch:940/1000\n",
      "Loss on train= 0.008217442780733109\n",
      "Loss on test= 0.014162551611661911\n",
      "acc for Lsat= 0.07085897028446199 \n",
      "acc for Psat= 0.09241686115662257 \n",
      "acc for optim= 0.14004224108325114\n",
      "Epoch:941/1000\n",
      "Loss on train= 0.008668987080454826\n",
      "Loss on test= 0.015825055539608\n",
      "acc for Lsat= 0.07914376656214397 \n",
      "acc for Psat= 0.09601684262355169 \n",
      "acc for optim= 0.1421093228790495\n",
      "Epoch:942/1000\n",
      "Loss on train= 0.008544904179871082\n",
      "Loss on test= 0.015001737512648106\n",
      "acc for Lsat= 0.07412178301148944 \n",
      "acc for Psat= 0.09827813920047551 \n",
      "acc for optim= 0.14189270486434302\n",
      "Epoch:943/1000\n",
      "Loss on train= 0.008529048413038254\n",
      "Loss on test= 0.01536557450890541\n",
      "acc for Lsat= 0.07477674583594003 \n",
      "acc for Psat= 0.10193936278422673 \n",
      "acc for optim= 0.1416000333097246\n",
      "Epoch:944/1000\n",
      "Loss on train= 0.00836943183094263\n",
      "Loss on test= 0.015529487282037735\n",
      "acc for Lsat= 0.07355110198259354 \n",
      "acc for Psat= 0.1003025402625402 \n",
      "acc for optim= 0.14125551333030065\n",
      "Epoch:945/1000\n",
      "Loss on train= 0.008588068187236786\n",
      "Loss on test= 0.013939852826297283\n",
      "acc for Lsat= 0.06869194325473574 \n",
      "acc for Psat= 0.0946550601058536 \n",
      "acc for optim= 0.14128682133224274\n",
      "Epoch:946/1000\n",
      "Loss on train= 0.008332285098731518\n",
      "Loss on test= 0.015190099366009235\n",
      "acc for Lsat= 0.07227215501997206 \n",
      "acc for Psat= 0.10035255071189668 \n",
      "acc for optim= 0.1417157805628247\n",
      "Epoch:947/1000\n",
      "Loss on train= 0.008465129882097244\n",
      "Loss on test= 0.016253719106316566\n",
      "acc for Lsat= 0.0690094961060418 \n",
      "acc for Psat= 0.09898613707886801 \n",
      "acc for optim= 0.14304268029001022\n",
      "Epoch:948/1000\n",
      "Loss on train= 0.008392279967665672\n",
      "Loss on test= 0.015736542642116547\n",
      "acc for Lsat= 0.06921183980173534 \n",
      "acc for Psat= 0.09514471888542174 \n",
      "acc for optim= 0.14227403170532651\n",
      "Epoch:949/1000\n",
      "Loss on train= 0.00884619727730751\n",
      "Loss on test= 0.015998991206288338\n",
      "acc for Lsat= 0.07196179181337357 \n",
      "acc for Psat= 0.09623496267530654 \n",
      "acc for optim= 0.14110284894704817\n",
      "Epoch:950/1000\n",
      "Loss on train= 0.00866953656077385\n",
      "Loss on test= 0.016129620373249054\n",
      "acc for Lsat= 0.07354548937744565 \n",
      "acc for Psat= 0.0954495083954599 \n",
      "acc for optim= 0.14188149687316684\n",
      "Epoch:951/1000\n",
      "Loss on train= 0.008425452746450901\n",
      "Loss on test= 0.014870794489979744\n",
      "acc for Lsat= 0.07637254612313377 \n",
      "acc for Psat= 0.10043926984071731 \n",
      "acc for optim= 0.14108881304661436\n",
      "Epoch:952/1000\n",
      "Loss on train= 0.008447890169918537\n",
      "Loss on test= 0.015134771354496479\n",
      "acc for Lsat= 0.07176379942231707 \n",
      "acc for Psat= 0.09808572547303306 \n",
      "acc for optim= 0.14129638224840163\n",
      "Epoch:953/1000\n",
      "Loss on train= 0.008637205697596073\n",
      "Loss on test= 0.015411930158734322\n",
      "acc for Lsat= 0.07017628318733639 \n",
      "acc for Psat= 0.09399833066595924 \n",
      "acc for optim= 0.14173233459393184\n",
      "Epoch:954/1000\n",
      "Loss on train= 0.008641030639410019\n",
      "Loss on test= 0.01604308933019638\n",
      "acc for Lsat= 0.0730565051237742 \n",
      "acc for Psat= 0.0938279108868705 \n",
      "acc for optim= 0.14249527636501522\n",
      "Epoch:955/1000\n",
      "Loss on train= 0.008495120331645012\n",
      "Loss on test= 0.015118383802473545\n",
      "acc for Lsat= 0.07089721577035056 \n",
      "acc for Psat= 0.09468221896224552 \n",
      "acc for optim= 0.14024788555171755\n",
      "Epoch:956/1000\n",
      "Loss on train= 0.008160850033164024\n",
      "Loss on test= 0.01605960540473461\n",
      "acc for Lsat= 0.06916340887546539 \n",
      "acc for Psat= 0.09191840257909563 \n",
      "acc for optim= 0.1415909179382854\n",
      "Epoch:957/1000\n",
      "Loss on train= 0.008816814981400967\n",
      "Loss on test= 0.015522528439760208\n",
      "acc for Lsat= 0.0722320177488857 \n",
      "acc for Psat= 0.09806030127737257 \n",
      "acc for optim= 0.1413613993260595\n",
      "Epoch:958/1000\n",
      "Loss on train= 0.008595713414251804\n",
      "Loss on test= 0.01515897735953331\n",
      "acc for Lsat= 0.07453548742665185 \n",
      "acc for Psat= 0.09481303724977706 \n",
      "acc for optim= 0.1401326659652922\n",
      "Epoch:959/1000\n",
      "Loss on train= 0.008188867010176182\n",
      "Loss on test= 0.015357229858636856\n",
      "acc for Lsat= 0.07058116015460757 \n",
      "acc for Psat= 0.09415360424253677 \n",
      "acc for optim= 0.1448318204945988\n",
      "Epoch:960/1000\n",
      "Loss on train= 0.008457423187792301\n",
      "Loss on test= 0.01585007831454277\n",
      "acc for Lsat= 0.07115187876754336 \n",
      "acc for Psat= 0.09419606294896868 \n",
      "acc for optim= 0.14130960173077056\n",
      "Epoch:961/1000\n",
      "Loss on train= 0.008513201028108597\n",
      "Loss on test= 0.016329925507307053\n",
      "acc for Lsat= 0.07435155593686633 \n",
      "acc for Psat= 0.09246725820832782 \n",
      "acc for optim= 0.14375373373428982\n",
      "Epoch:962/1000\n",
      "Loss on train= 0.008766818791627884\n",
      "Loss on test= 0.01386112067848444\n",
      "acc for Lsat= 0.07143773982922236 \n",
      "acc for Psat= 0.0989965827928649 \n",
      "acc for optim= 0.14409486651420594\n",
      "Epoch:963/1000\n",
      "Loss on train= 0.008600014261901379\n",
      "Loss on test= 0.01568937487900257\n",
      "acc for Lsat= 0.06940281026893191 \n",
      "acc for Psat= 0.09050179661975964 \n",
      "acc for optim= 0.14095357987615798\n",
      "Epoch:964/1000\n",
      "Loss on train= 0.008568912744522095\n",
      "Loss on test= 0.014874373562633991\n",
      "acc for Lsat= 0.07040763662921057 \n",
      "acc for Psat= 0.091638925506009 \n",
      "acc for optim= 0.14088348050912222\n",
      "Epoch:965/1000\n",
      "Loss on train= 0.008266144432127476\n",
      "Loss on test= 0.014809881336987019\n",
      "acc for Lsat= 0.07221927146116891 \n",
      "acc for Psat= 0.09523756388160917 \n",
      "acc for optim= 0.14219066848357514\n",
      "Epoch:966/1000\n",
      "Loss on train= 0.008231917396187782\n",
      "Loss on test= 0.014407585375010967\n",
      "acc for Lsat= 0.07349871281120512 \n",
      "acc for Psat= 0.09247004787127178 \n",
      "acc for optim= 0.14064204212692047\n",
      "Epoch:967/1000\n",
      "Loss on train= 0.008397053927183151\n",
      "Loss on test= 0.01525537297129631\n",
      "acc for Lsat= 0.07141347842084037 \n",
      "acc for Psat= 0.0911074279910988 \n",
      "acc for optim= 0.141430522998174\n",
      "Epoch:968/1000\n",
      "Loss on train= 0.008512110449373722\n",
      "Loss on test= 0.015335606411099434\n",
      "acc for Lsat= 0.07332183321317037 \n",
      "acc for Psat= 0.09473612639639113 \n",
      "acc for optim= 0.1424690806203418\n",
      "Epoch:969/1000\n",
      "Loss on train= 0.008583243936300278\n",
      "Loss on test= 0.015625610947608948\n",
      "acc for Lsat= 0.07439928071366417 \n",
      "acc for Psat= 0.09567901889483134 \n",
      "acc for optim= 0.14269437276654776\n",
      "Epoch:970/1000\n",
      "Loss on train= 0.00835410039871931\n",
      "Loss on test= 0.014405910857021809\n",
      "acc for Lsat= 0.0719754781987932 \n",
      "acc for Psat= 0.0960388594203525 \n",
      "acc for optim= 0.1415731396940019\n",
      "Epoch:971/1000\n",
      "Loss on train= 0.008780796080827713\n",
      "Loss on test= 0.014917787164449692\n",
      "acc for Lsat= 0.0694756277733379 \n",
      "acc for Psat= 0.09634040362305113 \n",
      "acc for optim= 0.14360972659455404\n",
      "Epoch:972/1000\n",
      "Loss on train= 0.008275432512164116\n",
      "Loss on test= 0.015222464688122272\n",
      "acc for Lsat= 0.07140195965766906 \n",
      "acc for Psat= 0.09810071223311953 \n",
      "acc for optim= 0.13946174068583383\n",
      "Epoch:973/1000\n",
      "Loss on train= 0.008511691354215145\n",
      "Loss on test= 0.0150296650826931\n",
      "acc for Lsat= 0.07561400334040323 \n",
      "acc for Psat= 0.09272816975911458 \n",
      "acc for optim= 0.13929213997390535\n",
      "Epoch:974/1000\n",
      "Loss on train= 0.008440046571195126\n",
      "Loss on test= 0.014575188048183918\n",
      "acc for Lsat= 0.08062107894155714 \n",
      "acc for Psat= 0.10807602322763868 \n",
      "acc for optim= 0.14216749403211804\n",
      "Epoch:975/1000\n",
      "Loss on train= 0.00810700561851263\n",
      "Loss on test= 0.015022588893771172\n",
      "acc for Lsat= 0.07582936386267343 \n",
      "acc for Psat= 0.10654329790009393 \n",
      "acc for optim= 0.13937337746222814\n",
      "Epoch:976/1000\n",
      "Loss on train= 0.008504378609359264\n",
      "Loss on test= 0.015711089596152306\n",
      "acc for Lsat= 0.08645548770825068 \n",
      "acc for Psat= 0.10484342541959552 \n",
      "acc for optim= 0.14002078937159645\n",
      "Epoch:977/1000\n",
      "Loss on train= 0.008523191325366497\n",
      "Loss on test= 0.015065799467265606\n",
      "acc for Lsat= 0.07597333176268471 \n",
      "acc for Psat= 0.1025160978237788 \n",
      "acc for optim= 0.1401527472668224\n",
      "Epoch:978/1000\n",
      "Loss on train= 0.008011863566935062\n",
      "Loss on test= 0.014328096061944962\n",
      "acc for Lsat= 0.07497635881106059 \n",
      "acc for Psat= 0.1001460462808609 \n",
      "acc for optim= 0.1406628943151898\n",
      "Epoch:979/1000\n",
      "Loss on train= 0.008333957754075527\n",
      "Loss on test= 0.014995472505688667\n",
      "acc for Lsat= 0.07178938455051846 \n",
      "acc for Psat= 0.0922274523311191 \n",
      "acc for optim= 0.13926675617694856\n",
      "Epoch:980/1000\n",
      "Loss on train= 0.008165575563907623\n",
      "Loss on test= 0.013800485990941525\n",
      "acc for Lsat= 0.06950155943632125 \n",
      "acc for Psat= 0.09505251463916567 \n",
      "acc for optim= 0.13918885125054256\n",
      "Epoch:981/1000\n",
      "Loss on train= 0.008078708313405514\n",
      "Loss on test= 0.014497865922749043\n",
      "acc for Lsat= 0.07289121813244288 \n",
      "acc for Psat= 0.09217169400718478 \n",
      "acc for optim= 0.1403698770536317\n",
      "Epoch:982/1000\n",
      "Loss on train= 0.008043448440730572\n",
      "Loss on test= 0.014965157024562359\n",
      "acc for Lsat= 0.07558374289009308 \n",
      "acc for Psat= 0.10287794371445973 \n",
      "acc for optim= 0.13870827439758515\n",
      "Epoch:983/1000\n",
      "Loss on train= 0.008358835242688656\n",
      "Loss on test= 0.01391523890197277\n",
      "acc for Lsat= 0.0723409288459354 \n",
      "acc for Psat= 0.09822999288638433 \n",
      "acc for optim= 0.138301531639364\n",
      "Epoch:984/1000\n",
      "Loss on train= 0.008406148292124271\n",
      "Loss on test= 0.016053911298513412\n",
      "acc for Lsat= 0.07143126262558831 \n",
      "acc for Psat= 0.09313384625646803 \n",
      "acc for optim= 0.14002218296130498\n",
      "Epoch:985/1000\n",
      "Loss on train= 0.007889567874372005\n",
      "Loss on test= 0.015862705186009407\n",
      "acc for Lsat= 0.06999486519230737 \n",
      "acc for Psat= 0.09160291602214177 \n",
      "acc for optim= 0.13897221899694864\n",
      "Epoch:986/1000\n",
      "Loss on train= 0.008531366474926472\n",
      "Loss on test= 0.014791252091526985\n",
      "acc for Lsat= 0.072868906127082 \n",
      "acc for Psat= 0.09679413023922179 \n",
      "acc for optim= 0.13992571963204278\n",
      "Epoch:987/1000\n",
      "Loss on train= 0.008226757869124413\n",
      "Loss on test= 0.015215075574815273\n",
      "acc for Lsat= 0.06937060554822286 \n",
      "acc for Psat= 0.09715939031706916 \n",
      "acc for optim= 0.14117683900727168\n",
      "Epoch:988/1000\n",
      "Loss on train= 0.00834438856691122\n",
      "Loss on test= 0.015561450272798538\n",
      "acc for Lsat= 0.07802816828091938 \n",
      "acc for Psat= 0.10391079915894401 \n",
      "acc for optim= 0.1411615169710583\n",
      "Epoch:989/1000\n",
      "Loss on train= 0.008404986932873726\n",
      "Loss on test= 0.015137231908738613\n",
      "acc for Lsat= 0.07569962839285532 \n",
      "acc for Psat= 0.10099924322631623 \n",
      "acc for optim= 0.14178135742743814\n",
      "Epoch:990/1000\n",
      "Loss on train= 0.008350797928869724\n",
      "Loss on test= 0.01441305223852396\n",
      "acc for Lsat= 0.07184829761584599 \n",
      "acc for Psat= 0.09802364541424645 \n",
      "acc for optim= 0.14182129849990213\n",
      "Epoch:991/1000\n",
      "Loss on train= 0.008529460057616234\n",
      "Loss on test= 0.01598237454891205\n",
      "acc for Lsat= 0.07306227385997771 \n",
      "acc for Psat= 0.10419402354293399 \n",
      "acc for optim= 0.1399946173032125\n",
      "Epoch:992/1000\n",
      "Loss on train= 0.008516737259924412\n",
      "Loss on test= 0.016260599717497826\n",
      "acc for Lsat= 0.08359208587143156 \n",
      "acc for Psat= 0.11216967387331857 \n",
      "acc for optim= 0.14069664196835624\n",
      "Epoch:993/1000\n",
      "Loss on train= 0.008465268649160862\n",
      "Loss on test= 0.014774967916309834\n",
      "acc for Lsat= 0.07411000265015497 \n",
      "acc for Psat= 0.10178262127770317 \n",
      "acc for optim= 0.14206085618999273\n",
      "Epoch:994/1000\n",
      "Loss on train= 0.007871342822909355\n",
      "Loss on test= 0.01530840341001749\n",
      "acc for Lsat= 0.07355874958965512 \n",
      "acc for Psat= 0.09464894284804663 \n",
      "acc for optim= 0.1392505303853088\n",
      "Epoch:995/1000\n",
      "Loss on train= 0.008410407230257988\n",
      "Loss on test= 0.01462759729474783\n",
      "acc for Lsat= 0.0713155531220966 \n",
      "acc for Psat= 0.09568681319554645 \n",
      "acc for optim= 0.14128703094191022\n",
      "Epoch:996/1000\n",
      "Loss on train= 0.008649620227515697\n",
      "Loss on test= 0.014005015604197979\n",
      "acc for Lsat= 0.07541496323214637 \n",
      "acc for Psat= 0.09596670418977739 \n",
      "acc for optim= 0.13943940334849889\n",
      "Epoch:997/1000\n",
      "Loss on train= 0.008311831392347813\n",
      "Loss on test= 0.01615186780691147\n",
      "acc for Lsat= 0.07442839658922619 \n",
      "acc for Psat= 0.0949288825194041 \n",
      "acc for optim= 0.13994490835401746\n",
      "Epoch:998/1000\n",
      "Loss on train= 0.0084290886297822\n",
      "Loss on test= 0.01688881404697895\n",
      "acc for Lsat= 0.07090341432227028 \n",
      "acc for Psat= 0.09731727623277239 \n",
      "acc for optim= 0.14132319357660084\n",
      "Epoch:999/1000\n",
      "Loss on train= 0.00840480625629425\n",
      "Loss on test= 0.01452916394919157\n",
      "acc for Lsat= 0.07537338121069802 \n",
      "acc for Psat= 0.10259769211212794 \n",
      "acc for optim= 0.14020797428157594\n",
      "Epoch:1000/1000\n",
      "Loss on train= 0.008151298388838768\n",
      "Loss on test= 0.01567520946264267\n",
      "acc for Lsat= 0.0751008411248525 \n",
      "acc for Psat= 0.09497692220740848 \n",
      "acc for optim= 0.13933118912908765\n"
     ]
    }
   ],
   "source": [
    "history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc1':[], 'test_acc2':[], \n",
    "          'test_acc3':[]}\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(df1)))):\n",
    "\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    \n",
    "    X_train = df1.iloc[train_idx]\n",
    "    X_test = df1.iloc[val_idx]\n",
    "    y_train = df2.iloc[train_idx]\n",
    "    y_test = df2.iloc[val_idx]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    scaler2 = MinMaxScaler()\n",
    "    scaler2.fit(y_train)\n",
    "\n",
    "    y_train = scaler2.transform(y_train)\n",
    "    y_test = scaler2.transform(y_test)\n",
    "    \n",
    "    \n",
    "    X_train = torch.Tensor(X_train) \n",
    "    y_train = torch.Tensor(y_train)\n",
    "\n",
    "    X_test = torch.Tensor(X_test)\n",
    "    y_test = torch.Tensor(y_test)\n",
    "\n",
    "    train_set = TensorDataset(X_train, y_train) \n",
    "    test_set = TensorDataset(X_test, y_test) \n",
    "\n",
    "\n",
    "    # Create Dataloader to read the data within batch sizes and put into memory. \n",
    "    train_loader = DataLoader(train_set, batch_size = 64, shuffle = True) \n",
    "    test_loader = DataLoader(test_set, batch_size = 20)\n",
    "\n",
    "    \n",
    "    model = Network(input_size,output_size).to(device) \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "    \n",
    "    lambda1 = lambda epoch: 0.998 ** epoch\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1, last_epoch = -1)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loss_on_train = train_epoch(model, optimizer, criterion, train_loader)\n",
    "        train_loss = float(np.mean(loss_on_train))\n",
    "        _, loss_on_test = validate(model, criterion, test_loader)\n",
    "        test_loss = float(np.mean(loss_on_test))\n",
    "        model.eval()\n",
    "        train_acc = test(model, train_loader)\n",
    "        test_acc = test(model, test_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(\"Epoch:{}/{}\".format(epoch + 1, num_epochs))\n",
    "        print('Loss on train=', train_loss)\n",
    "        print('Loss on test=', test_loss)\n",
    "        print('acc for Lsat=', test_acc[0],'\\n' 'acc for Psat=', test_acc[1], '\\n' 'acc for optim=', test_acc[2])\n",
    "        \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    #history['train_acc'].append(train_acc)\n",
    "    history['test_acc1'].append(test_acc[0])  \n",
    "    history['test_acc2'].append(test_acc[1])  \n",
    "    history['test_acc3'].append(test_acc[2])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "1cd2aea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of 5 fold cross validation\n",
      "Average Training Loss: 0.0085 \t Average Test Loss: 0.0161 \t Average Lsat Acc: 0.067 +- 0.005 \t Average Psat Acc: 0.092 +- 0.005 \t Average Loptim Acc: 0.142 +- 0.007\n"
     ]
    }
   ],
   "source": [
    "avg_train_loss = np.mean(history['train_loss'])\n",
    "avg_test_loss = np.mean(history['test_loss'])\n",
    "avg_test_acc1 = np.mean(history['test_acc1'])\n",
    "std1 = np.std(history['test_acc1'])\n",
    "avg_test_acc2 = np.mean(history['test_acc2'])\n",
    "std2 = np.std(history['test_acc2'])\n",
    "avg_test_acc3 = np.mean(history['test_acc3'])\n",
    "std3 = np.std(history['test_acc3'])\n",
    "\n",
    "print('Performance of {} fold cross validation'.format(k))\n",
    "print(\"Average Training Loss: {:.4f} \\t Average Test Loss: {:.4f} \\t Average Lsat Acc: {:.3f} +- {:.3f} \\t Average Psat Acc: {:.3f} +- {:.3f} \\t Average Loptim Acc: {:.3f} +- {:.3f}\".\n",
    "      format(avg_train_loss,avg_test_loss,avg_test_acc1,std1, avg_test_acc2, std2,\n",
    "avg_test_acc3, std3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6341f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
